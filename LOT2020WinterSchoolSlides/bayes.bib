% This file was created with JabRef 2.10.
% Encoding: ISO8859_1

@book{morin2016probability,
  title={Probability For the Enthusiastic Beginner},
  author={Morin, David J},
  year={2016},
  publisher={Createspace Independent Publishing Platform}
}

@book{lambert2018student,
  title={A Student’s Guide to {B}ayesian Statistics},
  author={Lambert, Ben},
  year={2018},
  publisher={Sage}
}

@article{VasishthMertzenJaegerGelman2018,
  Author = {Vasishth, Shravan and Mertzen, Daniela and J\"ager, Lena A. and Gelman, Andrew},
  journal = {Journal of Memory and Language},
  url = {https://osf.io/eyphj/},  
  doi = {https://doi.org/10.1016/j.jml.2018.07.004},
  Title = {The statistical significance filter leads to overoptimistic expectations of replicability},
  Year = {2018},
  volume = {103},
  pages = {151-175}
  }

 @book{blastland2014norm,
  title={The Norm Chronicles: Stories and Numbers about Danger and Death},
  author={Blastland, Michael and Spiegelhalter, David},
  year={2014},
  publisher={Basic Books (AZ)}
} 

@Article{brms,
    title = {{brms}: An {R} Package for {B}ayesian Multilevel Models Using {Stan}},
    author = {Paul-Christian B\"urkner},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {80},
    number = {1},
    pages = {1--28},
    doi = {10.18637/jss.v080.i01}
  }


@book{Ross,
  title={A first course in probability},
  author={Ross, Sheldon},
  year={2002},
  publisher={Pearson Education}
}


  @article{MalsburgAngele2016,
  title={False positives and other statistical errors in standard analyses of eye movements in reading},
  author={von der Malsburg, Titus and Angele, Bernhard},
  journal={Journal of Memory and Language},
  volume={94},
  pages={119--133},
  year={2017},
  publisher={Elsevier}
}



@article{NicenboimRoettgeretal,
  Author = {Bruno Nicenboim and Timo B. Roettger and Shravan Vasishth},
  Title = {Using meta-analysis for evidence synthesis: {The case of incomplete neutralization in German}},
  Year = {2018},
  journal = {Journal of Phonetics},
  doi = {https://doi.org/10.1016/j.wocn.2018.06.001},
  url = {https://osf.io/g5ndw/},
  pdf = {https://mfr.osf.io/render?url=https://osf.io/4k25w/?action=download%26mode=render},
  volume = {70},
  pages = {39-55}
  }


@Article{Colquhoun2014,
  Title                    = {An investigation of the false discovery rate and the misinterpretation of p-values},
  Author                   = {Colquhoun, David},
  Year                     = {2014},
  Doi                      = {10.1098/rsos.140216},
  Number                   = {3},
  Pages                    = {140216},
  Volume                   = {1},

  Journal                  = {Royal Society Open Science},
  Publisher                = {The Royal Society}
}

@book{blitzstein2014introduction,
  title={Introduction to probability},
  author={Blitzstein, Joseph K and Hwang, Jessica},
  year={2014},
  publisher={Chapman and Hall/CRC}
}

@article{neal2011mcmc,
  title={{MCMC using Hamiltonian dynamics}},
  author={Neal, Radford M and others},
  journal={Handbook of Markov Chain Monte Carlo},
  volume={2},
  number={11},
  pages={2},
  year={2011}
}

@Book{GelmanEtAl2014,
  Title                    = {{Bayesian Data Analysis}},
  Author                   = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Rubin, Donald B},
  Year                     = {2014},
  Edition                  = {Third Edition},
  Publisher                = {Taylor \& Francis}
}

@Article{HallerKrauss2002,
  Title                    = {Misinterpretations of significance: A problem students share with their teachers},
  Author                   = {Haller, Heiko and Krauss, Stefan},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {1--20},
  Volume                   = {7},

  Journal                  = {Methods of Psychological Research}
}

@book{vasishth:phdbook,
  Address = {New York},
  Author = {Shravan Vasishth},
  Note = {Published in the Garland series {Outstanding Dissertations in Linguistics, edited by Laurence Horn}},
  Publisher = {Garland Press},
  Title = {Working memory in sentence comprehension: {P}rocessing {H}indi center embeddings},
  Year = {2003}
  }

@Article{Hoekstra2014,
  Title                    = {Robust misinterpretation of confidence intervals},
  Author                   = {Hoekstra, Rink and Morey, Richard D and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
  Year                     = {2014},
  Doi                      = {10.3758/s13423-013-0572-3},
  Number                   = {5},
  Pages                    = {1157--1164},
  Volume                   = {21},

  Journal                  = {Psychonomic Bulletin \& Review},
  Publisher                = {Springer}
}

@Article{HoekstraEtAl2014,
  Title                    = {Robust misinterpretation of confidence intervals},
  Author                   = {Hoekstra, Rink and Morey, Richard D and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
  Year                     = {2014},
  Doi                      = {10.3758/s13423-013-0572-3},
  Number                   = {5},
  Pages                    = {1157--1164},
  Volume                   = {21},

  Journal                  = {Psychonomic Bulletin \& Review},
  Publisher                = {Springer}
}

@Book{mcelreath2015statistical,
  Title                    = {Statistical rethinking: A Bayesian course with R examples},
  Author                   = {McElreath, Richard},
  Year                     = {2015},
  ISBN                     = {9781482253443},
  Publisher                = {Chapman and Hall/CRC}
}

@Article{MoreyEtAl2015,
  Title                    = {The Fallacy of Placing Confidence in Confidence Intervals},
  Author                   = {Morey, Richard D and Hoekstra, Rink and Rouder, Jeffrey N. and Lee, Michael D and Wagenmakers, Eric-Jan},
  Year                     = {2016},
  Doi                      = {10.3758/s13423-015-0947-8},
  ISSN                     = {1531-5320},
  Number                   = {1},
  Pages                    = {103--123},
  Url                      = {http://dx.doi.org/10.3758/s13423-015-0947-8},
  Volume                   = {23},

  Abstract                 = {Interval estimates -- estimates of parameters that include an allowance for sampling uncertainty -- have long been touted as a key component of statistical analyses. There are several kinds of interval estimates, but the most popular are confidence intervals (CIs): intervals that contain the true parameter value in some known proportion of repeated samples, on average. The width of confidence intervals is thought to index the precision of an estimate; CIs are thought to be a guide to which parameter values are plausible or reasonable; and the confidence coefficient of the interval (e.g., 95 {\%}) is thought to index the plausibility that the true parameter is included in the interval. We show in a number of examples that CIs do not necessarily have any of these properties, and can lead to unjustified or arbitrary inferences. For this reason, we caution against relying upon confidence interval theory to justify interval estimates, and suggest that other theories of interval estimation should be used instead.}
}

@Article{ShiffrinEtAl2008,
  Title                    = {A Survey of Model Evaluation Approaches With a Tutorial on Hierarchical {Bayes}ian Methods},
  Author                   = {Shiffrin, Richard M. and Lee, Michael and Kim, Woojae and Wagenmakers, Eric-Jan},
  Year                     = {2008},
  Doi                      = {10.1080/03640210802414826},
  ISSN                     = {0364-0213},
  Month                    = {Dec},
  Number                   = {8},
  Pages                    = {1248--1284},
  Url                      = {http://dx.doi.org/10.1080/03640210802414826},
  Volume                   = {32},

  File                     = {:Shiffrin_et_al-2008-Cognitive_Science.pdf:PDF},
  Journal                  = {Cognitive Science},
  Publisher                = {Wiley-Blackwell}
}


@book{Lynch2007,
  title={Introduction to applied {B}ayesian statistics and estimation for social scientists},
  author={Lynch, Scott M.},
  year={2007},
  publisher={Springer Science \& Business Media}
}


@article{CarreirasClifton1999Anotherwordparsing,
  title = {Another Word on Parsing Relative Clauses: {{Eyetracking}} Evidence from {{Spanish}} and {{English}}},
  volume = {27},
  shorttitle = {Another Word on Parsing Relative Clauses},
  timestamp = {2016-11-27T17:11:07Z},
  number = {5},
  urldate = {2016-11-27},
  journal = {Memory \& cognition},
  author = {Carreiras, Manuel and Clifton, Charles},
  year = {1999},
  pages = {826--833},
  file = {CarreirasCliftonEyetracking.pdf:/home/bruno/.zotero/zotero/b5aal9uj.default/zotero/storage/UUJD3PNC/CarreirasCliftonEyetracking.pdf:application/pdf}
}




@article{SwetsEtAl2008Underspecificationsyntacticambiguities,
  title = {Underspecification of Syntactic Ambiguities: {{Evidence}} from Self-Paced Reading},
  volume = {36},
  issn = {0090-502X, 1532-5946},
  shorttitle = {Underspecification of Syntactic Ambiguities},
  doi = {10.3758/MC.36.1.201},
  language = {en},
  timestamp = {2016-11-29T14:41:11Z},
  number = {1},
  urldate = {2016-11-29},
  journal = {Memory \& Cognition},
  author = {Swets, Benjamin and Desmet, Timothy and Clifton, Charles and Ferreira, Fernanda},
  month = jan,
  year = {2008},
  pages = {201--216},
  file = {Swets et al 2008.pdf:/home/bruno/.zotero/zotero/b5aal9uj.default/zotero/storage/T4RIZXE2/Swets et al 2008.pdf:application/pdf}
}



@article{FrazierRayner1982making,
  title={Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences},
  author={Frazier, Lyn and Rayner, Keith},
  journal={Cognitive psychology},
  volume={14},
  number={2},
  pages={178--210},
  year={1982},
  publisher={Elsevier}
}


@book{GelmanHill2007,
  title={Data analysis using regression and multilevel/hierarchical models},
  author={Gelman, Andrew and Hill, Jennifer},
  year={2007},
  publisher={Cambridge University Press}
}


@article{Levy2008Expectationbasedsyntacticcomprehension,
  title = {Expectation-Based Syntactic Comprehension},
  volume = {106},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159-166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  timestamp = {2016-11-29T09:31:17Z},
  number = {3},
  journal = {Cognition},
  author = {Levy, Roger P.},
  month = mar,
  year = {2008},
  keywords = {Attitude,Cognition,expectations,Humans,linguistics,Models,Phd1,predictions,Psychological,Psychological Theory,Semantics},
  pages = {1126--1177},
  file = {Attachment:/home/bruno/.zotero/zotero/b5aal9uj.default/zotero/storage/668EQZIB/Levy - 2008 - Expectation-based syntactic comprehension.pdf:application/pdf},
  citation-verif = {pages; year; volume; month; number; issn; journaltitle; url; title; doi; publisher; author; were verified from given doi},
  mendeley-tags = {Phd1,expectations,predictions},
  pmid = {17662975}
}

@article {VasishthNicenboimStatMeth,
author = {Vasishth, Shravan and Nicenboim, Bruno},
title = {Statistical Methods for Linguistic Research: {F}oundational Ideas – {Part I}},
journal = {Language and Linguistics Compass},
volume = {10},
number = {8},
issn = {1749-818X},
pdf = {http://dx.doi.org/10.1111/lnc3.12201},
OPTdoi = {10.1111/lnc3.12201},
code = {https://github.com/vasishth/VasishthNicenboimPart1},
pages = {349--369},
year = {2016}
}



@article{SorensenEtAl2016,
    author =    {Sorensen, Tanner AND Hohenstein, Sven AND Vasishth, Shravan },
    journal =   {The Quantitative Methods for Psychology},
    publisher = {TQMP},
    title =     {Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists},
    year =      {2016},
    volume =    {12},
    number =    {3},
    url =       {http://www.tqmp.org/RegularArticles/vol12-3/p175/p175.pdf },
    pages =     {175-200},
    abstract =  {With the arrival of the R packages \fontencoding {T1}\texttt {nlme} and \fontencoding {T1}\texttt {lme4}, linear mixed models (LMMs) have come to be widely used in experimentally-driven areas like psychology, linguistics, and cognitive science. This tutorial provides a practical introduction to fitting LMMs in a Bayesian framework using the probabilistic programming language Stan. We choose Stan (rather than WinBUGS or JAGS) because it provides an elegant and scalable framework for fitting models in most of the standard applications of LMMs. We ease the reader into fitting increasingly complex LMMs, using a two-condition repeated measures self-paced reading study.},
    doi =       {10.20982/tqmp.12.3.p175}
}

@Book{WagenmakersLee2013book,
  Title                    = {Bayesian cognitive modeling: {A} practical course},
  Author                   = {Lee, Michael D and Wagenmakers, Eric-Jan},
  Year                     = {2013},
  Publisher                = {Cambridge University Press}
}

@Misc{Stan2017,
  Title                    = {Stan: {A} {C++} Library for Probability and Sampling, Version
 2.16.0},
  Author                   = {{Stan Development Team}},
  Year                     = {2017},
  Url                      = {http://mc-stan.org/}
}

@Article{Ratcliff1998,
  Title                    = {Modeling Response Times for Two-Choice Decisions},
  Author                   = {Ratcliff, Roger and Rouder, Jeffrey N.},
  Journaltitle             = {Psychological Science},
  Year                     = {1998},
  Doi                      = {10.1111/1467-9280.00067},
  ISSN                     = {0956-7976},
  Month                    = {Sep},
  Number                   = {5},
  Pages                    = {347--356},
  Url                      = {http://dx.doi.org/10.1111/1467-9280.00067},
  Volume                   = {9},

  Citation-verif           = {pages; year; volume; month; number; issn; journaltitle; url; title; doi; publisher; author; were verified from given doi},
  File                     = {:Ratcliff, Rouder - 1998 - Modeling Response Times for Two-Choice Decisions.pdf:PDF},
  Keywords                 = {Pavel,Phd1,modeling},
  Mendeley-tags            = {Pavel,Phd1,modeling},
  Publisher                = {Sage Publications}
}
@article{RatcliffEtAl2016,
title = "Diffusion Decision Model: {C}urrent Issues and History ",
journal = "Trends in Cognitive Sciences ",
volume = "20",
number = "4",
pages = "260 -- 281",
year = "2016",
note = "",
issn = "1364-6613",
doi = "10.1016/j.tics.2016.01.007",
url = "http://www.sciencedirect.com/science/article/pii/S1364661316000255",
author = "Roger Ratcliff and Philip L. Smith and Scott D. Brown and Gail McKoon",
keywords = "diffusion model",
keywords = "response time",
keywords = "optimality",
keywords = "nonstationarity "
}

@article{Ratcliff1978,
  title={A theory of memory retrieval.},
  author={Ratcliff, Roger},
  journal={Psychological review},
  volume={85},
  number={2},
  pages={59},
  year={1978},
  publisher={American Psychological Association}
}


@Article{RouderEtAl2014,
  Title                    = {The Lognormal Race: {A} Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties},
  Author                   = {Rouder, Jeffrey N. and Province, Jordan M. and Morey, Richard D. and Gomez, Pablo and Heathcote, Andrew},
  Year                     = {2014},
  Doi                      = {10.1007/s11336-013-9396-3},
  ISSN                     = {1860-0980},
  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {491–513},
  Url                      = {http://dx.doi.org/10.1007/S11336-013-9396-3},
  Volume                   = {80},

  File                     = {:home/bruno/Downloads/rouder2015.pdf:PDF},
  Journal                  = {Psychometrika},
  Publisher                = {Springer Science + Business Media}
}

@Article{HeathcoteLove2012,
  Title                    = {Linear Deterministic Accumulator Models of Simple Choice},
  Author                   = {Heathcote, Andrew and Love, Jonathon},
  Year                     = {2012},
  Doi                      = {10.3389/fpsyg.2012.00292},
  ISSN                     = {1664-1078},
  Url                      = {http://dx.doi.org/10.3389/fpsyg.2012.00292},
  Volume                   = {3},

  File                     = {:home/bruno/ownCloud/Papers/new-rts more/diff/Heathcote Love 2012.pdf:PDF},
  Journal                  = {Frontiers in Psychology},
  Publisher                = {Frontiers Media SA}
}

@Article{Rouder2005,
  Title                    = {Are unshifted distributional models appropriate for response time?},
  Author                   = {Rouder, Jeffrey N.},
  Year                     = {2005},
  Doi                      = {10.1007/s11336-005-1297-7},
  ISSN                     = {1860-0980},
  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {377–381},
  Url                      = {http://dx.doi.org/10.1007/s11336-005-1297-7},
  Volume                   = {70},

  File                     = {:Rouder-2005 shifted lognormal.pdf:PDF},
  Journal                  = {Psychometrika},
  Publisher                = {Springer Science + Business Media}
}

@book{jeffreys1998theory,
  title={The theory of probability},
  author={Jeffreys, Harold},
  edition={Third},
  year={1939/1998},
  publisher={Oxford University Press}
}


@article{Gelman2013,
  title={Two simple examples for understanding posterior p-values whose distributions are far from unform},
  author={Gelman, Andrew and others},
  journal={Electronic Journal of Statistics},
  volume={7},
  pages={2595--2602},
  year={2013},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@article{nieuwenhuis2011erroneous,
  title={Erroneous analyses of interactions in neuroscience: {A} problem of significance},
  author={Nieuwenhuis, Sander and Forstmann, Birte U and Wagenmakers, Eric-Jan},
  journal={Nature Neuroscience},
  volume={14},
  number={9},
  pages={1105--1107},
  year={2011},
  publisher={Nature Research}
}

@article{gibsonwu,
  title={Processing {C}hinese relative clauses in context},
  author={Gibson, Edward and Wu, H-H Iris},
  journal={Language and Cognitive Processes},
  volume={28},
  number={1-2},
  pages={125--155},
  year={2013},
  publisher={Taylor \& Francis}
}


@article{grodner,
  Author = {Daniel Grodner and Edward Gibson},
  Journal = {Cognitive Science},
  Pages = {261--290},
  Title = {Consequences of the serial nature of linguistic input},
  Volume = {29},
  Year = {2005}}


@unpublished{SchadEtAlcontrasts,
  Author = {Daniel J. Schad and Sven Hohenstein and Shravan Vasishth and Reinhold Kliegl},
  Note = {Unpublished manuscript},
  Title = {How to capitalize on a priori contrasts in linear (mixed) models: {A} tutorial},
  Year = {2018},
  pdf = {https://arxiv.org/abs/1807.10451}
}


@book{kruschke2015doing,
  title                = {Doing {B}ayesian Data Analysis},
  author                   = {Kruschke, John K. },
  Year                     = {2015},
  Doi                      = {http://dx.doi.org/10.1016/B978-0-12-405888-0.09999-2},
  Edition                  = {Second},
  ISBN                     = {978-0-12-405888-0},
  Publisher                = {Academic Press},
  Address                  = {Boston}
  }



@article {MonnahanEtAl2017,
author = {Monnahan, Cole C. and Thorson, James T. and Branch, Trevor A.},
title = {Faster estimation of Bayesian models in ecology using Hamiltonian Monte Carlo},
journal = {Methods in Ecology and Evolution},
volume = {8},
number = {3},
issn = {2041-210X},
url = {http://dx.doi.org/10.1111/2041-210X.12681},
doi = {10.1111/2041-210X.12681},
pages = {339--348},
keywords = {Bayesian inference, hierarchical modelling, Markov chain Monte Carlo, no-U-turn sampler, Stan},
year = {2017},
}


@book{lunn2012bugs,
  title={The {BUGS} book: {A} practical introduction to {B}ayesian analysis},
  author={Lunn, David and Jackson, Chris and Spiegelhalter, David J and Best, Nicky and Thomas, Andrew},
  volume={98},
  year={2012},
  publisher={CRC Press}
}

@article{lunn2000winbugs,
  title={{WinBUGS}-{A B}ayesian modelling framework: {C}oncepts, structure, and extensibility},
  author={Lunn, D.J. and Thomas, A. and Best, N. and Spiegelhalter, D.},
  journal={Statistics and computing},
  volume={10},
  number={4},
  pages={325--337},
  year={2000},
  publisher={Springer}
}


@misc{plummer2016jags,
  Author = {Plummer, Martin},
  Title = {JAGS Version 4.2.0 user manual},
  Year = {2016}}


@article{VasishthetalPLoSOne2013,
    author = {Vasishth, Shravan and Chen, Zhong and Li, Qiang and Guo, Gueilan},
    journal = {PLoS ONE},
    publisher = {Public Library of Science},
    title = {Processing {C}hinese Relative Clauses: {E}vidence for the Subject-Relative Advantage},
    year = {2013},
    month = {10},
    volume = {8},
    pdf = {http://dx.doi.org/10.1371%2Fjournal.pone.0077006},
    pages = {1--14},
    abstract = {A general fact about language is that subject relative clauses are easier to process than object relative clauses. Recently, several self-paced reading studies have presented surprising evidence that object relatives in Chinese are easier to process than subject relatives. We carried out three self-paced reading experiments that attempted to replicate these results. Two of our three studies found a subject-relative preference, and the third study found an object-relative advantage. Using a random effects Bayesian meta-analysis of fifteen studies (including our own), we show that the overall current evidence for the subject-relative advantage is quite strong (approximate posterior probability of a subject-relative advantage given the data: 78–80%). We argue that retrieval/integration based accounts would have difficulty explaining all three experimental results. These findings are important because they narrow the theoretical space by limiting the role of an important class of explanation—retrieval/integration cost—at least for relative clause processing in Chinese.},
    number = {10},
    code = {http://www.ling.uni-potsdam.de/~vasishth/code/PLoSOneVasishthetaldata.zip}
}        

@Article{JaegerEtAl2017,
  Title                    = {Similarity-based interference in sentence comprehension: {Literature} review and {Bayesian} meta-analysis
},
  Author                   = {J{\"a}ger, Lena A. and Engelmann, Felix and  Vasishth, Shravan},
 journal = {Journal of Memory and Language},
 volume = {94},
 pages = {316--339},
 year = {2017}
}

@article{NicenboimEtAlCogSci2018,
  year = {2018},
  author = {Bruno Nicenboim and Shravan Vasishth and Felix Engelmann and Katja Suckow},
  journal = {Cognitive Science},
  volume = {42},
  issue = {S4},
  page = {1075-1100},
  title = {Exploratory and confirmatory analyses in sentence processing: {A case study of number interference in German}},
  doi = {10.1111/cogs.12589}
}

@misc{NicenboimEtAl2016NIG,
  title={Number interference in {German}: {Evidence} for cue-based retrieval},
  url={osf.io/mmr7s},
  preprint={osf.io/mmr7s},
  DOI={10.17605/OSF.IO/MMR7S},
  publisher={Open Science Framework},
  author={Nicenboim, Bruno and Engelmann, Felix and Suckow, Katja and Vasishth, Shravan},
  year={Submitted},
  month={Dec}
}

@article{lewisvasishth:cogsci05,
  Author = {Richard L. Lewis and Shravan Vasishth},
  Journal = {Cognitive Science},
  Pages = {1--45},
  abstract = {We present a detailed process theory of the moment-by-moment working-memory retrievals and associated
  control structure that subserve sentence comprehension. The theory is derived from the application
  of independently motivated principles of memory and cognitive skill to the specialized task of sentence
  parsing. The resulting theory construes sentence processing as a series of skilled associative
  memory retrievals modulated by similarity-based interference and fluctuating activation. The cognitive
  principles are formalized in computational form in the Adaptive Control of Thought-Rational (ACT-R)
  architecture, and our process model is realized in ACT-R.We present the results of 6 sets of simulations:
  5 simulation sets provide quantitative accounts of the effects of length and structural interference on
  both unambiguous and garden-path structures. A final simulation set provides a graded taxonomy of
  double center embeddings ranging from relatively easy to extremely difficult. The explanation of center-
  embedding difficulty is a novel one that derives from the model‚Äôs complete reliance on discriminating
  retrieval cues in the absence of an explicit representation of serial order information. All fits were obtained
  with only 1 free scaling parameter fixed across the simulations; all other parameters were ACT-R
  defaults. The modeling results support the hypothesis that fluctuating activation and similarity-based interference
  are the key factors shaping working memory in sentence processing. We contrast the theory
  and empirical predictions with several related accounts of sentence-processing complexity.},
  Title = {An activation-based model of sentence processing as skilled memory retrieval},
  Volume = {29},
  Year = {2005},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/Lewis-VasishthCogSci2005.pdf},
  code = {http://www.ling.uni-potsdam.de/~vasishth/code/LewisVasishthModel05.tar.gz}
  }

@unpublished{EngelmannJaegerVasishthSubmitted2018,
  Author = {Engelmann, Felix and J{\"a}ger, Lena A. and Vasishth, Shravan},
  Note = {Manuscript submitted to Cognitive Science},
  Title = {The effect of prominence and cue association in retrieval processes: {A} computational account},
  pdf = {https://osf.io/b56qv/},
  abstract = {
We present a model of cue-based retrieval in sentence processing that formalizes (i) memory accessibility (prominence) and (ii) a theory of associative cues as extensions to the ACT-R model of Lewis and Vasishth (2005). The extensions are independently motivated and, compared to the original model, enable more differentiated predictions with respect to the experimental design of individual experiments as well as differences between retrieval contexts. The predictions of the original and the extended model are compared with the results of a comprehensive Bayesian meta-analysis of published studies on retrieval interference in reflexive-/reciprocal-antecedent and subject-verb dependencies (Jäger, Engelmann, Vasishth, submitted). Quantitative simulations show that the extended model accounts for effects that are outside the scope of the original model. The results emphasize the importance of accounting for different aspects of memory accessibility, for individual study design, and context-based feature-selectivity in order to generate accurate predictions of a model of cue-based memory retrieval. The simulation results thus shed new light on the cognitive mechanisms un- derlying interference effects and should be considered in the interpretation of the available data and in the design of future experiments.},
  Year = {2018}}


@ARTICLE{NicenboimVasishth2016,
   author = {Bruno Nicenboim and Shravan Vasishth},
    title = "{Statistical methods for linguistic research: {Foundational} Ideas - {Part} {II}}",
  journal = {Language and Linguistics Compass},
   eprint = {https://arxiv.org/abs/1602.00245},
  pages = {591--613},
  doi = {10.1111/lnc3.12207},
url = {http://dx.doi.org/10.1111/lnc3.12207},
  issn = {1749-818X},
  number = {11},
  volume = {10},
     year = "2016"
}

@Article{NicenboimEtAl2016Frontiersb,
  Title                    = { When high-capacity readers slow down and low-capacity readers speed up: {W}orking memory and locality effects },
  Author                   = { Bruno Nicenboim and Pavel Logačev and Carolina Gattei and Shravan Vasishth },
  JOURNAL={Frontiers in Psychology},      
  VOLUME={7},      
  YEAR={2016},      
  NUMBER={280},     
  URL={http://www.frontiersin.org/language_sciences/10.3389/fpsyg.2016.00280/abstract},       
  DOI={10.3389/fpsyg.2016.00280},      
  ISSN={1664-1078} ,      
  ABSTRACT={We examined the effects of argument-head distance in SVO and SOV languages (Spanish and German), while taking into account readers’ working memory capacity and controlling for expectation (Levy, 2008) and other factors. We predicted only locality effects, that is, a slow-down produced by increased dependency distance (Gibson, 2000; Lewis & Vasishth, 2005). Furthermore, we expected stronger locality effects for readers with low working memory capacity. Contrary to our predictions, low-capacity readers showed faster reading with increased distance, while high-capacity readers showed locality effects. We suggest that while the locality effects are compatible with memory-based explanations, the speedup of low-capacity readers can be explained by an increased probability of retrieval failure. We present a computational model based on ACT-R built under the previous assumptions, which is able to give a qualitative account for the present data and can be tested in future research. Our results suggest that in some cases, interpreting longer RTs as indexing increased processing difficulty and shorter RTs as facilitation may be too simplistic: The same increase in processing difficulty may lead to slowdowns in high-capacity readers and speedups in low-capacity ones. Ignoring individual level capacity differences when investigating locality effects may lead to misleading conclusions.}
}


@unpublished{NicenboimVasishth2016Models,
  Title                    = {Models of Retrieval in Sentence Comprehension: {A} computational evaluation using {Bayesian} hierarchical modeling},
  Author                   = { Bruno Nicenboim and Shravan Vasishth },
  Year                     = {Submitted},
  eprint = {https://arxiv.org/abs/1612.04174},
  notes={Under review in Journal of Memory and Language; arxiv e-print},
  url                      = {https://arxiv.org/abs/1612.04174},
  customa = {papers/NicenboimVasishth2016Models.pdf}
}

@incollection{gibson00,
  Address = {Cambridge, MA},
  Author = {Edward Gibson},
  Booktitle = {{Image, Language, Brain}: {Papers from the First Mind Articulation Project Symposium}},
  Editor = {Marantz, Alec and Miyashita, Yasushi and O'Neil, Wayne},
  Publisher = {MIT Press},
  Title = {Dependency Locality Theory: {A} Distance-Based Theory of Linguistic Complexity},
  Year = {2000}}


@article{vasishthlewisLanguage05,
  Author = {Shravan Vasishth and Richard L. Lewis},
  Date-Modified = {2009-08-21 11:35:57 +0200},
  Journal = {Language},
  Number = {4},
  Optmonth = {December},
  Pages = {767-794},
  abstract = {Although proximity between arguments and verbs (locality) is a relatively robust determinant
  of sentence-processing difficulty (Hawkins 1998, 2001, Gibson 2000), increasing argument-verb
  distance can also facilitate processing (Konieczny 2000). We present two self-paced reading
  (SPR) experiments involving Hindi that provide further evidence of antilocality, and a third SPR
  experiment which suggests that similarity-based interference can attenuate this distance-based
  facilitation. A unified explanation of interference, locality, and antilocality effects is proposed
  via an independently motivated theory of activation decay and retrieval interference (Anderson
  et al. 2004).},
  Title = {Argument-head distance and processing complexity: {E}xplaining both locality and antilocality effects},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/Vasishth-Lewis-Language2006.pdf},
  Volume = {82},
  Year = {2006},
  code = {http://www.ling.uni-potsdam.de/~vasishth/code/VasishthLewis2006.zip}
}


@Inproceedings{VasishthEtAl2017Modelling,
  Title                    = {Modelling dependency completion in sentence comprehension as a {Bayesian} hierarchical mixture process: {A} case study involving {Chinese} relative clauses},
  Author                   = {Shravan Vasishth and Nicolas Chopin and Robin Ryder and Bruno Nicenboim},
  eprint = {https://arxiv.org/abs/1702.00564v2},
       year = 2017,
  notes={arxiv e-print},
  Booktitle={Proceedings of Cognitive Science Conference},
Location={London, UK},
  url                      = {https://arxiv.org/abs/1702.00564v2},
  customa = {papers/VasishthEtAl2017Modelling.pdf}
}


@Inproceedings{VasishthEtAl2017Feature,
   author = {{Vasishth}, S. and {J{\"a}ger}, L.~A. and {Nicenboim}, B.},
    title = "{Feature overwriting as a finite mixture process: {Evidence} from comprehension data}",
archivePrefix = "arXiv",
   eprint = {https://arxiv.org/abs/1703.04081},
     Booktitle={Proceedings of MathPsych/ICCM Conference},
Location={Warwick, UK},
 notes={arxiv e-print},
     year = 2017,
  url = {https://arxiv.org/abs/1703.04081},
  customa = {papers/VasishthEtAl2017Feature.pdf}
  
}

@book{sloppy,
  title={Rigor mortis: {H}ow sloppy science creates worthless cures, crushes hope, and wastes billions},
  author={Harris, Richard},
  year={2017},
  publisher={Basic Books}
}

@book{kerns,
     Year = {2018},
  Author = {G. Jay Kerns},
  Title = {Introduction to Probability and Statistics Using R},
  url = {https://www.nongnu.org/ipsur/}}


@article{HofmeisterVasishth2014,
  author = {Philip Hofmeister and Shravan Vasishth},
  title = {Distinctiveness and encoding effects in online sentence comprehension},
  year = {2014},
  pages = {1--13},
  abstract = {In explicit memory recall and recognition tasks, elaboration and contextual isolation both facilitate memory performance. Here, we investigate these effects in the context of sentence processing: targets for retrieval during online sentence processing of English object relative clause constructions differ in the amount of elaboration associated with the target noun phrase, or the homogeneity of superficial features (text color). Experiment 1 shows that greater elaboration for targets during the encoding phase reduces reading times at retrieval sites, but elaboration of non-targets has considerably weaker effects. Experiment 2 illustrates that processing isolated superficial features of target noun phrases—here, a green word in a sentence with words colored white—does not lead to enhanced memory performance, despite triggering longer encoding times. These results are interpreted in the light of the memory models of Nairne, 1990, 2001, 2006, which state that encoding remnants contribute to the set of retrieval cues that provide the basis for similarity-based interference effects.},
  doi = {doi: 10.3389/fpsyg.2014.01237},
  pdf = {http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.01237/abstract},
  volume = {5},
  journal = {Frontiers in Psychology},
  note = {Article 1237},
  code = {http://privatewww.essex.ac.uk/~phofme/Univ3-Frontiers.zip}
}


@article{HusainEtAl2014,
  title={Strong Expectations Cancel Locality Effects: {E}vidence from {H}indi},
  author={Husain, Samar and Vasishth, Shravan and Srinivasan, Narayanan},
  journal={PLoS ONE},
  volume={9},
  number={7},
  pages={1--14},
  year={2014},
  abstract = {Expectation-driven facilitation (Hale, 2001; Levy, 2008) and locality-driven retrieval difficulty (Gibson, 1998, 2000; Lewis &
Vasishth, 2005) are widely recognized to be two critical factors in incremental sentence processing; there is accumulating
evidence that both can influence processing difficulty. However, it is unclear whether and how expectations and memory
interact. We first confirm a key prediction of the expectation account: a Hindi self-paced reading study shows that when an
expectation for an upcoming part of speech is dashed, building a rarer structure consumes more processing time than
building a less rare structure. This is a strong validation of the expectation-based account. In a second study, we show that
when expectation is strong, i.e., when a particular verb is predicted, strong facilitation effects are seen when the appearance
of the verb is delayed; however, when expectation is weak, i.e., when only the part of speech ``verb'' is predicted but a
particular verb is not predicted, the facilitation disappears and a tendency towards a locality effect is seen. The interaction
seen between expectation strength and distance shows that strong expectations cancel locality effects, and that weak
expectations allow locality effects to emerge.},
  publisher={Public Library of Science},
  pdf = {http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0100986},
  code={http://www.ling.uni-potsdam.de/~vasishth/code/HusainEtAl2014PLoSONE.zip}
}

@article{FrankEtAl2015,
  author = {Stefan L. Frank and Thijs Trompenaars and Shravan Vasishth},
  title = {Cross-linguistic differences in processing double-embedded relative clauses: {W}orking-memory constraints or language statistics?},
  year = {2015},
  pages = {n/a},
  abstract = {An English double-embedded relative clause from which the middle verb is omitted can often be processed more easily than its grammatical counterpart, a phenomenon known as the grammaticality illusion. This effect has been found to be reversed in German, suggesting that the illusion is language specific rather than a consequence of universal working memory constraints. We present results from three self-paced reading experiments which show that Dutch native speakers also do not show the grammaticality illusion in Dutch, whereas both German and Dutch native speakers do show the illusion when reading English sentences. These findings provide evidence against working memory constraints as an explanation for the observed effect in English. We propose an alternative account based on the statistical patterns of the languages involved. In support of this alternative, a single recurrent neural network model that is trained on both Dutch and English sentences indeed predicts the cross-linguistic difference in grammaticality effect.},
  journal = {Cognitive Science},
  code = {https://github.com/vasishth/StanJAGSexamples/tree/master/FrankEtAlCogSci2015},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/FrankTrompenaarsVasishthCogSci.pdf}
}

@Article{Lee2011,
  Title                    = {How cognitive modeling can benefit from hierarchical {Bayes}ian models},
  Author                   = {Lee, Michael D.},
  Year                     = {2011},
  Doi                      = {10.1016/j.jmp.2010.08.013},
  ISSN                     = {0022-2496},
  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {1--7},
  Url                      = {http://dx.doi.org/10.1016/j.jmp.2010.08.013},
  Volume                   = {55},

  File                     = {:Lee-inpress HIERARCHICAL.pdf:PDF},
  Journal                  = {Journal of Mathematical Psychology},
  Publisher                = {Elsevier BV}
}

@article{verdinelli1995computing,
  title={Computing {Bayes factors using a generalization of the Savage-Dickey density ratio}},
  author={Verdinelli, Isabella and Wasserman, Larry},
  journal={Journal of the American Statistical Association},
  volume={90},
  number={430},
  pages={614--618},
  year={1995},
  publisher={Taylor \& Francis}
}

@book{LeeWagenmakers2014,
  title={Bayesian cognitive modeling: A practical course},
  author={Lee, Michael D. and Wagenmakers, Eric-Jan},
  year={2014},
  publisher={Cambridge University Press}
}

@article {LogacevVasishth2015,
author = {Logačev, Pavel and Vasishth, Shravan},
title = {A Multiple-Channel Model of Task-Dependent Ambiguity Resolution in Sentence Comprehension},
journal = {Cognitive Science},
volume = {40},
number = {2},
issn = {1551-6709},
url = {http://dx.doi.org/10.1111/cogs.12228},
doi = {10.1111/cogs.12228},
pages = {266--298},
keywords = {Sentence processing, Ambiguity, Parallel processing, Cognitive modeling, Unrestricted race model, URM, Underspecification, Good-enough processing},
year = {2016},
}

@article{lme4new,
  Author = {Bates, D. and Maechler, M. and Bolker, B.M. and Walker, S.},
  Journal = {Journal of Statistical Software},
  Title = {Fitting Linear Mixed-Effects Models using lme4},
    Year = {2015},
    OPTdoi = {10.18637/jss.v067.i01},
    Volume = {67},
    Issue = {1},
    pages = {1--48}
    }

@article{HusainEtAl2014,
  title={Strong Expectations Cancel Locality Effects: {E}vidence from {H}indi},
  author={Husain, Samar and Vasishth, Shravan and Srinivasan, Narayanan},
  journal={PLoS ONE},
  volume={9},
  number={7},
  pages={1--14},
  year={2014},
  abstract = {Expectation-driven facilitation (Hale, 2001; Levy, 2008) and locality-driven retrieval difficulty (Gibson, 1998, 2000; Lewis &
Vasishth, 2005) are widely recognized to be two critical factors in incremental sentence processing; there is accumulating
evidence that both can influence processing difficulty. However, it is unclear whether and how expectations and memory
interact. We first confirm a key prediction of the expectation account: a Hindi self-paced reading study shows that when an
expectation for an upcoming part of speech is dashed, building a rarer structure consumes more processing time than
building a less rare structure. This is a strong validation of the expectation-based account. In a second study, we show that
when expectation is strong, i.e., when a particular verb is predicted, strong facilitation effects are seen when the appearance
of the verb is delayed; however, when expectation is weak, i.e., when only the part of speech ``verb'' is predicted but a
particular verb is not predicted, the facilitation disappears and a tendency towards a locality effect is seen. The interaction
seen between expectation strength and distance shows that strong expectations cancel locality effects, and that weak
expectations allow locality effects to emerge.},
  publisher={Public Library of Science},
  pdf = {http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0100986},
  code={http://www.ling.uni-potsdam.de/~vasishth/code/HusainEtAl2014PLoSONE.zip}
}
    


@article{barr2013,
  title={Random effects structure for confirmatory hypothesis testing: {K}eep it maximal},
  author={Barr, Dale J and Levy, Roger and Scheepers, Christoph and Tily, Harry J},
  journal={Journal of Memory and Language},
  volume={68},
  number={3},
  pages={255--278},
  year={2013},
  publisher={Elsevier}
}


@unpublished{BatesEtAlParsimonious,
  Author = {Douglas Bates and Reinhold Kliegl and Shravan Vasishth and Harald Baayen},
  Note = {ArXiv e-print},
  Title = {Parsimonious mixed models},
  Year = {2015},
  url = {http://arxiv.org/abs/1506.04967},
    abstract = {The analysis of experimental data with mixed-effects models requires
decisions about the specification of the appropriate random-effects structure.
Recently, Barr, et al 2013, recommended  fitting `maximal'
models with all possible random effect components included.  Estimation of
maximal models, however, may not converge.  We show that failure to converge
 typically is not due to a suboptimal estimation algorithm, but is
a consequence of attempting to fit a model that is too complex to be properly
supported by the data, irrespective of whether estimation is based on maximum
likelihood or on Bayesian hierarchical modeling with uninformative or weakly
informative priors.  Importantly, even under convergence, overparameterization
may lead to uninterpretable models.  We provide diagnostic tools for detecting
overparameterization and guiding model simplification.  Finally, we clarify
that the simulations on which Barr et al. base their recommendations are
atypical for real data.  A detailed example is provided of how subject-related
attentional fluctuation across trials may further qualify
statistical inferences about fixed effects, and of how such nonlinear effects
can be accommodated within the mixed-effects modeling framework.}
}


@article{GelmanEtAl2014understanding,
  title={Understanding predictive information criteria for {B}ayesian models},
  author={Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  journal={Statistics and Computing},
  volume={24},
  number={6},
  pages={997--1016},
  doi={10.1007/s11222-013-9416-2},
  year={2014},
  publisher={Springer}
}


@Article{VehtariOjanen2012,
  Title                    = {A survey of {Bayes}ian predictive methods for model assessment, selection and comparison},
  Author                   = {Vehtari, Aki and Ojanen, Janne},
  Year                     = {2012},
  Doi                      = {10.1214/12-ss102},
  ISSN                     = {1935-7516},
  Number                   = {0},
  Pages                    = {142--228},
  Url                      = {http://dx.doi.org/10.1214/12-SS102},
  Volume                   = {6},

  File                     = {:home/bruno/ownCloud/Papers/stats/Vehtari Ojanen 2012 A survey of Bayesian predictive methods for model assessment, selection and comparison.pdf:PDF},
  Journal                  = {Statistics Surveys},
  Publisher                = {Institute of Mathematical Statistics}
}

@Article{VehtariEtAl2017,
author="Vehtari, Aki
and Gelman, Andrew
and Gabry, Jonah",
title="Practical {Bayesian} model evaluation using leave-one-out cross-validation and {WAIC}",
journal="Statistics and Computing",
year="2017",
volume="27",
number="5",
pages="1413--1432",
abstract="Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.",
issn="1573-1375",
doi="10.1007/s11222-016-9696-4",
url="http://dx.doi.org/10.1007/s11222-016-9696-4"
}
