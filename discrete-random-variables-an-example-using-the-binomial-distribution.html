<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.3 Discrete random variables: An example using the Binomial distribution | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.13.2 and GitBook 2.6.7" />

  <meta property="og:title" content="1.3 Discrete random variables: An example using the Binomial distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bookdown.org/yihui/bookdown/" />
  <meta property="og:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="rstudio/bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.3 Discrete random variables: An example using the Binomial distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />

<meta name="author" content="Shravan Vasishth, Bruno Nicenboim, and Daniel Schad" />


<meta name="date" content="2019-11-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="conditional-probability.html"/>
<link rel="next" href="continuous-random-variables-an-example-using-the-normal-distribution.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.2</b> How to read this book</a></li>
<li class="chapter" data-level="0.3" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.3</b> Online materials</a></li>
<li class="chapter" data-level="0.4" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.4</b> Software needed</a></li>
<li class="chapter" data-level="0.5" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.5</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html"><i class="fa fa-check"></i><b>1.3</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.3.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.4</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.4.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><i class="fa fa-check"></i><b>1.5</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.6" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.7</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a><ul>
<li class="chapter" data-level="1.9.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.9.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.9.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.9.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.9.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.9.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.9.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.9.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.9.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="computational-bayesian-data-analysis.html"><a href="computational-bayesian-data-analysis.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using ‘Stan’: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="the-influence-of-priors-sensitivity-analysis.html"><a href="the-influence-of-priors-sensitivity-analysis.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="the-influence-of-priors-sensitivity-analysis.html"><a href="the-influence-of-priors-sensitivity-analysis.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-influence-of-priors-sensitivity-analysis.html"><a href="the-influence-of-priors-sensitivity-analysis.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="the-influence-of-priors-sensitivity-analysis.html"><a href="the-influence-of-priors-sensitivity-analysis.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="the-influence-of-priors-sensitivity-analysis.html"><a href="the-influence-of-priors-sensitivity-analysis.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="revisiting-the-button-pressing-example-with-different-priors.html"><a href="revisiting-the-button-pressing-example-with-different-priors.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="posterior-predictive-distribution.html"><a href="posterior-predictive-distribution.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="posterior-predictive-distribution.html"><a href="posterior-predictive-distribution.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="posterior-predictive-distribution.html"><a href="posterior-predictive-distribution.html#the-log-normal-likelihood"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="posterior-predictive-distribution.html"><a href="posterior-predictive-distribution.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a lognormal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3.9</b> Appendix</a><ul>
<li class="chapter" data-level="3.9.1" data-path="appendix.html"><a href="appendix.html#generating-prior-predictive-distributions-with-brms"><i class="fa fa-check"></i><b>3.9.1</b> Generating prior predictive distributions with <code>brms</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>4</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="5" data-path="bayes-factor-definition.html"><a href="bayes-factor-definition.html"><i class="fa fa-check"></i><b>5</b> Bayes factor: Definition</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="discrete-random-variables-an-example-using-the-binomial-distribution" class="section level2">
<h2><span class="header-section-number">1.3</span> Discrete random variables: An example using the Binomial distribution</h2>
<p>Consider the following sentence:</p>
<p><em>“It’s raining, I’m going to take the ….”</em></p>
<p>Suppose that our research goal is to estimate the probability, call it <span class="math inline">\(\theta\)</span>, of the word “umbrella” appearing in this sentence, versus any other word. If the sentence is completed with the word “umbrella”, we will refer to it as a success; any other completion will be referred to as a failure. This is an example of a Binomial random variable: there can be only two possible outcomes, a success or a failure, and there is some true unknown probability <span class="math inline">\(\theta\)</span> of success that we want to estimate.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>One way to empirically estimate this probability of success is to carry out a so-called cloze task. In a cloze task, participants are asked to complete a fragment of the original sentence, such as “It’s raining, I’m going to take the …”. The predictability or cloze probability of “umbrella” is then calculated as the proportion of times that the target word “umbrella” was produced as an answer by participants.</p>
<p>Assume for simplicity that <span class="math inline">\(10\)</span> participants are asked to complete the above sentence; each participant does this task only once. This gives us independent responses from <span class="math inline">\(10\)</span> trials that are either coded a success (“umbrella” was produced) or as a failure (some other word was produced). We can sum up the number of sucesses to calculate how many of the 10 trials had “umbrella” as a response. For example, if <span class="math inline">\(8\)</span> instances of “umbrella” are produced in <span class="math inline">\(10\)</span> trials, we would estimate the cloze probability of producing “umbrella” would be <span class="math inline">\(8/10\)</span>.</p>
<p>We can repeatedly generate simulated sequences of the number of successes in R (later on we will demonstrate how to generate such random sequences of simulated data). Here is a case where we run the same experiment <span class="math inline">\(20\)</span> times (the sample size is <span class="math inline">\(10\)</span> each time).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">rbinom</span>(<span class="dv">10</span>,<span class="dt">n=</span><span class="dv">20</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>##  [1] 7 7 4 7 6 5 6 3 6 6 5 6 7 4 5 7 8 3 5 5</code></pre>
<p>The number of successes in each of the <span class="math inline">\(20\)</span> simulated experiments above is being generated by a discrete random variable <span class="math inline">\(Y\)</span> with a probability distribution <span class="math inline">\(p(y|\theta)\)</span> called the <strong>Binomial distribution</strong>.</p>
<p>For discrete random variables such as the Binomial, the probability distribution <span class="math inline">\(p(y|\theta)\)</span> is called a <strong>probability mass function</strong> (PMF). The PMF defines the probability of each possible outcome. In the above example, with <span class="math inline">\(n=10\)</span> trials, there are 11 possible outcomes: <span class="math inline">\(y=0,1,2,...,10\)</span> successes. Which of these outcomes is most probable depends on the parameter <span class="math inline">\(\theta\)</span> in the Binomial distribution that represents the probability of success.</p>
<p>The left-hand side plot in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomplot">1.1</a> shows an example of a Binomial PMF with <span class="math inline">\(10\)</span> trials, with the parameter <span class="math inline">\(\theta\)</span> fixed at <span class="math inline">\(0.5\)</span>. Setting <span class="math inline">\(\theta\)</span> to <span class="math inline">\(0.5\)</span> leads to a PMF where the most probable outcome is <span class="math inline">\(5\)</span> successes out of <span class="math inline">\(10\)</span>. If we had set <span class="math inline">\(\theta\)</span> to, say 0.1, then the most probable outcome would be <span class="math inline">\(1\)</span> success out of <span class="math inline">\(10\)</span>; and if we had set <span class="math inline">\(\theta\)</span> to <span class="math inline">\(0.9\)</span>, then the most probable outcome would be <span class="math inline">\(9\)</span> successes out of <span class="math inline">\(10\)</span>.</p>
<div class="figure"><span id="fig:binomplot"></span>
<img src="bookdown_files/figure-html/binomplot-1.svg" alt="Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success." width="672" />
<p class="caption">
FIGURE 1.1: Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success.
</p>
</div>

<div class="rmdnote">
to-do bar or line graphs above, instead of points
</div>

<p>The probability mass function for the binomial is written as follows.</p>
<p><span class="math display">\[\begin{equation}
\hbox{Binomial}(k|n,\theta) = 
\binom{n}{k} \theta^{k} (1-\theta)^{n-k}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(n\)</span> represents the total number of trials, <span class="math inline">\(k\)</span> the number of successes (this could range from 0 to 10), and <span class="math inline">\(\theta\)</span> the probability of success. The term <span class="math inline">\(\binom{n}{k}\)</span>, pronounced n-choose-k, represents the number of ways in which one can choose <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(n\)</span> trials. For example, 1 success out of 10 can occur in 10 possible ways: the very first trial could be a 1, the secone trial could be a 1, etc.
The term <span class="math inline">\(\binom{n}{k}\)</span> expands to <span class="math inline">\(\frac{n!}{k!(n-k)!}\)</span>. In <code>R</code>, it is computed using the function <code>choose(n,k)</code>, with <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> representing positive integer values.</p>
<p>When we want to express the fact that the data is assumed to be generated from a Binomial random variable, we will write <span class="math inline">\(Y \sim Binomial(n,\theta)\)</span>, where <span class="math inline">\(\sim\)</span> should be read as “is being generated from”. If the data is generated from a random variable that has some other probability distribution <span class="math inline">\(f(\theta)\)</span>, we will write <span class="math inline">\(Y\sim f(\theta)\)</span>.</p>
<div id="the-mean-and-variance-of-the-binomial-distribution" class="section level3">
<h3><span class="header-section-number">1.3.1</span> The mean and variance of the Binomial distribution</h3>
<p>It is possible to analytically compute the mean and variance of the PMF associated with the Binomial random variable <span class="math inline">\(Y\)</span>. Without getting into the details of how these are derived mathematically, we just state here that the mean of <span class="math inline">\(Y\)</span> (also called the expectation, conventionally written <span class="math inline">\(E[Y]\)</span>) and variance of <span class="math inline">\(Y\)</span> (written <span class="math inline">\(Var(Y)\)</span>) of a Binomial distribution with parameter <span class="math inline">\(\theta\)</span> and <span class="math inline">\(n\)</span> trials are <span class="math inline">\(E[Y] = n\theta\)</span> and <span class="math inline">\(Var(Y) = n\theta (1-\theta)\)</span>.</p>
<p>Of course, <span class="math inline">\(n\)</span> is a fixed number because we decide on the total number of trials before running the experiment. In the PMF <span class="math inline">\(\theta\)</span> is also a fixed value; the only variable in a PMF is <span class="math inline">\(k\)</span>. In real experimental situations we never know the true value of <span class="math inline">\(\theta\)</span>. But <span class="math inline">\(\theta\)</span> can be estimated from the data. From the observed data, we can compute the estimate of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat \theta=k/n\)</span>. The quantity <span class="math inline">\(\hat \theta\)</span> is the observed proportion of successes, and is called the <strong>maximum likelihood estimate</strong> of the true (but unknown) expectation E[Y]. Once we have estimated <span class="math inline">\(\theta\)</span> in this way, we can also obtain an estimate (also a maximum likelihood estimate) of the variance by computing <span class="math inline">\(n\theta (1-\theta)\)</span>. These estimates are then used for statistical inference.</p>
<p>What does the term “maximum likelihood estimate” mean? The term <strong>likelihood</strong> refers to the Binomial distribution function, i.e., the PMF we saw above, <span class="math inline">\(p(k|n,\theta)\)</span>. Recall that the PMF assumes that <span class="math inline">\(\theta\)</span> and <span class="math inline">\(n\)</span> are fixed, and <span class="math inline">\(k\)</span> will vary from 0 to 10 when the experiment is repeated multiple times. The likelihood function is the same function as the PMF, <span class="math inline">\(p(k|n,\theta)\)</span>, but assumes that the data is fixed and only <span class="math inline">\(\theta\)</span> varies (from 0 to 1).</p>
<p>For example, suppose you record <span class="math inline">\(n=10\)</span> trials, and observe <span class="math inline">\(k=7\)</span> successes. What is the probability of observing <span class="math inline">\(7\)</span> successes out of <span class="math inline">\(10\)</span>? We need the Binomial distribution to compute this value:</p>
<p><span class="math display">\[\begin{equation}
\hbox{Binomial}(k=7|n=10,\theta) = 
\binom{10}{7} \theta^{7} (1-\theta)^{10-7}
\end{equation}\]</span></p>
<p>Once we have observed the data (k=7 successes), both <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> are fixed. The only variable in the above equation now is <span class="math inline">\(\theta\)</span>: the above function is now only dependent on the value of <span class="math inline">\(\theta\)</span>.</p>
<p>When the data are fixed, the probability mass function is only dependent on the value of the parameter <span class="math inline">\(\theta\)</span>, and is called a <strong>likelihood function</strong>. It is therefore often expressed as a function of <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math inline">\(p( k=7, n=10 | \theta) = \mathcal{L}(\theta)\)</span></p>
<p>Since the PMF and the likelihood refer to the same function seen in two different ways, sometimes the likelihood is written <span class="math inline">\(p(\theta | k=7, n=10)\)</span> to distinguish it from the PMF, which has the data appearing first (<span class="math inline">\(p(k|n,\theta)\)</span>). We will write both the PMF and the likelihood identically in this book; context will disambiguate what we are referring to.</p>
<p>If we now plot the likelihood function for all possible values of <span class="math inline">\(\theta\)</span> ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>, we get the plot shown in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomlik">1.2</a>.</p>
<div class="figure"><span id="fig:binomlik"></span>
<img src="bookdown_files/figure-html/binomlik-1.svg" alt="The likelihood function for 7 successes out of 10." width="672" />
<p class="caption">
FIGURE 1.2: The likelihood function for 7 successes out of 10.
</p>
</div>

<div class="rmdnote">
DS comment: do we want to show the code for computing all likelihood values? (maybe this comes later?)
</div>

<p>What is important about this plot is that it shows that, given the data, the maximum point is at the point <span class="math inline">\(0.7\)</span>, which corresponds to the estimated mean using the formula shown above: <span class="math inline">\(k/n = 7/10\)</span>. Thus, the maximum likelihood estimate (MLE) gives us the most likely value that the parameter <span class="math inline">\(\theta\)</span> has, given the data.</p>
<p>It is crucial to note here that the phrase “most likely” here does not mean that the MLE from a <em>particular</em> sample of data invariably gives us an accurate estimate of <span class="math inline">\(\theta\)</span>. For example, if we run our experiment for <span class="math inline">\(10\)</span> trials and get <span class="math inline">\(1\)</span> success out of <span class="math inline">\(10\)</span>, the MLE is <span class="math inline">\(0.10\)</span>. We could have happened to observe only one success out of ten even if the true <span class="math inline">\(\theta\)</span> were <span class="math inline">\(0.5\)</span>. The MLE would however give an accurate estimate of the true parameter as <span class="math inline">\(n\)</span> approaches infinity.</p>
</div>
<div id="what-information-does-a-probability-distribution-provide" class="section level3">
<h3><span class="header-section-number">1.3.2</span> What information does a probability distribution provide?</h3>
<p>In Bayesian data analysis, we will constantly be asking the question: what information does a probability distribution give us? In particular, we will treat each parameter <span class="math inline">\(\theta\)</span> as a random variable; this will raise questions like: “what is the probability that the parameter <span class="math inline">\(\theta\)</span> lies between two values <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>;”what is the range over which we can be 95% certain that the true value of the parameter lies&quot;? In order to be able to answer questions like these, we need to know what information we can obtain once we have a probability distribution, and how to extract this information. We therefore discuss the different kinds of information we can obtain from a probability distribution. For now we focus only on the Binomial random variable discussed above.</p>
<div id="compute-the-probability-of-a-particular-outcome-discrete-case-only" class="section level4">
<h4><span class="header-section-number">1.3.2.1</span> Compute the probability of a particular outcome (discrete case only)</h4>
<p>The Binomial distribution shown in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomplot">1.1</a> already shows the probability of each possible outcome under a different value for <span class="math inline">\(\theta\)</span>. In R, there is a built-in function that allows us to calculate the probability of <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(n\)</span>, given a particular value of <span class="math inline">\(k\)</span> (this number constitutes our data), the number of trials <span class="math inline">\(n\)</span>, and given a particular value of <span class="math inline">\(\theta\)</span>; this is the <code>dbinom</code> function. For example, the probability of 5 successes out of 10 when <span class="math inline">\(\theta\)</span> is 0.5 is:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">dbinom</span>(<span class="dv">5</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 0.2461</code></pre>
<p>The probabilities of success when <span class="math inline">\(\theta\)</span> is 0.1 or 0.9 can be computed by replacing 0.5 above by each of these probabilities. One can just do this by giving <code>dbinom</code> a vector of probabilities:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">dbinom</span>(<span class="dv">5</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="kw">c</span>(<span class="fl">0.1</span>,<span class="fl">0.9</span>))</a></code></pre></div>
<pre><code>## [1] 0.001488 0.001488</code></pre>
<p>Note that the probability of a particular outcome is only computable in the discrete case; in the continuous case, this probability will always be zero (we discuss this when we turn to continuous probability distributions below).</p>
</div>
<div id="compute-the-cumulative-probability-of-k-or-less-more-than-k-successes" class="section level4">
<h4><span class="header-section-number">1.3.2.2</span> Compute the cumulative probability of k or less (more) than k successes</h4>
<p>Using the <code>dbinom</code> function, we can compute the cumulative probability of obtaining 1 or less, 2 or less successes etc. This is done through a simple summation procedure:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="co">## the cumulative probability of obtaining</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2"><span class="co">## 0, 1, or 2 successes out of 10,</span></a>
<a class="sourceLine" id="cb8-3" data-line-number="3"><span class="co">## with theta=0.5:</span></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="kw">dbinom</span>(<span class="dv">0</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)<span class="op">+</span><span class="kw">dbinom</span>(<span class="dv">1</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb8-5" data-line-number="5"><span class="st">  </span><span class="kw">dbinom</span>(<span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 0.05469</code></pre>
<p>Mathematically, we could write the above summation as:</p>
<p><span class="math display">\[\begin{equation}
\sum_{k=0}^2 \binom{n}{k} \theta^{k} (1-\theta)^{n-k} 
\end{equation}\]</span></p>
<p>An alternative to the cumbersome addition in the R code above is this more compact statement, which closely mimics the above mathematical expression:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw">sum</span>(<span class="kw">dbinom</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>))</a></code></pre></div>
<pre><code>## [1] 0.05469</code></pre>
<p>R has a built-in function called <code>pbinom</code> that does this summation for us. If we want to know the probability of <span class="math inline">\(2\)</span> or less successes as in the above example, we can write:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="kw">pbinom</span>(<span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>,<span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 0.05469</code></pre>
<p>The specification <code>lower.tail=TRUE</code> ensures that the summation goes from <span class="math inline">\(2\)</span> to numbers smaller than <span class="math inline">\(2\)</span> (which lie in the lower tail of the distribution in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomplot">1.1</a>). If we wanted to know what the probability is of obtaining <span class="math inline">\(2\)</span> or more successes out of <span class="math inline">\(10\)</span>, we can set <code>lower.tail</code> to <code>FALSE</code>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="kw">pbinom</span>(<span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>,<span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## [1] 0.9453</code></pre>
<p>The cumulative distribution function or CDF can be plotted by computing the cumulative probabilities for any value <span class="math inline">\(k\)</span> or less than <span class="math inline">\(k\)</span>, where <span class="math inline">\(k\)</span> ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(10\)</span> in our running example. The CDF is shown in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomcdf">1.3</a>.</p>
<div class="figure"><span id="fig:binomcdf"></span>
<img src="bookdown_files/figure-html/binomcdf-1.svg" alt="The cumulative distribution function for a Binomial distribution assuming 10 trials, with 50% probability of success." width="672" />
<p class="caption">
FIGURE 1.3: The cumulative distribution function for a Binomial distribution assuming 10 trials, with 50% probability of success.
</p>
</div>
</div>
<div id="compute-the-inverse-of-the-cumulative-distribution-function-the-quantile-function" class="section level4">
<h4><span class="header-section-number">1.3.2.3</span> Compute the inverse of the cumulative distribution function (the quantile function)</h4>
<p>We can also find out the value of the variable <span class="math inline">\(k\)</span> (the quantile) such that the probability of obtaining <span class="math inline">\(k\)</span> or less than <span class="math inline">\(k\)</span> successes is some specific probability value <span class="math inline">\(p\)</span>. If we switch the x and y axes of Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomcdf">1.3</a>, we obtain another very useful function, the inverse CDF.</p>
<p>The inverse of the CDF (known as the quantile function in R because it returns the quantile, the value k) is available in R as the function <code>qbinom</code>. The usage is as follows: to find out what the value <span class="math inline">\(k\)</span> of the outcome is such that the probability of obtaining <span class="math inline">\(k\)</span> or less successes is <span class="math inline">\(0.37\)</span>, type:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="kw">qbinom</span>(<span class="fl">0.37</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 4</code></pre>

<div class="rmdnote">
to-do: explain why qbinom(0.77 gives 5 as an answer and not 4)
</div>


<div class="rmdnote">
DS comment: maybe it’s good to include an additional Figure for the inverse CDF and an example
</div>

</div>
<div id="generate-simulated-data-from-a-hboxbinomialntheta-distribution" class="section level4">
<h4><span class="header-section-number">1.3.2.4</span> Generate simulated data from a <span class="math inline">\(\hbox{Binomial}(n,\theta)\)</span> distribution</h4>
<p>We can generate simulated data from a Binomial distribution by specifying the number of trials and the probability of success <span class="math inline">\(\theta\)</span>. In <code>R</code>, we do this as follows:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw">rbinom</span>(<span class="dv">1</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 7</code></pre>

<div class="rmdnote">
to-do: introduce Bernoulli here and link it with the code below
</div>

<p>The above code generates the number of successes in an experiment with <span class="math inline">\(10\)</span> trials. Repeatedly run the above code; you will get different sequences each time. For each generated sequence, one can calculate the number of successes by just summing up the vector, or computing its mean and multiplying by the number of trials, here <span class="math inline">\(10\)</span>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1">y&lt;-<span class="kw">rbinom</span>(<span class="dv">10</span>,<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb20-2" data-line-number="2"><span class="kw">mean</span>(y)<span class="op">*</span><span class="dv">10</span> ; <span class="kw">sum</span>(y)</a></code></pre></div>
<pre><code>## [1] 6</code></pre>
<pre><code>## [1] 6</code></pre>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>Technically, each single trial that can result in either a success or failure is called a Bernoulli random variable—but this random variable is just a special case of the Binomial when the number of trials is 1.<a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="conditional-probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="continuous-random-variables-an-example-using-the-normal-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/01-introductionBayesCogSci.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
