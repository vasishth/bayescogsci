<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.1 Hierarchical models with Stan | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.20.2 and GitBook 2.6.7" />

  <meta property="og:title" content="12.1 Hierarchical models with Stan | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.1 Hierarchical models with Stan | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2020-09-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-complexstan.html"/>
<link rel="next" href="summary-7.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script type="text/javascript">
 $(document).ready(function() {
     $folds = $(".solution");
     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
     $folds.prepend("<button class=\"solution-btn\">Show solution</button>");  // add a button
     $(".solution-blck").toggle();  // fold all blocks
     $(".solution-btn").on("click", function() {  // add onClick event
         $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution" 
         $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
     })
 });
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i><b>0.2</b> Developing the right mindset for this book</a></li>
<li class="chapter" data-level="0.3" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.3</b> How to read this book</a></li>
<li class="chapter" data-level="0.4" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.4</b> Online materials</a></li>
<li class="chapter" data-level="0.5" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.5</b> Software needed</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="the-law-of-total-probability.html"><a href="the-law-of-total-probability.html"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-2-continuous-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#generate-simulated-bivariate-multivariate-data"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="sec-marginal.html"><a href="sec-marginal.html"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.9</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.10" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.11</b> Exercises</a><ul>
<li class="chapter" data-level="1.11.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.11.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.11.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.11.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.11.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.11.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.11.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.11.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.11.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.11.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#sec:choosepriortheta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#exercise-deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Exercise: Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Exercise: Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Exercise: Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Exercise: Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Exercise: Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Regression models</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="sec-sampling.html"><a href="sec-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec-sampling.html"><a href="sec-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using ‘Stan’: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3.9</b> Appendix</a><ul>
<li class="chapter" data-level="3.9.1" data-path="appendix.html"><a href="appendix.html#app:pp"><i class="fa fa-check"></i><b>3.9.1</b> Generating prior predictive distributions with <code>brms</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect reaction times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-the-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#sec:comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="appendix-1.html"><a href="appendix-1.html"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="appendix-1.html"><a href="appendix-1.html#sec:preprocessingpupil"><i class="fa fa-check"></i><b>4.7.1</b> Preparation of the pupil size data (section @ref(sec:pupil))</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal models–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.2.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>5.3</b> Summary</a><ul>
<li class="chapter" data-level="5.3.1" data-path="summary-2.html"><a href="summary-2.html#why-should-we-take-the-trouble-of-fitting-a-bayesian-hierarchical-model"><i class="fa fa-check"></i><b>5.3.1</b> Why should we take the trouble of fitting a Bayesian hierarchical model?</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.4</b> Further reading</a></li>
<li class="chapter" data-level="5.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a><ul>
<li class="chapter" data-level="6.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="6.1.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining hypotheses</a></li>
<li class="chapter" data-level="6.1.3" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#cell-means-parameterization-and-posterior-comparisons"><i class="fa fa-check"></i><b>6.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="6.2.1" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><i class="fa fa-check"></i><b>6.3</b> Further examples of contrasts illustrated with a factor with four levels</a><ul>
<li class="chapter" data-level="6.3.1" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.2</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="6.3.3" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.3</b> Polynomial contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="6.4.1" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html"><i class="fa fa-check"></i><b>6.5</b> Examples of contrast coding in a factorial design with two factors</a><ul>
<li class="chapter" data-level="6.5.1" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html#the-difference-between-an-anova-and-a-multiple-regression"><i class="fa fa-check"></i><b>6.5.1</b> The difference between an ANOVA and a multiple regression</a></li>
<li class="chapter" data-level="6.5.2" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html#nestedEffects"><i class="fa fa-check"></i><b>6.5.2</b> Nested effects</a></li>
<li class="chapter" data-level="6.5.3" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>6.5.3</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>7</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="7.1" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>7.1</b> Meta-analysis</a></li>
<li class="chapter" data-level="7.2" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>7.2</b> Measurement-error models</a></li>
<li class="chapter" data-level="7.3" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>7.3</b> Further reading</a></li>
<li class="chapter" data-level="7.4" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>7.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Model comparison</b></span></li>
<li class="chapter" data-level="8" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>8</b> Introduction to model comparison</a></li>
<li class="chapter" data-level="9" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>9</b> Bayes factors</a><ul>
<li class="chapter" data-level="9.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html"><i class="fa fa-check"></i><b>9.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="9.1.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#marginal-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#bayes-factor"><i class="fa fa-check"></i><b>9.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sec-N400BF.html"><a href="sec-N400BF.html"><i class="fa fa-check"></i><b>9.2</b> Testing the N400 effect using null hypothesis testing</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-N400BF.html"><a href="sec-N400BF.html#sec:BFnonnested"><i class="fa fa-check"></i><b>9.2.1</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="understanding-the-in-stability-of-bayes-factors.html"><a href="understanding-the-in-stability-of-bayes-factors.html"><i class="fa fa-check"></i><b>9.3</b> Understanding the (in-)stability of Bayes factors</a><ul>
<li class="chapter" data-level="9.3.1" data-path="understanding-the-in-stability-of-bayes-factors.html"><a href="understanding-the-in-stability-of-bayes-factors.html#instability-due-to-the-number-of-iterations-of-the-posterior-sampler"><i class="fa fa-check"></i><b>9.3.1</b> Instability due to the number of iterations of the posterior sampler</a></li>
<li class="chapter" data-level="9.3.2" data-path="understanding-the-in-stability-of-bayes-factors.html"><a href="understanding-the-in-stability-of-bayes-factors.html#instability-due-to-posterior-uncertainty-and-noise-associated-with-subjects-items-and-residual-variability"><i class="fa fa-check"></i><b>9.3.2</b> Instability due to posterior uncertainty and noise associated with subjects, items, and residual variability</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>10</b> Cross validation</a><ul>
<li class="chapter" data-level="10.1" data-path="expected-log-predictive-density-of-a-model.html"><a href="expected-log-predictive-density-of-a-model.html"><i class="fa fa-check"></i><b>10.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="10.2" data-path="k-fold-and-leave-one-out-cross-validation.html"><a href="k-fold-and-leave-one-out-cross-validation.html"><i class="fa fa-check"></i><b>10.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="10.3" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html"><i class="fa fa-check"></i><b>10.3</b> Testing the N400 effect using cross-validation</a></li>
<li class="chapter" data-level="10.4" data-path="summary-5.html"><a href="summary-5.html"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="further-reading-7.html"><a href="further-reading-7.html"><i class="fa fa-check"></i><b>10.5</b> Further reading</a></li>
<li class="chapter" data-level="10.6" data-path="exercises-6.html"><a href="exercises-6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Advanced models with Stan</b></span></li>
<li class="chapter" data-level="11" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>11</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="11.1" data-path="stan-syntax.html"><a href="stan-syntax.html"><i class="fa fa-check"></i><b>11.1</b> Stan syntax</a></li>
<li class="chapter" data-level="11.2" data-path="sec-firststan.html"><a href="sec-firststan.html"><i class="fa fa-check"></i><b>11.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="11.3" data-path="sec-clozestan.html"><a href="sec-clozestan.html"><i class="fa fa-check"></i><b>11.3</b> Another simple example: Cloze probability with Stan: Binomial likelihood</a></li>
<li class="chapter" data-level="11.4" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html"><i class="fa fa-check"></i><b>11.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="11.4.1" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:pupilstan"><i class="fa fa-check"></i><b>11.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="11.4.2" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:interstan"><i class="fa fa-check"></i><b>11.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="11.4.3" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:logisticstan"><i class="fa fa-check"></i><b>11.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html"><i class="fa fa-check"></i><b>11.5</b> Model comparison in Stan</a><ul>
<li class="chapter" data-level="11.5.1" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html#bayes-factor-in-stan"><i class="fa fa-check"></i><b>11.5.1</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="11.5.2" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>11.5.2</b> Cross validation in Stan</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="summary-6.html"><a href="summary-6.html"><i class="fa fa-check"></i><b>11.6</b> Summary</a></li>
<li class="chapter" data-level="11.7" data-path="further-reading-8.html"><a href="further-reading-8.html"><i class="fa fa-check"></i><b>11.7</b> Further reading</a></li>
<li class="chapter" data-level="11.8" data-path="exercises-7.html"><a href="exercises-7.html"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>12</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="12.1" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html"><i class="fa fa-check"></i><b>12.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="12.1.1" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>12.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="12.1.2" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:uncorrstan"><i class="fa fa-check"></i><b>12.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="12.1.3" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:corrstan"><i class="fa fa-check"></i><b>12.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="12.1.4" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:crosscorrstan"><i class="fa fa-check"></i><b>12.1.4</b> By-participant and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="summary-7.html"><a href="summary-7.html"><i class="fa fa-check"></i><b>12.2</b> Summary</a></li>
<li class="chapter" data-level="12.3" data-path="further-reading-9.html"><a href="further-reading-9.html"><i class="fa fa-check"></i><b>12.3</b> Further reading</a></li>
<li class="chapter" data-level="12.4" data-path="exercises-8.html"><a href="exercises-8.html"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Computational cognitive modeling</b></span></li>
<li class="chapter" data-level="13" data-path="introduction-to-computational-cognitive-modeling.html"><a href="introduction-to-computational-cognitive-modeling.html"><i class="fa fa-check"></i><b>13</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="13.1" data-path="further-reading-10.html"><a href="further-reading-10.html"><i class="fa fa-check"></i><b>13.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>14</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="14.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>14.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="14.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>14.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="14.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>14.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html"><i class="fa fa-check"></i><b>14.2</b> Multinomial processing tree (MPT) models</a><ul>
<li class="chapter" data-level="14.2.1" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html#mpts-for-modeling-picture-naming-abilities-in-aphasia"><i class="fa fa-check"></i><b>14.2.1</b> MPTs for modeling picture naming abilities in aphasia</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="further-reading-11.html"><a href="further-reading-11.html"><i class="fa fa-check"></i><b>14.3</b> Further reading</a></li>
<li class="chapter" data-level="14.4" data-path="exercises-9.html"><a href="exercises-9.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>15</b> Mixture models</a><ul>
<li class="chapter" data-level="15.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off.html"><i class="fa fa-check"></i><b>15.1</b> A mixture model of the speed-accuracy trade-off</a><ul>
<li class="chapter" data-level="15.1.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#a-fast-guess-model-account-of-the-global-motion-detection-task"><i class="fa fa-check"></i><b>15.1.1</b> A fast guess model account of the global motion detection task</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="summary-8.html"><a href="summary-8.html"><i class="fa fa-check"></i><b>15.2</b> Summary</a></li>
<li class="chapter" data-level="15.3" data-path="further-reading-12.html"><a href="further-reading-12.html"><i class="fa fa-check"></i><b>15.3</b> Further reading</a></li>
<li class="chapter" data-level="15.4" data-path="exercises-10.html"><a href="exercises-10.html"><i class="fa fa-check"></i><b>15.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Appendix</b></span></li>
<li class="chapter" data-level="16" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>16</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hierarchical-models-with-stan" class="section level2">
<h2><span class="header-section-number">12.1</span> Hierarchical models with Stan</h2>
<p>In the following sections, we will revisit and expand on some of the examples from chapter <a href="ch-hierarchical.html#ch:hierarchical">5</a>.</p>
<div id="varying-intercept-model-with-stan" class="section level3">
<h3><span class="header-section-number">12.1.1</span> Varying intercept model with Stan</h3>
<p>Recall that in section <a href="sec-N400hierarchical.html#sec:N400hierarchical">5.1</a> we fit models to investigate the effect of Cloze probability on EEG averages in the N400 spatiotemporal time window. For our first model, we’ll make the (unfounded) assumption that only the average signal varies across participants, but all participants share the same effect of Cloze probability. This means that the likelihood incorporates the assumption that the intercept, <span class="math inline">\(\alpha\)</span>, is adjusted with the term <span class="math inline">\(u_i\)</span> for each participant.</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim Normal(\alpha + u_{subj[n]} + c\_cloze_n \cdot \beta,\sigma)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp;\sim Normal(0,10)\\
 \beta  &amp;\sim Normal(0,10)\\
 u &amp;\sim Normal(0,\tau_u)\\
 \tau_{u} &amp;\sim Normal_+(0,20) \\
 \sigma  &amp;\sim Normal_+(0,50)
 \end{aligned}
 \end{equation}\]</span></p>
<p>Here <span class="math inline">\(n\)</span> represents each observation, the <span class="math inline">\(n\)</span>th row in the data frame and <span class="math inline">\(subj[n]\)</span> is the participant that corresponds to observation <span class="math inline">\(n\)</span>. We present the mathematical notation of the likelihood with “multiple indexing”: the index of <span class="math inline">\(u\)</span> is provided by the vector <span class="math inline">\(subj\)</span>.</p>
<p>Before we discuss the Stan implementation, let’s see how the location of the normal likelihood looks like, the vector <span class="math inline">\(\mu\)</span>. There are in total 2827 observations, that means that <span class="math inline">\(\boldsymbol{\mu}=\{\mu_1,\mu_2, \ldots, \mu_{2827}\}\)</span>, and we have 37 participants which means that <span class="math inline">\(\boldsymbol{u}=\{u_1,u_2, \ldots, u_{37}\}\)</span>. The following equality shows that the use of multiple indexing allows us to have a vector of adjustments with only 37 different elements, but that has a length of 2827.</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
    \boldsymbol{\mu} &amp;=
    \begin{bmatrix}
        \mu_1 \\
        \mu_2 \\
        \ldots \\
        \mu_{101} \\
        \mu_{102} \\
        \ldots \\
        \mu_{215} \\
        \mu_{216} \\
        \mu_{217} \\
        \ldots \\
        \mu_{1000} \\
        \ldots \\
        \mu_{2827}
    \end{bmatrix}
=
    \begin{bmatrix}
        \alpha \\
        \alpha \\
        \ldots \\
        \alpha \\
        \alpha \\
        \ldots \\
        \alpha \\
        \alpha \\
        \alpha \\
        \ldots \\
        \alpha \\
        \ldots \\
        \alpha
    \end{bmatrix}
+
\begin{bmatrix}
u_{subj[1]} \\
u_{subj[2]} \\
\ldots \\
u_{subj[101]} \\
u_{subj[102]} \\
\ldots \\
u_{subj[215]} \\
u_{subj[216]} \\
u_{subj[217]} \\
\ldots \\
u_{subj[1000]} \\
\ldots \\
u_{i[2827]}
\end{bmatrix}
+
\begin{bmatrix}
ccloze_1 \\
ccloze_2 \\
\ldots \\
ccloze_{101} \\
ccloze_{102} \\
\ldots \\
ccloze_{215} \\
ccloze_{216} \\
ccloze_{217} \\
\ldots \\
ccloze_{1000} \\
\ldots \\
ccloze_{2827}
\end{bmatrix}
\circ
\begin{bmatrix}
\beta \\
\beta \\
\ldots \\
\beta \\
\beta \\
\ldots \\
\beta \\
\beta \\
\beta \\
\ldots \\
\beta \\
\ldots \\
\beta
\end{bmatrix} \\
&amp; =
\begin{bmatrix}
\alpha \\
\alpha \\
\ldots \\
\alpha \\
\alpha \\
\ldots \\
\alpha \\
\alpha \\
\alpha \\
\ldots \\
\alpha \\
\ldots \\
\alpha
\end{bmatrix}
+
\begin{bmatrix}
u_{1} \\
u_{1} \\
\ldots \\
u_{2} \\
u_{2} \\
\ldots \\
u_{3} \\
u_{3} \\
u_{3} \\
\ldots \\
u_{14} \\
\ldots \\
u_{37 }
\end{bmatrix}
+
\begin{bmatrix}
{-0.471} \\
{-0.441} \\
\ldots \\
{0.499} \\
{0.459} \\
\ldots \\
{0.529} \\
{-0.471} \\
{0.459} \\
\ldots \\
{-0.471} \\
\ldots \\
{0.499 }
\end{bmatrix}
\circ
\begin{bmatrix}
\beta \\
\beta \\
\ldots \\
\beta \\
\beta \\
\ldots \\
\beta \\
\beta \\
\beta \\
\ldots \\
\beta \\
\ldots \\
\beta
\end{bmatrix}
\end{aligned}
\end{equation}\]</span></p>
<p>In this model each subject has their own intercept adjustment <span class="math inline">\(u_i\)</span>, if <span class="math inline">\(u_i\)</span> is positive, the subject will have a more positive EEG signal than the average subject. As we discussed in <a href="sec-N400hierarchical.html#sec:uncorrelated">5.1.3</a>, since we are estimating <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(u\)</span> at the same time and we assume that the average of the <span class="math inline">\(u\)</span>’s is 0 (since it is assumed to be normally distributed with mean of 0), whatever the subjects have in common “goes” to <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(u\)</span> only “absorbs” the differences between participants through the variance component <span class="math inline">\(\tau_u\)</span>.</p>
<p>We can implement this in Stan in the following way:</p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  // The following line creates an array of integers;
  int&lt;lower = 1, upper = N_subj&gt; subj[N]; 
}
parameters {
  real&lt;lower = 0&gt; sigma;
  real&lt;lower = 0&gt;  tau_u;   
  real alpha;
  real beta;
  vector[N_subj] u;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u | 0, 20)  -
    normal_lccdf(0 | 0, 20);
  target += normal_lpdf(u | 0, tau_u);
  target += normal_lpdf(signal | alpha + u[subj] +
                        c_cloze * beta, sigma);
}
</code></pre>
<p>In the previous Stan code we use <code>int&lt;lower = 1, upper = N_subj&gt; i[N];</code> to define a one-dimensional <em>array</em> of <code>N</code> elements that contains integers (bounded between 1 and <code>N_subj</code>). As we explain in Box <a href="regression-models-in-stan.html#thm:stancontainers">11.2</a>, the difference between vectors and one-dimensional arrays is that vectors can only contain real numbers and can be used with matrix algebra functions, and arrays can contain any type but can’t be used in matrix algebra. Notice that we use <code>normal_lpdf</code> rather than <code>normal_glm_lpdf</code> since at the moment there is no efficient likelihood implementation of hierarchical generalized linear models.</p>
<p>Save the model as <code>stan_models/hierarchical1.stan</code>, load the data, and fit it.</p>
<div class="sourceCode" id="cb587"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb587-1" data-line-number="1">df_eeg_data &lt;-<span class="st"> </span><span class="kw">read_tsv</span>(<span class="st">&quot;data/public_noun_data.txt&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb587-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(lab<span class="op">==</span><span class="st">&quot;edin&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb587-3" data-line-number="3"><span class="st">  </span><span class="co"># choose only the relevant columns:</span></a>
<a class="sourceLine" id="cb587-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(subject, cloze, item, n400) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb587-5" data-line-number="5"><span class="st">  </span><span class="co"># we simplify the subjects id </span></a>
<a class="sourceLine" id="cb587-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subject =</span> <span class="kw">as.factor</span>(subject) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb587-7" data-line-number="7"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cloze=</span> cloze<span class="op">/</span><span class="dv">100</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cloze<span class="op">/</span><span class="dv">100</span>) )</a>
<a class="sourceLine" id="cb587-8" data-line-number="8"></a>
<a class="sourceLine" id="cb587-9" data-line-number="9">ls_eeg_data &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N=</span><span class="kw">nrow</span>(df_eeg_data),</a>
<a class="sourceLine" id="cb587-10" data-line-number="10">                    <span class="dt">signal =</span> df_eeg_data<span class="op">$</span>n400,</a>
<a class="sourceLine" id="cb587-11" data-line-number="11">                    <span class="dt">c_cloze =</span> df_eeg_data<span class="op">$</span>c_cloze,</a>
<a class="sourceLine" id="cb587-12" data-line-number="12">                    <span class="dt">subj =</span> df_eeg_data<span class="op">$</span>subject,</a>
<a class="sourceLine" id="cb587-13" data-line-number="13">                    <span class="dt">N_subj =</span> <span class="kw">max</span>(df_eeg_data<span class="op">$</span>subject))</a></code></pre></div>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb588-1" data-line-number="1">fit_eeg1 &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/hierarchical1.stan&#39;</span>, </a>
<a class="sourceLine" id="cb588-2" data-line-number="2">                 <span class="dt">data =</span> ls_eeg_data)</a></code></pre></div>
<div class="sourceCode" id="cb589"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb589-1" data-line-number="1"><span class="kw">print</span>(fit_eeg1, <span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<pre><code>##        mean  2.5% 97.5% n_eff Rhat
## alpha  3.63  2.80  4.45  1756    1
## beta   2.33  1.30  3.36  4729    1
## sigma 11.65 11.34 11.96  5480    1
## tau_u  2.21  1.59  3.00  2615    1</code></pre>
</div>
<div id="sec:uncorrstan" class="section level3">
<h3><span class="header-section-number">12.1.2</span> Uncorrelated varying intercept and slopes model with Stan</h3>
<p>In the following model, we relax the strong assumption that every participant will be affected equally by the manipulation. For ease of exposition, we start by assuming that the adjustments for the intercept and slope are not correlated, as we did in section <a href="sec-N400hierarchical.html#sec:uncorrelated">5.1.3</a>.</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim Normal(\alpha + u_{1,subj[n]} + c\_cloze_n \cdot (\beta+ u_{2,subj[n]}),\sigma)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp;\sim Normal(0,10)\\
 \beta  &amp;\sim Normal(0,10)\\
 u_1 &amp;\sim Normal(0,\tau_{u_1})\\
 u_2 &amp;\sim Normal(0,\tau_{u_2})\\
 \tau_{u_1} &amp;\sim Normal_+(0,20) \\
 \tau_{u_2} &amp;\sim Normal_+(0,20) \\
 \sigma  &amp;\sim Normal_+(0,50)
 \end{aligned}
 \end{equation}\]</span></p>
<p>We can implement this in Stan in the following way:</p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  int&lt;lower = 1, upper = N_subj&gt; subj[N]; 
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;   
  real alpha;
  real beta;
  matrix[N_subj, 2] u;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u[1] | 0, 20)  - 
    normal_lccdf(0 | 0, 20);
  target += normal_lpdf(tau_u[2] | 0, 20)  - 
    normal_lccdf(0 | 0, 20);
  target += normal_lpdf(u[, 1]| 0, tau_u[1]);
  target += normal_lpdf(u[, 2]| 0, tau_u[2]);
  target += normal_lpdf(signal | alpha + u[subj, 1] +
                        c_cloze .* (beta + u[subj, 2]), sigma);
}
</code></pre>
<p>In the previous model, we assign the same prior distribution to both <code>tau_u[1]</code> and <code>tau_u[2]</code>, and thus in principle we could have written the two statements in one (notice that we multiply by 2, because there are two PDF that need to be corrected for the truncation):</p>
<pre><code>  target += normal_lpdf(tau_u | 0, 20)  - 
    2 * normal_lccdf(0 | 0, 20);</code></pre>
<p>Fit the model as follows:</p>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb593-1" data-line-number="1">fit_eeg2 &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/hierarchical2.stan&#39;</span>, </a>
<a class="sourceLine" id="cb593-2" data-line-number="2">                 <span class="dt">data =</span> ls_eeg_data)</a></code></pre></div>
<pre><code>## Warning: There were 3 chains where the estimated Bayesian Fraction of Missing Information was low. See
## http://mc-stan.org/misc/warnings.html#bfmi-low</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>We see that there are warnings. As we increase the complexity and the number of parameters, the sampler has a harder time exploring the parameter space:</p>
<div class="sourceCode" id="cb598"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb598-1" data-line-number="1"><span class="kw">print</span>(fit_eeg2, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<pre><code>##           mean  2.5% 97.5% n_eff Rhat
## alpha     3.64  2.82  4.48  1379 1.00
## beta      2.36  1.14  3.59  4329 1.00
## sigma    11.64 11.33 11.95  5213 1.00
## tau_u[1]  2.22  1.55  3.02  2233 1.00
## tau_u[2]  1.62  0.30  3.46   105 1.01</code></pre>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb600-1" data-line-number="1"><span class="kw">traceplot</span>(fit_eeg2, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-332-1.svg" width="672" /></p>
<p>We see that <code>tau_u[2]</code> has a low number of effective samples (<code>n_eff</code>) and its chains are not mixing properly. This parameter is specially problematic because there is not too much information about it (every subject is providing only one data point), it’s quite small (in comparison with <code>sigma</code>), it’s bounded by zero, and there is a dependency between this parameter and <code>u[, 2]</code>. This makes the exploration of the sampler quite hard.</p>
<p>Pairs plots can be useful to uncover pathologies in the sampling, since we can visualize correlations between samples, which are in general problematic. Here we see the samples of <code>tau_u[2]</code> against some of the adjustments to the slope <code>u</code>:</p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb601-1" data-line-number="1"><span class="kw">pairs</span>(fit_eeg2, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;u[1,2]&quot;</span>, <span class="st">&quot;u[2,2]&quot;</span>, <span class="st">&quot;u[3,2]&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-333-1.svg" width="672" /></p>
<p>Compare with <code>tau_u[1]</code> plotted against the by-participant adjustments to the intercept. Here, instead of funnels we see blobs, indicating no strong correlation between the parameters:</p>
<div class="sourceCode" id="cb602"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb602-1" data-line-number="1"><span class="kw">pairs</span>(fit_eeg2, <span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&quot;tau_u[1]&quot;</span>, <span class="st">&quot;u[1,1]&quot;</span>, <span class="st">&quot;u[2,1]&quot;</span>,<span class="st">&quot;u[3,1]&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-334-1.svg" width="672" /></p>
<p>In fact, the problem that the sampler faces is more serious than what our initial plots show. Stan samples in an <em>unconstrained</em> space where all the parameters can range from minus infinity to infinity, and then transforms back the parameters to the constrained space that we specified, where, for example, a variance parameter is restricted to be positive. This means that Stan is actually sampling from an auxiliary parameter equivalent to <code>log(tau_u[2])</code> rather than from <code>tau_u[2]</code>. We can use <code>mcmc_scatter</code> to see the actual funnel:</p>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb603-1" data-line-number="1"><span class="kw">mcmc_pairs</span>(<span class="kw">as.array</span>(fit_eeg2), <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;u[1,2]&quot;</span>),</a>
<a class="sourceLine" id="cb603-2" data-line-number="2">           <span class="dt">transform =</span> <span class="kw">list</span>(<span class="st">`</span><span class="dt">tau_u[2]</span><span class="st">`</span> =<span class="st"> &quot;log&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-335-1.svg" width="672" /></p>
<p>At the neck of the funnel, <code>tau_u[2]</code> is close to zero (and <code>log(tau_u[2])</code> is a negative number) and thus the adjustment <code>u</code> is constrained to be near 0. This a problem because a step size that’s optimized to work well in the broad part of the funnel will fail to work appropriately in the neck of the funnel and vice versa; see also Neal’s funnel <span class="citation">(Neal <a href="#ref-neal2003">2003</a>)</span> and the optimization chapter of Stan’s manual (<a href="https://mc-stan.org/docs/2_24/stan-users-guide/reparameterization-section.html#ref-Neal:2003" class="uri">https://mc-stan.org/docs/2_24/stan-users-guide/reparameterization-section.html#ref-Neal:2003</a>).
There are two options, we might just remove the by-participant varying slope since it’s not giving us much information anyway, or we can alleviate this problem by re-parameterizing the model. In general, this is the trickiest and probably most annoying part of modeling. A model can be theoretically and mathematically sound, but still fail to converge. The best advice to solve this type of problems is to start small with simulated data where we know the true values of the parameters, and increase the complexity of the models gradually. Although in this example, the problem was clearly in the parametrization of <code>tau_u[2]</code>, in many cases the biggest hurdle is to identify where the problem lies. Fortunately, the issue with <code>tau_u[2]</code> is a common problem which is easy to solve by using a non-centered parametrization <span class="citation">(Papaspiliopoulos, Roberts, and Sköld <a href="#ref-papaspiliopoulos2007">2007</a>)</span>. The following Box explains the specific re-parametrization we use for the improved version of our Stan code.</p>

<div class="extra">

<div class="theorem">
<span id="thm:noncenterparam" class="theorem"><strong>Box 12.1  </strong></span><strong>A simple non-centered re-parametrization</strong>
</div>
<p>The sampler can explore the parameter space more easily if its step size is appropriate for all the parameters. This is achieved when there are no strong correlations between parameters. We want to assume the following</p>
<p><span class="math display">\[\begin{equation}
\mathbf{u}_{2} \sim Normal(0, \tau_{u_2})
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{u}_{2}\)</span> is the column vector of <span class="math inline">\(u_{i,2}\)</span>’s.</p>
<p>We can transform <span class="math inline">\(u_2\)</span> to z-scores as follows</p>
<p><span class="math display">\[\begin{equation}
\mathbf{u}_{raw2} =\frac{\mathbf{u}_{2} - 0}{\tau_{u_2}}
\end{equation}\]</span></p>
<p>where
<span class="math display">\[\begin{equation}
\mathbf{u}_{raw2} \sim Normal(0, 1)
\end{equation}\]</span></p>
<p>Now <span class="math inline">\(\mathbf{u}_{raw2}\)</span> is easier to sample because it doesn’t depend on another parameter and its scale is 1. We can derive the actual parameter we care about by doing the following</p>
<p><span class="math display">\[\begin{equation}
\mathbf{u}_{2} = \mathbf{u}_{raw2} \cdot \tau_{u_2}
\end{equation}\]</span></p>
<p>A question that might be raised here is whether using a non-centered parametrization is always a good idea. <span class="citation">Betancourt and Girolami (<a href="#ref-betancourt2013hamiltonian">2013</a>)</span> point out that the extremity of the correlation depends on the amount of data, and the efficacy of the parametrization depends on the relative strength of the data. When there is <em>enough</em> data this parametrization is unnecessary and can be harmful. However, cases where there is enough data to render this parametrization useless are also many times cases where the partial pooling of the hierarchical models isn’t needed in the first place. Although data from lab-size experiments seem to benefit from the non-centered parametrization, the jury is still our for larger datasets with thousands of participants from crowdsourcing websites.</p>
</div>

<p>The following Stan code uses the previous re-parametrization for both <code>tau_u</code>s. Although it’s not strictly necessary for <code>tau_u[1]</code>, it won’t hurt either and the code will be simpler.</p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  int&lt;lower = 1, upper = N_subj&gt; subj[N]; 
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;   
  real alpha;
  real beta;
  matrix[N_subj, 2] z_u;
}
transformed parameters {
  matrix[N_subj, 2] u;
  u[, 1] = z_u[, 1] * tau_u[1];
  u[, 2] = z_u[, 2] * tau_u[2];
 }
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u[1] | 0, 20)  - 
    normal_lccdf(0 | 0, 20);
 target += normal_lpdf(tau_u[2] | 0, 20)  - 
    normal_lccdf(0 | 0, 20);
  target += std_normal_lpdf(to_vector(z_u));
  target += normal_lpdf(signal | alpha + u[subj, 1] +
                        c_cloze .* (beta + u[subj, 2]), sigma);
}
</code></pre>
<p>By re-parametrizing the model we can also optimize it more, we can convert the matrix <code>z_u</code> into a long column vector (in column-major order) and use a single call of <code>std_normal_lpdf</code>, which implements the log PDF of a standard normal distribution, a normal distribution with location 0 and scale 1. The function <code>std_normal_lpdf</code> is more efficient than <code>normal_lpdf(... | 0, 1)</code>. Save the model as <code>stan_models/hierarchical3.stan</code> and fit it.</p>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb605-1" data-line-number="1">fit_eeg3 &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/hierarchical3.stan&#39;</span>, </a>
<a class="sourceLine" id="cb605-2" data-line-number="2">                 <span class="dt">data =</span> ls_eeg_data)</a></code></pre></div>
<div class="sourceCode" id="cb606"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb606-1" data-line-number="1"><span class="kw">print</span>(fit_eeg3, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<pre><code>##           mean  2.5% 97.5% n_eff Rhat
## alpha     3.62  2.77  4.50  1504    1
## beta      2.35  1.17  3.52  4517    1
## sigma    11.63 11.33 11.95  6381    1
## tau_u[1]  2.21  1.56  3.02  1576    1
## tau_u[2]  1.56  0.09  3.31  1089    1</code></pre>
<div class="sourceCode" id="cb608"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb608-1" data-line-number="1"><span class="kw">traceplot</span>(fit_eeg3, <span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-338-1.svg" width="672" /></p>
<p>Although the samples of <code>tau_u[2]</code> are still correlated with the adjustments for the slope, <code>u[,2]</code>, these latter parameters are not the ones explored by the model, the auxiliary parameters, <code>z_u</code>, are the relevant ones for the sampler:</p>
<div class="sourceCode" id="cb609"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb609-1" data-line-number="1"><span class="kw">mcmc_pairs</span>(<span class="kw">as.array</span>(fit_eeg3), <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;u[1,2]&quot;</span>),</a>
<a class="sourceLine" id="cb609-2" data-line-number="2">           <span class="dt">transform =</span> <span class="kw">list</span>(<span class="st">`</span><span class="dt">tau_u[2]</span><span class="st">`</span> =<span class="st"> &quot;log&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-339-1.svg" width="672" /></p>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb610-1" data-line-number="1"><span class="kw">mcmc_pairs</span>(<span class="kw">as.array</span>(fit_eeg3), <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;z_u[1,2]&quot;</span>),</a>
<a class="sourceLine" id="cb610-2" data-line-number="2">           <span class="dt">transform =</span> <span class="kw">list</span>(<span class="st">`</span><span class="dt">tau_u[2]</span><span class="st">`</span> =<span class="st"> &quot;log&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-339-2.svg" width="672" /></p>
<!-- Before examine how each subject is being affected by the manipulation as we did with `brms`, we'll fit a correlated intercepts and slopes model. -->
</div>
<div id="sec:corrstan" class="section level3">
<h3><span class="header-section-number">12.1.3</span> Correlated varying intercept varying slopes model</h3>
<p>The likelihood remains identical to the model without a correlation between group-level intercepts and slopes, but priors and hyperpriors change to reflect the potential correlation between by-subject adjustments to intercepts and slopes:</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim Normal(\alpha + u_{subj[n],0} + c\_cloze_n \cdot  (\beta + u_{subj[n],1}),\sigma)
\end{equation}\]</span></p>
<p>The correlation is indicated in the priors on the adjustments for intercept <span class="math inline">\(u_{,1}\)</span> and slope <span class="math inline">\(u_{,2}\)</span>.</p>
<ul>
<li>Priors:
<span class="math display">\[\begin{equation}
 \begin{aligned} 
 \alpha &amp; \sim Normal(0,10) \\
 \beta  &amp; \sim Normal(0,10) \\
  \sigma  &amp;\sim Normal_+(0,50)\\
  {\begin{pmatrix}
  u_{i,1} \\
  u_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right)
 \end{aligned}
 \end{equation}\]</span></li>
</ul>
<p>where <span class="math inline">\(i = \{1, .., N_{subj} \}\)</span></p>
<p><span class="math display">\[\begin{equation}
\boldsymbol{\Sigma_u} = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_2} &amp; \tau_{u_2}^2
\end{pmatrix}}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim Normal_+(0,20)\\
\tau_{u_2} &amp;\sim Normal_+(0,20)\\
\rho_u &amp;\sim LKJcorr(2) 
\end{aligned}
\end{equation}\]</span></p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  int&lt;lower = 1, upper = N_subj&gt; subj[N]; 
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;   
  real alpha;
  real beta;
  matrix[N_subj, 2] u;
  corr_matrix[2] rho_u;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u[1] | 0, 20)  - 
    normal_lccdf(0 | 0, 20);
 target += normal_lpdf(tau_u[2] | 0, 20)  - 
    normal_lccdf(0 | 0, 20);
  target += lkj_corr_lpdf(rho_u | 2);
  for(i in 1:N_subj)
    target +=  multi_normal_lpdf(u[i,] |
                                 rep_row_vector(0, 2),
                                 quad_form_diag(rho_u, tau_u));
  target += normal_lpdf(signal | alpha + u[subj, 1] +
                        c_cloze .* (beta + u[subj, 2]), sigma);
}</code></pre>
<p>In this Stan model, we use some new functions and types:</p>
<ul>
<li><code>corr_matrix[n] M;</code> defines a (square) matrix of n rows and n columns called M, symmetrical around a diagonal of ones.</li>
<li><code>rep_vector(X, n)</code> creates a vector with n columns filled with X.</li>
<li><code>quad_form_diag(M, V)</code> a <em>quadratic form</em> using the column vector V as a diagonal matrix (a matrix with all zeros except for its diagonal), this function corresponds in Stan to: <code>diag_matrix(V) * M * diag_matrix(V)</code> and in R to <code>diag(V) %*% M %*% diag(V)</code>.</li>
</ul>
<!-- diag_matrix(vector x) -->
<!-- The diagonal matrix with diagonal x -->
<p>Notice that problematic aspects of the first model presented in <a href="hierarchical-models-with-stan.html#sec:uncorrstan">12.1.2</a> (before the reparameterization), that is dependencies between parameters, are also present here. Save the model as <code>stan_models/hierarchical_corr.stan</code> and fit it as follows:</p>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb612-1" data-line-number="1">fit_eeg_corr &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/hierarchical_corr.stan&#39;</span>, </a>
<a class="sourceLine" id="cb612-2" data-line-number="2">                 <span class="dt">data =</span> ls_eeg_data)</a></code></pre></div>
<pre><code>## Warning: There were 28 divergent transitions after warmup. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
## to find out why this is a problem and how to eliminate them.</code></pre>
<pre><code>## Warning: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See
## http://mc-stan.org/misc/warnings.html#bfmi-low</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<pre><code>## Warning: The largest R-hat is NA, indicating chains have not mixed.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#r-hat</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>As we expected, there are warnings and bad mixing of the chains for <code>tau_u[2]</code></p>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb619-1" data-line-number="1"><span class="kw">print</span>(fit_eeg_corr, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<pre><code>##           mean  2.5% 97.5% n_eff Rhat
## alpha     3.66  2.84  4.49  1306 1.00
## beta      2.35  1.16  3.58  3706 1.00
## sigma    11.64 11.32 11.96  2612 1.00
## tau_u[1]  2.21  1.56  3.04  2105 1.00
## tau_u[2]  1.56  0.28  3.39    88 1.08</code></pre>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb621-1" data-line-number="1"><span class="kw">traceplot</span>(fit_eeg_corr, <span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-342-1.svg" width="672" /></p>
<p>Again, we need to reparametrize this model. The non-centered parametrization for this type of model is the so-called Cholesky factorization; the math and the intuition behind this parametrization is explained in Box <a href="hierarchical-models-with-stan.html#thm:cholesky">12.2</a>.</p>

<div class="extra">


<div class="theorem">
<span id="thm:cholesky" class="theorem"><strong>Box 12.2  </strong></span><strong>Cholesky factorization</strong>
</div>

<p>Given a (symmetric positive definite or semi-definite matrix) correlation matrix <span class="math inline">\(\boldsymbol{\rho_u}\)</span>, we can get a lower triangular matrix <span class="math inline">\(\mathbf{L_u}\)</span> such that <span class="math inline">\(\mathbf{L_u} \mathbf{L_u}^T=\boldsymbol{\rho_u}\)</span>. The matrix <span class="math inline">\(\mathbf{L_u}\)</span> is called the Cholesky factor of <span class="math inline">\(\mathbf{\rho_u}\)</span>. Intuitively, you can think of <span class="math inline">\(L_u\)</span> as the matrix equivalent of the <em>square root</em> of <span class="math inline">\(\boldsymbol{\rho_u}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\mathbf{L_u}  =
{\begin{pmatrix} 
l_{11} &amp; 0 \\ 
l_{21}  &amp; l_{22}
\end{pmatrix}}
\end{equation}\]</span></p>
<p>For a model without a correlation between adjustments for the intercept and slope, we assumed that adjustments <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span> were generated by normal distributions, but now we want those adjustments to be correlated. We can use the Cholesky factor to generate correlated random variables in the following way.</p>
<ol style="list-style-type: decimal">
<li>We generate uncorrelated vectors, <span class="math inline">\(z_{u_1}\)</span> and <span class="math inline">\(z_{u_2}\)</span>, for each vector of
adjustments <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span>, as sampled from <span class="math inline">\(Normal(0,1)\)</span>:</li>
</ol>
<p><span class="math display">\[z_{u_1} \sim Normal(0,1)\]</span>
<span class="math display">\[z_{u_2} \sim Normal(0,1)\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>By multiplying the Cholesky factor by our <span class="math inline">\(z\)</span>’s we generate a matrix that contains two row vectors of correlated variables (with standard deviation of 1).</li>
</ol>
<p><span class="math display">\[
  \mathbf{L_u}\cdot \mathbf{z_u}  =
  {\begin{pmatrix} 
  l_{11} &amp; 0 \\ 
  l_{21}  &amp; l_{22}
  \end{pmatrix}}
  {\begin{pmatrix}
  z_{u_{1,subj=1}} &amp; z_{u_{1,subj=2}} &amp; ... &amp; z_{u_{1,subj=N_{subj}}} \\
  z_{u_{2,subj=1}} &amp; z_{u_{2,subj=2}} &amp; ... &amp; z_{u_{2,subj=N_{subj}}}
  \end{pmatrix}}
  \]</span></p>
<p><span class="math display">\[
  \mathbf{L_u}\cdot \mathbf{z_u}  =
  {\begin{pmatrix}
  l_{11} \cdot z_{u_{1,1}} + 0 \cdot z_{u_{2,1}} &amp;   ... &amp; l_{11} \cdot z_{u_{1,N_{subj}}} \\
  l_{21} \cdot z_{u_{1,1}} + l_{22} \cdot z_{u_{2,1}} &amp; ... &amp; l_{11} \cdot z_{u_{1,N_{subj}}} + l_{22} \cdot z_{u_{2,N_{subj}}}
  \end{pmatrix}}
  \]</span></p>
<p>A very informal explanation of why this works is that we are making the
variable that corresponds to the slope to be a function of a scaled version of
the intercept.</p>
<ol start="3" style="list-style-type: decimal">
<li>The last step is to scale the previous matrix to the desired standard deviation. We define the diagonalized matrix <span class="math inline">\(diag\_matrix(\tau_u)\)</span> as before:</li>
</ol>
<p><span class="math display">\[
  {\begin{pmatrix} 
  \tau_{u_1} &amp; 0 \\ 
  0  &amp; \tau_{u_2}
  \end{pmatrix}}
  \]</span></p>
<p>And we pre-multiply it by the correlated variables with SD of 1 from before:</p>
<p><span class="math display">\[\mathbf{u} = diag\_matrix(\tau_u) \cdot \mathbf{L_u}\cdot \mathbf{z_u} = \]</span></p>
<p><span class="math display">\[ 
  {\begin{pmatrix} 
  \tau_{u_1} &amp; 0 \\ 
  0  &amp; \tau_{u_2}
  \end{pmatrix}}
  {\begin{pmatrix}
  l_{11} \cdot z_{u_{1,1}}  &amp; ...  \\
  l_{21} \cdot z_{u_{1,1}} + l_{22} \cdot z_{u_{2,1}} &amp; ... 
  \end{pmatrix}}
  \]</span></p>
<p><span class="math display">\[ 
  {\begin{pmatrix}
  \tau_{u_1} \cdot l_{11} \cdot z_{u_{1,1}}  &amp; \tau_{u_1} \cdot l_{11} \cdot  z_{u_{1,2}} &amp; ...  \\
  \tau_{u_2} \cdot (l_{21} \cdot z_{u_{1,1}} + l_{22} \cdot z_{u_{2,1}}) &amp; \tau_{u_2} \cdot (l_{21} \cdot  z_{u_{1,2}} + l_{22} \cdot z_{u_{2,2}}) &amp; ... 
  \end{pmatrix}}
  \]</span></p>
<p>It might be helpful to see how one would implement this in R:</p>
<ul>
<li>Let’s assume a correlation of <span class="math inline">\(0.8\)</span>.</li>
</ul>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb622-1" data-line-number="1">rho &lt;-<span class="st"> </span><span class="fl">.8</span></a>
<a class="sourceLine" id="cb622-2" data-line-number="2"><span class="co">#Correlation matrix</span></a>
<a class="sourceLine" id="cb622-3" data-line-number="3">(rho_u &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, rho, rho,<span class="dv">1</span>), <span class="dt">ncol =</span> <span class="dv">2</span>))</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  1.0  0.8
## [2,]  0.8  1.0</code></pre>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb624-1" data-line-number="1"><span class="co"># Cholesky factor:</span></a>
<a class="sourceLine" id="cb624-2" data-line-number="2"><span class="co"># (we transpose it so that it looks the same as in Stan)</span></a>
<a class="sourceLine" id="cb624-3" data-line-number="3">(L_u &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">chol</span>(rho_u))) </a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  1.0  0.0
## [2,]  0.8  0.6</code></pre>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb626-1" data-line-number="1"><span class="co"># We verify that we recover rho_u,</span></a>
<a class="sourceLine" id="cb626-2" data-line-number="2"><span class="co"># Recall that %*% indicates matrix multiplication</span></a>
<a class="sourceLine" id="cb626-3" data-line-number="3">L_u <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(L_u)</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  1.0  0.8
## [2,]  0.8  1.0</code></pre>
<p>1 <em>We generate uncorrelated z from a standard normal distribution assuming only 10 subjects.</em></p>
<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb628-1" data-line-number="1"> N_subj &lt;-<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb628-2" data-line-number="2"> (z_u1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_subj, <span class="dv">0</span>, <span class="dv">1</span>))</a></code></pre></div>
<pre><code>##  [1] -0.336 -0.347 -1.780  0.286 -1.459  0.964 -0.550 -0.294
##  [9] -0.268  0.860</code></pre>
<div class="sourceCode" id="cb630"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb630-1" data-line-number="1"> (z_u2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_subj, <span class="dv">0</span>, <span class="dv">1</span>))</a></code></pre></div>
<pre><code>##  [1] -0.2413  1.9984  0.0288 -0.1962  0.4199  1.2495 -0.0866
##  [8]  0.7659 -0.9106 -0.1701</code></pre>
<p>2 <em>Matrix of correlated parameters:</em></p>
<div class="sourceCode" id="cb632"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb632-1" data-line-number="1"><span class="co"># matrix z_u</span></a>
<a class="sourceLine" id="cb632-2" data-line-number="2"> (z_u &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(z_u1, z_u2), <span class="dt">ncol =</span> N_subj, <span class="dt">byrow =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##        [,1]   [,2]    [,3]   [,4]  [,5]  [,6]    [,7]
## [1,] -0.336 -0.347 -1.7798  0.286 -1.46 0.964 -0.5504
## [2,] -0.241  1.998  0.0288 -0.196  0.42 1.249 -0.0866
##        [,8]   [,9] [,10]
## [1,] -0.294 -0.268  0.86
## [2,]  0.766 -0.911 -0.17</code></pre>
<div class="sourceCode" id="cb634"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb634-1" data-line-number="1"> L_u <span class="op">%*%</span><span class="st"> </span>z_u</a></code></pre></div>
<pre><code>##        [,1]   [,2]  [,3]  [,4]   [,5]  [,6]   [,7]   [,8]
## [1,] -0.336 -0.347 -1.78 0.286 -1.459 0.964 -0.550 -0.294
## [2,] -0.413  0.921 -1.41 0.111 -0.915 1.521 -0.492  0.225
##        [,9] [,10]
## [1,] -0.268 0.860
## [2,] -0.760 0.586</code></pre>
<p>3 <em>We’ll use the following diagonal matrix to scale the z_u</em></p>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb636-1" data-line-number="1"> tau_u1 &lt;-<span class="st"> </span><span class="fl">.2</span></a>
<a class="sourceLine" id="cb636-2" data-line-number="2"> tau_u2 &lt;-<span class="st"> </span><span class="fl">.01</span></a>
<a class="sourceLine" id="cb636-3" data-line-number="3"> (diag_matrix_tau &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(tau_u1, <span class="dv">0</span>, <span class="dv">0</span>, tau_u2), <span class="dt">ncol =</span> <span class="dv">2</span>))</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  0.2 0.00
## [2,]  0.0 0.01</code></pre>
<p>4 <em>We finally generate the adjustments for each subject u:</em></p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb638-1" data-line-number="1"> (u &lt;-<span class="st"> </span>diag_matrix_tau <span class="op">%*%</span><span class="st"> </span>L_u <span class="op">%*%</span><span class="st"> </span>z_u)</a></code></pre></div>
<pre><code>##          [,1]     [,2]    [,3]    [,4]     [,5]   [,6]
## [1,] -0.06717 -0.06942 -0.3560 0.05728 -0.29179 0.1928
## [2,] -0.00413  0.00921 -0.0141 0.00111 -0.00915 0.0152
##          [,7]     [,8]    [,9]   [,10]
## [1,] -0.11007 -0.05876 -0.0535 0.17197
## [2,] -0.00492  0.00225 -0.0076 0.00586</code></pre>
<div class="sourceCode" id="cb640"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb640-1" data-line-number="1"> <span class="co"># We should find that the rows are correlated ~.8</span></a>
<a class="sourceLine" id="cb640-2" data-line-number="2"> <span class="kw">cor</span>(u[<span class="dv">1</span>,], u[<span class="dv">2</span>,])</a></code></pre></div>
<pre><code>## [1] 0.828</code></pre>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb642-1" data-line-number="1"> <span class="co"># We should be able to recover the tau&#39;s as well:</span></a>
<a class="sourceLine" id="cb642-2" data-line-number="2"> <span class="kw">sd</span>(u[<span class="dv">1</span>,])</a></code></pre></div>
<pre><code>## [1] 0.175</code></pre>
<div class="sourceCode" id="cb644"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb644-1" data-line-number="1"> <span class="kw">sd</span>(u[<span class="dv">2</span>,])</a></code></pre></div>
<pre><code>## [1] 0.00903</code></pre>

<div/>

<p>The reparameterization of the model which allows for a correlation between between adjustments for the intercepts and slopes is the following:</p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  int&lt;lower = 1, upper = N_subj&gt; subj[N]; 
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;   
  real alpha;
  real beta;
  matrix[2, N_subj] z_u;
  cholesky_factor_corr[2] L_u;
}
transformed parameters {
  matrix[N_subj, 2] u;
  u = (diag_pre_multiply(tau_u, L_u) * z_u)&#39;;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u[1] | 0, 20)  - 
    normal_lccdf(0 | 0, 20);
 target += normal_lpdf(tau_u[2] | 0, 20)  - 
    normal_lccdf(0 | 0, 20);
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += std_normal_lpdf(to_vector(z_u));
  target += normal_lpdf(signal | alpha + u[subj, 1] +
                        c_cloze .* (beta + u[subj, 2]), sigma);
}
generated quantities {
  corr_matrix[2] rho_u= L_u * L_u&#39;;
  vector[N_subj] effect_by_subj;
  for(i in 1:N_subj)
    effect_by_subj[i] = beta + u[i,2];
}</code></pre>
<p>In this Stan model, we also created a <code>effect_by_subject</code> in the generated quantities. This would allow us to plot or to summarize by-subject effects of cloze probability.</p>
<p>The code implements the following new types and functions:</p>
<ul>
<li><code>cholesky_factor_corr[2] L_u</code>, which defines <code>L_u</code> as a lower triangular (<span class="math inline">\(2 \times 2\)</span>)
matrix which has to be the Cholesky factor of a correlation.</li>
<li><code>diag_pre_multiply(tau_u,L_u)</code> which makes a diagonal matrix out of
the vector <code>tau_u</code> and multiplies it by <code>L_u</code>.</li>
<li><code>to_vector(z_u)</code> makes a long vector out the matrix <code>z_u</code></li>
<li><code>L_u ~ lkj_corr_cholesky(2);</code> is the Cholesky factor associated with the lkj
correlation distribution, such that it implies that <code>L_u * L_u' ~ lkj_corr(2.0);</code>. Notice that <code>'</code> indicates transposition (while it is <code>t(.)</code> in R).</li>
</ul>
<p>We can recover the correlation by adding in the <code>generated quantities</code> section a <span class="math inline">\(2 \times 2\)</span> matrix <code>rho_u</code>, defined as <code>rho_u = L_u * L_u';</code>.</p>
<p>Fit the new model:</p>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb647-1" data-line-number="1">fit_eeg_corr2 &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/hierarchical_corr2.stan&#39;</span>, </a>
<a class="sourceLine" id="cb647-2" data-line-number="2">                 <span class="dt">data =</span> ls_eeg_data)</a></code></pre></div>
<p>Unfortunately <code>rstan</code> version 2.21.1 throws spurious warnings here. This is because the Cholesky matrix has elements which are always zero or one, and thus the variance within and between changes, and thus Rhat, is not defined.<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> However, the parameters of the model have an appropriate number of effective sample size, Rhat, and the chains are mixing well.</p>
<div class="sourceCode" id="cb648"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb648-1" data-line-number="1"><span class="kw">print</span>(fit_eeg_corr2, <span class="dt">pars =</span></a>
<a class="sourceLine" id="cb648-2" data-line-number="2">                       <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;rho_u&quot;</span>, <span class="st">&quot;L_u&quot;</span>))</a></code></pre></div>
<pre><code>##             mean  2.5% 97.5% n_eff Rhat
## alpha       3.66  2.81  4.53  1228    1
## beta        2.35  1.17  3.55  4838    1
## sigma      11.63 11.33 11.95  6284    1
## tau_u[1]    2.24  1.60  3.08  1383    1
## tau_u[2]    1.48  0.07  3.33  1025    1
## rho_u[1,1]  1.00  1.00  1.00   NaN  NaN
## rho_u[1,2]  0.19 -0.56  0.79  3512    1
## rho_u[2,1]  0.19 -0.56  0.79  3512    1
## rho_u[2,2]  1.00  1.00  1.00  1425    1
## L_u[1,1]    1.00  1.00  1.00   NaN  NaN
## L_u[1,2]    0.00  0.00  0.00   NaN  NaN
## L_u[2,1]    0.19 -0.56  0.79  3512    1
## L_u[2,2]    0.91  0.58  1.00  2453    1</code></pre>
<div class="sourceCode" id="cb650"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb650-1" data-line-number="1"><span class="kw">traceplot</span>(fit_eeg_corr2, <span class="dt">pars =</span></a>
<a class="sourceLine" id="cb650-2" data-line-number="2">                           <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;L_u&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-351-1.svg" width="672" /></p>
<p>Is there a correlation between the by-participant intercept and slope?</p>
<p>Let’s visualize some of the posteriors:</p>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb651-1" data-line-number="1"><span class="kw">mcmc_hist</span>(<span class="kw">as.data.frame</span>(fit_eeg_corr2),</a>
<a class="sourceLine" id="cb651-2" data-line-number="2">          <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;beta&quot;</span>, <span class="st">&quot;rho_u[1,2]&quot;</span>))</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-352-1.svg" width="672" /></p>
<p>We can’t really know if they are correlated, it’s clear that we don’t have enough data, the correlation might well be between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>.</p>
</div>
<div id="sec:crosscorrstan" class="section level3">
<h3><span class="header-section-number">12.1.4</span> By-participant and by-items correlated varying intercept varying slopes model</h3>
<p>We extend the previous model by adding by-items intercepts and slopes, and priors and hyperpriors that reflect the potential correlation between by-items adjustments to intercepts and slopes:</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim Normal(\alpha + u_{subj[n], 1} + w_{item[n], 2} + c\_cloze_n \cdot  (\beta + u_{subj[n],2} + w_{item[n], 2}),\sigma)
\end{equation}\]</span></p>
<p>The correlation is indicated in the priors on the adjustments for intercept <span class="math inline">\(u_{,1}\)</span> and slopes <span class="math inline">\(u_{,2}\)</span>.</p>
<ul>
<li>Priors:
<span class="math display">\[\begin{equation}
 \begin{aligned} 
 \alpha &amp; \sim Normal(0,10) \\
 \beta  &amp; \sim Normal(0,10) \\
  \sigma  &amp;\sim Normal_+(0, 50)\\
  {\begin{pmatrix}
  u_{i,1} \\
  u_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right) \\
   {\begin{pmatrix}
  w_{i,1} \\
  w_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_w} \right)
 \end{aligned}
 \end{equation}\]</span></li>
</ul>
<p><span class="math display">\[\begin{equation}
\boldsymbol{\Sigma_u} = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_1} &amp; \tau_{u_2}^2
\end{pmatrix}}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim Normal_+(0,20)\\
\tau_{u_2} &amp;\sim Normal_+(0,20)\\
\rho_u &amp;\sim LKJcorr(2) 
\end{aligned}
\end{equation}\]</span></p>
<p>The translation to Stan looks as follows:</p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  int&lt;lower = 1&gt; N_item;
  vector[N] c_cloze;
  int&lt;lower = 1, upper = N_subj&gt; subj[N]; 
  int&lt;lower = 1, upper = N_item&gt; item[N]; 
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;   
  vector&lt;lower = 0&gt;[2]  tau_w;   
  real alpha;
  real beta;
  matrix[2, N_subj] z_u;
  matrix[2, N_item] z_w;
  cholesky_factor_corr[2] L_u;
  cholesky_factor_corr[2] L_w;
}
transformed parameters {
  matrix[N_subj, 2] u;
  matrix[N_item, 2] w;
  u = (diag_pre_multiply(tau_u, L_u) * z_u)&#39;;
  w = (diag_pre_multiply(tau_w, L_w) * z_w)&#39;;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u | 0, 20) -
    2 * normal_lccdf(0 | 0, 20);
 target += normal_lpdf(tau_w | 0, 20) -
    2* normal_lccdf(0 | 0, 20);
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += lkj_corr_cholesky_lpdf(L_w | 2);
  target += std_normal_lpdf(to_vector(z_u));
  target += std_normal_lpdf(to_vector(z_w));
  target += normal_lpdf(signal | alpha + u[subj, 1] + w[item, 1]+
                        c_cloze .* (beta + u[subj, 2] + w[item, 2]), sigma);
}
generated quantities {
  corr_matrix[2] rho_u = L_u * L_u&#39;;
  corr_matrix[2] rho_w = L_w * L_w&#39;;
}</code></pre>
<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb653-1" data-line-number="1">df_eeg_data &lt;-<span class="st"> </span>df_eeg_data <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb653-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">item =</span> <span class="kw">as.numeric</span>(<span class="kw">as.factor</span>(item)))</a>
<a class="sourceLine" id="cb653-3" data-line-number="3"></a>
<a class="sourceLine" id="cb653-4" data-line-number="4">ls_eeg_data &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> <span class="kw">nrow</span>(df_eeg_data),</a>
<a class="sourceLine" id="cb653-5" data-line-number="5">                    <span class="dt">signal =</span> df_eeg_data<span class="op">$</span>n400,</a>
<a class="sourceLine" id="cb653-6" data-line-number="6">                    <span class="dt">c_cloze =</span> df_eeg_data<span class="op">$</span>c_cloze,</a>
<a class="sourceLine" id="cb653-7" data-line-number="7">                    <span class="dt">subj =</span> df_eeg_data<span class="op">$</span>subject,</a>
<a class="sourceLine" id="cb653-8" data-line-number="8">                    <span class="dt">item =</span> df_eeg_data<span class="op">$</span>item,</a>
<a class="sourceLine" id="cb653-9" data-line-number="9">                    <span class="dt">N_subj =</span> <span class="kw">max</span>(df_eeg_data<span class="op">$</span>subject),</a>
<a class="sourceLine" id="cb653-10" data-line-number="10">                    <span class="dt">N_item =</span>  <span class="kw">max</span>(df_eeg_data<span class="op">$</span>item))</a></code></pre></div>
<div class="sourceCode" id="cb654"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb654-1" data-line-number="1">fit_eeg_corr_by &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/hierarchical_corr_by.stan&#39;</span>, </a>
<a class="sourceLine" id="cb654-2" data-line-number="2">                 <span class="dt">data =</span> ls_eeg_data)</a></code></pre></div>
<div class="sourceCode" id="cb655"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb655-1" data-line-number="1"><span class="kw">print</span>(fit_eeg_corr_by, <span class="dt">pars =</span></a>
<a class="sourceLine" id="cb655-2" data-line-number="2">                       <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;tau_w&quot;</span>, <span class="st">&quot;rho_u&quot;</span>, <span class="st">&quot;rho_w&quot;</span>))</a></code></pre></div>
<pre><code>##             mean  2.5% 97.5% n_eff Rhat
## alpha       3.64  2.76  4.57  1601 1.00
## beta        2.32  1.01  3.64  3490 1.00
## sigma      11.50 11.20 11.82  5209 1.00
## tau_u[1]    2.25  1.59  3.08  1604 1.00
## tau_u[2]    1.38  0.08  3.21  1162 1.00
## tau_w[1]    1.53  0.83  2.22  1199 1.00
## tau_w[2]    2.23  0.22  4.21   692 1.01
## rho_u[1,1]  1.00  1.00  1.00   NaN  NaN
## rho_u[1,2]  0.14 -0.62  0.78  3666 1.00
## rho_u[2,1]  0.14 -0.62  0.78  3666 1.00
## rho_u[2,2]  1.00  1.00  1.00   258 1.00
## rho_w[1,1]  1.00  1.00  1.00   NaN  NaN
## rho_w[1,2] -0.40 -0.88  0.33  1926 1.00
## rho_w[2,1] -0.40 -0.88  0.33  1926 1.00
## rho_w[2,2]  1.00  1.00  1.00  1636 1.00</code></pre>
<!-- A hierarchical log-normal model: The Stroop effect + add interaction here -->
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-betancourt2013hamiltonian">
<p>Betancourt, M. J., and Mark Girolami. 2013. “Hamiltonian Monte Carlo for Hierarchical Models.”</p>
</div>
<div id="ref-neal2003">
<p>Neal, Radford M. 2003. “Slice Sampling.” <em>Ann. Statist.</em> 31 (3). The Institute of Mathematical Statistics: 705–67. <a href="https://doi.org/10.1214/aos/1056562461" class="uri">https://doi.org/10.1214/aos/1056562461</a>.</p>
</div>
<div id="ref-papaspiliopoulos2007">
<p>Papaspiliopoulos, Omiros, Gareth O. Roberts, and Martin Sköld. 2007. “A General Framework for the Parametrization of Hierarchical Models.” <em>Statist. Sci.</em> 22 (1). The Institute of Mathematical Statistics: 59–73. <a href="https://doi.org/10.1214/088342307000000014" class="uri">https://doi.org/10.1214/088342307000000014</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="29">
<li id="fn29"><p>The lightweight interface <code>cmdstanr</code> version 0.0.0.9008 for cmdstan version 2.24.0 is not throwing this warning though.<a href="hierarchical-models-with-stan.html#fnref29" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-complexstan.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-7.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/22-complexstan.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
