<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Contrast coding | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Contrast coding | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Contrast coding | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2022-09-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-workflow.html"/>
<link rel="next" href="ch-coding2x2.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-anyway"><i class="fa fa-check"></i>Why read this book anyway?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b> Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b> Bayesian Regression Models using Stan: brms</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b> Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.6.1" data-path="ch-compbda.html"><a href="ch-compbda.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.6.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.6.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.6.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.6.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.6.3</b> Re-fitting a single subject pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.7</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.9</b> Further reading</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect response times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b> No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a><ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#model-building"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#exemplary-data-analysis"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-workflow.html"><a href="ch-workflow.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-workflow.html"><a href="ch-workflow.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a><ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-7"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-9"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-10"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a><ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b> Validation of a computed posterior distribution</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#simulation-based-calibration-revealing-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> Simulation-based calibration revealing a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-and-limitation-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues and limitation of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#a-custom-distribution-re-implementing-the-exponential-distribution-manually"><i class="fa fa-check"></i><b>12.3</b> A custom distribution: Re-implementing the exponential distribution manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-11"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Other useful models</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-12"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a><ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a><ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a><ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a><ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#summary-13"><i class="fa fa-check"></i><b>15.6</b> Summary</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.7</b> Further reading</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a><ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#issues-with-cross-validation"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a><ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-14"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a><ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.3</b> Further reading</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a><ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a><ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a><ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a><ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-contr" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 8</span> Contrast coding<a href="ch-contr.html#ch-contr" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Whenever one uses a categorical factor as a predictor in a Bayesian regression model, for example when estimating the difference in a dependent variable between two or three experimental conditions, then it is necessary to code the discrete factor levels into numeric predictor variables. This coding is termed <em>contrast coding</em>. For example, in the previous chapter (section <a href="ch-hierarchical.html#sec-stroop">5.3</a>), we coded two experimental conditions as <span class="math inline">\(-1\)</span> and <span class="math inline">\(+1\)</span>, i.e., implementing a sum contrast. Those <em>contrasts</em> are the values that we assign to predictor variables to encode specific comparisons between factor levels and to create predictor terms to estimate these comparisons in any type of regression, including Bayesian regressions.</p>
<p>Contrast coding in Bayesian models works more or less the same way as in frequentist models, and the same principles and tools can be used in both cases. This chapter will introduce contrast coding using Bayesian models. The descriptions are in large parts taken from <span class="citation">Schad et al. (<a href="#ref-schad2020capitalize">2020</a>)</span> (which is published under a CC-BY 4.0 license) and adapted for the current context.</p>
<p>Consider a situation where we want to estimate differences in a dependent variable between three factor levels. An example could be differences in response times between three levels of word class (noun, verb, adjective). We might be interested in whether word class influences response times. In frequentist statistics, one way to approach this question would be to run an ANOVA and compute an omnibus <span class="math inline">\(F\)</span>-test for whether word class explains response times. A Bayesian equivalent to the frequentist omnibus <span class="math inline">\(F\)</span>-test is Bayesian model comparison (i.e., Bayes factors), where we might compare an alternative model including word class as a predictor term with a null model lacking this predictor. We will discuss such Bayesian model comparison using Bayes factors in chapter (ch-BF). However, if based on such omnibus approaches we find support for an influence of word class on response times, it remains unclear where this effect actually comes from, i.e., whether it originated from the nouns, verbs, or adjectives. However, scientists typically have a priori expectations about which groups differ from each other. In this chapter, we will show how to estimate specific comparisons directly in a Bayesian linear model. This gives the researcher a lot of control over Bayesian analyses. Specifically, we show how planned comparisons between specific conditions (groups) or clusters of conditions, are implemented as contrasts. This is a very effective way to align expectations with the statistical model. In Bayesian models, any specific comparisons can also be computed after the model is fit. However, coding a priori expectations into contrasts for model fitting will make it much more straightforward to estimate certain comparisons between experimental conditions, and to perform Bayesian model comparisons using Bayes factors to provide evidence for or against very specific hypotheses.</p>
<p>For this and the next chapter, although knowledge of the matrix formulation of the linear model is not necessary, for a deeper understanding of contrast coding some exposure to the matrix formulation is desirable. We discuss the matrix formulation in Box <a href="ch-hierarchical.html#thm:matrixHierachicalModel">5.3</a> and in the frequentist textbook <span class="citation">(Vasishth et al. <a href="#ref-VasishthEtAlFreq2021">2021</a>)</span>.</p>
<div id="basic-concepts-illustrated-using-a-two-level-factor" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.1</span> Basic concepts illustrated using a two-level factor<a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We first consider the simplest case: suppose we want to compare the means of a dependent variable (DV) such as response times between two groups of subjects. A simulated data set is available in the package <code>bcogsci</code> as the data set <code>df_contrasts1</code>. The simulations assumed longer response times in condition <span class="math inline">\(F1\)</span> (<span class="math inline">\(\mu_1 = 0.8\)</span> sec) than <span class="math inline">\(F2\)</span> (<span class="math inline">\(\mu_2 = 0.4\)</span> sec). The data from the <span class="math inline">\(10\)</span> simulated subjects are aggregated and summary statistics are computed for the two groups.</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb366-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_contrasts1&quot;</span>)</a>
<a class="sourceLine" id="cb366-2" data-line-number="2">df_contrasts1</a></code></pre></div>
<pre><code>## # A tibble: 10 × 3
##   F        DV    id
##   &lt;fct&gt; &lt;dbl&gt; &lt;int&gt;
## 1 F1    0.636     1
## 2 F1    0.841     2
## 3 F1    0.555     3
## # … with 7 more rows</code></pre>
<pre><code>## [1] 0.6</code></pre>
<table>
<caption>
<span id="tab:cTab1Means">TABLE 8.1: </span>Summary statistics per condition for the simulated data.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Factor
</th>
<th style="text-align:right;">
N data
</th>
<th style="text-align:right;">
Est. means
</th>
<th style="text-align:right;">
Std. dev.
</th>
<th style="text-align:right;">
Std. errors
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
F1
</td>
<td style="text-align:right;">
<span class="math inline">\(5\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.8\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.2\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.1\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
F2
</td>
<td style="text-align:right;">
<span class="math inline">\(5\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.4\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.2\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.1\)</span>
</td>
</tr>
</tbody>
</table>

<div class="figure"><span style="display:block;" id="fig:cFig1Means"></span>
<img src="bookdown_files/figure-html/cFig1Means-1.svg" alt="Means and standard errors of the simulated dependent variable (e.g., response times in seconds) in two conditions \(F1\) and \(F2\)." width="384" />
<p class="caption">
FIGURE 8.1: Means and standard errors of the simulated dependent variable (e.g., response times in seconds) in two conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>.
</p>
</div>
<p>The results, displayed in Figure <a href="ch-contr.html#fig:cFig1Means">8.1</a> and shown in Table <a href="ch-contr.html#tab:cTab1Means">8.1</a>, show that the assumed true condition means are exactly realized with the simulated data. The numbers are exact because the used <code>mvrnorm()</code> function (see <code>?df_contrasts1</code>) ensures that the data are generated so that the sample mean yields the true means for each level. In real data sets, of course, the sample means will vary from experiment to experiment.</p>
<p>A simple Bayesian linear model of <code>DV</code> on <span class="math inline">\(F\)</span> yields a straightforward estimate of the difference between the group means. We use relatively uninformative priors. The estimates for the population-level effects are presented below using the function <code>fixef()</code>:</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb369-1" data-line-number="1">fit_F &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb369-2" data-line-number="2">  <span class="dt">data =</span> df_contrasts1,</a>
<a class="sourceLine" id="cb369-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb369-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb369-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb369-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb369-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb369-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb369-9" data-line-number="9">)</a></code></pre></div>
<div class="sourceCode" id="cb370"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb370-1" data-line-number="1"><span class="kw">fixef</span>(fit_F)</a></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept     0.79      0.11  0.57  1.02
## FF2          -0.39      0.16 -0.72 -0.06</code></pre>
<p>Comparing the means for each condition with the coefficients (<code>Estimates</code>) reveals that (i) the intercept (<span class="math inline">\(0.8\)</span>) is the mean for condition <span class="math inline">\(F1\)</span>, <span class="math inline">\(\hat\mu_1\)</span>; and (ii) the slope (<code>FF2</code>: <span class="math inline">\(-0.4\)</span>) is the difference between the estimated means for the two groups, <span class="math inline">\(\hat\mu_2 - \hat\mu_1\)</span> <span class="citation">(Bolker <a href="#ref-Bolker2018">2018</a>)</span>:</p>
<p><span class="math display" id="eq:betac">\[\begin{equation}
\begin{array}{lcl}
\text{Intercept} = &amp; \hat{\mu}_1 &amp; = \text{estimated mean for } F1 \\
\text{Slope (FF2)} = &amp; \hat{\mu}_2 - \hat{\mu}_1 &amp; = \text{estim. mean for } F2 - \text{estim. mean for }F1
\end{array}
\tag{8.1}
\end{equation}\]</span></p>
<p>The new information are the credible intervals for the difference between the two groups.</p>
<div id="treatmentcontrasts" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.1.1</span> Default contrast coding: Treatment contrasts<a href="ch-contr.html#treatmentcontrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How does the function <code>brm</code> arrive at these particular values for the intercept and slope? That is, why does the intercept assess the mean of condition <span class="math inline">\(F1\)</span> and how do we know the slope measures the difference in means between <span class="math inline">\(F2\)</span>-<span class="math inline">\(F1\)</span>? This result is a consequence of the default contrast coding of the factor <span class="math inline">\(F\)</span>. R assigns treatment contrasts to factors and orders their levels alphabetically. The first factor level (here: <span class="math inline">\(F1\)</span>) is coded as <span class="math inline">\(0\)</span> and the second level (here: <span class="math inline">\(F2\)</span>) is coded as <span class="math inline">\(1\)</span>. This becomes clear when we inspect the current contrast attribute of the factor using the <code>contrasts()</code> command:</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb372-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts1<span class="op">$</span>F)</a></code></pre></div>
<pre><code>##    F2
## F1  0
## F2  1</code></pre>
<p>Why does this contrast coding yield these particular regression coefficients? Let’s take a look at the regression equation.
Let <span class="math inline">\(\alpha\)</span> represent the intercept, and <span class="math inline">\(\beta_1\)</span> the slope. Then, the simple regression above expresses the belief that the expected response time <span class="math inline">\(\hat{y}\)</span> (or <span class="math inline">\(E(y)\)</span>) is a linear function of the factor <span class="math inline">\(F\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\hat{y} = \alpha + \beta_1 x
\label{eq:lm1}
\end{equation}\]</span></p>
<p>This equation is part of the likelihood in a Bayesian model.
So, if <span class="math inline">\(x = 0\)</span> (condition <span class="math inline">\(F1\)</span>), <span class="math inline">\(y\)</span> is <span class="math inline">\(\alpha + \beta_1 \cdot 0 = \alpha\)</span>; and if <span class="math inline">\(x = 1\)</span> (condition <span class="math inline">\(F2\)</span>), <span class="math inline">\(y\)</span> is <span class="math inline">\(\alpha + \beta_1 \cdot 1 = \alpha + \beta_1\)</span>.</p>
<p>Expressing the above in terms of the estimated coefficients:</p>
<p><span class="math display">\[\begin{equation}
\begin{array}{lccll}
\text{estim. value for }F1 = &amp; \hat{\mu}_1 = &amp; \hat{\alpha} = &amp; \text{Intercept} \\
\text{estim. value for }F2 = &amp; \hat{\mu}_2 = &amp; \hat{\alpha} + \hat{\beta}_1 = &amp; \text{Intercept} + \text{Slope (FF2)}
\end{array}
\label{eq:predVal}
\end{equation}\]</span></p>
<p>It is useful to think of such unstandardized regression coefficients as difference scores; they express the increase in the dependent variable <span class="math inline">\(y\)</span> associated with a change in the independent variable <span class="math inline">\(x\)</span> of <span class="math inline">\(1\)</span> unit, such as going from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> in this example. The difference between condition means is <span class="math inline">\(0.4 - 0.8 = -0.4\)</span>, which is the estimated regression coefficient <span class="math inline">\(\hat{\beta}_1\)</span>. The sign of the slope is negative because we have chosen to subtract the larger mean <span class="math inline">\(F1\)</span> score from the smaller mean <span class="math inline">\(F2\)</span> score.</p>
</div>
<div id="inverseMatrix" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.1.2</span> Defining comparisons<a href="ch-contr.html#inverseMatrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The analysis of the regression equation demonstrates that in the treatment contrast the intercept assesses the average response in the baseline condition, whereas the slope estimates the difference between condition means. However, these are just verbal descriptions of what each coefficient assesses. Is it also possible to formally write down what each coefficient assesses?</p>
<p>From the perspective of parameter estimation, the slope represents the effect of main interest, so we consider this first. The treatment contrast specifies that the slope <span class="math inline">\(\beta_1\)</span> estimates the difference in means between the two levels of the factor <span class="math inline">\(F\)</span>. This can formally be written as:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = \mu_{F2} - \mu_{F1}
\end{equation}\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = - 1 \cdot \mu_{F1} + 1 \cdot \mu_{F2}
\end{equation}\]</span></p>
<p>The <span class="math inline">\(\pm 1\)</span> weights in the parameter estimation directly express which means are compared by the treatment contrast.</p>
<p>The intercept in the treatment contrast estimates a quantity that is usually of little interest: it estimates the mean in condition <span class="math inline">\(F1\)</span>.
Formally, the parameter <span class="math inline">\(\alpha\)</span> estimates the following quantity:</p>
<p><span class="math display">\[\begin{equation}
\alpha = \mu_{F1}
\end{equation}\]</span></p>
<p>
or equivalently:</p>
<p><span class="math display">\[\begin{equation}
\alpha = 1 \cdot \mu_{F1} + 0 \cdot \mu_{F2} .
\end{equation}\]</span></p>
<p>
The fact that the intercept term formally estimates the mean of condition <span class="math inline">\(F1\)</span> is in line with our previous derivation (see equation <a href="ch-contr.html#eq:betac">(8.1)</a>).</p>
<p>In R, factor levels are ordered alphabetically and by default the first level is used as the baseline in treatment contrasts. Obviously, this default mapping will only be correct for a given data set if the levels’ alphabetical ordering matches the desired contrast coding. When it does not, it is possible to re-order the levels. Here is one way of re-ordering the levels:</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb374-1" data-line-number="1">df_contrasts1<span class="op">$</span>Fb &lt;-<span class="st"> </span><span class="kw">factor</span>(df_contrasts1<span class="op">$</span>F,</a>
<a class="sourceLine" id="cb374-2" data-line-number="2">  <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;F2&quot;</span>, <span class="st">&quot;F1&quot;</span>)</a>
<a class="sourceLine" id="cb374-3" data-line-number="3">)</a>
<a class="sourceLine" id="cb374-4" data-line-number="4"><span class="kw">contrasts</span>(df_contrasts1<span class="op">$</span>Fb)</a></code></pre></div>
<pre><code>##    F1
## F2  0
## F1  1</code></pre>
<p>
This re-ordering did not change any data associated with the factor, only one of its attributes. With this new contrast attribute a simple Bayesian model yields the following result.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb376-1" data-line-number="1">fit_Fb &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Fb,</a>
<a class="sourceLine" id="cb376-2" data-line-number="2">  <span class="dt">data =</span> df_contrasts1,</a>
<a class="sourceLine" id="cb376-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb376-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb376-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb376-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb376-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb376-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb376-9" data-line-number="9">)</a></code></pre></div>
<div class="sourceCode" id="cb377"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb377-1" data-line-number="1"><span class="kw">fixef</span>(fit_Fb)</a></code></pre></div>
<pre><code>##           Estimate Est.Error Q2.5 Q97.5
## Intercept     0.40      0.11 0.18  0.62
## FbF1          0.39      0.16 0.06  0.69</code></pre>
<p>The model now estimates different quantities. The intercept now codes the mean of condition <span class="math inline">\(F2\)</span>, and the slope measures the difference in means between <span class="math inline">\(F1\)</span> minus <span class="math inline">\(F2\)</span>. This represents an alternative coding of the treatment contrast.</p>
<p>Importantly, these model posteriors do not provide evidence for the hypothesis that the effect of factor <span class="math inline">\(F\)</span> is different from zero. If the research focus is on such hypothesis testing, Bayesian hypothesis tests can be carried out using Bayes factors, by comparing a model containing a contrast of interest with a model lacking this contrast. We will the discuss details of Bayesian hypothesis testing based on Bayes factors in chapter <a href="ch-bf.html#ch-bf">15</a>.</p>
</div>
<div id="effectcoding" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.1.3</span> Sum contrasts<a href="ch-contr.html#effectcoding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Treatment contrasts are only one of many options. It is also possible to use sum contrasts, which code one of the conditions as <span class="math inline">\(-1\)</span> and the other as <span class="math inline">\(+1\)</span>, effectively <em>centering</em> the effects at the grand mean (GM, i.e., the mean of the two group means). Here, we rescale the contrast to values of <span class="math inline">\(-0.5\)</span> and <span class="math inline">\(+0.5\)</span>, which makes the estimated treatment effect the same as for treatment coding and easier to interpret.</p>
<p>To use this contrast in a linear regression, use the <code>contrasts</code> function:</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb379-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts1<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">+0.5</span>)</a>
<a class="sourceLine" id="cb379-2" data-line-number="2">fit_mSum &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb379-3" data-line-number="3">  <span class="dt">data =</span> df_contrasts1,</a>
<a class="sourceLine" id="cb379-4" data-line-number="4">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb379-5" data-line-number="5">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb379-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb379-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb379-8" data-line-number="8">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb379-9" data-line-number="9">  )</a>
<a class="sourceLine" id="cb379-10" data-line-number="10">)</a></code></pre></div>
<div class="sourceCode" id="cb380"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb380-1" data-line-number="1"><span class="kw">fixef</span>(fit_mSum)</a></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept     0.60      0.08  0.44  0.76
## F1           -0.38      0.16 -0.68 -0.05</code></pre>
<p>Here, the slope (<span class="math inline">\(F1\)</span>) again codes the difference of the groups associated with the first and second factor levels. It has the same value as in the treatment contrast.
However, the intercept now represents the estimate of the average of condition means for <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>, that is, the grand mean. This differs from the treatment contrast. For the scaled sum contrast:</p>
<p><span class="math display">\[\begin{equation}
\begin{array}{lcl}
\text{Intercept} = &amp; (\hat{\mu}_1 + \hat{\mu}_2)/2 &amp; = \text{estimated mean of }F1 \text{ and }F2 \\
\text{Slope (F1)} = &amp; \hat{\mu}_2 - \hat{\mu}_1 &amp; = \text{estim. mean for }F2 - \text{estim. mean for} F1 
\end{array}
\label{eq:beta2}
\end{equation}\]</span></p>
<p>Why does the intercept assess the grand mean and why does the slope estimate the group difference? This is the result of rescaling the sum contrast. The first factor level (<span class="math inline">\(F1\)</span>) was coded as <span class="math inline">\(-0.5\)</span>, and the second factor level (<span class="math inline">\(F2\)</span>) as <span class="math inline">\(+0.5\)</span>:</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb382-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts1<span class="op">$</span>F)</a></code></pre></div>
<pre><code>##    [,1]
## F1 -0.5
## F2  0.5</code></pre>
<p>Let’s again look at the regression equation to better understand what computations are performed. Again, <span class="math inline">\(\alpha\)</span> represents the intercept, <span class="math inline">\(\beta_1\)</span> represents the slope, and the predictor variable <span class="math inline">\(x\)</span> represents the factor <span class="math inline">\(F\)</span>. The regression equation is written as:</p>
<p><span class="math display">\[\begin{equation}
\hat{y} = \alpha + \beta_1 x
\label{eq:lm2}
\end{equation}\]</span></p>
<p>The group of <span class="math inline">\(F1\)</span> subjects is then coded as <span class="math inline">\(-0.5\)</span>, and the response time for the group of <span class="math inline">\(F1\)</span> subjects is <span class="math inline">\(\alpha + \beta_1 \cdot x_1 = 0.6 + (-0.4) \cdot (-0.5) = 0.8\)</span>. By contrast, the <span class="math inline">\(F2\)</span> group is coded as <span class="math inline">\(+0.5\)</span>. By implication, the mean of the <span class="math inline">\(F2\)</span> group must be <span class="math inline">\(\alpha + \beta_1 \cdot x_1 = 0.6 + (-0.4) \cdot 0.5 = 0.4\)</span>.
Expressed in terms of the estimated coefficients:</p>
<p><span class="math display">\[\begin{equation}
\begin{array}{lccll}
\text{estim. value for }F1 = &amp; \hat{\mu}_1 = &amp; \hat{\alpha} - 0.5 \cdot \hat{\beta}_1 = &amp; \text{Intercept} - 0.5 \cdot \text{Slope (F1)}\\
\text{estim. value for }F2 = &amp; \hat{\mu}_2 = &amp; \hat{\alpha} + 0.5 \cdot \hat{\beta}_1 = &amp; \text{Intercept} + 0.5 \cdot \text{Slope (F1)}
\end{array}
\label{eq:predVal2}
\end{equation}\]</span></p>
<p>The unstandardized regression coefficient is a difference score: Taking a step of one unit on the predictor variable <span class="math inline">\(x\)</span>, e.g., from <span class="math inline">\(-0.5\)</span> to <span class="math inline">\(+0.5\)</span>, reflecting a step from condition <span class="math inline">\(F1\)</span> to <span class="math inline">\(F2\)</span>, changes the dependent variable from <span class="math inline">\(0.8\)</span> (for condition <span class="math inline">\(F1\)</span>) to <span class="math inline">\(0.4\)</span> (condition <span class="math inline">\(F2\)</span>). This reflects a difference of <span class="math inline">\(0.4 - 0.8 = -0.4\)</span>; this is again the estimated regression coefficient <span class="math inline">\(\hat{\beta}_1\)</span>.
Moreover, as mentioned above, the intercept now assesses the grand mean of conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>: it is in the middle between condition means for <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>.</p>
<p>So far we gave verbal statements about what is estimated by the intercept and the slope in the case of the scaled sum contrast. It is possible to write these statements as formal parameter estimates.
In sum contrasts, the slope parameter <span class="math inline">\(\beta_1\)</span> assesses the following quantity:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = -1 \cdot \mu_{F1} + 1 \cdot \mu_{F2}
\end{equation}\]</span></p>
<p>
This estimates the same quantity as the slope in the treatment contrast.
The intercept, however, now assesses a different quantity: it estimates the average of the two conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\alpha = 1/2 \cdot \mu_{F1} + 1/2 \cdot \mu_{F2} = \frac{\mu_{F1} + \mu_{F2}}{2}
\end{equation}\]</span></p>
<p>
In balanced data, i.e., in data sets where there are no missing data points, the average of the two conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span> is the grand mean. In unbalanced data sets, where there are missing values, this average is the weighted grand mean.
To illustrate this point, consider an example with fully balanced data and two equal group sizes of <span class="math inline">\(5\)</span> subjects for each group <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>. Here, the grand mean is also the mean across all subjects. Next, consider a highly simplified unbalanced data set, where in condition <span class="math inline">\(F1\)</span> two observations of the dependent variable are available with values of <span class="math inline">\(2\)</span> and <span class="math inline">\(3\)</span>, and where in condition <span class="math inline">\(F2\)</span> only one observation of the dependent variable is available with a value of <span class="math inline">\(4\)</span>. In this data set, the mean across all subjects is <span class="math inline">\(\frac{2 + 3 + 4}{3} = \frac{9}{3} = 3\)</span>. However, the (weighted) grand mean as assessed in the intercept in a model using sum contrasts for factor <span class="math inline">\(F\)</span> would first compute the mean for each group separately (i.e., <span class="math inline">\(\frac{2 + 3}{2} = 2.5\)</span>, and <span class="math inline">\(4\)</span>), and then compute the mean across conditions <span class="math inline">\(\frac{2.5 + 4}{2} = \frac{6.5}{2} = 3.25\)</span>. The grand mean of <span class="math inline">\(3.25\)</span> is different from the mean across subjects of <span class="math inline">\(3\)</span>.</p>
<p>To summarize, treatment contrasts and sum contrasts are two possible ways to parameterize the difference between two groups; they generally estimate different quantities. Treatment contrasts compare one or more means against a baseline condition, whereas sum contrasts compare a condition’s mean to the grand mean (which in the two-group case also implies estimating the difference between the two group means). One question that comes up here is: how does one know or how can one formally derive what quantities are estimated by a given set of contrasts? (In the context of Bayes factors, the question would be: what hypothesis test does the contrast coding encode?) This question will be discussed in detail below for the general case of any arbitrary contrast coding.</p>
</div>
<div id="sec-cellMeans" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.1.4</span> Cell means parameterization and posterior comparisons<a href="ch-contr.html#sec-cellMeans" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One alternative option is to use what is called the cell means parameterization (this coding is also called <code>one-hot encoding</code> in the context of machine learning). In this approach, one does not estimate an intercept term, and then differences between factor levels. Instead, each free parameter is used to simply estimate the mean of one of the factor levels. As a consequence, no comparisons between condition means are estimated, but simply the mean of each experimental condition is estimated. Cell means parameterization is specified by explicitly removing the intercept term (which is added automatically in <code>brms</code>) by adding a <span class="math inline">\(-1\)</span> in the regression formula:</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb384-1" data-line-number="1">fit_mCM &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb384-2" data-line-number="2">  <span class="dt">data =</span> df_contrasts1,</a>
<a class="sourceLine" id="cb384-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb384-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb384-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb384-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb384-7" data-line-number="7">  )</a>
<a class="sourceLine" id="cb384-8" data-line-number="8">)</a></code></pre></div>
<div class="sourceCode" id="cb385"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb385-1" data-line-number="1"><span class="kw">fixef</span>(fit_mCM)</a></code></pre></div>
<pre><code>##     Estimate Est.Error Q2.5 Q97.5
## FF1     0.79      0.11 0.57  1.02
## FF2     0.40      0.11 0.18  0.63</code></pre>
<p>Now, the regression coefficients (see the column labeled <code>Estimate</code>) estimate the mean of the first factor level (<span class="math inline">\(0.8\)</span>) and the mean of the second factor level (<span class="math inline">\(0.4\)</span>). This cell means parameterization usually does not allow us to make inferences about the hypotheses of interest using Bayes factors, as these hypotheses usually relate to differences between conditions rather than to whether each condition differs from zero.
However, the cell means parameterization provides a good example demonstrating an advantage of Bayesian data analysis: In Bayesian models, it is possible to use the posterior samples to compute new estimates that were not directly contained in the fitted model. To implement this, we first extract the posterior samples from the <code>brm</code> model object:</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb387-1" data-line-number="1">df_postSamp &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_mCM)</a></code></pre></div>
<p>In a second step, we can then compute comparisons from these posterior samples. For example, we can compute the difference between conditions <span class="math inline">\(F2\)</span> and <span class="math inline">\(F1\)</span>. To do so, we simply take the posterior samples for each condition, and compute their difference.</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb388-1" data-line-number="1">df_postSamp<span class="op">$</span>b_dif &lt;-<span class="st"> </span>df_postSamp<span class="op">$</span>b_FF2 <span class="op">-</span><span class="st"> </span>df_postSamp<span class="op">$</span>b_FF1</a></code></pre></div>
<p>This provides a posterior sample of the difference between conditions. It is possible to investigate this posterior sample by looking at its mean and 95% credibility intervals:</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb389-1" data-line-number="1"><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb389-2" data-line-number="2">  <span class="dt">Estimate =</span> <span class="kw">mean</span>(df_postSamp<span class="op">$</span>b_dif),</a>
<a class="sourceLine" id="cb389-3" data-line-number="3">  <span class="kw">quantile</span>(df_postSamp<span class="op">$</span>b_dif, <span class="dt">p =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</a>
<a class="sourceLine" id="cb389-4" data-line-number="4">)</a></code></pre></div>
<pre><code>## Estimate     2.5%    97.5% 
##   -0.392   -0.708   -0.075</code></pre>
<p>Interestingly, this provides the same estimate of roughly <span class="math inline">\(-0.4\)</span> as we obtained previously when using the treatment contrast or the scaled sum contrasts.
Thus, Bayesian models provide a lot of flexibility in computing new comparisons post-hoc from the posterior samples and in obtaining their posterior distributions. However, what these posterior computations do not provide directly are inferences on null hypotheses, i.e., they do not allow us to make inference on whether a given contrast is best explained by a null model assuming no difference, or by an alternative model assuming a difference between conditions.</p>
</div>
</div>
<div id="the-hypothesis-matrix-illustrated-with-a-three-level-factor" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.2</span> The hypothesis matrix illustrated with a three-level factor<a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider an example with the three word classes nouns, verbs, and adjectives. We load simulated data from a lexical decision task with response times as dependent variable. The research question is: do response times differ as a function of the between-subject factor word class with three levels: nouns, verbs, and adjectives? Here, just to illustrate the case of a three-level factor, we make the arbitrary assumption that nouns have longest response times and that adjectives the shortest response times. Word class is specified as a between-subject factor. In cognitive science experiments, word class will usually vary within subjects and between items. However, the within- or between-subjects status of an effect is independent of its contrast coding; we assume the manipulation to be between subjects for ease of exposition. The concepts presented here extend to repeated measures designs that are often analyzed using hierarchical Bayesian (linear mixed) models.</p>
<p>Load and display the simulated data.</p>
<div class="sourceCode" id="cb391"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb391-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_contrasts2&quot;</span>)</a>
<a class="sourceLine" id="cb391-2" data-line-number="2"><span class="kw">head</span>(df_contrasts2)</a></code></pre></div>
<pre><code>## # A tibble: 6 × 3
##   F        DV    id
##   &lt;fct&gt; &lt;int&gt; &lt;int&gt;
## 1 nouns   476     1
## 2 nouns   517     2
## 3 nouns   491     3
## # … with 3 more rows</code></pre>
<table>
<caption>
<span id="tab:cTab2Means">TABLE 8.2: </span>Summary statistics per condition for the simulated data.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Factor
</th>
<th style="text-align:right;">
N data
</th>
<th style="text-align:right;">
Est. means
</th>
<th style="text-align:right;">
Std. dev.
</th>
<th style="text-align:right;">
Std. errors
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
adjectives
</td>
<td style="text-align:right;">
<span class="math inline">\(4\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(400.2\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(19.9\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(9.9\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
nouns
</td>
<td style="text-align:right;">
<span class="math inline">\(4\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(500.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(20.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(10.0\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
verbs
</td>
<td style="text-align:right;">
<span class="math inline">\(4\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(450.2\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(20.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(10.0\)</span>
</td>
</tr>
</tbody>
</table>
<p>As shown in Table <a href="ch-contr.html#tab:cTab2Means">8.2</a>, the estimated means reflect our assumptions about the true means in the data simulation: Response times are longest for nouns and shortest for adjectives.
In the following sections, we use this data set to illustrate sum contrasts. Furthermore, we will use an additional data set to illustrate repeated, Helmert, polynomial, and custom contrasts. In practice, usually only one set of contrasts is selected when the expected pattern of means is formulated during the design of the experiment.</p>
<div id="sumcontrasts" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.1</span> Sum contrasts<a href="ch-contr.html#sumcontrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We begin with sum contrasts. Suppose that the expectation is that nouns are responded to slower and adjectives are responded to faster than the grand mean response time. Then, the research question could be: How much do nouns differ from the grand mean and how much do adjectives differ from the grand mean? And are the responses slower or faster than the grand mean? We want to estimate the following two quantities:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = \mu_1 - \frac{\mu_1+\mu_2+\mu_3}{3} = \mu_1 - GM
\end{equation}\]</span></p>
<p>
and</p>
<p><span class="math display">\[\begin{equation}
\beta_2 = \mu_2 - \frac{\mu_1+\mu_2+\mu_3}{3} = \mu_2 - GM
\end{equation}\]</span></p>
<p><span class="math inline">\(\beta_1\)</span> can also be written as:</p>
<p><span class="math display">\[\begin{align} \label{h01}
\beta_1 &amp;= \mu_1 - \frac{\mu_1+\mu_2+\mu_3}{3}\\
\Leftrightarrow \beta_1 &amp;= \frac{2}{3} \mu_1 - \frac{1}{3}\mu_2 - \frac{1}{3}\mu_3 
\end{align}\]</span></p>
<p>
Here, the weights <span class="math inline">\(2/3, -1/3, -1/3\)</span> are informative about how to combine the condition means to estimate the linear model coefficient.</p>
<p><span class="math inline">\(\beta_2\)</span> is also rewritten as:</p>
<p><span class="math display">\[\begin{align}\label{h02}
\beta_2 =&amp; \mu_2 - \frac{\mu_1+\mu_2+\mu_3}{3} \\
\Leftrightarrow \beta_2 =&amp; -\frac{1}{3}\mu_1 + \frac{2}{3} \mu_2 - \frac{1}{3} \mu_3
\end{align}\]</span></p>
<p>
Here, the weights are <span class="math inline">\(-1/3, 2/3, -1/3\)</span>, and they again indicate how to combine the condition means for estimating the regression coefficient.</p>
</div>
<div id="the-hypothesis-matrix" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.2</span> The hypothesis matrix<a href="ch-contr.html#the-hypothesis-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The weights of the condition means are not only useful to define parameter estimates. They also provide the starting step in a very powerful method which allows the researcher to generate the contrasts that are needed to estimate these comparisons in a linear model. That is, what we did so far is to explain some kinds of different contrast codings that exist and what comparions are estimated by these contrasts. That is, if a certain data set is given and the goal is to estimate certain comparisons, then the procedure would be to check whether any of the contrasts that we encountered above happen to estimate these comparisons of interest. Sometimes it suffices to use one of these existing contrasts. However, at other times, our research questions may not correspond exactly to any of the contrasts in the default set of standard contrasts provided in R. For these cases, or simply for more complex designs, it is very useful to know how contrast matrices are created. Indeed, a relatively simple procedure exists in which we write our comparisons formally, extract the weights of the condition means from the comparisons, and then automatically generate the correct contrast matrix that we need in order to estimate these comparisons in a linear model. Using this powerful method, it is not necessary to find a match to a contrast matrix provided by the family of functions in R starting with the prefix <code>contr</code>. Instead, it is possible to simply define the comparisons that one wants to estimate, and to obtain the correct contrast matrix for these in an automatic procedure. Here, for pedagogical reasons, we show some examples of how to apply this procedure in cases where the comparisons <em>do</em> correspond to some of the existing contrasts.</p>
<p>Defining a custom contrast matrix involves four steps:</p>
<ol style="list-style-type: decimal">
<li>Write down the estimated comparisons</li>
<li>Extract the weights and write them into what we will call a <em>hypothesis matrix</em> (and can also be viewed as a <em>comparision matrix</em>)</li>
<li>Apply the <em>generalized matrix inverse</em> to the hypothesis matrix to create the contrast matrix</li>
<li>Assign the contrast matrix to the factor and run the (Bayesian) model</li>
</ol>
<p>Let us apply this four-step procedure to our example of the sum contrast. The first step, writing down the estimated comparisons, is shown above. The second step involves writing down the weights that each comparison gives to condition means. The weights for the first comparison are <code>wH01=c(+2/3, -1/3, -1/3)</code>, and the weights for the second comparison are <code>wH02=c(-1/3, +2/3, -1/3)</code>.</p>
<p>Before writing these into a hypothesis matrix, we also define the estimated quantity for the intercept term. The intercept parameter estimates the mean across all conditions:</p>
<p><span class="math display">\[\begin{align}
\alpha = \frac{\mu_1 + \mu_2 + \mu_3}{3} \\
\alpha = \frac{1}{3} \mu_1 + \frac{1}{3}\mu_2 + \frac{1}{3}\mu_3
\end{align}\]</span></p>
<p>This estimate has weights of <span class="math inline">\(1/3\)</span> for all condition means.
The weights from all three model parameters that were defined are now combined and written into a matrix that we refer to as the <em>hypothesis matrix</em> (<code>Hc</code>):</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb393-1" data-line-number="1">HcSum &lt;-<span class="st"> </span><span class="kw">rbind</span>(</a>
<a class="sourceLine" id="cb393-2" data-line-number="2">  <span class="dt">cH00 =</span> <span class="kw">c</span>(<span class="dt">adjectives =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">nouns =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">verbs =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>),</a>
<a class="sourceLine" id="cb393-3" data-line-number="3">  <span class="dt">cH01 =</span> <span class="kw">c</span>(<span class="dt">adjectives =</span> <span class="op">+</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">nouns =</span> <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">verbs =</span> <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>),</a>
<a class="sourceLine" id="cb393-4" data-line-number="4">  <span class="dt">cH02 =</span> <span class="kw">c</span>(<span class="dt">adjectives =</span> <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">nouns =</span> <span class="op">+</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">verbs =</span> <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb393-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb393-6" data-line-number="6"><span class="kw">fractions</span>(<span class="kw">t</span>(HcSum))</a></code></pre></div>
<pre><code>##            cH00 cH01 cH02
## adjectives  1/3  2/3 -1/3
## nouns       1/3 -1/3  2/3
## verbs       1/3 -1/3 -1/3</code></pre>
<p>Each set of weights is first entered as a row into the matrix (command <code>rbind()</code>)<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a>. However, we then switch rows and columns of the matrix for easier readability using the command <code>t()</code> (this transposes the matrix, i.e., switches rows and columns). The command <code>fractions()</code> from <code>MASS</code> library turns the decimals into fractions to improve readability.</p>
<p>Now that the condition weights have been written into the hypothesis matrix, the third step of the procedure is implemented: a matrix operation called the <em>generalized matrix inverse</em><a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> is used to obtain the contrast matrix that is needed to estimate these comparisons in a linear model.</p>
<p>Use the function <code>ginv()</code> from the <code>MASS</code> package for this next step. Define a function <code>ginv2()</code> for nicer formatting of the output.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a></p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb395-1" data-line-number="1">ginv2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="co"># define a function to make the output nicer</span></a>
<a class="sourceLine" id="cb395-2" data-line-number="2">  <span class="kw">fractions</span>(<span class="kw">provideDimnames</span>(<span class="kw">ginv</span>(x),</a>
<a class="sourceLine" id="cb395-3" data-line-number="3">    <span class="dt">base =</span> <span class="kw">dimnames</span>(x)[<span class="dv">2</span><span class="op">:</span><span class="dv">1</span>]</a>
<a class="sourceLine" id="cb395-4" data-line-number="4">  ))</a>
<a class="sourceLine" id="cb395-5" data-line-number="5">}</a></code></pre></div>
<p>Applying the generalized inverse to the hypothesis matrix results in the new matrix <code>XcSum</code>. This is the contrast matrix <span class="math inline">\(X_c\)</span> that estimates exactly those comparisons that were specified earlier:</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb396-1" data-line-number="1">(XcSum &lt;-<span class="st"> </span><span class="kw">ginv2</span>(HcSum))</a></code></pre></div>
<pre><code>##            cH00 cH01 cH02
## adjectives  1    1    0  
## nouns       1    0    1  
## verbs       1   -1   -1</code></pre>
<p>This contrast matrix corresponds exactly to the sum contrasts described above. In the case of the sum contrast, the contrast matrix looks very different from the hypothesis matrix. The contrast matrix in sum contrasts codes with <span class="math inline">\(+1\)</span> the condition that is to be compared to the grand mean. The condition that is never compared to the grand mean is coded as <span class="math inline">\(-1\)</span>. Without knowing the relationship between the hypothesis matrix and the contrast matrix, the meaning of the coefficients is completely opaque.</p>
<p>To verify this custom-made contrast matrix, it is compared to the sum contrast matrix as generated by the <code>R</code> function <code>contr.sum()</code> in the <code>stats</code> package. The resulting contrast matrix is identical to the result when adding the intercept term, a column of ones, to the contrast matrix:</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb398-1" data-line-number="1"><span class="kw">fractions</span>(<span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">contr.sum</span>(<span class="dv">3</span>)))</a></code></pre></div>
<pre><code>##   [,1] [,2] [,3]
## 1  1    1    0  
## 2  1    0    1  
## 3  1   -1   -1</code></pre>
<p>In order to estimate model parameters, step four in our procedure involves assigning sum contrasts to the factor <span class="math inline">\(F\)</span> in our example data, and running a (Bayesian) linear model. This allows us to estimate the regression coefficients associated with each contrast. We compare these to the data shown above (Table <a href="ch-contr.html#tab:cTab2Means">8.2</a>) to test whether the regression coefficients actually correspond to the differences of condition means, as intended. To define the contrast, it is necessary to remove the intercept term, as this is automatically added by the modeling function <code>brm()</code>.</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb400-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F) &lt;-<span class="st"> </span>XcSum[, <span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]</a>
<a class="sourceLine" id="cb400-2" data-line-number="2">fit_Sum &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb400-3" data-line-number="3">  <span class="dt">data =</span> df_contrasts2,</a>
<a class="sourceLine" id="cb400-4" data-line-number="4">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb400-5" data-line-number="5">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb400-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">500</span>, <span class="dv">100</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb400-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb400-8" data-line-number="8">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb400-9" data-line-number="9">  )</a>
<a class="sourceLine" id="cb400-10" data-line-number="10">)</a></code></pre></div>
<div class="sourceCode" id="cb401"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb401-1" data-line-number="1"><span class="kw">fixef</span>(fit_Sum)</a></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    450.3      7.22 435.7 464.6
## FcH01        -49.1     10.01 -69.0 -29.0
## FcH02         49.0      9.87  28.1  68.2</code></pre>
<p>The (Bayesian) linear model regression coefficients show the grand mean response time of <span class="math inline">\(450\)</span> ms in the intercept. Remember that the first regression coefficient <code>FcH01</code> was designed to estimate the extent to which adjectives are responded to faster than the grand mean. The regression coefficient <code>FcH01</code> (<code>Estimate</code>) of <span class="math inline">\(-50\)</span> reflects the difference between adjectives (<span class="math inline">\(400\)</span> ms) and the grand mean of <span class="math inline">\(450\)</span> ms. The second estimate of interest tells us the extent to which response times for nouns differ from the grand mean. The fact that the second regression coefficient <code>FcH02</code> is close to <span class="math inline">\(50\)</span> indicates that response times for nouns (<span class="math inline">\(500\)</span> ms) slower than the grand mean of <span class="math inline">\(450\)</span> ms. Although the nouns are estimated to have <span class="math inline">\(50\)</span> ms longer reading times than the grand mean, the reading times for adjectives are <span class="math inline">\(50\)</span> ms faster than the grand mean.</p>
<p>We have now not only derived contrasts, parameter estimates, and comparisons for the sum contrast, we have also used a powerful and highly general procedure that is used to generate contrasts for many kinds of different comparisons and experimental designs.</p>
</div>
<div id="generating-contrasts-the-hypr-package" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.3</span> Generating contrasts: The <code>hypr</code> package<a href="ch-contr.html#generating-contrasts-the-hypr-package" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To work with the four-step procedure, i.e., to flexibly design contrasts to estimate specific comparisons, we have developed the R package <code>hypr</code> <span class="citation">(Rabe, Vasishth, Hohenstein, Kliegl, and Schad <a href="#ref-rabe2020hypr">2020</a><a href="#ref-rabe2020hypr">b</a>)</span>. This package allows the researcher to specify the desired comparisons, and based on these comparisons, it automatically generates contrast matrices that allow us to estimate these comparisons in linear models. The functions available in this package thus considerably simplify the implementation of the four-step procedure outlined above. Because <code>hypr</code> was originally written with the frequentist framework in mind, the comparisons are expressed as null hypotheses. In the Bayesian framework, these should be treated as comparisons between (bundles of) condition means.<br />
To illustrate the functionality of the <code>hypr</code> package, we will use the two comparisons that we had defined and analyzed in the previous section:</p>
<p><span class="math display">\[\begin{equation}
\beta_1 = \mu_1 - \frac{\mu_1+\mu_2+\mu_3}{3} = \mu_1 - GM
\end{equation}\]</span></p>
<p>
and</p>
<p><span class="math display">\[\begin{equation}
\beta_2 = \mu_2 - \frac{\mu_1+\mu_2+\mu_3}{3} = \mu_2 - GM
\end{equation}\]</span></p>
<p>These estimates are effectively comparisons between condition means or between bundles of condition means. That is, <span class="math inline">\(\mu_1\)</span> is compared to the grand mean and <span class="math inline">\(\mu_2\)</span> is compared to the GM. These two comparisons can be directly entered into R using the <code>hypr()</code> function from the <code>hypr</code> package.
To do so, we use some labels to indicate factor levels. E.g., <code>adjectives</code>, <code>nouns</code>, and <code>verbs</code> can represent factor levels <span class="math inline">\(\mu_1\)</span>, <span class="math inline">\(\mu_2\)</span>, and <span class="math inline">\(\mu_3\)</span>. The first comparison specifies that <span class="math inline">\(\mu_1\)</span> is compared to <span class="math inline">\(\frac{\mu_1+\mu_2+\mu_3}{3}\)</span>. This can be written as a formula in R: <code>adjectives ~ (adjectives + nouns + verbs)/3</code>. The second comparison is that <span class="math inline">\(\mu_2\)</span> is compared to <span class="math inline">\(\frac{\mu_1+\mu_2+\mu_3}{3}\)</span>, which can be written as <code>nouns ~ (adjectives + nouns + verbs)/3</code>.</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb403-1" data-line-number="1">HcSum &lt;-<span class="st"> </span><span class="kw">hypr</span>(</a>
<a class="sourceLine" id="cb403-2" data-line-number="2">  <span class="dt">b1 =</span> adjectives <span class="op">~</span><span class="st"> </span>(adjectives <span class="op">+</span><span class="st"> </span>nouns <span class="op">+</span><span class="st"> </span>verbs) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>,</a>
<a class="sourceLine" id="cb403-3" data-line-number="3">  <span class="dt">b2 =</span> nouns <span class="op">~</span><span class="st"> </span>(adjectives <span class="op">+</span><span class="st"> </span>nouns <span class="op">+</span><span class="st"> </span>verbs) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>,</a>
<a class="sourceLine" id="cb403-4" data-line-number="4">  <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;adjectives&quot;</span>, <span class="st">&quot;nouns&quot;</span>, <span class="st">&quot;verbs&quot;</span>)</a>
<a class="sourceLine" id="cb403-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb403-6" data-line-number="6">HcSum</a></code></pre></div>
<pre><code>## hypr object containing 2 null hypotheses:
## H0.b1: 0 = (2*adjectives - nouns - verbs)/3
## H0.b2: 0 = (2*nouns - adjectives - verbs)/3
## 
## Call:
## hypr(b1 = ~2/3 * adjectives - 1/3 * nouns - 1/3 * verbs, b2 = ~2/3 * 
##     nouns - 1/3 * adjectives - 1/3 * verbs, levels = c(&quot;adjectives&quot;, 
## &quot;nouns&quot;, &quot;verbs&quot;))
## 
## Hypothesis matrix (transposed):
##            b1   b2  
## adjectives  2/3 -1/3
## nouns      -1/3  2/3
## verbs      -1/3 -1/3
## 
## Contrast matrix:
##            b1 b2
## adjectives  1  0
## nouns       0  1
## verbs      -1 -1</code></pre>
<p>The results show that the comparisons between condition means have been re-written into a form where <span class="math inline">\(0\)</span> is coded on the left side of the equation, and the condition means together with associated weights are written on the right side of the equation. This presentation makes it easy to see the weights of the condition means to code a certain comparison. The next part of the results shows the hypothesis matrix, which contains the weights from the condition means. Thus, <code>hypr</code> takes comparisons between condition means as input, and automatically extracts the corresponding weights and encodes them into the hypothesis matrix. <code>hypr</code> moreover applies the generalized matrix inverse to obtain the contrast matrix from the hypothesis matrix. The different steps correspond exactly to the steps we had carried out manually in the preceding section. <code>hypr</code> automatically performs these steps for us. We can now extract the contrast matrix by a simple function call:</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb405-1" data-line-number="1"><span class="kw">contr.hypothesis</span>(HcSum)</a></code></pre></div>
<pre><code>##            b1 b2
## adjectives  1  0
## nouns       0  1
## verbs      -1 -1
## attr(,&quot;class&quot;)
## [1] &quot;hypr_cmat&quot; &quot;matrix&quot;    &quot;array&quot;</code></pre>
<p>We can assign this contrast to our factor as we did before.</p>
<div class="sourceCode" id="cb407"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb407-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcSum)</a></code></pre></div>
<p>Now, we could again run the same model. However, since the contrast matrix is now the same as used before, the results would also be exactly the same, and we therefore skip the model fitting for brevity.</p>
<p>The <code>hypr</code> package can be used to create contrasts for Bayesian models, where the focus lies on estimation of contrasts that code comparisons between condition means or bundles of condition means. Thus, the comparison that one specifies imply the estimation of a difference between condition means or bundles of condition means. We see this in the output of the <code>hypr()</code> function (see the first section of the results) - these formulate the comparison in a way that also illustrates the estimation of model parameters. I.e., the comparison <span class="math inline">\(\mu_1 ~ \frac{\mu_1+\mu_2+\mu_3}{3}\)</span> corresponds to a parameter estimate of <code>b1 = 2/3*m1 - 1/3*m2 - 1/3*m3</code>, where <span class="math inline">\(m1\)</span> to <span class="math inline">\(m3\)</span> are the means for each of the conditions. The resulting contrasts will then allow us to estimate the specified differences between condition means or bundles of condition means.</p>
</div>
</div>
<div id="sec-4levelFactor" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.3</span> Other types of contrasts: illustration with a factor with four levels<a href="ch-contr.html#sec-4levelFactor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here, we introduce repeated difference, Helmert, and polynomial contrasts. For these, it may be instructive to consider an experiment with one between-subject factor with four levels.
We load a corresponding data set, which contains simulated data about response times with a four-level between-subject factor.
The sample sizes for each level and the means and standard errors are shown in Table <a href="ch-contr.html#tab:cTab3Means">8.3</a>, and the means and standard errors are also shown graphically in Figure <a href="ch-contr.html#fig:helmertsimdatFig">8.2</a>.</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb408-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_contrasts3&quot;</span>)</a></code></pre></div>
<pre><code>## [1] 20</code></pre>
<div class="figure"><span style="display:block;" id="fig:helmertsimdatFig"></span>
<img src="bookdown_files/figure-html/helmertsimdatFig-1.svg" alt="Means and error bars (showing standard errors) for a simulated data set with one between-subjects factor with four levels." width="451.2" />
<p class="caption">
FIGURE 8.2: Means and error bars (showing standard errors) for a simulated data set with one between-subjects factor with four levels.
</p>
</div>
<table>
<caption>
<span id="tab:cTab3Means">TABLE 8.3: </span>Summary statistics per condition for the simulated data.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Factor
</th>
<th style="text-align:right;">
N data
</th>
<th style="text-align:right;">
Est. means
</th>
<th style="text-align:right;">
Std. dev.
</th>
<th style="text-align:right;">
Std. errors
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
F1
</td>
<td style="text-align:right;">
<span class="math inline">\(5\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(10.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(10.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(4.5\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
F2
</td>
<td style="text-align:right;">
<span class="math inline">\(5\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(20.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(10.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(4.5\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
F3
</td>
<td style="text-align:right;">
<span class="math inline">\(5\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(10.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(10.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(4.5\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
F4
</td>
<td style="text-align:right;">
<span class="math inline">\(5\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(40.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(10.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(4.5\)</span>
</td>
</tr>
</tbody>
</table>
<p>We assume that the four factor levels <span class="math inline">\(F1\)</span> to <span class="math inline">\(F4\)</span> reflect levels of word frequency, including the levels <code>low</code>, <code>medium-low</code>, <code>medium-high</code>, and <code>high</code> frequency words, and that the dependent variable reflects response time. (Qualitatively, the simulated pattern of results is actually empirically observed for word frequency effects on single fixation durations <span class="citation">(Heister, Würzner, and Kliegl <a href="#ref-heister2012analysing">2012</a>)</span>.)</p>
<div id="repeatedcontrasts" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.3.1</span> Repeated contrasts<a href="ch-contr.html#repeatedcontrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Arguably, the most popular contrast psychologists and psycholinguists are interested in is the comparison between neighboring levels of a factor. This type of contrast is called the repeated contrast. In our example, our research question might be whether the frequency level  leads to slower response times than frequency level , whether frequency level  leads to slower response times than frequency level , and whether frequency level  leads to slower response times than frequency level .</p>
<p>Repeated contrasts are used to implement these comparisons. Consider first how to derive the contrast matrix for repeated contrasts, starting out by specifying the comparisons that are to be estimated. Importantly, this again applies the general strategy of how to translate (any) comparisons between groups or conditions into a set of contrasts, yielding a powerful tool of great value in many research settings. We follow the four-step procedure outlined above.</p>
<p>The first step is to specify our comparisons, and to write them down in a way such that their weights can be extracted easily. For a four-level factor, the three comparisons are:</p>
<p><span class="math display">\[\begin{equation}
\beta_{2-1} = -1 \cdot \mu_1 + 1 \cdot \mu_2 + 0 \cdot \mu_3 + 0 \cdot \mu_4
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\beta_{3-2} = 0 \cdot \mu_1 - 1 \cdot \mu_2 + 1 \cdot \mu_3 + 0 \cdot \mu_4
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\beta_{4-3} = 0 \cdot \mu_1 + 0 \cdot \mu_2 - 1 \cdot \mu_3 + 1 \cdot \mu_4
\end{equation}\]</span></p>
<p>Here, the <span class="math inline">\(\mu_x\)</span> are the mean response times in condition <span class="math inline">\(x\)</span>. Each regression coefficient gives weights to the different condition means. For example, the first estimate (<span class="math inline">\(\beta_{2-1}\)</span>) estimates the difference between condition mean for <span class="math inline">\(F2\)</span> (<span class="math inline">\(\mu_2\)</span>) minus the condition mean for <span class="math inline">\(F1\)</span> (<span class="math inline">\(\mu_1\)</span>), but ignores condition means for <span class="math inline">\(F3\)</span> and <span class="math inline">\(F4\)</span> (<span class="math inline">\(\mu_3\)</span>, <span class="math inline">\(\mu_4\)</span>). <span class="math inline">\(\mu_1\)</span> has a weight of <span class="math inline">\(-1\)</span>, <span class="math inline">\(\mu_2\)</span> has a weight of <span class="math inline">\(+1\)</span>, and <span class="math inline">\(\mu_3\)</span> and <span class="math inline">\(\mu_4\)</span> have weights of <span class="math inline">\(0\)</span>.</p>
<p>We can write these comparisons into hypr:</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb410-1" data-line-number="1">HcRep &lt;-<span class="st"> </span><span class="kw">hypr</span>(</a>
<a class="sourceLine" id="cb410-2" data-line-number="2">  <span class="dt">c2vs1 =</span> F2 <span class="op">~</span><span class="st"> </span>F1,</a>
<a class="sourceLine" id="cb410-3" data-line-number="3">  <span class="dt">c3vs2 =</span> F3 <span class="op">~</span><span class="st"> </span>F2,</a>
<a class="sourceLine" id="cb410-4" data-line-number="4">  <span class="dt">c4vs3 =</span> F4 <span class="op">~</span><span class="st"> </span>F3,</a>
<a class="sourceLine" id="cb410-5" data-line-number="5">  <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;F1&quot;</span>, <span class="st">&quot;F2&quot;</span>, <span class="st">&quot;F3&quot;</span>, <span class="st">&quot;F4&quot;</span>)</a>
<a class="sourceLine" id="cb410-6" data-line-number="6">)</a>
<a class="sourceLine" id="cb410-7" data-line-number="7">HcRep</a></code></pre></div>
<pre><code>## hypr object containing 3 null hypotheses:
## H0.c2vs1: 0 = F2 - F1
## H0.c3vs2: 0 = F3 - F2
## H0.c4vs3: 0 = F4 - F3
## 
## Call:
## hypr(c2vs1 = ~F2 - F1, c3vs2 = ~F3 - F2, c4vs3 = ~F4 - F3, levels = c(&quot;F1&quot;, 
## &quot;F2&quot;, &quot;F3&quot;, &quot;F4&quot;))
## 
## Hypothesis matrix (transposed):
##    c2vs1 c3vs2 c4vs3
## F1 -1     0     0   
## F2  1    -1     0   
## F3  0     1    -1   
## F4  0     0     1   
## 
## Contrast matrix:
##    c2vs1 c3vs2 c4vs3
## F1 -3/4  -1/2  -1/4 
## F2  1/4  -1/2  -1/4 
## F3  1/4   1/2  -1/4 
## F4  1/4   1/2   3/4</code></pre>
<p>The hypothesis matrix shows exactly the weights that we had written down above. Moreover, we see the contrast matrix. In the case of the repeated contrast, the contrast matrix again looks very different from the hypothesis matrix. In this case, the contrast matrix looks a lot less intuitive than the hypothesis matrix, and if one did not know the associated hypothesis matrix, it seems unclear what the contrast matrix would actually test. To verify this custom-made contrast matrix, we compare it to the repeated contrast matrix as generated by the <code>R</code> function <code>contr.sdif()</code> in the  package <span class="citation">(Ripley <a href="#ref-R-MASS">2019</a>)</span>. The resulting contrast matrix is identical to our result:</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb412-1" data-line-number="1"><span class="kw">fractions</span>(<span class="kw">contr.sdif</span>(<span class="dv">4</span>))</a></code></pre></div>
<pre><code>##   2-1  3-2  4-3 
## 1 -3/4 -1/2 -1/4
## 2  1/4 -1/2 -1/4
## 3  1/4  1/2 -1/4
## 4  1/4  1/2  3/4</code></pre>
<p>We can thus use either approach (<code>hypr()</code> or <code>contr.sdif()</code>) to obtain the contrast matrix in this case.
Next, we apply the repeated contrasts to the factor <span class="math inline">\(F\)</span> in the example data and run a linear model. This allows us to estimate the regression coefficients associated with each contrast. These are compared to the data in Figure <a href="ch-contr.html#fig:helmertsimdatFig">8.2</a> to test whether the regression coefficients actually correspond to the differences between successive condition means, as intended.</p>
<div class="sourceCode" id="cb414"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb414-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcRep)</a>
<a class="sourceLine" id="cb414-2" data-line-number="2">fit_Rep &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb414-3" data-line-number="3">  <span class="dt">data =</span> df_contrasts3,</a>
<a class="sourceLine" id="cb414-4" data-line-number="4">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb414-5" data-line-number="5">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb414-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb414-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb414-8" data-line-number="8">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb414-9" data-line-number="9">  )</a>
<a class="sourceLine" id="cb414-10" data-line-number="10">)</a></code></pre></div>
<div class="sourceCode" id="cb415"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb415-1" data-line-number="1"><span class="kw">fixef</span>(fit_Rep)</a></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    19.97      2.41  14.94 24.64
## Fc2vs1        9.73      6.93  -4.11 23.43
## Fc3vs2       -9.39      6.85 -22.89  4.45
## Fc4vs3       29.28      6.78  15.64 42.36</code></pre>
<p>The results show that as expected, the regression coefficients reflect the differences that were of interest: the regression coefficient (<code>Estimate</code>) <code>Fc2vs1</code> has a value of <span class="math inline">\(10\)</span>, which exactly corresponds to the difference between the condition mean for <span class="math inline">\(F2\)</span> (<span class="math inline">\(20\)</span>) minus the condition mean for <span class="math inline">\(F1\)</span> (<span class="math inline">\(10\)</span>), i.e., <span class="math inline">\(20 - 10 = 10\)</span>. Likewise, the regression coefficient <code>Fc3vs2</code> has a value of <span class="math inline">\(-10\)</span>, which corresponds to the difference between the condition mean for <span class="math inline">\(F3\)</span> (<span class="math inline">\(10\)</span>) minus the condition mean for <span class="math inline">\(F2\)</span> (<span class="math inline">\(20\)</span>), i.e., <span class="math inline">\(10 - 20 = -10\)</span>. Finally, the regression coefficient <code>Fc4vs3</code> has a value of roughly <span class="math inline">\(30\)</span>, which reflects the difference between condition <span class="math inline">\(F4\)</span> (<span class="math inline">\(40\)</span>) minus condition <span class="math inline">\(F3\)</span> (<span class="math inline">\(10\)</span>), i.e., <span class="math inline">\(40 - 10 = 30\)</span>. Thus, the regression coefficients estimate differences between successive or neighboring condition means.</p>
</div>
<div id="helmertcontrasts" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.3.2</span> Helmert contrasts<a href="ch-contr.html#helmertcontrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another common contrast is the Helmert contrast. In a Helmert contrast for our 4-level factor, the first contrast compares level <span class="math inline">\(F1\)</span> versus <span class="math inline">\(F2\)</span>. The second contrast compares level <span class="math inline">\(F3\)</span> to the average of the first two, i.e., <code>F3 ~ (F1+F2)/2</code>. The third contrast then compares level <span class="math inline">\(F4\)</span> to the average of the first three. We can code this contrast in <code>hypr</code>:</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb417-1" data-line-number="1">HcHel &lt;-<span class="st"> </span><span class="kw">hypr</span>(</a>
<a class="sourceLine" id="cb417-2" data-line-number="2">  <span class="dt">b1 =</span> F2 <span class="op">~</span><span class="st"> </span>F1,</a>
<a class="sourceLine" id="cb417-3" data-line-number="3">  <span class="dt">b2 =</span> F3 <span class="op">~</span><span class="st"> </span>(F1 <span class="op">+</span><span class="st"> </span>F2) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb417-4" data-line-number="4">  <span class="dt">b3 =</span> F4 <span class="op">~</span><span class="st"> </span>(F1 <span class="op">+</span><span class="st"> </span>F2 <span class="op">+</span><span class="st"> </span>F3) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>,</a>
<a class="sourceLine" id="cb417-5" data-line-number="5">  <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;F1&quot;</span>, <span class="st">&quot;F2&quot;</span>, <span class="st">&quot;F3&quot;</span>, <span class="st">&quot;F4&quot;</span>)</a>
<a class="sourceLine" id="cb417-6" data-line-number="6">)</a>
<a class="sourceLine" id="cb417-7" data-line-number="7">HcHel</a></code></pre></div>
<pre><code>## hypr object containing 3 null hypotheses:
## H0.b1: 0 = F2 - F1
## H0.b2: 0 = (2*F3 - F1 - F2)/2
## H0.b3: 0 = (3*F4 - F1 - F2 - F3)/3
## 
## Call:
## hypr(b1 = ~F2 - F1, b2 = ~F3 - 1/2 * F1 - 1/2 * F2, b3 = ~F4 - 
##     1/3 * F1 - 1/3 * F2 - 1/3 * F3, levels = c(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;, 
## &quot;F4&quot;))
## 
## Hypothesis matrix (transposed):
##    b1   b2   b3  
## F1   -1 -1/2 -1/3
## F2    1 -1/2 -1/3
## F3    0    1 -1/3
## F4    0    0    1
## 
## Contrast matrix:
##    b1   b2   b3  
## F1 -1/2 -1/3 -1/4
## F2  1/2 -1/3 -1/4
## F3    0  2/3 -1/4
## F4    0    0  3/4</code></pre>
<p>The classical Helmert contrast coded by the function <code>contr.helmert()</code> yields a similar but slightly different result:</p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb419-1" data-line-number="1"><span class="kw">contr.helmert</span>(<span class="dv">4</span>)</a></code></pre></div>
<pre><code>##   [,1] [,2] [,3]
## 1   -1   -1   -1
## 2    1   -1   -1
## 3    0    2   -1
## 4    0    0    3</code></pre>
<p>These contrasts are scaled versions of our custom Helmert contrast. I.e., the first column of our custom Helmert contrast has to be multiplied by 2 to get the classical version, the second column has to be multiplied by 3, and the fourth column has to be multiplied by 4 to get to our custom Helmert contrast. <!-- In fact, the classical Helmert contrast does not directly estimate the differences as explained above, but it estimates scaled versions of these differences. This means that the estimates in the classical Helmert contrast do not directly estimate the condition differences as outlined above, but rather the scaled condition differences. --> Therefore, we suggest that our custom Helmert contrast defined using the <code>hypr</code> function is more appropriate and intuitive to use. Probably the only reason the classical Helmert contrast uses these scaled differences is that the rescaling yields an easier contrast matrix, which consists of integers rather than fractions. However, this shouldn’t be a concern to us, and the intuitive estimates from our custom Helmert contrast seem much more relevant in Bayesian approaches today.</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb421-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcHel)</a>
<a class="sourceLine" id="cb421-2" data-line-number="2">fit_Hel &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb421-3" data-line-number="3">  <span class="dt">data =</span> df_contrasts3,</a>
<a class="sourceLine" id="cb421-4" data-line-number="4">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb421-5" data-line-number="5">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb421-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb421-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb421-8" data-line-number="8">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb421-9" data-line-number="9">  )</a>
<a class="sourceLine" id="cb421-10" data-line-number="10">)</a></code></pre></div>
<div class="sourceCode" id="cb422"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb422-1" data-line-number="1"><span class="kw">fixef</span>(fit_Hel)</a></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    19.98      2.49  15.11 24.87
## Fb1           9.92      6.90  -3.85 23.62
## Fb2          -5.01      6.13 -17.19  7.07
## Fb3          26.27      5.59  15.01 37.18</code></pre>
<p>When we fit the Bayesian model using our custom Helmert contrast, we can see that the estimates reflect the comparisons outlined above. The first estimate <code>Fb1</code> has a value of roughly <span class="math inline">\(10\)</span>, reflecting the difference between conditions <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>. The second estimate <code>Fb2</code> has a value of <span class="math inline">\(5\)</span>, which reflects the difference between condition <span class="math inline">\(F3\)</span> (<span class="math inline">\(10\)</span>) and the average of the first two conditions (<span class="math inline">\((10+20)/2=15\)</span>). Last, the estimate <code>Fb3</code> reflects the difference between <span class="math inline">\(F4\)</span> (<span class="math inline">\(40\)</span>) minus the average of the first three, which is <span class="math inline">\((10+20+10)/3=13.3\)</span>, and is thus <span class="math inline">\(40-13.3=26.7\)</span>.</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:cTreatGM" class="theorem"><strong>Box 8.1  </strong></span><strong>Treatment contrast with intercept as the grand mean.</strong></p>
</div>
<p>Above, we have introduced the treatment contrast, where each contrast compares one condition to a baseline condition. We have discussed that the intercept in the treatment contrast estimates the condition mean for the baseline condition. There are some applications where this behavior may seem sub-optimal. This can be the case in experimental designs with multiple factors, where we may want to use centered contrasts (we will discuss this below). Moreover, the contrast coding of the population-level (or fixed) effects also defines what the group-level (or random) effects assess. If the intercept assesses the grand mean - rather than the baseline condition - in hierarchical models, then the population-level intercepts reflect the grand mean variance, rather than the variance in the baseline condition.</p>
<p>Interestingly, it is possible to design a treatment contrast where the intercept reflects the grand mean. We implement this using the <code>hypr</code> package. The trick is to add the intercept explicitly as a comparison of the average of all four condition means:</p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb424-1" data-line-number="1">HcTrGM &lt;-<span class="st"> </span><span class="kw">hypr</span>(</a>
<a class="sourceLine" id="cb424-2" data-line-number="2">  <span class="dt">b0 =</span> <span class="op">~</span><span class="st"> </span>(F1 <span class="op">+</span><span class="st"> </span>F2 <span class="op">+</span><span class="st"> </span>F3 <span class="op">+</span><span class="st"> </span>F4) <span class="op">/</span><span class="st"> </span><span class="dv">4</span>,</a>
<a class="sourceLine" id="cb424-3" data-line-number="3">  <span class="dt">b1 =</span> F2 <span class="op">~</span><span class="st"> </span>F1,</a>
<a class="sourceLine" id="cb424-4" data-line-number="4">  <span class="dt">b2 =</span> F3 <span class="op">~</span><span class="st"> </span>F1,</a>
<a class="sourceLine" id="cb424-5" data-line-number="5">  <span class="dt">b3 =</span> F4 <span class="op">~</span><span class="st"> </span>F1,</a>
<a class="sourceLine" id="cb424-6" data-line-number="6">  <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;F1&quot;</span>, <span class="st">&quot;F2&quot;</span>, <span class="st">&quot;F3&quot;</span>, <span class="st">&quot;F4&quot;</span>)</a>
<a class="sourceLine" id="cb424-7" data-line-number="7">)</a>
<a class="sourceLine" id="cb424-8" data-line-number="8">HcTrGM</a></code></pre></div>
<pre><code>## hypr object containing 4 null hypotheses:
## H0.b0: 0 = (F1 + F2 + F3 + F4)/4  (Intercept)
## H0.b1: 0 = F2 - F1
## H0.b2: 0 = F3 - F1
## H0.b3: 0 = F4 - F1
## 
## Call:
## hypr(b0 = ~1/4 * F1 + 1/4 * F2 + 1/4 * F3 + 1/4 * F4, b1 = ~F2 - 
##     F1, b2 = ~F3 - F1, b3 = ~F4 - F1, levels = c(&quot;F1&quot;, &quot;F2&quot;, 
## &quot;F3&quot;, &quot;F4&quot;))
## 
## Hypothesis matrix (transposed):
##    b0  b1  b2  b3 
## F1 1/4  -1  -1  -1
## F2 1/4   1   0   0
## F3 1/4   0   1   0
## F4 1/4   0   0   1
## 
## Contrast matrix:
##    b0   b1   b2   b3  
## F1    1 -1/4 -1/4 -1/4
## F2    1  3/4 -1/4 -1/4
## F3    1 -1/4  3/4 -1/4
## F4    1 -1/4 -1/4  3/4</code></pre>
<p>The hypothesis matrix now explicitly codes the intercept as the first column, where all hypothesis weights are equal and sum up to one. This is coding the intercept. The other hypothesis weights are as expected for the treatment contrast. The contrast matrix now looks very different compared to the standard treatment contrast. Next, we fit a model with this adapted treatment contrast. The function <code>contr.hypothesis</code> automatically removes the intercept that is encoded in <code>HcTrGM</code>, since this is automatically added by <code>brms</code>.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb426-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcTrGM)</a>
<a class="sourceLine" id="cb426-2" data-line-number="2">fit_TrGM &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb426-3" data-line-number="3">  <span class="dt">data =</span> df_contrasts3,</a>
<a class="sourceLine" id="cb426-4" data-line-number="4">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb426-5" data-line-number="5">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb426-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb426-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb426-8" data-line-number="8">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb426-9" data-line-number="9">  )</a>
<a class="sourceLine" id="cb426-10" data-line-number="10">)</a></code></pre></div>
<div class="sourceCode" id="cb427"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb427-1" data-line-number="1"><span class="kw">fixef</span>(fit_TrGM)</a></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    20.03      2.53  15.04  25.0
## Fb1           9.58      6.92  -4.21  23.4
## Fb2          -0.41      6.95 -14.40  12.9
## Fb3          29.46      6.94  15.60  43.4</code></pre>
<p>The results show that the coefficients reflect comparisons of each condition <span class="math inline">\(F2\)</span>, F3, and F4 to the baseline condition <span class="math inline">\(F1\)</span>. However, the intercept now captures the grand mean across all four conditions of <span class="math inline">\(20\)</span>.</p>
</div>
</div>
<div id="contrasts-in-linear-regression-analysis-the-design-or-model-matrix" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.3.3</span> Contrasts in linear regression analysis: The design or model matrix<a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have now discussed how different contrasts are created from the hypothesis matrix. However, we have not treated in detail how exactly contrasts are used in a linear model. Here, we will see that the contrasts for a factor in a linear model are just the same thing as continuous numeric predictors (i.e., covariates) in a linear/multiple regression analysis. That is, contrasts are the way to encode discrete factor levels into numeric predictor variables to use in linear/multiple regression analysis, by encoding which differences between factor levels are estimated.
The contrast matrix <span class="math inline">\(X_c\)</span> that we have looked at so far has one entry (row) for each experimental condition. For use in a linear model, however, the contrast matrix is coded into a design or model matrix <span class="math inline">\(X\)</span>, where each individual data point has one row. The design matrix <span class="math inline">\(X\)</span> can be extracted using the function <code>model.matrix()</code>:</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb429-1" data-line-number="1">(<span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcRep)) <span class="co"># contrast matrix</span></a></code></pre></div>
<pre><code>##    c2vs1 c3vs2 c4vs3
## F1 -0.75  -0.5 -0.25
## F2  0.25  -0.5 -0.25
## F3  0.25   0.5 -0.25
## F4  0.25   0.5  0.75
## attr(,&quot;class&quot;)
## [1] &quot;hypr_cmat&quot; &quot;matrix&quot;    &quot;array&quot;</code></pre>
<div class="sourceCode" id="cb431"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb431-1" data-line-number="1">covars &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F, df_contrasts3) <span class="co"># design matrix</span></a>
<a class="sourceLine" id="cb431-2" data-line-number="2">(covars &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(covars))</a></code></pre></div>
<pre><code>##    (Intercept) Fc2vs1 Fc3vs2 Fc4vs3
## 1            1  -0.75   -0.5  -0.25
## 2            1  -0.75   -0.5  -0.25
## 3            1  -0.75   -0.5  -0.25
## 4            1  -0.75   -0.5  -0.25
## 5            1  -0.75   -0.5  -0.25
## 6            1   0.25   -0.5  -0.25
## 7            1   0.25   -0.5  -0.25
## 8            1   0.25   -0.5  -0.25
## 9            1   0.25   -0.5  -0.25
## 10           1   0.25   -0.5  -0.25
## 11           1   0.25    0.5  -0.25
## 12           1   0.25    0.5  -0.25
## 13           1   0.25    0.5  -0.25
## 14           1   0.25    0.5  -0.25
## 15           1   0.25    0.5  -0.25
## 16           1   0.25    0.5   0.75
## 17           1   0.25    0.5   0.75
## 18           1   0.25    0.5   0.75
## 19           1   0.25    0.5   0.75
## 20           1   0.25    0.5   0.75</code></pre>
<p>For each of the <span class="math inline">\(20\)</span> subjects, four numbers are stored in this model matrix. They represent the three values of three predictor variables used to predict response times in the task. Indeed, this matrix is exactly the design matrix <span class="math inline">\(X\)</span> commonly used in multiple regression analysis, where each column represents one numeric predictor variable (covariate), and the first column codes the intercept term.</p>
<p>To further illustrate this, the covariates are extracted from this design matrix and stored separately as numeric predictor variables in the data frame:</p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb433-1" data-line-number="1">df_contrasts3[, <span class="kw">c</span>(<span class="st">&quot;Fc2vs1&quot;</span>, <span class="st">&quot;Fc3vs2&quot;</span>, <span class="st">&quot;Fc4vs3&quot;</span>)] &lt;-<span class="st"> </span>covars[, <span class="dv">2</span><span class="op">:</span><span class="dv">4</span>]</a></code></pre></div>
<p>They are now used as numeric predictor variables in a multiple regression analysis:</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb434-1" data-line-number="1">fit_m3 &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>Fc2vs1 <span class="op">+</span><span class="st"> </span>Fc3vs2 <span class="op">+</span><span class="st"> </span>Fc4vs3,</a>
<a class="sourceLine" id="cb434-2" data-line-number="2">  <span class="dt">data =</span> df_contrasts3,</a>
<a class="sourceLine" id="cb434-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb434-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb434-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb434-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb434-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb434-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb434-9" data-line-number="9">)</a></code></pre></div>
<div class="sourceCode" id="cb435"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb435-1" data-line-number="1"><span class="kw">fixef</span>(fit_m3)</a></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    19.94      2.44  15.03 24.85
## Fc2vs1        9.81      6.89  -3.59 23.75
## Fc3vs2       -9.50      6.90 -22.98  4.01
## Fc4vs3       29.31      6.94  15.49 42.98</code></pre>
<p>The results show that the regression coefficients are the same as in the contrast-based analysis shown in the previous section. This demonstrates that contrasts serve to code discrete factor levels into a linear/multiple regression analysis by numerically encoding comparisons between specific condition means.</p>
</div>
<div id="polynomialContrasts" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.3.4</span> Polynomial contrasts<a href="ch-contr.html#polynomialContrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Polynomial contrasts are another option for analyzing factors. Suppose that we expect a linear trend across conditions, where the response increases by a constant magnitude with each successive factor level. This could be the expectation when four levels of a factor reflect decreasing levels of word frequency (i.e., four factor levels: high, medium-high, medium-low, and low word frequency), where one expects the lowest response for high frequency words, and successively higher responses for lower word frequencies. The effect for each individual level of a factor may not be strong enough for detecting it in the statistical model. Specifying a linear trend in a polynomial contrast allows us to pool the whole increase into a single coefficient for the linear trend, increasing statistical sensitivity for estimating/detecting the increase. Such a specification constrains the estimate to one interpretable parameter, e.g., a linear increase across factor levels. The larger the number of factor levels, the more parsimonious are polynomial contrasts compared to contrast-based specifications as introduced in the previous sections. Going beyond a linear trend, one may also have expectations about quadratic trends. For example, one may expect an increase only among very low frequency words, but no difference between high and medium-high frequency words.</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb437-1" data-line-number="1">Xpol &lt;-<span class="st"> </span><span class="kw">contr.poly</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb437-2" data-line-number="2">(<span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span>Xpol)</a></code></pre></div>
<pre><code>##          .L   .Q     .C
## [1,] -0.671  0.5 -0.224
## [2,] -0.224 -0.5  0.671
## [3,]  0.224 -0.5 -0.671
## [4,]  0.671  0.5  0.224</code></pre>
<div class="sourceCode" id="cb439"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb439-1" data-line-number="1">fit_Pol &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb439-2" data-line-number="2">  <span class="dt">data =</span> df_contrasts3,</a>
<a class="sourceLine" id="cb439-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb439-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb439-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb439-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb439-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb439-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb439-9" data-line-number="9">)</a></code></pre></div>
<div class="sourceCode" id="cb440"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb440-1" data-line-number="1"><span class="kw">fixef</span>(fit_Pol)</a></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    20.00      2.41 15.27  24.8
## F.L          17.69      5.01  7.81  27.6
## F.Q           9.95      4.90  0.13  19.9
## F.C          13.10      4.91  2.91  22.5</code></pre>
<p>In this example, condition means increase across factor levels in a linear fashion, but there may also be quadratic and cubic trends.</p>
</div>
<div id="an-alternative-to-contrasts-monotonic-effects" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.3.5</span> An alternative to contrasts: monotonic effects<a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An alternative to specifying contrasts to estimate specific comparisons between factor levels is monotonic effects <span class="citation">(<a href="https://paul-buerkner.github.io/brms/articles/brms_monotonic.html" class="uri">https://paul-buerkner.github.io/brms/articles/brms_monotonic.html</a>; Bürkner and Charpentier <a href="#ref-burkner2020modelling">2020</a>)</span>. This simply assumes that the dependent variable increases (or decreases) in a monotonic fashion across levels of an ordered factor. In this kind of analysis, one does not define contrasts specifying differences between (groups of) factor levels. Instead, one estimates one parameter which captures the average increase (or decrease) in the dependent variable associated with two neighboring factor levels. Moreover, one estimates the percentages of the overall increase (or decrease) that is associated with each of the differences between neighboring factor levels (i.e., similar to simple difference contrasts, but measured in percentage increase, and assuming monotonicity, i.e., that the same increase or decrease is present for all simple differences).</p>
<p>To implement a monotonic analysis, we first code the factor <span class="math inline">\(F\)</span> as being an ordered factor (i.e.,<code>ordered=TRUE</code>). Then, we specify that we want to estimate a monotonic effect of <span class="math inline">\(F\)</span> using the notation <code>mo(F)</code> in our call to <code>brms</code>:</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" data-line-number="1">df_contrasts3<span class="op">$</span>F &lt;-<span class="st"> </span><span class="kw">factor</span>(df_contrasts3<span class="op">$</span>F, <span class="dt">ordered =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb442-2" data-line-number="2">fit_mo &lt;-<span class="st"> </span><span class="kw">brm</span>(DV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">mo</span>(F),</a>
<a class="sourceLine" id="cb442-3" data-line-number="3">  <span class="dt">data =</span> df_contrasts3,</a>
<a class="sourceLine" id="cb442-4" data-line-number="4">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb442-5" data-line-number="5">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb442-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">20</span>, <span class="dv">50</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb442-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb442-8" data-line-number="8">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb442-9" data-line-number="9">  )</a>
<a class="sourceLine" id="cb442-10" data-line-number="10">)</a></code></pre></div>
<div class="sourceCode" id="cb443"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb443-1" data-line-number="1"><span class="kw">summary</span>(fit_mo)</a></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: DV ~ 1 + mo(F) 
##    Data: df_contrasts3 (Number of observations: 20) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     9.30      4.34    -0.00    17.03 1.00     1915     1854
## moF           9.49      2.48     4.58    14.30 1.00     1981     2064
## 
## Simplex Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## moF1[1]     0.20      0.14     0.01     0.52 1.00     2122     1541
## moF1[2]     0.11      0.11     0.00     0.42 1.00     2565     2107
## moF1[3]     0.69      0.17     0.30     0.94 1.00     2088     2167
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    11.79      2.32     8.31    17.37 1.00     2032     2296
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>The results show that there is an overall positive population-level effect of the factor <span class="math inline">\(F\)</span> with an estimate (<code>moF</code>) of <span class="math inline">\(9.49\)</span>, reflecting an average increase in the dependent variables of <span class="math inline">\(9.49\)</span> with each level of <span class="math inline">\(F\)</span>. The model summary shows estimates for the simplex parameters, which represent the ratios of the overall increase associated with <span class="math inline">\(F\)</span> that can be attributed to each of the differences between neighboring factor levels. The results show that most of the increase is associated with <code>moF1[3]</code>, i.e., with the last difference, reflecting the difference between <span class="math inline">\(F3\)</span> and <span class="math inline">\(F4\)</span>, whereas the other two differences (<code>moF1[1]</code>, reflecting the difference between <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>, and <code>moF1[2]</code>, reflecting the difference between <span class="math inline">\(F2\)</span> and <span class="math inline">\(F3\)</span>) are smaller. Comparing conditional effects between a model using polynomial contrasts and a model assuming monotonic effects makes it clear that the current model “forces” the effects to increase in a monotonic fashion; see Figure <a href="ch-contr.html#fig:condmopol">8.3</a>.</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb445-1" data-line-number="1"><span class="kw">conditional_effects</span>(fit_Pol) </a>
<a class="sourceLine" id="cb445-2" data-line-number="2"><span class="kw">conditional_effects</span>(fit_mo)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:condmopol"></span>
<img src="bookdown_files/figure-html/condmopol-1.svg" alt="Conditional effects using the polynomial contrasts on the left side vs. assuming monotic effects on the right side." width="48%" /><img src="bookdown_files/figure-html/condmopol-2.svg" alt="Conditional effects using the polynomial contrasts on the left side vs. assuming monotic effects on the right side." width="48%" />
<p class="caption">
FIGURE 8.3: Conditional effects using the polynomial contrasts on the left side vs. assuming monotic effects on the right side.
</p>
</div>
<p>This is regardless of the information provided in the data; see the posterior predictive checks in Figure <a href="ch-contr.html#fig:checkmopol">8.4</a>.</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb446-1" data-line-number="1"></a>
<a class="sourceLine" id="cb446-2" data-line-number="2"><span class="kw">pp_check</span>(fit_Pol, <span class="dt">type =</span> <span class="st">&quot;violin_grouped&quot;</span>,</a>
<a class="sourceLine" id="cb446-3" data-line-number="3">         <span class="dt">group =</span> <span class="st">&quot;F&quot;</span>, <span class="dt">y_draw =</span> <span class="st">&quot;points&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb446-4" data-line-number="4"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb446-5" data-line-number="5"><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">55</span>, <span class="dv">105</span>))</a>
<a class="sourceLine" id="cb446-6" data-line-number="6"><span class="kw">pp_check</span>(fit_mo, <span class="dt">type =</span> <span class="st">&quot;violin_grouped&quot;</span>,</a>
<a class="sourceLine" id="cb446-7" data-line-number="7">         <span class="dt">group =</span> <span class="st">&quot;F&quot;</span>, <span class="dt">y_draw =</span> <span class="st">&quot;points&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb446-8" data-line-number="8"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb446-9" data-line-number="9"><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">55</span>, <span class="dv">105</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:checkmopol"></span>
<img src="bookdown_files/figure-html/checkmopol-1.svg" alt="Posterior predictive distributions by condition using the polynomial contrasts on the left side vs. assuming monotonic effects on the right side." width="48%" /><img src="bookdown_files/figure-html/checkmopol-2.svg" alt="Posterior predictive distributions by condition using the polynomial contrasts on the left side vs. assuming monotonic effects on the right side." width="48%" />
<p class="caption">
FIGURE 8.4: Posterior predictive distributions by condition using the polynomial contrasts on the left side vs. assuming monotonic effects on the right side.
</p>
</div>
<p>Interestingly, the monotonicity assumption is violated in the current data set, since the mean is larger in condition <span class="math inline">\(F2\)</span> than in condition <span class="math inline">\(F3\)</span>. The monotonic model thus assumes this (negative) difference is due to chance; see Figure <a href="ch-contr.html#fig:checkmopol">8.4</a>.</p>
<p>Estimating such monotonic effects provides an alternative to the contrast coding we treat in the rest of this chapter. It may be relevant when the specific differences between factor levels are not of interest, but when instead the goal is to estimate the overall monotonic effect of a factor and when this overall effect is not well approximated by a simple linear trend.</p>
</div>
</div>
<div id="nonOrthogonal" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.4</span> What makes a good set of contrasts?<a href="ch-contr.html#nonOrthogonal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!--Contrasts decompose ANOVA omnibus F tests into several component comparisons [@Baguley2012]. Orthogonal contrasts decompose the sum of squares of the F test into additive independent subcomponents, which allows for clarity in interpreting each effect.-->
<p>For a factor with <span class="math inline">\(I\)</span> levels one can make only <span class="math inline">\(I-1\)</span> comparisons within a single model. For example, in a design with one factor with two levels, only one comparison is possible (between the two factor levels). The reason for this is that the intercept is also estimated. More generally, if we have a factor with <span class="math inline">\(I_1\)</span> and another factor with <span class="math inline">\(I_2\)</span> levels, then the total number of conditions is <span class="math inline">\(I_1\times I_2 = \nu\)</span> (not <span class="math inline">\(I_1 + I_2\)</span>!), which implies a maximum of <span class="math inline">\(\nu-1\)</span> contrasts.</p>
<p>For example, in a design with one factor with three levels, <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span>, in principle one could make three comparisons (<span class="math inline">\(A\)</span> vs. <span class="math inline">\(B\)</span>, <span class="math inline">\(A\)</span> vs. <span class="math inline">\(C\)</span>, <span class="math inline">\(B\)</span> vs. <span class="math inline">\(C\)</span>).
However, after defining an intercept, only two means can be compared. Therefore, for a factor with three levels, we define two comparisons within one statistical model.</p>
<p>One critical precondition for contrasts is that they implement different comparisons that are not completely collinear, that is, that none of the contrasts can be generated from the other contrasts by linear combination. For example, the contrast <code>c1 = c(1,2,3)</code> can be generated from the contrast <code>c2 = c(3,4,5)</code> simply by computing <code>c2 - 2</code>. Therefore, contrasts c1 and c2 cannot be used simultaneously. That is, each contrast needs to encode some independent information about the data.</p>
<p>There are (at least) two criteria to decide what a good contrast is. First, <em>orthogonal contrasts</em> have advantages as they estimate mutually independent comparisons in the data <span class="citation">(see Dobson and Barnett <a href="#ref-dobson2011introduction">2011</a>, sec. 6.2.5, p. 91 for a detailed explanation of orthogonality)</span>. As independent predictors in a regression yield more accurate estimates (tighter posterior distributions) of slope parameters than correlated predictors, in a similar way, orthogonal contrasts yield more accurate estimates of slopes than non-orthogonal contrasts. Second, it is crucial that contrasts are defined in a way such that they answer the research questions. One way to accomplish this second point is to use the hypothesis matrix to generate contrasts (e.g., via the <code>hypr</code> package), as this ensures that one uses contrasts that exactly estimate the comparisons of interest in a given study.</p>
<div id="centered-contrasts" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.1</span> Centered contrasts<a href="ch-contr.html#centered-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Contrasts are often constrained to be centered, such that the individual contrast coefficients <span class="math inline">\(c_i\)</span> for different factor levels <span class="math inline">\(i\)</span> sum to <span class="math inline">\(0\)</span>: <span class="math inline">\(\sum_{i=1}^I c_i = 0\)</span>. This has advantages when estimating interactions with other factors or covariates (we discuss interactions between factors in the next chapter).
All contrasts discussed here are centered except for the treatment contrast, in which the contrast coefficients for each contrast do not sum to zero:</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb447-1" data-line-number="1"><span class="kw">colSums</span>(<span class="kw">contr.treatment</span>(<span class="dv">4</span>))</a></code></pre></div>
<pre><code>## 2 3 4 
## 1 1 1</code></pre>
<p>Other contrasts, such as repeated contrasts, are centered and the contrast coefficients for each contrast sum to <span class="math inline">\(0\)</span>:</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb449-1" data-line-number="1"><span class="kw">colSums</span>(<span class="kw">contr.sdif</span>(<span class="dv">4</span>))</a></code></pre></div>
<pre><code>## 2-1 3-2 4-3 
##   0   0   0</code></pre>
<p>The contrast coefficients mentioned above appear in the contrast matrix. By contrast, the weights in the hypothesis matrix are always centered. This is also true for the treatment contrast. The reason is that they code comparisons between conditions or bundles of conditions.
The only exception are the weights for the intercept, which are all the same and together always sum to <span class="math inline">\(1\)</span> in the hypothesis matrix. This is done to ensure that when applying the generalized matrix inverse, the intercept results in a constant term with values of <span class="math inline">\(1\)</span> in the contrast matrix.
An important question concerns whether (or when) the intercept needs to be considered in the generalized matrix inversion, and whether (or when) it can be ignored. This question is closely related to orthogonal contrasts, a concept we turn to below.</p>
</div>
<div id="orthogonal-contrasts" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.2</span> Orthogonal contrasts<a href="ch-contr.html#orthogonal-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two centered contrasts <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> are orthogonal to each other if the following condition applies. Here, <span class="math inline">\(i\)</span> is the <span class="math inline">\(i\)</span>-th cell of the vector representing the contrast.</p>
<p><span class="math display">\[\begin{equation}
\sum_{i=1}^I c_{1,i} \cdot c_{2,i} = 0
\end{equation}\]</span></p>
<p>Orthogonality can be determined easily by computing the correlation between two contrasts. Orthogonal contrasts have a correlation of <span class="math inline">\(0\)</span>. Contrasts are therefore just a special case for the general case of predictors in regression models, where two numeric predictor variables are orthogonal if they are un-correlated.</p>
<p>For example, coding two factors in a <span class="math inline">\(2 \times 2\)</span> design (we return to this case in a section on designs with two factors below) using sum contrasts, these sum contrasts and their interaction are orthogonal to each other:</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb451-1" data-line-number="1">(Xsum &lt;-<span class="st"> </span><span class="kw">cbind</span>(</a>
<a class="sourceLine" id="cb451-2" data-line-number="2">  <span class="dt">F1 =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">-1</span>),</a>
<a class="sourceLine" id="cb451-3" data-line-number="3">  <span class="dt">F2 =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dv">-1</span>),</a>
<a class="sourceLine" id="cb451-4" data-line-number="4">  <span class="dt">F1xF2 =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">-1</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb451-5" data-line-number="5">))</a></code></pre></div>
<pre><code>##      F1 F2 F1xF2
## [1,]  1  1     1
## [2,]  1 -1    -1
## [3,] -1  1    -1
## [4,] -1 -1     1</code></pre>
<div class="sourceCode" id="cb453"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb453-1" data-line-number="1"><span class="kw">cor</span>(Xsum)</a></code></pre></div>
<pre><code>##       F1 F2 F1xF2
## F1     1  0     0
## F2     0  1     0
## F1xF2  0  0     1</code></pre>
<p>
The correlations between the different contrasts (i.e., the off-diagonals) are exactly <span class="math inline">\(0\)</span>. Sum contrasts coding one multi-level factor, however, are not orthogonal to each other:</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb455-1" data-line-number="1"><span class="kw">cor</span>(<span class="kw">contr.sum</span>(<span class="dv">4</span>))</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  1.0  0.5  0.5
## [2,]  0.5  1.0  0.5
## [3,]  0.5  0.5  1.0</code></pre>
<p>
Here, the correlations between individual contrasts, which appear in the off-diagonals, deviate from <span class="math inline">\(0\)</span>, indicating non-orthogonality. The same is also true for treatment and repeated contrasts:</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" data-line-number="1"><span class="kw">cor</span>(<span class="kw">contr.sdif</span>(<span class="dv">4</span>))</a></code></pre></div>
<pre><code>##       2-1   3-2   4-3
## 2-1 1.000 0.577 0.333
## 3-2 0.577 1.000 0.577
## 4-3 0.333 0.577 1.000</code></pre>
<div class="sourceCode" id="cb459"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" data-line-number="1"><span class="kw">cor</span>(<span class="kw">contr.treatment</span>(<span class="dv">4</span>))</a></code></pre></div>
<pre><code>##        2      3      4
## 2  1.000 -0.333 -0.333
## 3 -0.333  1.000 -0.333
## 4 -0.333 -0.333  1.000</code></pre>
<p>Orthogonality of contrasts plays a critical role when computing the generalized inverse. In the inversion operation, orthogonal contrasts are converted independently from each other. That is, the presence or absence of another orthogonal contrast does not change the resulting weights. In fact, for orthogonal contrasts, applying the generalized matrix inverse to the hypothesis matrix simply furnishes a scaled version of the hypothesis matrix in the contrast matrix <span class="citation">(for mathematical details see Schad et al. <a href="#ref-schad2020capitalize">2020</a>)</span>. However, in Bayesian models, scaling is always important, since we need to interpret the scale in order to define priors or interpret posteriors. Therefore, when working with contrasts in Bayesian models, the generalized matrix inverse is always a good procedure to use.</p>
</div>
<div id="the-role-of-the-intercept-in-non-centered-contrasts" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.3</span> The role of the intercept in non-centered contrasts<a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A related question concerns whether the intercept needs to be considered when computing the generalized inverse for a contrast. This is of key importance when using the generalized matrix inverse to define contrasts: the resulting contrast matrix and also the definition of estimates can completely change between a situation where the intercept is explicitly considered or not considered, and can thus change the resulting estimates in possibly unintended ways. Thus, if the definition of the intercept is incorrect, the estimates of slopes may also be wrong.</p>
<p>More specifically, it turns out that considering the intercept is necessary for contrasts that are not centered. This is the case for treatment contrasts which are not centered; e.g., the treatment contrast for two factor levels <code>c1vs0 = c(0,1)</code>: <span class="math inline">\(\sum_i c_i = 0 + 1 = 1\)</span>. One can actually show that the formula to determine whether contrasts are centered (i.e., <span class="math inline">\(\sum_i c_i = 0\)</span>) is the same formula as the formula to test whether a contrast is “orthogonal to the intercept”. Remember that for the intercept, all contrast coefficients are equal to one: <span class="math inline">\(c_{1,i} = 1\)</span> (here, <span class="math inline">\(c_{1,i}\)</span> indicates the vector of contrast coefficients associated with the intercept). We enter these contrast coefficient values into the formula testing whether a contrast is orthogonal to the intercept (here, <span class="math inline">\(c_{2,i}\)</span> indicates the vector of contrast coefficients associated with some contrast for which we want to test whether it is “orthogonal to the intercept”): <span class="math inline">\(\sum_i c_{1,i} \cdot c_{2,i} = \sum_i 1 \cdot c_{2,i} = \sum_i c_{2,i} = 0\)</span>. The resulting formula is: <span class="math inline">\(\sum_i c_{2,i} = 0\)</span>, which is exactly the formula for whether a contrast is centered. Because of this analogy, treatment contrasts can be viewed to be `not orthogonal to the intercept.’ This means that the intercept needs to be considered when computing the generalized inverse for treatment contrasts. As we have discussed above, when the intercept is included in the hypothesis matrix, the weights for this intercept term should sum to one, as this yields a column of ones for the intercept term in the contrast matrix.</p>
<p>We can see that considering the intercept makes a difference for the treatment contrast. First, we define the comparisons involved in a treatment contrast, where two experimental conditions <code>b</code> and <code>c</code> are each compared to a baseline condition <code>a</code> (<code>b~a</code> and <code>c~a</code>). In addition, we explicitly code the intercept term, which involves a comparison of the baseline to 0 (<code>a~0</code>). We take a look at the resulting contrast matrix:</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" data-line-number="1"><span class="kw">hypr</span>(<span class="dt">int =</span> a <span class="op">~</span><span class="st"> </span><span class="dv">0</span>, <span class="dt">b1 =</span> b <span class="op">~</span><span class="st"> </span>a, <span class="dt">b2 =</span> c <span class="op">~</span><span class="st"> </span>a)</a></code></pre></div>
<pre><code>## hypr object containing 3 null hypotheses:
## H0.int: 0 = a      (Intercept)
##  H0.b1: 0 = b - a
##  H0.b2: 0 = c - a
## 
## Call:
## hypr(int = ~a, b1 = ~b - a, b2 = ~c - a, levels = c(&quot;a&quot;, &quot;b&quot;, 
## &quot;c&quot;))
## 
## Hypothesis matrix (transposed):
##   int b1 b2
## a  1  -1 -1
## b  0   1  0
## c  0   0  1
## 
## Contrast matrix:
##   int b1 b2
## a 1   0  0 
## b 1   1  0 
## c 1   0  1</code></pre>
<div class="sourceCode" id="cb463"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb463-1" data-line-number="1"><span class="kw">contr.treatment</span>(<span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>, <span class="st">&quot;c&quot;</span>))</a></code></pre></div>
<pre><code>##   b c
## a 0 0
## b 1 0
## c 0 1</code></pre>
<p>This shows a contrast matrix that we know from the treatment contrast. The intercept is coded as a column of ones. And each of the comparisons is coded as a <span class="math inline">\(1\)</span> in the condition which is compared to the baseline, and a <span class="math inline">\(0\)</span> in other conditions. The point is here that this gives us the contrast matrix that is expected and known for the treatment contrast.</p>
<p>However, we can also ignore the intercept in the specification of the comparisons:</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb465-1" data-line-number="1"><span class="kw">hypr</span>(<span class="dt">b1 =</span> m1 <span class="op">~</span><span class="st"> </span>m0, <span class="dt">b2 =</span> m2 <span class="op">~</span><span class="st"> </span>m0)</a></code></pre></div>
<pre><code>## hypr object containing 2 null hypotheses:
## H0.b1: 0 = m1 - m0
## H0.b2: 0 = m2 - m0
## 
## Call:
## hypr(b1 = ~m1 - m0, b2 = ~m2 - m0, levels = c(&quot;m0&quot;, &quot;m1&quot;, &quot;m2&quot;
## ))
## 
## Hypothesis matrix (transposed):
##    b1 b2
## m0 -1 -1
## m1  1  0
## m2  0  1
## 
## Contrast matrix:
##    b1   b2  
## m0 -1/3 -1/3
## m1  2/3 -1/3
## m2 -1/3  2/3</code></pre>
<p>Interestingly, the resulting contrast matrix now looks very different from the contrast matrix that we know from the treatment contrast. Indeed, this contrast also estimates a reasonable set of quantities. It again estimates whether the condition mean <code>m1</code> differs from the baseline and whether <code>m2</code> differs from baseline. The intercept, however, now estimates the average dependent variable across all three conditions (i.e., the grand mean). This can be seen by explicitly adding a comparison of the average of all three conditions to <span class="math inline">\(0\)</span>:</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" data-line-number="1"><span class="kw">hypr</span>(<span class="dt">int =</span> (m0 <span class="op">+</span><span class="st"> </span>m1 <span class="op">+</span><span class="st"> </span>m2) <span class="op">/</span><span class="st"> </span><span class="dv">3</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>, <span class="dt">b1 =</span> m1 <span class="op">~</span><span class="st"> </span>m0, <span class="dt">b2 =</span> m2 <span class="op">~</span><span class="st"> </span>m0)</a></code></pre></div>
<pre><code>## hypr object containing 3 null hypotheses:
## H0.int: 0 = (m0 + m1 + m2)/3  (Intercept)
##  H0.b1: 0 = m1 - m0
##  H0.b2: 0 = m2 - m0
## 
## Call:
## hypr(int = ~1/3 * m0 + 1/3 * m1 + 1/3 * m2, b1 = ~m1 - m0, b2 = ~m2 - 
##     m0, levels = c(&quot;m0&quot;, &quot;m1&quot;, &quot;m2&quot;))
## 
## Hypothesis matrix (transposed):
##    int b1  b2 
## m0 1/3  -1  -1
## m1 1/3   1   0
## m2 1/3   0   1
## 
## Contrast matrix:
##    int  b1   b2  
## m0    1 -1/3 -1/3
## m1    1  2/3 -1/3
## m2    1 -1/3  2/3</code></pre>
<p>The last two columns of the resulting contrast matrix are now the same as when the intercept was ignored, which confirms that the two columns encode the same comparison.</p>
</div>
</div>
<div id="computing-condition-means-from-estimated-contrasts" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.5</span> Computing condition means from estimated contrasts<a href="ch-contr.html#computing-condition-means-from-estimated-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As mentioned earlier, one advantage of Bayesian modeling is that based on the posterior samples, it is possible to very flexibly compute new comparisons and estimates. Above (see section <a href="ch-contr.html#sec-cellMeans">8.1.4</a>), we had discussed the case where the Bayesian model estimated the condition means instead of contrasts by removing the intercept from the <code>brms</code> model (the formula in <code>brms</code> was: <code>DV ~ -1 + F</code>). This allowed us to get posterior samples from each condition mean, and then to compute any possible comparison between condition means by subtracting the corresponding samples.</p>
<p>Importantly, posterior samples for the condition means can also be obtained after fitting a model with contrasts. We illustrate this here for the case of sum contrasts. Let’s use our above example of a design where we assess response times (in milliseconds, <code>DV</code>) for three different word classes adjectives, nouns, and verbs, that is, for a 3-level factor <span class="math inline">\(F\)</span>. In the above example, factor <span class="math inline">\(F\)</span> was coded using a sum contrast, where the first contrast coded the difference of adjectives from the grand mean, and the second contrast coded the difference of nouns from the grand mean. This was the corresponding contrast matrix:</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb469-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcSum)</a>
<a class="sourceLine" id="cb469-2" data-line-number="2"><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F)</a></code></pre></div>
<pre><code>##            b1 b2
## adjectives  1  0
## nouns       0  1
## verbs      -1 -1</code></pre>
<p>We had estimated a <code>brms</code> model for this data. The posterior estimates show results for the intercept (which is estimated to be <span class="math inline">\(450\)</span> ms) and for our two coded comparisons. The effect <code>FcH01</code> codes our first comparison that response times for adjectives differ from the grand mean, and show an estimate that response times for adjectives are about <span class="math inline">\(50\)</span> ms shorter than the grand mean. Moreover, the effect <code>FcH02</code> codes our second comparison that response times for nouns differ from the grand mean, and show the estimate that response times for nouns are <span class="math inline">\(50\)</span> ms longer than the grand mean.</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb471-1" data-line-number="1"><span class="kw">fixef</span>(fit_Sum)</a></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept    450.3      7.22 435.7 464.6
## FcH01        -49.1     10.01 -69.0 -29.0
## FcH02         49.0      9.87  28.1  68.2</code></pre>
<p>However, of course other comparisons might be of interest to us as well. For example, we might be interested in estimating how strongly response times for verbs differ from the grand mean.</p>
<p>To do so, one possible first step is to obtain the posteriors for the response times in each of the three conditions. How can this be done? The first step is to again extract the posterior samples from the model:</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" data-line-number="1">df_postSamp_Sum &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_Sum)</a></code></pre></div>
<p>We can see the samples for our first contrast (<code>b_FcH01</code>) and for our second contrast (<code>b_FcH02</code>). How can we now compute the posterior samples for each of the condition means, i.e., for adjectives, nouns, and verbs? For this, we need to take another look at the contrast matrix.</p>
<div class="sourceCode" id="cb474"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb474-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts2<span class="op">$</span>F)</a></code></pre></div>
<pre><code>##            b1 b2
## adjectives  1  0
## nouns       0  1
## verbs      -1 -1</code></pre>
<p>It tells us how the condition means are computed. For adjectives (see the first row of the contrast matrix), we can see that the response time is computed by taking <span class="math inline">\(1\)</span> times the coefficient for <code>b1</code> (i.e., <code>FcH01</code>) and <span class="math inline">\(0\)</span> times the coefficient for <code>b2</code> (i.e., <code>FcH02</code>). Thus, response times for adjectives are simply the samples for the <code>b1</code> (i.e., <code>FcH01</code>) contrast. The contrast matrix does not show the intercept term, which is implicitly added. Thus, we also have to add the estimates for the intercept. Thus, the condition mean for adjectives is computed as <code>b_adjectives &lt;- b_Intercept + b_FcH01</code>:</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb476-1" data-line-number="1">df_postSamp_Sum<span class="op">$</span>b_adjectives &lt;-</a>
<a class="sourceLine" id="cb476-2" data-line-number="2"><span class="st">  </span>df_postSamp_Sum<span class="op">$</span>b_Intercept <span class="op">+</span><span class="st"> </span>df_postSamp_Sum<span class="op">$</span>b_FcH01</a></code></pre></div>
<p>Similarly, we can obtain the posterior samples for the response times for nouns. The computation can be seen from the second row of the contrast matrix, which shows that the contrast <code>b1</code> (i.e., <code>FcH01</code>) has weight <span class="math inline">\(0\)</span> times, whereas the contrast <code>b2</code> (i.e., <code>FcH02</code>) has weight <span class="math inline">\(1\)</span>. Adding the intercept thus gives:</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb477-1" data-line-number="1">df_postSamp_Sum<span class="op">$</span>b_nouns &lt;-</a>
<a class="sourceLine" id="cb477-2" data-line-number="2"><span class="st">  </span>df_postSamp_Sum<span class="op">$</span>b_Intercept <span class="op">+</span><span class="st"> </span>df_postSamp_Sum<span class="op">$</span>b_FcH02</a></code></pre></div>
<p>Finally, we want to obtain posterior samples for the average response times for verbs. For verbs, the third row of the contrast matrix shows two times a <span class="math inline">\(-1\)</span>. Thus, contrasts <code>b1</code> (i.e., <code>FcH01</code>) and <code>b2</code> (i.e., <code>FcH02</code>) have to be subtracted from the intercept:</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb478-1" data-line-number="1">df_postSamp_Sum<span class="op">$</span>b_verbs &lt;-</a>
<a class="sourceLine" id="cb478-2" data-line-number="2"><span class="st">  </span>df_postSamp_Sum<span class="op">$</span>b_Intercept <span class="op">-</span><span class="st"> </span>df_postSamp_Sum<span class="op">$</span>b_FcH01 <span class="op">-</span></a>
<a class="sourceLine" id="cb478-3" data-line-number="3"><span class="st">  </span>df_postSamp_Sum<span class="op">$</span>b_FcH02</a></code></pre></div>
<p>This yields posterior samples for the mean response times for verbs.</p>
<p>We can now look at the posterior means and 95% credible intervals for adjectives, nouns, and verbs by computing the means and quantiles across all computed samples.</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb479-1" data-line-number="1">postTab &lt;-<span class="st"> </span>df_postSamp_Sum <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb479-2" data-line-number="2"><span class="st">  </span><span class="co"># removes the meta data:</span></a>
<a class="sourceLine" id="cb479-3" data-line-number="3"><span class="st">  </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb479-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(b_adjectives, b_nouns, b_verbs) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb479-5" data-line-number="5"><span class="st">  </span><span class="co"># transform from wide to long with tidyr:</span></a>
<a class="sourceLine" id="cb479-6" data-line-number="6"><span class="st">  </span><span class="kw">pivot_longer</span>(</a>
<a class="sourceLine" id="cb479-7" data-line-number="7">    <span class="dt">cols =</span> <span class="kw">everything</span>(),</a>
<a class="sourceLine" id="cb479-8" data-line-number="8">    <span class="dt">names_to =</span> <span class="st">&quot;condition&quot;</span>,</a>
<a class="sourceLine" id="cb479-9" data-line-number="9">    <span class="dt">values_to =</span> <span class="st">&quot;samp&quot;</span></a>
<a class="sourceLine" id="cb479-10" data-line-number="10">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb479-11" data-line-number="11"><span class="st">  </span><span class="kw">group_by</span>(condition) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb479-12" data-line-number="12"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb479-13" data-line-number="13">    <span class="dt">post_mean =</span> <span class="kw">round</span>(<span class="kw">mean</span>(samp)),</a>
<a class="sourceLine" id="cb479-14" data-line-number="14">    <span class="st">`</span><span class="dt">2.5%</span><span class="st">`</span> =<span class="st"> </span><span class="kw">round</span>(<span class="kw">quantile</span>(samp, <span class="dt">p =</span> <span class="fl">0.025</span>)),</a>
<a class="sourceLine" id="cb479-15" data-line-number="15">    <span class="st">`</span><span class="dt">97.5%</span><span class="st">`</span> =<span class="st"> </span><span class="kw">round</span>(<span class="kw">quantile</span>(samp, <span class="dt">p =</span> <span class="fl">0.975</span>))</a>
<a class="sourceLine" id="cb479-16" data-line-number="16">  )</a></code></pre></div>
<div class="sourceCode" id="cb480"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb480-1" data-line-number="1">postTab</a></code></pre></div>
<pre><code>## # A tibble: 3 × 4
##   condition    post_mean `2.5%` `97.5%`
##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
## 1 b_adjectives       401    377     427
## 2 b_nouns            499    476     524
## 3 b_verbs            450    426     475</code></pre>
<p>The results show that as expected the posterior mean for adjectives is <span class="math inline">\(400\)</span> ms, for nouns it is <span class="math inline">\(500\)</span> ms, and for verbs, the posterior mean is <span class="math inline">\(450\)</span> ms. Moreover, we have now posterior credible intervals for each of these estimates.</p>
<p>In fact, <code>brms</code> has a very convenient built-in function that allows us to compute these nested effects automatically (<code>robust = FALSE</code> shows the posterior mean; by default <code>brms</code> shows the posterior median). Notice that you need to add a <code>[]</code> after the function call, otherwise <code>brms</code> will plot the results.</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb482-1" data-line-number="1"><span class="kw">conditional_effects</span>(fit_Sum, <span class="dt">robust =</span> <span class="ot">FALSE</span>)[]</a></code></pre></div>
<pre><code>## $F
##            F  DV cond__  effect1__ estimate__ se__ lower__ upper__
## 1 adjectives 450      1 adjectives        401 12.2     377     427
## 2      nouns 450      1      nouns        499 12.3     476     524
## 3      verbs 450      1      verbs        450 12.3     426     475</code></pre>
<p>The same function allows us to visualize the effects, as shown in Figure <a href="ch-contr.html#fig:cFigCond">8.5</a>.</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb484-1" data-line-number="1"><span class="kw">conditional_effects</span>(fit_Sum, <span class="dt">robust =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:cFigCond"></span>
<img src="bookdown_files/figure-html/cFigCond-1.svg" alt="Estimated condition means, computed from a brms model fitted with a sum contrast." width="384" />
<p class="caption">
FIGURE 8.5: Estimated condition means, computed from a brms model fitted with a sum contrast.
</p>
</div>
<p>Importantly, coming back to our hand-crafted computations, the computed posterior samples can be used to compute additional comparisons. For example, we might be interested in how much response times for verbs differ from the grand mean. This can be computed based on the samples for the condition means: we first compute the grand mean from the three condition means, <code>b_GM &lt;- (b_adjectives + b_nouns + b_verbs)/3</code>, and then we compare this to the estimate for verbs.</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb485-1" data-line-number="1">df_postSamp_Sum &lt;-<span class="st"> </span>df_postSamp_Sum <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb485-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">GM =</span> (b_adjectives <span class="op">+</span><span class="st"> </span>b_nouns <span class="op">+</span><span class="st"> </span>b_verbs) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>,</a>
<a class="sourceLine" id="cb485-3" data-line-number="3">         <span class="dt">b_FcH03 =</span> b_verbs <span class="op">-</span><span class="st"> </span>GM)</a>
<a class="sourceLine" id="cb485-4" data-line-number="4"><span class="kw">c</span>(<span class="dt">post_mean =</span> <span class="kw">mean</span>(df_postSamp_Sum<span class="op">$</span>b_FcH03),</a>
<a class="sourceLine" id="cb485-5" data-line-number="5">  <span class="kw">quantile</span>(df_postSamp_Sum<span class="op">$</span>b_FcH03, <span class="dt">p =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>## post_mean      2.5%     97.5% 
##     0.102   -19.965    19.866</code></pre>
<p>The results show that reading times for verbs are quite the same as the grand mean, with a posterior mean estimate for the differences of nearly <span class="math inline">\(0\)</span> ms, and with a 95% credible interval ranging between <span class="math inline">\(-20\)</span> and <span class="math inline">\(+20\)</span> ms.</p>
<p>The key message here is that based on the contrast matrix, it is possible to compute posterior samples for the condition means, and then to compute any arbitrary further comparisons or contrasts. We want to stress again that just obtaining the posterior distribution of a comparison does not allow us to argue that we have evidence for the effect; to argue that we have evidence for an effect being present/absent, we need Bayes factors. But the approach we outline above does allow us to obtain posterior means and credible intervals for arbitrary comparisons.</p>
<p>We briefly show how to compute posterior samples for condition means for one more example contrast, namely for repeated contrasts. Here, the contrast matrix is:</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb487-1" data-line-number="1"><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">contr.hypothesis</span>(HcRep)</a>
<a class="sourceLine" id="cb487-2" data-line-number="2"><span class="kw">contrasts</span>(df_contrasts3<span class="op">$</span>F)</a></code></pre></div>
<pre><code>##    c2vs1 c3vs2 c4vs3
## F1 -0.75  -0.5 -0.25
## F2  0.25  -0.5 -0.25
## F3  0.25   0.5 -0.25
## F4  0.25   0.5  0.75</code></pre>
<p>The model estimates were:</p>
<div class="sourceCode" id="cb489"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb489-1" data-line-number="1"><span class="kw">fixef</span>(fit_Rep)</a></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5 Q97.5
## Intercept    19.97      2.41  14.94 24.64
## Fc2vs1        9.73      6.93  -4.11 23.43
## Fc3vs2       -9.39      6.85 -22.89  4.45
## Fc4vs3       29.28      6.78  15.64 42.36</code></pre>
<p>We first obtain the posterior samples for the contrasts:</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb491-1" data-line-number="1">df_postSamp_Rep &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_Rep)</a></code></pre></div>
<p>Then we compute the posterior samples for condition <span class="math inline">\(F1\)</span>. First, we have to add the intercept. Then, we can see in the contrast matrix that to compute the condition mean for <span class="math inline">\(F1\)</span>, we have to add up all contrasts, using the weights <code>c(-3/4, -1/2, -1/4)</code> for each of the three contrasts (see first row of the contrast matrix). Thus, the posterior samples are computed as follows:</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb492-1" data-line-number="1">df_postSamp_Rep &lt;-<span class="st"> </span>df_postSamp_Rep <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb492-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">b_F1 =</span> b_Intercept <span class="op">+</span></a>
<a class="sourceLine" id="cb492-3" data-line-number="3"><span class="st">           </span><span class="dv">-3</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc2vs1 <span class="op">+</span></a>
<a class="sourceLine" id="cb492-4" data-line-number="4"><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>b_Fc3vs2 <span class="op">+</span></a>
<a class="sourceLine" id="cb492-5" data-line-number="5"><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc4vs3)</a></code></pre></div>
<p>The other condition means are computed correspondingly:</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb493-1" data-line-number="1">df_postSamp_Rep &lt;-<span class="st"> </span>df_postSamp_Rep <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb493-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">b_F2 =</span> b_Intercept <span class="op">+</span></a>
<a class="sourceLine" id="cb493-3" data-line-number="3"><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc2vs1 <span class="op">+</span></a>
<a class="sourceLine" id="cb493-4" data-line-number="4"><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>b_Fc3vs2 <span class="op">+</span></a>
<a class="sourceLine" id="cb493-5" data-line-number="5"><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc4vs3,</a>
<a class="sourceLine" id="cb493-6" data-line-number="6">         <span class="dt">b_F3 =</span> b_Intercept <span class="op">+</span></a>
<a class="sourceLine" id="cb493-7" data-line-number="7"><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc2vs1 <span class="op">+</span></a>
<a class="sourceLine" id="cb493-8" data-line-number="8"><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>b_Fc3vs2 <span class="op">+</span></a>
<a class="sourceLine" id="cb493-9" data-line-number="9"><span class="st">           </span><span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span>b_Fc4vs3,</a>
<a class="sourceLine" id="cb493-10" data-line-number="10">         <span class="dt">b_F4 =</span> b_Intercept <span class="op">+</span></a>
<a class="sourceLine" id="cb493-11" data-line-number="11"><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc2vs1 <span class="op">+</span></a>
<a class="sourceLine" id="cb493-12" data-line-number="12"><span class="st">           </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>b_Fc3vs2 <span class="op">+</span></a>
<a class="sourceLine" id="cb493-13" data-line-number="13"><span class="st">           </span><span class="dv">3</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>b_Fc4vs3)</a></code></pre></div>
<p>Now we can look at the posterior means and credible intervals:</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb494-1" data-line-number="1">postTab &lt;-<span class="st"> </span>df_postSamp_Rep <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb494-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(b_F1, b_F2, b_F3, b_F4) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb494-3" data-line-number="3"><span class="st">  </span><span class="kw">pivot_longer</span>(</a>
<a class="sourceLine" id="cb494-4" data-line-number="4">    <span class="dt">cols =</span> <span class="kw">everything</span>(),</a>
<a class="sourceLine" id="cb494-5" data-line-number="5">    <span class="dt">names_to =</span> <span class="st">&quot;condition&quot;</span>,</a>
<a class="sourceLine" id="cb494-6" data-line-number="6">    <span class="dt">values_to =</span> <span class="st">&quot;samp&quot;</span></a>
<a class="sourceLine" id="cb494-7" data-line-number="7">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb494-8" data-line-number="8"><span class="st">  </span><span class="kw">group_by</span>(condition) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb494-9" data-line-number="9"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb494-10" data-line-number="10">    <span class="dt">post_mean =</span> <span class="kw">round</span>(<span class="kw">mean</span>(samp)),</a>
<a class="sourceLine" id="cb494-11" data-line-number="11">    <span class="st">`</span><span class="dt">2.5%</span><span class="st">`</span> =<span class="st"> </span><span class="kw">round</span>(<span class="kw">quantile</span>(samp, <span class="dt">p =</span> <span class="fl">0.025</span>)),</a>
<a class="sourceLine" id="cb494-12" data-line-number="12">    <span class="st">`</span><span class="dt">97.5%</span><span class="st">`</span> =<span class="st"> </span><span class="kw">round</span>(<span class="kw">quantile</span>(samp, <span class="dt">p =</span> <span class="fl">0.975</span>))</a>
<a class="sourceLine" id="cb494-13" data-line-number="13">  )</a></code></pre></div>
<div class="sourceCode" id="cb495"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb495-1" data-line-number="1"><span class="kw">print</span>(postTab, <span class="dt">n =</span> <span class="dv">4</span>)</a></code></pre></div>
<pre><code>## # A tibble: 4 × 4
##   condition post_mean `2.5%` `97.5%`
##   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
## 1 b_F1             10      0      20
## 2 b_F2             20     10      30
## 3 b_F3             10      1      20
## 4 b_F4             40     30      49</code></pre>
<p>We can verify that <code>brms</code> function return the same values:</p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb497-1" data-line-number="1"><span class="kw">conditional_effects</span>(fit_Rep, <span class="dt">robust =</span> <span class="ot">FALSE</span>)[]</a></code></pre></div>
<pre><code>## $F
##    F DV cond__ effect1__ estimate__ se__ lower__ upper__
## 1 F1 20      1        F1       10.0 4.93   0.342    19.8
## 2 F2 20      1        F2       19.8 4.86  10.196    29.5
## 3 F3 20      1        F3       10.4 4.83   0.661    19.9
## 4 F4 20      1        F4       39.7 4.95  29.663    49.3</code></pre>
<p>The posterior means reflect exactly the means in the data (for comparison see Figure <a href="ch-contr.html#fig:helmertsimdatFig">8.2</a> and Table <a href="ch-contr.html#tab:cTab3Means">8.3</a>). However, we now have posterior samples for each of the conditions and can compute posterior credible intervals as well as new comparisons between conditions.</p>
</div>
<div id="summary-7" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.6</span> Summary<a href="ch-contr.html#summary-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Contrasts in Bayesian models work in exactly the same way as in frequentist models. Contrasts provide a way to tell the model how to code factors into numeric covariates. That is, they provide a way to define which comparisons between which condition means or bundles of condition means should be estimated in the Bayesian model. There are a number of default contrasts, like treatment contrasts, sum contrasts, repeated contrasts, or Helmert contrasts, that are known to estimate specific comparisons between condition means. A much more powerful procedure is to use the generalized matrix inverse, e.g., as implemented in the <code>hypr</code> package, to derive contrasts automatically after specifying the comparisons that a contrast should estimate. We have seen that in Bayesian models, it is quite straightforward to compute posterior samples for new contrasts post-hoc, after the model is fit. However, specifying precise contrasts is still of key importance when doing model comparisons (via Bayes factors) to answer the question of whether the data provide evidence for an effect of interest. If the effect of interest relates to a factor, then it has to be defined using contrast coding.</p>
</div>
<div id="further-reading-5" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.7</span> Further reading<a href="ch-contr.html#further-reading-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A good discussion on contrast coding appears in Chapter 15 of <span class="citation">Baguley (<a href="#ref-baguley2012serious">2012</a>)</span>. A book-length treatment is by <span class="citation">Rosenthal, Rosnow, and Rubin (<a href="#ref-rosenthal2000contrasts">2000</a>)</span>. A brief discussion on contrast coding appears in <span class="citation">Venables and Ripley (<a href="#ref-venablesripley">2002</a>)</span>.</p>
</div>
<div id="sec-Contrastsexercises" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.8</span> Exercises<a href="ch-contr.html#sec-Contrastsexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:ContrastsPersian" class="exercise"><strong>Exercise 8.1  </strong></span>Contrast coding for a four-condition design</p>
</div>
<p>Load the following data. These data are from Experiment 1 in a set of reading studies on Persian <span class="citation">(Safavi, Husain, and Vasishth <a href="#ref-SafaviEtAlFrontiers2016">2016</a>)</span>. This is a self-paced reading study on particle-verb constructions, with a <span class="math inline">\(2\times 2\)</span> design: distance (short, long) and predictability (predictable, unpredictable). The data are from a critical region in the sentence. All the data from the <span class="citation">Safavi, Husain, and Vasishth (<a href="#ref-SafaviEtAlFrontiers2016">2016</a>)</span> paper are available from <a href="https://github.com/vasishth/SafaviEtAl2016" class="uri">https://github.com/vasishth/SafaviEtAl2016</a>.</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb499-1" data-line-number="1"><span class="kw">library</span>(bcogsci)</a>
<a class="sourceLine" id="cb499-2" data-line-number="2"><span class="kw">data</span>(<span class="st">&quot;df_persianE1&quot;</span>)</a>
<a class="sourceLine" id="cb499-3" data-line-number="3">dat1 &lt;-<span class="st"> </span>df_persianE1</a>
<a class="sourceLine" id="cb499-4" data-line-number="4"><span class="kw">head</span>(dat1)</a></code></pre></div>
<pre><code>##     subj item   rt distance   predability
## 60     4    6  568    short   predictable
## 94     4   17  517     long unpredictable
## 146    4   22  675    short   predictable
## 185    4    5  575     long unpredictable
## 215    4    3  581     long   predictable
## 285    4    7 1171     long   predictable</code></pre>
<p>The four conditions are:</p>
<ul>
<li>Distance=short and Predictability=unpredictable</li>
<li>Distance=short and Predictability=predictable</li>
<li>Distance=long and Predictability=unpredictable</li>
<li>Distance=long and Predictability=predictable</li>
</ul>
<p>The researcher wants to do the following sets of comparisons between condition means:</p>
<p>Compare the condition labeled Distance=short and Predictability=unpredictable with each of the following conditions:</p>
<ul>
<li>Distance=short and Predictability=predictable</li>
<li>Distance=long and Predictability=unpredictable</li>
<li>Distance=long and Predictability=predictable</li>
</ul>
<p>Questions:</p>
<ul>
<li>Which contrast coding is needed for such a comparison?</li>
<li>First, define the relevant contrast coding. Hint: You can do it by creating a condition column labeled a,b,c,d and then use a built-in contrast coding function.</li>
<li>Then, use the <code>hypr</code> library function to confirm that your contrast coding actually does the comparison you need.</li>
<li>Fit a simple linear model with the above contrast coding and display the slopes, which constitute the relevant comparisons.</li>
<li>Now, compute each of the four conditions’ means and check that the slopes from the linear model correspond to the relevant differences between means that you obtained from the data.</li>
</ul>
<div class="exercise">
<p><span id="exr:ContrastsNPIHelmert" class="exercise"><strong>Exercise 8.2  </strong></span>Helmert coding for a four-condition design.</p>
</div>
<p>Load the following data:</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb501-1" data-line-number="1"><span class="kw">library</span>(bcogsci)</a>
<a class="sourceLine" id="cb501-2" data-line-number="2"><span class="kw">data</span>(<span class="st">&quot;df_polarity&quot;</span>)</a>
<a class="sourceLine" id="cb501-3" data-line-number="3"><span class="kw">head</span>(df_polarity)</a></code></pre></div>
<pre><code>##   subject item condition times value
## 1       1    6         f   SFD   328
## 2       1   24         f   SFD   206
## 3       1   35         e   SFD   315
## 4       1   17         e   SFD   265
## 5       1   34         d   SFD   252
## 6       1    7         a   SFD   156</code></pre>
<p>The data come from an eyetracking study in German reported in <span class="citation">Vasishth et al. (<a href="#ref-VBLD07">2008</a>)</span>. The experiment is a reading study involving six conditions. The sentences are in English, but the original design was involved German sentences. In German, the word <em>durchaus</em> (certainly) is a positive polarity item: in the constructions used in this experiment, <em>durchaus</em> cannot have a c-commanding element that is a negative polarity item licensor. Here are the conditions:</p>
<ul>
<li>Negative polarity items
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Grammatical: No man who had a beard was ever thrifty.</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Ungrammatical (Intrusive NPI licensor): A man who had no beard was ever thrifty.</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Ungrammatical: A man who had a beard was ever thrifty.</li>
</ol></li>
</ul></li>
<li>Positive polarity items
<ul>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Ungrammatical: No man who had a beard was certainly thrifty.</li>
</ol></li>
<li><ol start="5" style="list-style-type: lower-alpha">
<li>Grammatical (Intrusive NPI licensor): A man who had no beard was certainly thrifty.</li>
</ol></li>
<li><ol start="6" style="list-style-type: lower-alpha">
<li>Grammatical: A man who had a beard was certainly thrifty.</li>
</ol></li>
</ul></li>
</ul>
<p>We will focus only on re-reading time in this data set. Subset the data so that we only have re-reading times in the data frame:</p>
<div class="sourceCode" id="cb503"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb503-1" data-line-number="1">dat2 &lt;-<span class="st"> </span><span class="kw">subset</span>(df_polarity, times <span class="op">==</span><span class="st"> &quot;RRT&quot;</span>)</a>
<a class="sourceLine" id="cb503-2" data-line-number="2"><span class="kw">head</span>(dat2)</a></code></pre></div>
<pre><code>##      subject item condition times value
## 6365       1   20         b   RRT   240
## 6366       1    3         c   RRT  1866
## 6367       1   13         a   RRT   530
## 6368       1   19         a   RRT   269
## 6369       1   27         c   RRT   845
## 6370       1   26         b   RRT   635</code></pre>
<p>The comparisons we are interested in are:</p>
<ul>
<li>What is the difference in reading time between negative polarity items and positive polarity items?</li>
<li>Within negative polarity items, what is the difference between grammatical and ungrammatical conditions?</li>
<li>Within negative polarity items, what is the difference between the two ungrammatical conditions?</li>
<li>Within positive polarity items, what is the difference between grammatical and ungrammatical conditions?</li>
<li>Within positive polarity items, what is the difference between the two grammatical conditions?</li>
</ul>
<p>Use the <code>hypr</code> package to specify the comparisons specified above, and then extract the contrast matrix. Finally, specify the contrasts to the condition column in the data frame. Fit a linear model using this contrast specification, and then check that the estimates from the model match the mean differences between the conditions being compared.</p>
<div class="exercise">
<p><span id="exr:ContrastsNcomparisons" class="exercise"><strong>Exercise 8.3  </strong></span>Number of possible comparisions in a single model.</p>
</div>
<ul>
<li>How many comparisons can one make in a single model when there is a single factor with four levels? Why can we not code four comparisons?</li>
<li>How many comparisons can one code in a model where there are two factors, one with 3 levels and one with 2 levels?</li>
<li>How about a model for a 2 x 2 x 3 design?</li>
</ul>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-baguley2012serious">
<p>Baguley, Thomas. 2012. <em>Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences</em>. Macmillan International Higher Education.</p>
</div>
<div id="ref-Bolker2018">
<p>Bolker, Ben. 2018. “Https://Github.com/Bbolker/Mixedmodels-Misc/Blob/Master/Notes/Contrasts.rmd.”</p>
</div>
<div id="ref-burkner2020modelling">
<p>Bürkner, Paul-Christian, and Emmanuel Charpentier. 2020. “Modelling Monotonic Effects of Ordinal Predictors in Bayesian Regression Models.” <em>British Journal of Mathematical and Statistical Psychology</em>. Wiley Online Library.</p>
</div>
<div id="ref-dobson2011introduction">
<p>Dobson, Annette J, and Adrian Barnett. 2011. <em>An Introduction to Generalized Linear Models</em>. CRC press.</p>
</div>
<div id="ref-friendly_matlib">
<p>Friendly, Michael, John Fox, and Phil Chalmers. 2020. <em>Matlib: Matrix Functions for Teaching and Learning Linear Algebra and Multivariate Statistics</em>. <a href="https://CRAN.R-project.org/package=matlib" class="uri">https://CRAN.R-project.org/package=matlib</a>.</p>
</div>
<div id="ref-heister2012analysing">
<p>Heister, Julian, Kay-Michael Würzner, and Reinhold Kliegl. 2012. “Analysing Large Datasets of Eye Movements During Reading.” <em>Visual Word Recognition</em> 2: 102–30.</p>
</div>
<div id="ref-rabe2020hypr">
<p>Rabe, Maximilian M., Shravan Vasishth, Sven Hohenstein, Reinhold Kliegl, and Daniel J Schad. 2020b. “Hypr: An R Package for Hypothesis-Driven Contrast Coding.” <em>Journal of Open Source Software</em> 5 (48): 2134.</p>
</div>
<div id="ref-R-MASS">
<p>Ripley, Brian. 2019. <em>MASS: Support Functions and Datasets for Venables and Ripley’s Mass</em>. <a href="https://CRAN.R-project.org/package=MASS" class="uri">https://CRAN.R-project.org/package=MASS</a>.</p>
</div>
<div id="ref-rosenthal2000contrasts">
<p>Rosenthal, Robert, Ralph L Rosnow, and Donald B Rubin. 2000. <em>Contrasts and Effect Sizes in Behavioral Research: A Correlational Approach</em>. Cambridge University Press.</p>
</div>
<div id="ref-SafaviEtAlFrontiers2016">
<p>Safavi, Molood Sadat, Samar Husain, and Shravan Vasishth. 2016. “Dependency Resolution Difficulty Increases with Distance in Persian Separable Complex Predicates: Implications for Expectation and Memory-Based Accounts.” <em>Frontiers in Psychology</em> 7 (403).</p>
</div>
<div id="ref-schad2020capitalize">
<p>Schad, Daniel J., Shravan Vasishth, Sven Hohenstein, and Reinhold Kliegl. 2019. “How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.” <em>Journal of Memory and Language</em> 110. <a href="https://doi.org/10.1016/j.jml.2019.104038" class="uri">https://doi.org/10.1016/j.jml.2019.104038</a>.</p> 2020. “How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.” <em>Journal of Memory and Language</em> 110. Elsevier: 104038.</p>
</div>
<div id="ref-VBLD07">
<p>Vasishth, Shravan, Sven Bruessow, Richard L. Lewis, and Heiner Drenhaus. 2008. “Processing Polarity: How the Ungrammatical Intrudes on the Grammatical.” <em>Cognitive Science</em> 32 (4, 4): 685–712.</p>
</div>
<div id="ref-VasishthEtAlFreq2021">
<p>Vasishth, Shravan, Daniel J. Schad, Audrey Bürki, and Reinhold Kliegl. 2021. <em>Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction</em>. CRC Press. <a href="https://vasishth.github.io/Freq_CogSci/" class="uri">https://vasishth.github.io/Freq_CogSci/</a>.</p>
</div>
<div id="ref-venablesripley">
<p>Venables, William N., and Brian D. Ripley. 2002. <em>Modern Applied Statistics with S-PLUS</em>. New York: Springer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="28">
<li id="fn28"><p>The reason for this is that mathematically, individual comparisons in the hypothesis matrix are coded as rows rather than as columns <span class="citation">(see Schad et al. <a href="#ref-schad2020capitalize">2020</a>)</span>.<a href="ch-contr.html#fnref28" class="footnote-back">↩</a></p></li>
<li id="fn29"><p>At this point, there is no need to understand in detail what this means. We refer the interested reader to <span class="citation">Schad et al. (<a href="#ref-schad2020capitalize">2020</a>)</span>. For a quick overview, we recommend a vignette explaining the generalized inverse in the <a href="https://cran.r-project.org/web/packages/matlib/vignettes/ginv.html">matlib package</a> <span class="citation">(Friendly, Fox, and Chalmers <a href="#ref-friendly_matlib">2020</a>)</span>.<a href="ch-contr.html#fnref29" class="footnote-back">↩</a></p></li>
<li id="fn30"><p>The function  from the  package is used to make the output more easily readable, and the function  is used to keep row and column names.<a href="ch-contr.html#fnref30" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-workflow.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-coding2x2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
