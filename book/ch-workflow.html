<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Workflow | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Workflow | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Workflow | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2023-02-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-priors.html"/>
<link rel="next" href="ch-contr.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b> Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b> Bayesian Regression Models using Stan: brms</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b> Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b> Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a><ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
<li class="chapter" data-level="3.11" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect finger tapping times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b> No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a><ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#model-building"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#exemplary-data-analysis"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-workflow.html"><a href="ch-workflow.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-workflow.html"><a href="ch-workflow.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a><ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-7"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-9"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparameterization</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-10"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a><ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b> Validation of a computed posterior distribution</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#simulation-based-calibration-revealing-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> Simulation-based calibration revealing a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-and-limitation-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues and limitation of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-re-implementing-the-exponential-distribution-manually"><i class="fa fa-check"></i><b>12.3</b> Another custom distribution: Re-implementing the exponential distribution manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-11"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-12"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a><ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a><ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a><ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a><ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="15.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>15.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#summary-13"><i class="fa fa-check"></i><b>15.6</b> Summary</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.7</b> Further reading</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a><ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#issues-with-cross-validation"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a><ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-14"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a><ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-15"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
<li class="chapter" data-level="18.5" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a><ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a><ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-16"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a><ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a><ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-17"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-workflow" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 7</span> Workflow<a href="ch-workflow.html#ch-workflow" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Although modern Bayesian analysis tools (such as <code>brms</code>) greatly facilitate Bayesian computations, the model specification is still (as it should be) the responsibility of the user. In the previous chapters (e.g., chapter <a href="ch-compbda.html#ch-compbda">3</a>), we have outlined some of the steps needed to arrive at a useful and robust analysis. In this chapter, we bring these ideas together to spell out a principled approach to developing a workflow. This chapter is based on a recent introduction of a principled Bayesian workflow to cognitive science <span class="citation">(Schad, Betancourt, and Vasishth <a href="#ref-schad2019towardarXiv">2019</a>, for a revised published version see <a href="#ref-schad2020toward">2020</a>)</span>.</p>
<p>Much research has been carried out in recent years to develop tools to ensure robust Bayesian data analyses <span class="citation">(e.g., Gabry et al. <a href="#ref-Gabry:2017aa">2017</a>; Talts et al. <a href="#ref-talts2018validating">2018</a>)</span>. One of the most recent end-products of this research has been the formulation of a principled Bayesian workflow for conducting a probabilistic analysis <span class="citation">(Betancourt <a href="#ref-Betancourt:2018aa">2018</a>; Schad, Betancourt, and Vasishth <a href="#ref-schad2019towardarXiv">2019</a>)</span>. This workflow provides an initial coherent set of steps to take for a robust analysis, leaving room for further improvements and methodological developments. At an abstract level, parts of this workflow can be applied to any kind of data analysis, be it frequentist or Bayesian, be it based on sampling or on analytic procedures.</p>
<p>Here, we introduce parts of this principled Bayesian workflow by illustrating its use with experimental data from a reading-time experiment. <!-- We demonstrate how to implement analyses in R, and use the R package `brms` [@Buerkner2017brms] for statistical analysis.  -->Some parts of this principled Bayesian workflow are specifically recommended when using advanced/non-standard models.</p>
<p>A number of questions should be asked when fitting a model, and several checks should be performed to validate a probabilistic model. Before going into the details of this discussion, we first treat the process of model building, and how different traditions have yielded different approaches to this questions.</p>
<div id="model-building" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.1</span> Model building<a href="ch-workflow.html#model-building" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One strategy for model building is to start with a minimal model that captures just the phenomenon of interest but not much other structure in the data. For example, this could be a linear model with just the factor or covariate of main interest. For this model, we perform a number of checks described in detail in the following sections. If the model passes all checks and does not show signs of inadequacy, then it can be applied in practice and we can be confident that the model provides reasonably robust inferences on our scientific question. However, if the model shows signs of trouble on one or more of these checks, then the model may need to be improved. Alternatively, we may need to be more modest with respect to our scientific question. For example, in a repeated measures data set, we may be interested in estimating the correlation parameter between by-group adjustments (their random effects correlation) based on a sample of <span class="math inline">\(30\)</span> subjects. If model analysis reveals that our sample size is not sufficiently large to estimate the correlation reliably, then we may need to either increase our sample size, or give up on our plan.</p>
<p>During the model building process, we make use of an aspirational model <span class="math inline">\(\mathcal{M}_A\)</span>: we mentally imagine a model with all the possible details that the phenomenon and measurement process contain; i.e., we imagine a model that one would fit if there were no limitations in resources, time, mathematical and computational tools, subjects, and so forth. It would contain all systematic effects that might influence the measurement process. For example, influences of time or heterogeneity across individuals. This should be taken to guide and inform model development; such a procedure prevents random walks in model space during model development. The model has to consider both the latent phenomenon of interest as well as the environment and experiment used to probe it.</p>
<p>In contrast to the aspirational model, the initial model <span class="math inline">\(\mathcal{M}_1\)</span> may only contain enough structure to incorporate the phenomenon of core scientific interest, but none of the additional aspects/structures relevant for the modeling or measurement. The additional, initially left-out structures, which reflect the difference between the initial (<span class="math inline">\(\mathcal{M}_1\)</span>) and the aspirational model (<span class="math inline">\(\mathcal{M}_A\)</span>), can then be probed for using specifically designed summary statistics. These summary statistics can thus inform model expansion from the initial model <span class="math inline">\(\mathcal{M}_1\)</span> into the direction of the aspirational model <span class="math inline">\(\mathcal{M}_A\)</span>. If the initial model proves inadequate, then the aspirational model and the associated summary statistics guide model development. If the expanded model is still not adequate, then another cycle of model development is conducted.</p>
<p>The range of prior and posterior predictive checks discussed in the following sections serve as a basis for a principled approach to model expansion. The notion of <em>expansion</em> is critical here. If an expanded model does not prove more adequate, one can always fall back to the previous model version.</p>
<p>Some researchers suggest an alternative strategy of fitting models with all the group-level variance components (e.g., by-participant and by-items) allowed by the experimental design and a full variance covariance matrix for all the group-level parameters (as we did in section <a href="ch-hierarchical.html#sec-sih">5.2.5</a>).
<!-- As an alternative analysis strategy, a tradition in the cognitive and other experimental sciences relies on *maximal models* for a given experimental design [@barr2013]. This maximal model contains all effects from experimental manipulations (main effects and interactions) as well as differences in these effects varying across subjects, items, or other random factors in the experimental design. --> This type of model is sometimes called a “maximal” model <span class="citation">(e.g., Barr et al. <a href="#ref-barr2013">2013</a>)</span>. However, this model is maximal within the scope of a linear regression. In section <a href="ch-hierarchical.html#sec-distrmodel">5.2.6</a>, for example, we saw distributional models, which are more complex than the so-called maximal models. However, a maximal model can provide an alternative starting point for the principled Bayesian workflow. In this case, the focus does not lie so much on model expansion. Instead, for maximal models the workflow can be used to specify priors encoding domain expertise (possibly to ensure computational faithfulness and model sensitivity), and to ensure model adequacy. Some steps in the principled Bayesian workflow (e.g., the computationally more demanding steps of assessing computational faithfulness and model sensitivity) may even be performed only for models coded in Stan or only once for a given research program, where similar designs are repeatedly used. We will explain this in more detail below.</p>
<p>In the maximal model, “maximal” refers to maximal specification of the variance components within the scope of the linear regression approximation, not maximal with respect to the actual data generating process. Models that are bound by the linear regression structure cannot capture effects such as selection bias in the data, dynamical changes in processes across time, or measurement error. Importantly, the “maximal” models are not the aspirational model, which is an image of the true data generating process. <!-- but rather just an approximation. Indeed, aiming to formulate models closer to the aspiration model, which go beyond the linear model framework, may be one reason to consider investing in learning to express models directly within probabilistic programming languages such as Stan instead of the limited range of models provided in packages like `brms`. --></p>
<p>Finally, sometimes the results from the Bayesian workflow will show that our experimental design or data is not sufficient to answer our scientific question at hand. In this case, ambition needs to be reduced, or new data needs to be collected, possibly with a different experimental design more sensitive to the phenomenon of interest.</p>
<p>One important development in open science practices is pre-registration of experimental analyses before the data are collected <span class="citation">(Chambers <a href="#ref-chambers2019seven">2019</a>)</span>. This can be done using online-platforms such as the <a href="https://osf.io/">Open Science Foundation</a> or <a href="https://aspredicted.org/">AsPredicted</a> <span class="citation">(but see Szollosi et al. <a href="#ref-szollosi2019preregistration">2020</a>)</span>. What information can or should one document in preregistration of the Bayesian workflow? If one plans on using the maximal model for analysis, then this maximal model, including contrast coding <span class="citation">(Schad et al. <a href="#ref-schad2020capitalize">2020</a>)</span>, population- and group-level effects (also known as fixed and random effects) should be described. In the case of incremental model building, if a model isn’t a good fit to the data, then any resulting inference will be limited if not useless, so a rigid preregistration is useless unless one knows exactly what the model is. Thus, the deeper issue with preregistration is that a model cannot be confirmed until the phenomenon <em>and</em> experiment are all extremely well understood.
One practical possibility is to describe the initial and the aspirational model, and the incremental strategy used to probe the initial model to move more towards the aspirational model. This can also include delineation of summary statistics that one plans to use for probing the tested models. Even if it is difficult to spell out the aspirational model fully, it can be useful to preregister the initial model, summary statistics, and the principles one intends to apply in model selection.
Although the maximal modeling approach clearly reflects confirmatory hypothesis testing, the incremental model building strategy towards the aspirational model may be seen as lying at the boundary between confirmatory and exploratory, and becomes more confirmatory the more clearly the aspirational model can be spelled out a priori.</p>
</div>
<div id="principled-questions-on-a-model" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.2</span> Principled questions on a model<a href="ch-workflow.html#principled-questions-on-a-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What characterizes a useful probabilistic model? A useful probabilistic model should be consistent with domain expertise. Moreover, a useful probabilistic model should be rich enough to capture the structure of the true data generating process needed to answer scientific questions. When very complex or non-standard models are developed, there are two additional requirements that must be met (we will briefly touch upon these in the present chapter): it is key for the model to allow accurate posterior approximation, and the model must capture enough of the experimental design to give useful answers to our questions.</p>
<p>So what can we do aiming to meet these properties of our probabilistic model? In the following, we will outline a number of analysis steps to take and questions to ask in order to improve these properties for our model.</p>
<p>In a first step, we will use prior predictive checks to investigate whether our model is consistent with our domain expertise. Moreover, posterior predictive checks assess model adequacy for the given data set, that is, they investigate the question whether the model captures the relevant structure of the true data generating process.
We will also briefly discuss two additional steps that are computationally very expensive, and can be used e.g., when coding advanced/non-standard models, but which are also part of the principled workflow: this includes investigating computational faithfulness by studying whether posterior estimation is accurate, and it includes studing model sensitivity and the question whether we can recover model parameters with the given design and model.</p>
<div id="prior-predictive-checks-checking-consistency-with-domain-expertise" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.1</span> Prior predictive checks: Checking consistency with domain expertise<a href="ch-workflow.html#prior-predictive-checks-checking-consistency-with-domain-expertise" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first key question for checking the model is whether the model and the distributions of prior parameters are consistent with domain expertise. Prior distributions can be selected based on prior research or plausibility. However, for complex models it is often difficult to know which prior distributions should be chosen, and what consequences distributions of prior model parameters have for expected data. A viable solution is to use prior distributions to simulate hypothetical data from the model and to check whether the simulated data are plausible and consistent with domain expertise. This approach is often much easier to judge compared to assessing prior distributions in complex models directly.</p>
<p>In practice, this approach can be implemented by the following steps:</p>
<ol style="list-style-type: decimal">
<li>Take the prior <span class="math inline">\(p(\boldsymbol{\Theta})\)</span> and randomly draw a parameter set <span class="math inline">\(\boldsymbol{\Theta_{pred}}\)</span> from it: <span class="math inline">\(\boldsymbol{\Theta_{pred}} \sim p(\boldsymbol{\Theta})\)</span></li>
<li>Use this parameter set <span class="math inline">\(\boldsymbol{\Theta_{pred}}\)</span> to simulate hypothetical data <span class="math inline">\(\boldsymbol{y_{pred}}\)</span> from the model: <span class="math inline">\(\boldsymbol{y_{pred}} \sim p(\boldsymbol{y} \mid \boldsymbol{\Theta_{pred}})\)</span></li>
</ol>
<p>To assess whether prior model predictions are consistent with domain expertise, it is useful to compute summary statistics of the simulated data <span class="math inline">\(t(\boldsymbol{y_{pred}})\)</span>. The distribution of these summary statistics can be visualized using, for example, histograms (see Figure <a href="ch-workflow.html#fig:figPriorPredCh">7.1</a>). This can quickly reveal whether the data falls in an expected range, or whether a substantial amount of extreme data points are expected a priori. For example, in a study using self-paced reading times, extreme values may be considered to be reading times smaller than <span class="math inline">\(50\)</span> ms or larger than <span class="math inline">\(2000\)</span> ms. Reading times for a word larger than <span class="math inline">\(2000\)</span> ms are not impossible, but would be implausible and largely inconsistent with domain expertise. Experience with reading studies shows that a small number of observations may actually take extreme values. However, if we observe a large number of extreme data points in the hypothetical data, and if these are inconsistent with domain expertise, the priors or the model should be adjusted so that they yield hypothetical data within the range of reasonable values.</p>
<div class="figure"><span style="display:block;" id="fig:figPriorPredCh"></span>
<img src="bookdown_files/figure-html/figPriorPredCh-1.svg" alt="Prior predictive checks. a) In a first step, define a summary statistic that one wants to investigate. b) Second, define extremity thresholds (shaded areas), beyond which one does not expect a lot of data to be observed. c) Third, simulate prior model predictions for the data (histogram) and compare them with the extreme values (shaded areas)." width="864" />
<p class="caption">
FIGURE 7.1: Prior predictive checks. a) In a first step, define a summary statistic that one wants to investigate. b) Second, define extremity thresholds (shaded areas), beyond which one does not expect a lot of data to be observed. c) Third, simulate prior model predictions for the data (histogram) and compare them with the extreme values (shaded areas).
</p>
</div>
<!--\FloatBarrier-->
<p>Choosing good summary statistics is more an art than a science. However, the choice of summary statistics will be crucial, as they provide key markers of what we want the model to account for in the data. They should thus be carefully chosen and designed based on the expectations that we have about the true data generating process and about the kinds of structures and effects we expect the data may exhibit. Interestingly, summary statistics can also be used to critique the model: if someone wants to criticize an analysis, then they can formalize that criticism into a summary statistic they expect to show undesired behavior. Such criticism can serve as a very constructive way to write reviews in a peer-review setting. Here, we will show some examples of useful summary statistics below when discussing data analysis for a concrete example data set.</p>
<p>Choosing good priors will be particularly relevant in cases where the likelihood is not sufficiently informed by the data (see Figure <a href="ch-workflow.html#fig:FigBayes">7.2</a>, in particular g-i). In hierarchical models, for example, this often occurs in cases where a “maximal” model is fitted for a small data set that does not constrain estimation of all group-level effects variance and covariance parameters.<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a></p>
<p>In such situations, using a prior in a Bayesian analysis (or a more informative prior rather than a relatively uninformative one) should incorporate just enough domain expertise to suppress extreme, although not impossible parameter values. This may allow the model to be fit, as the posterior is now sufficiently constrained. Thus, introducing prior information in Bayesian computation allows us to fit and interpret models that cannot be validly estimated using frequentist tools.</p>
<p>A welcome side-effect of incorporating more domain expertise (into what still constitutes weakly informative priors) is thus more concentrated prior distributions, which can facilitate Bayesian computation. This allows more complex models to be estimated; that is, using prior knowledge can make it possible to fit models that could otherwise not be estimated using the available tools. In other words, incorporating prior knowledge can allow us to get closer to the aspirational model in the iterative model building procedure. Moreover, more informative priors also lead to faster convergence of MCMC algorithms.</p>
<div class="figure"><span style="display:block;" id="fig:FigBayes"></span>
<img src="bookdown_files/figure-html/FigBayes-1.svg" alt="The role of priors for informative and uninformative data. a)-c) When the data provides good information via the likelihood (b), then a flat uninformative prior (a) is sufficient to obtain a concentrated posterior (c). d)-f) When the data does not sufficiently constrain the parameters through the likelihood (e), then using a flat uninformative prior (d) also leaves the posterior (f) widely spread out. g)-i) When the data does not constrain the parameter through the likelihood (h), then including domain expertise through an informative prior (g) can help to constrain the posterior (i) to reasonable values." width="576" />
<p class="caption">
FIGURE 7.2: The role of priors for informative and uninformative data. a)-c) When the data provides good information via the likelihood (b), then a flat uninformative prior (a) is sufficient to obtain a concentrated posterior (c). d)-f) When the data does not sufficiently constrain the parameters through the likelihood (e), then using a flat uninformative prior (d) also leaves the posterior (f) widely spread out. g)-i) When the data does not constrain the parameter through the likelihood (h), then including domain expertise through an informative prior (g) can help to constrain the posterior (i) to reasonable values.
</p>
</div>
<!--\FloatBarrier-->
<p>Incorporating more domain expertise into the prior also has crucial consequences for Bayesian modeling when computing Bayes factors (see chapter <a href="ch-bf.html#ch-bf">15</a>, on Bayes factors). <!--
BN: All this appears too soon, we discuss it in the BF chapter
Bayes factors are highly sensitive to the prior, and in particular to the prior uncertainty. Priors are thus never uninformative when it comes to Bayes factors. Choosing (relatively) uninformative priors makes it very difficult to find posterior evidence in favor of an expanded model, and will often support the simpler model. For example, in nested model comparison of hierarchical models, large prior uncertainty implies the assumption that the effect of interest could be very (implausibly) large. Using Bayes factors for such nested model comparison with high prior uncertainty thus tests whether there is evidence for a very big effect size for the predictor term in question, which is usually not supported by the data (because the relatively uninformative prior covers implausibly large effect sizes). When using a weakly informative or even an informative prior with much smaller uncertainty, the Bayes factor tests whether there is evidence for a small effect of the additional predictor term, which is much more likely to be the case. Thus, using prior knowledge and specifying priors with reasonable uncertainty (rather than uninformative priors with large uncertainty) are crucial when carrying out model comparison using Bayes factors [@SchadEtAlBF]. --></p>
<p>Importantly, this first step simulates from the <em>prior predictive distribution</em>, which specifies how the prior interacts with the likelihood. Mathematically, it computes an average (the integral) over different possible (prior) parameter values. The prior predictive distribution is (also see chapter <a href="ch-compbda.html#ch-compbda">3</a>):</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p(\boldsymbol{y_{pred}}) &amp;= \int p(\boldsymbol{y_{pred}}, \boldsymbol{\Theta}) \; d\boldsymbol{\Theta} = \int p(\boldsymbol{y_{pred}} \mid \boldsymbol{\Theta}) p(\boldsymbol{\Theta}) \; d\boldsymbol{\Theta}\\ &amp;= \int \mathrm{likelihood}(\boldsymbol{y_{pred}} \mid \boldsymbol{\Theta}) \cdot \mathrm{prior}(\boldsymbol{\Theta}) \; d\boldsymbol{\Theta}
\end{aligned}
\end{equation}\]</span></p>
<p>As a concrete example, suppose we assume that our likelihood is a Normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. Suppose that we now define the following priors on the parameters: <span class="math inline">\(\mu \sim \mathit{Normal}(0,1)\)</span>, and <span class="math inline">\(\sigma \sim \mathit{Uniform}(1,2)\)</span>. We can generate the prior predictive distribution using the following steps:</p>
<ul>
<li>Do the following 100000 times:
<ul>
<li>Take one sample m from a Normal(0,1) distribution</li>
<li>Take one sample s from a Uniform(1,2) distribution</li>
<li>Generate and save a data point from Normal(m,s)</li>
<li>The generated data is the prior predictive distribution.</li>
</ul></li>
</ul>
<p>More complex generative processes involving repeated measures data can also be defined.</p>
</div>
<div id="computational-faithfulness-testing-for-correct-posterior-approximations" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.2</span> Computational faithfulness: Testing for correct posterior approximations<a href="ch-workflow.html#computational-faithfulness-testing-for-correct-posterior-approximations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- A key aim in Bayesian data analysis is to compute posterior expectations, such as the posterior mean or posterior credible intervals (quantiles) of some parameter. For some simple models and prior distributions, these posterior expectations can be computed exactly by analytical derivation. However, this is not possible in most interesting and realistic, more complex models, where analytical solutions cannot be computed. Instead, computational approximations are needed for estimation. Here, we use one option: although it is often not possible to compute the posterior exactly, it is possible to draw samples from it, and we accordingly use (Markov Chain Monte Carlo) sampling to approximate posterior expectations. -->
<p>Approximations of posterior expectations can be inaccurate. For example, a computer program that is designed to sample from a posterior can be erroneous. This could involve an error in the specification of the likelihood (e.g., caused by an error in the R syntax formula), or insufficient sampling of the full density of the posterior. The sampler may be biased, sampling parameter values that are larger or smaller than the true posterior, or the variance of the posterior samples may be larger or smaller than the true posterior uncertainty. However, posterior sampling from simple and standard models should work properly in most cases. Thus, we think that in many applications, a further check of computational faithfulness may be asking for too much, and might need to be performed only once for a given research program, where different experiments are rather similar to each other. However, checking computational faithfulness can become an important issue when dealing with more advanced/non-standard models (such as those discussed in the later chapters of this book). Here, errors in the specification of the likelihood can occur more easily.</p>
<p>Given that posterior approximations can be inaccurate, it is important to design a procedure to test whether the posterior approximation of choice is indeed accurate, e.g., that the software used to implement the sampling works without errors for the specific problem at hand. This checking can be performed using simulation-based calibration <span class="citation">(SBC; Talts et al. <a href="#ref-talts2018validating">2018</a>; Schad, Betancourt, and Vasishth <a href="#ref-schad2019towardarXiv">2019</a>)</span>. This is a very simulation-intensive procedure, which can take a long time to run for considerably complex models and larger data sets. We do not discuss SBC in detail here, but refer the reader to its later treatment in chapter <a href="ch-MPT.html#ch-MPT">18</a>, where SBC is applied for models coded in Stan directly, as well as to the description in <span class="citation">Schad, Betancourt, and Vasishth (<a href="#ref-schad2019towardarXiv">2019</a>)</span>.</p>
<!-- Even powerful tools like Hamiltonian Monte Carlo (HMC) sampling do not always provide accurate posterior estimation. For a given experimental design, model, and data set, it is therefore important to check computational faithfulness.  -->
<p>Assuming that our posterior computations are accurate and faithful, we can take a next step, namely looking at the sensitivity of the model analyses.</p>
</div>
<div id="model-sensitivity" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.3</span> Model sensitivity<a href="ch-workflow.html#model-sensitivity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What can we realistically expect from the posterior of a model, and how can we check whether these expectations are justified for the current setup? First, we might expect that the posterior recovers the true parameters generating the data without bias. That is, when we simulate hypothetical data based on a true parameter value, we may expect that the posterior mean is close to the true value. However, for a given model, experimental design, and data set, this expectation may or may not be justified. Indeed, parameter estimation for some, e.g., non-linear, models may be biased, such that the true value of the parameter can practically not be recovered from the data. At the same time, we might expect from the posterior that it is highly informative with respect to the parameters that generated the data. That is, we may hope for small posterior uncertainty (a small posterior standard deviation) relative to our prior knowledge. However, posterior certainty may sometimes be low. Some experimental designs, models, or data sets may yield highly uninformative estimates, where uncertainty is not reduced compared to our prior information. This can be the case when we have very little data, or when the experimental design does not allow us to constrain certain model parameters; i.e., the model is too complex for the experimental design.</p>
<p>To study model sensitivity, one can investigate two questions about the model:</p>
<ol style="list-style-type: decimal">
<li>How well does the estimated posterior mean match the true simulating parameter?</li>
<li>How much is uncertainty reduced from the prior to the posterior?</li>
</ol>
<p>To investigate these questions, it is again possible to perform extensive simulation studies.
This is crucial to do for complex, non-standard, or cognitive models, but may be less important for simpler and more standard models.
Indeed, the same set of simulations can be used that are also used in SBC. Therefore, both analyses can be usefully applied in tandem. Again, here we skip the details of how these computations can be implemented, and refer the interested reader to <span class="citation">Schad, Betancourt, and Vasishth (<a href="#ref-schad2019towardarXiv">2019</a>)</span>.</p>
</div>
<div id="posterior-predictive-checks-does-the-model-adequately-capture-the-data" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.4</span> Posterior predictive checks: Does the model adequately capture the data?<a href="ch-workflow.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>“<em>All models are wrong but some are useful.</em>” <span class="citation">(Box <a href="#ref-box1979robustness">1979</a>, 2)</span>. We know that our model probably does not fully capture the true data generating process, which is noisily reflected in the observed data. Our question therefore is whether our model is close enough to the true process that has generated the data, and whether the model is useful for informing our scientific question. To compare the model to the true data generating process (i.e., to the data), we can simulate data from the model and compare the simulated to the real data. This can be formulated via a posterior predictive distribution (see chapter <a href="ch-compbda.html#ch-compbda">3</a>): the model is fit to the data, and the estimated posterior model parameters are used to simulate new data.</p>
<p>Mathematically, the posterior predictive distribution is written:</p>
<p><span class="math display">\[\begin{equation}
p(\boldsymbol{y_{pred}} \mid \boldsymbol{y}) = \int p(\boldsymbol{y_{pred}} \mid \boldsymbol{\Theta}) p(\boldsymbol{\Theta} \mid \boldsymbol{y}) \; d \boldsymbol{\Theta} 
\end{equation}\]</span></p>
<p></p>
<p>Here, the observed data <span class="math inline">\(\boldsymbol{y}\)</span> is used to infer the posterior distribution over model parameters,
<span class="math inline">\(p(\boldsymbol{\Theta} \mid \boldsymbol{y})\)</span>. This is combined with the model or likelihood function, <span class="math inline">\(p(\boldsymbol{y_{pred}} \mid \boldsymbol{\Theta})\)</span>, to yield new, now simulated, data, <span class="math inline">\(\boldsymbol{y_{pred}}\)</span>. The integral <span class="math inline">\(\int d \boldsymbol{\Theta}\)</span> indicates averaging across different possible values for the posterior model parameters (<span class="math inline">\(\boldsymbol{\Theta}\)</span>).</p>
<p>As mentioned in chapter <a href="ch-compbda.html#ch-compbda">3</a>, we can’t evaluate this integral exactly: <span class="math inline">\(\boldsymbol{\Theta}\)</span> can be a vector of many parameters, making this a very complicated integral with no analytical solution. However, we can approximate it using sampling. <!-- Specifically, we can obtain samples from the posterior distribution, e.g., using HMC or a different MCMC sampling scheme. --> We can now use each of the posterior samples as parameters to simulate new data from the model. This procedure then approximates the integral and yields an approximation to the posterior predictive distribution.</p>
<p>To summarize, in the posterior predictive distribution, the model is fit to the data, and the estimated posterior model parameters are used to simulate new data. Critically, the question then is how close the simulated data is to the observed data.</p>
<p>One approach is to use features of the data that we care about, and to test how well the model can capture these features. Indeed, we had already defined summary statistics in the prior predictive checks. We can now compute these summary statistics for the data simulated from the posterior predictive distribution. This will yield a distribution for each summary statistic. In addition, we compute the summary statistic for the observed data, and can now check whether the data falls within the distribution of the model predictions (cf. Figure <a href="ch-workflow.html#fig:FigPostPredCh">7.3</a>a), or whether the model predictions are far from the observed data (see Figure <a href="ch-workflow.html#fig:FigPostPredCh">7.3</a>b). If the observed data is similar to the posterior-predicted data, then this supports model adequacy. If we observe a large discrepancy, then this indicates that our model likely is missing some important structure of the true process that has generated the data, and that we have to use our domain expertise to further improve the model. Alternatively, a large discrepancy can be due to the data being an extreme observation, which was nevertheless generated by the process captured in our model. In general, we can’t discriminate between these two possibilities. Consequently, we have to use our best judgement as to which possibility is more relevant, in particular changing the model only if the discrepancy is consistent with a known missing model feature.</p>
<div class="figure"><span style="display:block;" id="fig:FigPostPredCh"></span>
<img src="bookdown_files/figure-html/FigPostPredCh-1.svg" alt="Posterior predictive checks. Compare posterior model predictions (histogram) with observed data (vertical line) for a specific summary statistic, t(y). a) This displays a case where the observed summary statistic (vertical line) lies within the posterior model predictions (histogram). b) This displays a case where the summary statistic of the observed data (vertical line) lies clearly outside of what the model predicts a posteriori (histogram)." width="480" />
<p class="caption">
FIGURE 7.3: Posterior predictive checks. Compare posterior model predictions (histogram) with observed data (vertical line) for a specific summary statistic, t(y). a) This displays a case where the observed summary statistic (vertical line) lies within the posterior model predictions (histogram). b) This displays a case where the summary statistic of the observed data (vertical line) lies clearly outside of what the model predicts a posteriori (histogram).
</p>
</div>
<!--\FloatBarrier-->
</div>
</div>
<div id="exemplary-data-analysis" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.3</span> Exemplary data analysis<a href="ch-workflow.html#exemplary-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We perform an exemplary analysis of a data set from <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span>, a data set that we have already encountered in previous chapters. The methodology they used is the familiar method (self-paced reading) that we encountered in earlier chapters. <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span> collected self-paced reading data using Chinese relative clauses. Relative clauses are sentences like: <em>The student who praised the teacher was very happy</em>. Here, the head noun, <em>student</em>, is modified by a relative clause <em>whoteacher</em>, and the head noun is the subject of the relative clause as well: the student praised the teacher. Such relative clauses are called subject relatives. By contrast, one can also have object relative clauses, where the head noun is modified by a relative clause which takes the head noun as an object. An example is: <em>The student whom the teacher praised was very happy</em>. Here, the teacher praised the student.
<span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span> were interested in testing the hypothesis that Chinese shows an object relative (OR) processing advantage compared to the corresponding subject relative (SR). The theoretical reason for this processing advantage is that, in Chinese, the distance (which can be approximated by counting the number of words intervening) between the relative clause verb (<em>praised</em>) and the head noun is shorter in ORs than SRs . This prediction arises because, unlike English, the relative clause appears before the head noun in Chinese; see <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span> for a detailed explanation.</p>
<p>Their experimental design had one factor with two levels: (i) object relative sentences, and (ii) subject relative sentences. We use sum coding (-1, +1) for this factor, which we call <code>so</code>, an abbreviation for subject-object. Following <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span>, we analyze reading time on the target word, which was the head noun of the relative clause. As mentioned above, in Chinese, the head noun appears after the relative clause. By the time the subject reads the head noun, they already know whether they are reading a subject or an object relative. Because the distance between the relative clause verb and the head noun is shorter in Chinese object relatives compared to subject relatives, reading the head noun is expected to be easier in object relatives.</p>
<p>The data set contains reading time measurements in milliseconds from <span class="math inline">\(37\)</span> subjects and from <span class="math inline">\(15\)</span> items (there were <span class="math inline">\(16\)</span> items originally, but one item was removed during analysis). The design is a classic repeated measures Latin square design.</p>
<div id="prior-predictive-checks" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.3.1</span> Prior predictive checks<a href="ch-workflow.html#prior-predictive-checks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first step in Bayesian data analysis is to specify the statistical model and the priors for the model parameters. As a statistical model, we use what is called the maximal model <span class="citation">(Barr et al. <a href="#ref-barr2013">2013</a>)</span> for the design. Such a model includes population-level effects for the intercept and the slope (coded using sum contrast coding: <span class="math inline">\(+1\)</span> for object relatives, and <span class="math inline">\(-1\)</span> for subject relatives), correlated group-level intercepts and slopes for subjects, and correlated group-level intercepts and slopes for items. We define the likelihood as follows:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim \mathit{LogNormal}(\alpha + u_{subj[n],1} + w_{item[n],1} + so_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma)
\end{equation}\]</span></p>
<p>In <code>brms</code> syntax, the model would be specified as follows:</p>
<p><code>rt ~  so + ( so | subj) + (so | item), family = lognormal()</code>.</p>
<p>Because we assume a possible correlation between group-level (or random) effects, the adjustments to the intercept and slope for subjects and items have multivariate (in this case, bivariate) normal distributions with means zero, as in Equation <a href="ch-workflow.html#eq:adjstr">(7.1)</a>.</p>
<p><span class="math display" id="eq:adjstr">\[\begin{equation}
 \begin{aligned}
    {\begin{pmatrix}
    u_{i,1} \\
    u_{i,2}
    \end{pmatrix}}
   &amp;\sim {\mathcal {N}}
    \left(
   {\begin{pmatrix} 
    0\\
    0
   \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right) \\
     {\begin{pmatrix}
    w_{j,1} \\
    w_{j,2}
    \end{pmatrix}}
   &amp;\sim {\mathcal {N}}
    \left(
   {\begin{pmatrix} 
    0\\
    0
   \end{pmatrix}}
 ,\boldsymbol{\Sigma_w} \right) 
 \end{aligned}
\tag{7.1}
 \end{equation}\]</span></p>
<p>One possible standard setup for (relatively) uninformative priors which is sometimes used in reading studies <span class="citation">(e.g., Paape, Nicenboim, and Vasishth <a href="#ref-Paape:2017aa">2017</a>; Vasishth, Mertzen, Jäger, et al. <a href="#ref-Vasishth2018aa">2018</a><a href="#ref-Vasishth2018aa">b</a>)</span> is as follows:</p>
<p><span class="math display">\[\begin{align}
\alpha &amp;\sim \mathit{Normal}(0, 10)\\
\beta &amp;\sim \mathit{Normal}(0, 1)\\
\sigma &amp;\sim \mathit{Normal}_+(0, 1)\\
\tau_{\{1,2\}} &amp;\sim \mathit{Normal}_+(0, 1)\\
\rho  &amp;\sim \mathit{LKJ}(2)
\end{align}\]</span></p>
<p>We define these priors in <code>brms</code> syntax as follows:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb356-1" data-line-number="1">priors &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb356-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb356-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> so),</a>
<a class="sourceLine" id="cb356-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb356-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb356-6" data-line-number="6">  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb356-7" data-line-number="7">)</a></code></pre></div>
<p>For the intercept (<span class="math inline">\(\alpha\)</span>) we use a normal distribution with mean <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(10\)</span>. This is on the log scale as we assume a log-normal distribution of reading times. That is, this approach assumes a priori that the intercept for reading times varies between <span class="math inline">\(0\)</span> seconds and (one standard deviation) <span class="math inline">\(exp(10) = 22026\)</span> ms (i.e., <span class="math inline">\(22\)</span> sec) or (two standard deviations) <span class="math inline">\(exp(20) = 485165195\)</span> ms (i.e., <span class="math inline">\(135\)</span> hours). Going from seconds to hours within one standard deviation shows how uninformative this prior is.</p>
<p>Moreover, for the effect of linguistic manipulations on reading times (<span class="math inline">\(\beta\)</span>), one common standard prior is to assume a mean of <span class="math inline">\(0\)</span> and a standard deviation of <span class="math inline">\(1\)</span> (also on the log scale). The prior on the effect size on log scale is a multiplicative factor, that is, the prediction for the effect size depends on the intercept.
For an intercept of <span class="math inline">\(exp(6) = 403\)</span> ms, a variation to one standard deviation above multiplies the base effect by <span class="math inline">\(2.71\)</span>, increasing the mean from <span class="math inline">\(403\)</span> to <span class="math inline">\(exp(6) \times exp(1) = 1097\)</span>. Likewise a variation to one standard deviation below multiplies the base effect by <span class="math inline">\(1 / 2.71\)</span>, decreasing the mean from <span class="math inline">\(403\)</span> to <span class="math inline">\(exp(6) \times exp(-1) = 148\)</span>.
This effect size is strongly changed when assuming a different intercept: for a slightly smaller value for the intercept of <span class="math inline">\(exp(5) = 148\)</span> ms, the expected condition difference is reduced to <span class="math inline">\(37\)</span>% (<span class="math inline">\(349\)</span> ms), and for a slightly larger value for the intercept of <span class="math inline">\(exp(7) = 1097\)</span> ms, the condition difference is enhanced to <span class="math inline">\(272\)</span>% (<span class="math inline">\(2578\)</span> ms). Also see Box <a href="ch-reg.html#thm:lognormal">4.3</a> in chapter <a href="ch-reg.html#ch-reg">4</a> for an explanation about the non-linear behavior of the log-normal model.
Even though it seems <span class="math inline">\(\mathit{Normal}(0,1)\)</span> is not entirely appropriate as a prior for the difference between object-relative and subject-relative sentences (i.e., the slope), we use it for illustrative purposes. We use the same <span class="math inline">\(\mathit{Normal}(0, 1)\)</span> prior for the <span class="math inline">\(\tau\)</span> parameter and <span class="math inline">\(\sigma\)</span>. Finally, for the group-level effects correlation between the intercept and the slope, we use an LKJ prior <span class="citation">(Lewandowski, Kurowicka, and Joe <a href="#ref-Lewandowski:2009aa">2009</a>)</span> with a relatively uninformative/regularizing prior parameter value of <span class="math inline">\(2\)</span> (for visualization of the prior see Figure <a href="ch-workflow.html#fig:FigLKJ">7.4</a>).</p>
<div class="figure"><span style="display:block;" id="fig:FigLKJ"></span>
<img src="bookdown_files/figure-html/FigLKJ-1.svg" alt="The shape of the LKJ prior with the parameter set to 2. This is the prior density for the group-level effects correlation parameter, here used as a prior for the correlation between the effect size (so) and the intercept. The shape shows that correlation estimates close to zero are expected, and that very strong positive correlations (close to 1) or negative correlations (close to -1) are increasingly unlikely. Thus, correlation estimates are regularized towards values of zero." width="288" />
<p class="caption">
FIGURE 7.4: The shape of the LKJ prior with the parameter set to 2. This is the prior density for the group-level effects correlation parameter, here used as a prior for the correlation between the effect size (so) and the intercept. The shape shows that correlation estimates close to zero are expected, and that very strong positive correlations (close to 1) or negative correlations (close to -1) are increasingly unlikely. Thus, correlation estimates are regularized towards values of zero.
</p>
</div>
<!--\FloatBarrier-->
<p>For the prior predictive checks, we use these priors to draw random parameter sets from the distributions, and to simulate hypothetical data using the statistical model. We load the data and code the predictor variable <code>so</code>. We next use <code>brms</code> to simulate prior predictive data from the hierarchical model.</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb357-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_gibsonwu&quot;</span>)</a>
<a class="sourceLine" id="cb357-2" data-line-number="2">df_gibsonwu &lt;-<span class="st"> </span>df_gibsonwu <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb357-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">so =</span> <span class="kw">ifelse</span>(type <span class="op">==</span><span class="st"> &quot;obj-ext&quot;</span>, <span class="dv">1</span>, <span class="dv">-1</span>))</a>
<a class="sourceLine" id="cb357-4" data-line-number="4">fit_prior_gibsonwu &lt;-<span class="st"> </span><span class="kw">brm</span>(rt <span class="op">~</span><span class="st"> </span>so <span class="op">+</span><span class="st"> </span>(so <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(so <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb357-5" data-line-number="5">                          <span class="dt">data =</span> df_gibsonwu,</a>
<a class="sourceLine" id="cb357-6" data-line-number="6">                          <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb357-7" data-line-number="7">                          <span class="dt">prior =</span> priors,</a>
<a class="sourceLine" id="cb357-8" data-line-number="8">                          <span class="dt">sample_prior =</span> <span class="st">&quot;only&quot;</span></a>
<a class="sourceLine" id="cb357-9" data-line-number="9">)</a></code></pre></div>
<p>Based on the simulated data we can now perform prior predictive checks: we compute summary statistics, and plot the distributions of the summary statistic across simulated data sets. First, we visualize the distribution of the simulated data. For a single data set, this could be visualized as a histogram. Here, we have a large number of simulated data sets, and thus a large number of histograms. We represent this uncertainty: for each bin, we plot the median as well as quantiles showing where 10%-90%, 20%-80%, 30%-70%, and 40%-60% of the histograms lie <span class="citation">(for R code, see Schad, Betancourt, and Vasishth <a href="#ref-schad2019towardarXiv">2019</a>)</span>. For the current prior data simulations, this shows (see Figure <a href="ch-workflow.html#fig:figPrior1a">7.5</a>) that most of the hypothetical reading times are close to zero or larger than <span class="math inline">\(2000\)</span> ms. It is immediately clear that the data predicted by this prior follows a very implausible distribution: it looks exponential; we would expect a log-normal distribution for reading times. Most data points take on extreme values.</p>
<div class="figure"><span style="display:block;" id="fig:figPrior1a"></span>
<img src="bookdown_files/figure-html/figPrior1a-1.svg" alt="Prior predictive checks for a high-variance prior. Multivariate summary statistic: Distribution of histograms of reading times shows very short and also very long reading times are expected too frequently by the uninformative prior. Values larger 2000 ms are shown as 2000 ms." width="672" />
<p class="caption">
FIGURE 7.5: Prior predictive checks for a high-variance prior. Multivariate summary statistic: Distribution of histograms of reading times shows very short and also very long reading times are expected too frequently by the uninformative prior. Values larger 2000 ms are shown as 2000 ms.
</p>
</div>
<p>As an additional summary statistic, in Figure <a href="ch-workflow.html#fig:s2000">7.6</a> we take a look at the mean per simulated data set and also at the standard deviation. We create functions that use as summary statistics the mean and standard deviations; the functions collapse all the values over 2000 ms, making the figure more readable. Then we use these summary statistics to visualize prior predictive checks using the <code>pp_check()</code> function, where we enter the prior predictions (<code>fit_prior_gibsonwu</code>) and the relevant summary statistic (<code>mean_2000</code> and <code>sd_2000</code>).</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb358-1" data-line-number="1">mean_<span class="dv">2000</span> &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb358-2" data-line-number="2">  tmp &lt;-<span class="st"> </span><span class="kw">mean</span>(x)</a>
<a class="sourceLine" id="cb358-3" data-line-number="3">  tmp[tmp <span class="op">&gt;</span><span class="st"> </span><span class="dv">2000</span>] &lt;-<span class="st"> </span><span class="dv">2000</span></a>
<a class="sourceLine" id="cb358-4" data-line-number="4">  tmp</a>
<a class="sourceLine" id="cb358-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb358-6" data-line-number="6">sd_<span class="dv">2000</span> &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb358-7" data-line-number="7">  tmp &lt;-<span class="st"> </span><span class="kw">sd</span>(x)</a>
<a class="sourceLine" id="cb358-8" data-line-number="8">  tmp[tmp <span class="op">&gt;</span><span class="st"> </span><span class="dv">2000</span>] &lt;-<span class="st"> </span><span class="dv">2000</span></a>
<a class="sourceLine" id="cb358-9" data-line-number="9">  tmp</a>
<a class="sourceLine" id="cb358-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb358-11" data-line-number="11">fig_pri_1b &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_prior_gibsonwu,</a>
<a class="sourceLine" id="cb358-12" data-line-number="12">                       <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>,</a>
<a class="sourceLine" id="cb358-13" data-line-number="13">                       <span class="dt">stat =</span> <span class="st">&quot;mean_2000&quot;</span>,</a>
<a class="sourceLine" id="cb358-14" data-line-number="14">                       <span class="dt">prefix =</span> <span class="st">&quot;ppd&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb358-15" data-line-number="15"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Mean RT [ms]&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb358-16" data-line-number="16"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb358-17" data-line-number="17">fig_pri_1b</a>
<a class="sourceLine" id="cb358-18" data-line-number="18">fig_pri_1c &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_prior_gibsonwu,</a>
<a class="sourceLine" id="cb358-19" data-line-number="19">                       <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>,</a>
<a class="sourceLine" id="cb358-20" data-line-number="20">                       <span class="dt">stat =</span> <span class="st">&quot;sd_2000&quot;</span>,</a>
<a class="sourceLine" id="cb358-21" data-line-number="21">                       <span class="dt">prefix =</span> <span class="st">&quot;ppd&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb358-22" data-line-number="22"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Standard Deviation RT [ms]&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb358-23" data-line-number="23"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb358-24" data-line-number="24">fig_pri_1c</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:s2000"></span>
<img src="bookdown_files/figure-html/s2000-1.svg" alt="Prior predictive distribution of average reading times shows that extremely large reading times of more than 2000 ms are too frequently expected. Distribution of standard deviations of reading times shows that very large standard deviations are too frequently expected in the priors. Values larger than 2000 are plotted at a value of 2000 for visualization." width="48%" /><img src="bookdown_files/figure-html/s2000-2.svg" alt="Prior predictive distribution of average reading times shows that extremely large reading times of more than 2000 ms are too frequently expected. Distribution of standard deviations of reading times shows that very large standard deviations are too frequently expected in the priors. Values larger than 2000 are plotted at a value of 2000 for visualization." width="48%" />
<p class="caption">
FIGURE 7.6: Prior predictive distribution of average reading times shows that extremely large reading times of more than 2000 ms are too frequently expected. Distribution of standard deviations of reading times shows that very large standard deviations are too frequently expected in the priors. Values larger than 2000 are plotted at a value of 2000 for visualization.
</p>
</div>
<p>The results, displayed in Figure <a href="ch-workflow.html#fig:s2000">7.6</a>, show that the mean (or the standard deviation) varies across a wide range, with a substantial number of data sets having a mean (or the standard deviation) larger than <span class="math inline">\(2000\)</span> ms. Again, this reveals a highly implausible assumption about the intercept parameter.</p>
<p>Moreover, we also plot the size of the effect of object relative minus subject relative sentence as a measure of effect size with the following code. We first compute the summary statistic, here the difference in reading times between subject versus object relative sentences (i.e., variable <code>so</code>). Then we define difference-values larger than <span class="math inline">\(2000\)</span> ms as <span class="math inline">\(2000\)</span> ms, and values smaller than <span class="math inline">\(-2000\)</span> ms as <span class="math inline">\(-2000\)</span> ms because these represent implausibly large/small values and make it easier to read the plot. We plot these prior predictive data using <code>pp_check</code> by supplying the prior samples (<code>fit_prior_gibsonwu</code>) and the summary statistic (<code>effsize_2000</code>).</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb359-1" data-line-number="1">effsize_<span class="dv">2000</span> &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb359-2" data-line-number="2">  tmp &lt;-<span class="st"> </span><span class="kw">mean</span>(x[df_gibsonwu<span class="op">$</span>so <span class="op">==</span><span class="st"> </span><span class="op">+</span><span class="dv">1</span>]) <span class="op">-</span></a>
<a class="sourceLine" id="cb359-3" data-line-number="3"><span class="st">    </span><span class="kw">mean</span>(x[df_gibsonwu<span class="op">$</span>so <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>])</a>
<a class="sourceLine" id="cb359-4" data-line-number="4">  tmp[tmp <span class="op">&gt;</span><span class="st"> </span><span class="op">+</span><span class="dv">2000</span>] &lt;-<span class="st"> </span><span class="op">+</span><span class="dv">2000</span></a>
<a class="sourceLine" id="cb359-5" data-line-number="5">  tmp[tmp <span class="op">&lt;</span><span class="st"> </span><span class="dv">-2000</span>] &lt;-<span class="st"> </span><span class="dv">-2000</span></a>
<a class="sourceLine" id="cb359-6" data-line-number="6">  tmp</a>
<a class="sourceLine" id="cb359-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb359-8" data-line-number="8">fig_pri_1d &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_prior_gibsonwu,</a>
<a class="sourceLine" id="cb359-9" data-line-number="9">                       <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>,</a>
<a class="sourceLine" id="cb359-10" data-line-number="10">                       <span class="dt">stat =</span> <span class="st">&quot;effsize_2000&quot;</span>,</a>
<a class="sourceLine" id="cb359-11" data-line-number="11">                       <span class="dt">prefix =</span> <span class="st">&quot;ppd&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb359-12" data-line-number="12"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Object - Subject [S-O RT]&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb359-13" data-line-number="13"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb359-14" data-line-number="14">fig_pri_1d</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:diff2000"></span>
<img src="bookdown_files/figure-html/diff2000-1.svg" alt="Prior predictive distribution of differences in reading times between object minus subject relatives shows that very large effect sizes are far too frequently expected. Values larger than 2000 (or smaller than -2000) are plotted at a value of 2000 (or -2000) for visualization." width="672" />
<p class="caption">
FIGURE 7.7: Prior predictive distribution of differences in reading times between object minus subject relatives shows that very large effect sizes are far too frequently expected. Values larger than 2000 (or smaller than -2000) are plotted at a value of 2000 (or -2000) for visualization.
</p>
</div>
<p>The results in Figure <a href="ch-workflow.html#fig:diff2000">7.7</a> show that our priors commonly assume differences in reading times between conditions of more than <span class="math inline">\(2000\)</span> ms, which are larger than we would expect for a psycholinguistic manipulation of the kind investigated here. More specifically, given that we model reading times using a log-normal distribution, the expected effect size depends on the value for the intercept. To take an extreme example, for an intercept of <span class="math inline">\(exp(1) = 2.7\)</span> ms and an effect size in log space of <span class="math inline">\(1\)</span> (i.e., one standard deviation of the prior for the effect size), expected reading times for the two conditions are <span class="math inline">\(exp(1-1) = 1\)</span> ms and <span class="math inline">\(exp(1+1) = 7\)</span> ms. By contrast, for an intercept of <span class="math inline">\(exp(10) = 22026\)</span> ms the corresponding reading times for the two conditions would be <span class="math inline">\(exp(10-1) = 8103\)</span> ms and <span class="math inline">\(exp(10+1) = 59874\)</span> ms.</p>
<p>This implies highly variable expectations for the effect size, including the possibility of very large effect sizes. If there is good reason to believe that the effect size is likely to be relatively small, priors with smaller expected effect sizes may be more reasonable. In chapter <a href="ch-priors.html#ch-priors">6</a> we discussed different methods for working out ballpark estimates of the range of variation in expected effect sizes in reading studies on relative clause processing.</p>
<p>It is also useful to look at individual-level differences in the effect of object versus subject relatives. Figure <a href="ch-workflow.html#fig:maximal2000">7.8</a> shows the subject with the largest (absolute) difference in reading times between object versus subject relatives.</p>
<p>Here, we first assign the prior simulated reading time to the data frame <code>df_gibsonwu</code> (terming it <code>rtfake</code>). Then we group the data frame by subject and by experimental condition (<code>so</code>), average prior predictive reading times for each subject and condition, and then compute the difference in reading times between subject versus object relative sentences for each subject (the subset where <code>so == 1</code> minus the subset where <code>so == -1</code>). Now, we can take the absolute value of this difference (<code>abs(tmp$dif)</code>), and take the maximum or standard deviation across all subjects. Again, we set values larger than 2000 ms to a value of 2000 ms for better visibility.</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb360-1" data-line-number="1">effsize_max_<span class="dv">2000</span> &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb360-2" data-line-number="2">  df_gibsonwu<span class="op">$</span>rtfake &lt;-<span class="st"> </span>x</a>
<a class="sourceLine" id="cb360-3" data-line-number="3">  tmp &lt;-<span class="st"> </span>df_gibsonwu <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb360-4" data-line-number="4"><span class="st">    </span><span class="kw">group_by</span>(subj, so) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb360-5" data-line-number="5"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">rtfake =</span> <span class="kw">mean</span>(rtfake)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb360-6" data-line-number="6"><span class="st">    </span><span class="co"># calculates the difference between conditions:</span></a>
<a class="sourceLine" id="cb360-7" data-line-number="7"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">dif =</span> rtfake[so <span class="op">==</span><span class="st"> </span><span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>rtfake[so <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>])</a>
<a class="sourceLine" id="cb360-8" data-line-number="8">  effect_size_max &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(tmp<span class="op">$</span>dif), <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb360-9" data-line-number="9">  effect_size_max[effect_size_max <span class="op">&gt;</span><span class="st"> </span><span class="dv">2000</span>] &lt;-<span class="st"> </span><span class="dv">2000</span></a>
<a class="sourceLine" id="cb360-10" data-line-number="10">  effect_size_max</a>
<a class="sourceLine" id="cb360-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb360-12" data-line-number="12">effsize_sd_<span class="dv">2000</span> &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb360-13" data-line-number="13">  df_gibsonwu<span class="op">$</span>rtfake &lt;-<span class="st"> </span>x</a>
<a class="sourceLine" id="cb360-14" data-line-number="14">  tmp &lt;-<span class="st"> </span>df_gibsonwu <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb360-15" data-line-number="15"><span class="st">    </span><span class="kw">group_by</span>(subj, so) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb360-16" data-line-number="16"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">rtfake =</span> <span class="kw">mean</span>(rtfake)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb360-17" data-line-number="17"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">dif =</span> rtfake[so <span class="op">==</span><span class="st"> </span><span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>rtfake[so <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>])</a>
<a class="sourceLine" id="cb360-18" data-line-number="18">  effect_size_SD &lt;-<span class="st"> </span><span class="kw">sd</span>(tmp<span class="op">$</span>dif, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb360-19" data-line-number="19">  effect_size_SD[effect_size_SD <span class="op">&gt;</span><span class="st"> </span><span class="dv">2000</span>] &lt;-<span class="st"> </span><span class="dv">2000</span></a>
<a class="sourceLine" id="cb360-20" data-line-number="20">  effect_size_SD</a>
<a class="sourceLine" id="cb360-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb360-22" data-line-number="22">fig_pri_1e &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_prior_gibsonwu,</a>
<a class="sourceLine" id="cb360-23" data-line-number="23">  <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>,</a>
<a class="sourceLine" id="cb360-24" data-line-number="24">  <span class="dt">stat =</span> <span class="st">&quot;effsize_max_2000&quot;</span>, </a>
<a class="sourceLine" id="cb360-25" data-line-number="25">  <span class="dt">prefix =</span> <span class="st">&quot;ppd&quot;</span></a>
<a class="sourceLine" id="cb360-26" data-line-number="26">) <span class="op">+</span></a>
<a class="sourceLine" id="cb360-27" data-line-number="27"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Max Effect Size [S-O RT]&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb360-28" data-line-number="28"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb360-29" data-line-number="29">fig_pri_1e</a>
<a class="sourceLine" id="cb360-30" data-line-number="30">fig_pri_1f &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_prior_gibsonwu,</a>
<a class="sourceLine" id="cb360-31" data-line-number="31">  <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>,</a>
<a class="sourceLine" id="cb360-32" data-line-number="32">  <span class="dt">stat =</span> <span class="st">&quot;effsize_sd_2000&quot;</span>, </a>
<a class="sourceLine" id="cb360-33" data-line-number="33">  <span class="dt">prefix =</span> <span class="st">&quot;ppd&quot;</span></a>
<a class="sourceLine" id="cb360-34" data-line-number="34">) <span class="op">+</span></a>
<a class="sourceLine" id="cb360-35" data-line-number="35"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;SD Effect Size [S-O RT]&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb360-36" data-line-number="36"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb360-37" data-line-number="37">fig_pri_1f</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:maximal2000"></span>
<img src="bookdown_files/figure-html/maximal2000-1.svg" alt="Maximal prior predicted effect size (object - subject relatives) across subjects again shows far too many extreme values and standard deviation of effect size (object - subject relatives) across subjects; again far too many extreme values are expected. Values &gt; 2000 or &lt; -2000 are plotted at a value of 2000 or -2000 for visualization." width="48%" /><img src="bookdown_files/figure-html/maximal2000-2.svg" alt="Maximal prior predicted effect size (object - subject relatives) across subjects again shows far too many extreme values and standard deviation of effect size (object - subject relatives) across subjects; again far too many extreme values are expected. Values &gt; 2000 or &lt; -2000 are plotted at a value of 2000 or -2000 for visualization." width="48%" />
<p class="caption">
FIGURE 7.8: Maximal prior predicted effect size (object - subject relatives) across subjects again shows far too many extreme values and standard deviation of effect size (object - subject relatives) across subjects; again far too many extreme values are expected. Values &gt; 2000 or &lt; -2000 are plotted at a value of 2000 or -2000 for visualization.
</p>
</div>
<!--\FloatBarrier-->
<p>The prior simulations in Figure <a href="ch-workflow.html#fig:maximal2000">7.8</a> show common maximal effect sizes of larger than <span class="math inline">\(2000\)</span> ms, which is more than we would expect for observed data; similarly, the variance in hypothetical effect sizes is large, with many SDs larger than <span class="math inline">\(2000\)</span> ms, and thus again takes many values that are inconsistent with our domain expertise about reading experiments.</p>
</div>
<div id="adjusting-priors" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.3.2</span> Adjusting priors<a href="ch-workflow.html#adjusting-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Based on these analyses of prior predictive data, we can next use our domain expertise to refine our priors and adjust them to values for which we expect more plausible prior predictive hypothetical data as captured in the summary statistics.</p>
<p>First, we adapt the intercept; recall that in chapter <a href="ch-priors.html#ch-priors">6</a> we made a first attempt at coming up with priors for the intercept; now we take our reasoning one step further. Given our prior knowledge about mean reading times (see the discussion in the previous chapter), we could choose a normal distribution in log-space with a mean of <span class="math inline">\(6\)</span>. This corresponds to an expected grand average reading time of <span class="math inline">\(exp(6) = 403\)</span> ms. For the standard deviation, we use a value of SD = <span class="math inline">\(0.6\)</span>. For these prior values, we expect a strongly reduced mean reading time and a strongly reduced residual standard deviation in the simulated hypothetical data. Moreover, we expect that implausibly small or large values for reading times will no longer be expected. For a visualization of the prior distribution of the intercept parameter in log-space and in ms-space see Figure <a href="ch-workflow.html#fig:figPriorAdjust1">7.9</a>a+b. Other values for the standard deviation that are close to <span class="math inline">\(0.6\)</span>, (e.g., SD = <span class="math inline">\(0.5\)</span> or <span class="math inline">\(0.7\)</span>), may yield similar results. Our goal is not to specify a precise value, but rather to use prior parameter values that are qualitatively in line with our domain expertise about expected observed reading time data, and that do not produce highly implausible hypothetical data.</p>
<div class="figure"><span style="display:block;" id="fig:figPriorAdjust1"></span>
<img src="bookdown_files/figure-html/figPriorAdjust1-1.svg" alt="Prior distribution in log-space and in ms-space for a toy example of a linear regression. a) Displays the prior distribution of the intercept in log-space. b) Displays the prior distribution of the intercept in ms-space. c) Displays the prior distribution of the effect size in log-space. d) Displays the prior distribution of the effect size in ms-space." width="528" />
<p class="caption">
FIGURE 7.9: Prior distribution in log-space and in ms-space for a toy example of a linear regression. a) Displays the prior distribution of the intercept in log-space. b) Displays the prior distribution of the intercept in ms-space. c) Displays the prior distribution of the effect size in log-space. d) Displays the prior distribution of the effect size in ms-space.
</p>
</div>
<!--\FloatBarrier-->
<p>Next, for the effect of object minus subject relative sentences, we define a normally distributed prior with mean <span class="math inline">\(0\)</span> and a much smaller standard deviation of <span class="math inline">\(0.05\)</span>. Again, we do not have precise information on the specific value for the standard deviation, but as we saw in chapter <a href="ch-priors.html#ch-priors">6</a>, we have some understanding of the range of variation seen in reading studies involving relative clauses. We expect a generally smaller effect size (see the meta-analysis in chapter <a href="ch-priors.html#ch-priors">6</a>), and we can check through prior predictive checks (data simulation and investigation of summary statistics) whether this yields a plausible pattern of expected results. Figures <a href="ch-workflow.html#fig:figPriorAdjust1">7.9</a>c+d show expected effects in log-scale and in ms-scale for a simple linear regression example.</p>
<p>In addition, we assume much smaller values for the standard deviations in how the intercept and the slope vary across subjects and across items of <span class="math inline">\(0.1\)</span>, and a smaller residual standard deviation of <span class="math inline">\(0.5\)</span>. Our expectation for the correlation between group-level effects is unchanged. In summary, we settle on the following prior specification:</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb361-1" data-line-number="1">priors2 &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb361-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">0.6</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb361-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.05</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> so),</a>
<a class="sourceLine" id="cb361-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb361-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb361-6" data-line-number="6">  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb361-7" data-line-number="7">)</a></code></pre></div>
<div id="prior-predictive-checks-after-increasing-the-informativity-of-the-priors" class="section level4 hasAnchor">
<h4><span class="header-section-number">7.3.2.1</span> Prior predictive checks after increasing the informativity of the priors<a href="ch-workflow.html#prior-predictive-checks-after-increasing-the-informativity-of-the-priors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We have now adjusted the priors increasing their informativity. These priors are still not too informative, but they are somewhat principled since they include some of the theory-neutral information that we have. Based on this new set of now weakly informative priors, we can again perform prior predictive checks as we did before. The new prior predictive checks are shown in Figure <a href="ch-workflow.html#fig:figPrior2">7.10</a>.</p>
<div class="figure"><span style="display:block;" id="fig:figPrior2"></span>
<img src="bookdown_files/figure-html/figPrior2-1.svg" alt="Prior predictive checks after adjusting the priors. The figures show prior predictive distributions. a) Histograms of reading times. Shaded areas correspond to 10-90 percent, 20-80 percent, 30-70 percent, and 40-60 percent quantiles across histograms. This now provides a much more reasonable range of expectations. b)-f) Prior predictive distributions. b) Average reading times now span a more reasonable range of values. c) Differences in reading times between object minus subject relatives; the values are now much more constrained without too many extreme values. d) Standard deviations of reading times; in contrast to the relatively uninformative priors, values are in a reasonable range. e) Maximal effect size (object - subject relatives) across subjects; again, prior expectations are now much more reasonable compared to the uninformative prior. f) The standard deviation of effect size (object minus subject relative reading times) across subjects; this no longer shows a dominance of extreme values any more. a)-f) Values &gt; 2000 or &lt; -2000 are plotted at 2000 or -2000 for visualization." width="403.2" />
<p class="caption">
FIGURE 7.10: Prior predictive checks after adjusting the priors. The figures show prior predictive distributions. a) Histograms of reading times. Shaded areas correspond to 10-90 percent, 20-80 percent, 30-70 percent, and 40-60 percent quantiles across histograms. This now provides a much more reasonable range of expectations. b)-f) Prior predictive distributions. b) Average reading times now span a more reasonable range of values. c) Differences in reading times between object minus subject relatives; the values are now much more constrained without too many extreme values. d) Standard deviations of reading times; in contrast to the relatively uninformative priors, values are in a reasonable range. e) Maximal effect size (object - subject relatives) across subjects; again, prior expectations are now much more reasonable compared to the uninformative prior. f) The standard deviation of effect size (object minus subject relative reading times) across subjects; this no longer shows a dominance of extreme values any more. a)-f) Values &gt; 2000 or &lt; -2000 are plotted at 2000 or -2000 for visualization.
</p>
</div>
<!--\FloatBarrier-->
<p>Figure <a href="ch-workflow.html#fig:figPrior2">7.10</a>a shows that now the distribution over histograms of the data looks much more reasonable, i.e., more like what we would expect for a histogram of observed data. Very small values for reading times are now rare, and not heavily inflated any more. Moreover, extremely large values for reading times larger than <span class="math inline">\(2000\)</span> ms are rather unlikely.</p>
<p>We also take a look at the hypothetical average reading times (Figure <a href="ch-workflow.html#fig:figPrior2">7.10</a>b), and find that our expectations are now much more reasonable. We expect average reading times of around <span class="math inline">\(log(6) = 403\)</span> ms. Most of the expected average reading times lie between <span class="math inline">\(50\)</span> ms and <span class="math inline">\(1500\)</span> ms, and only relatively few extreme values beyond these numbers are observed. The standard deviations of reading times are also in a much more reasonable range (see Figure <a href="ch-workflow.html#fig:figPrior2">7.10</a>d), with only very few values larger than the extreme value of <span class="math inline">\(2000\)</span> ms.</p>
<p>As a next step, we look at the expected effect size (OR minus SR) in the hypothetical data (Figure <a href="ch-workflow.html#fig:figPrior2">7.10</a>c). Extreme values of larger or smaller than <span class="math inline">\(2000\)</span> ms are now very rare, and most of the absolute values of expected effect sizes are smaller than <span class="math inline">\(200\)</span> ms. More specifically, we also check the maximal effect size among all subjects (Figure <a href="ch-workflow.html#fig:figPrior2">7.10</a>e). Most of the distribution centers below a value of <span class="math inline">\(1000\)</span> ms, reflecting a more plausible range of expected values. Likewise, the standard deviation of the psycholinguistically interesting effect size now rarely takes values larger than <span class="math inline">\(500\)</span> ms (Figure <a href="ch-workflow.html#fig:figPrior2">7.10</a>f), reflecting more realistic a priori assumptions than in our initial (relatively) uninformative prior.</p>
</div>
</div>
<div id="computational-faithfulness-and-model-sensitivity" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.3.3</span> Computational faithfulness and model sensitivity<a href="ch-workflow.html#computational-faithfulness-and-model-sensitivity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The next formal steps in the principled Bayesian workflow are to investigate computational faithfulness (using SBC) and model sensitivity. These allow the researcher to determine whether the posterior is estimated accurately for the given problem. Moreover, model sensitivity can be used to test whether parameter estimates are unbiased and whether anything can be learned by sampling data using the given design. Computational faithfulness (i.e., accurate posterior estimation) and model sensitivity need to be checked for non-standard and more complex models, but for simpler/standard models may be performed only once for a research program where experimental designs and models are similar across studies. These steps are computationally very expensive, and can take a very long time to run for realistic data sets and models. For details on how to implement these steps, we refer the interested readers to <span class="citation">Schad, Betancourt, and Vasishth (<a href="#ref-schad2019towardarXiv">2019</a>)</span> and <span class="citation">Betancourt (<a href="#ref-Betancourt:2018aa">2018</a>)</span>. We discuss a simplified version inspired by SBC for more advanced custom models implemented directly in Stan in chapter <a href="ch-MPT.html#ch-MPT">18</a>.</p>
</div>
<div id="posterior-predictive-checks-model-adequacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.3.4</span> Posterior predictive checks: Model adequacy<a href="ch-workflow.html#posterior-predictive-checks-model-adequacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having examined the prior predictive data in detail, we can now take the observed data and perform posterior inference on it. We start by fitting a maximal <code>brms</code> model to the observed data.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb362-1" data-line-number="1">fit_gibsonwu &lt;-<span class="st"> </span><span class="kw">brm</span>(rt <span class="op">~</span><span class="st"> </span>so <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>so <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>so <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb362-2" data-line-number="2">  <span class="dt">data =</span> df_gibsonwu,</a>
<a class="sourceLine" id="cb362-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb362-4" data-line-number="4">  <span class="dt">prior =</span> priors2</a>
<a class="sourceLine" id="cb362-5" data-line-number="5">)</a></code></pre></div>
<p>One could examine the posteriors from this model; we skip this step for brevity, but the reader should run the above code and examine the posterior summaries by typing:</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb363-1" data-line-number="1">fit_gibsonwu</a></code></pre></div>
<p>Figure <a href="ch-workflow.html#fig:FigPosteriorB">7.11</a> shows the posterior distribution for the slope parameter, which estimates the difference in reading times between object minus subject relative sentences.</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb364-1" data-line-number="1"><span class="kw">mcmc_hist</span>(fit_gibsonwu, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;b_so&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb364-2" data-line-number="2"><span class="st">  </span><span class="kw">labs</span>(</a>
<a class="sourceLine" id="cb364-3" data-line-number="3">    <span class="dt">x =</span> <span class="st">&quot;Object - subject relatives&quot;</span>,</a>
<a class="sourceLine" id="cb364-4" data-line-number="4">    <span class="dt">title =</span> <span class="st">&quot;Posterior distribution&quot;</span></a>
<a class="sourceLine" id="cb364-5" data-line-number="5">  )</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:FigPosteriorB"></span>
<img src="bookdown_files/figure-html/FigPosteriorB-1.svg" alt="Posterior distribution for the slope parameter, estimating the difference in reading times between object relative minus subject relative sentences." width="384" />
<p class="caption">
FIGURE 7.11: Posterior distribution for the slope parameter, estimating the difference in reading times between object relative minus subject relative sentences.
</p>
</div>
<div class="sourceCode" id="cb365"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb365-1" data-line-number="1">postgw &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_gibsonwu)</a>
<a class="sourceLine" id="cb365-2" data-line-number="2"><span class="kw">mean</span>(postgw<span class="op">$</span>b_so <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] 0.882</code></pre>
<!--\FloatBarrier-->
<p>Figure <a href="ch-workflow.html#fig:FigPosteriorB">7.11</a> shows that the reading times in object relative sentences tends to be slightly faster than in subject relative sentences (<span class="math inline">\(P(\beta&lt;0) = 0.88\)</span>); this is as predicted by <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span>. However, given the wide <span class="math inline">\(95\)</span>% credible intervals, it is difficult to rule out the possibility that there is effectively no difference in reading time between the two conditions without doing model comparison (with Bayes factor or cross-validation).</p>
<p>To assess model adequacy, we perform posterior predictive checks. We simulate data based on posterior samples of parameters. This then allows us to investigate the simulated data by computing the summary statistics that we used in the prior predictive checks, and by comparing model predictions with the observed data.</p>
<div class="figure"><span style="display:block;" id="fig:FigPost"></span>
<img src="bookdown_files/figure-html/FigPost-1.svg" alt="Posterior predictive checks for weakly informative priors. Distributions are over posterior predictive simulated data. a) Histograms of reading times. 10-90 percent, 20-80 percent, 30-70 percent, and 40-60 percent quantiles across histograms are shown as shaded areas; the median is shown as a dotted line and the observed data as a solid line. For illustration, values &gt; 2000 are plotted as 2000; modeling was done on the original data. b)-f) Grey shows the posterior predictive distributions, the dark line shows the relevant estimate from the observed data. b) Average reading times. c) Differences in reading times between object minus subject relatives. d) Standard deviations of reading times. e) Maximal effect size (object - subject relatives) across subjects. f) The standard deviation of effect size across subjects." width="576" />
<p class="caption">
FIGURE 7.12: Posterior predictive checks for weakly informative priors. Distributions are over posterior predictive simulated data. a) Histograms of reading times. 10-90 percent, 20-80 percent, 30-70 percent, and 40-60 percent quantiles across histograms are shown as shaded areas; the median is shown as a dotted line and the observed data as a solid line. For illustration, values &gt; 2000 are plotted as 2000; modeling was done on the original data. b)-f) Grey shows the posterior predictive distributions, the dark line shows the relevant estimate from the observed data. b) Average reading times. c) Differences in reading times between object minus subject relatives. d) Standard deviations of reading times. e) Maximal effect size (object - subject relatives) across subjects. f) The standard deviation of effect size across subjects.
</p>
</div>
<!--\FloatBarrier-->
<p>The results from these analyses show that the log-normal distribution (see Figure <a href="ch-workflow.html#fig:FigPost">7.12</a>a) provides an approximation to the distribution of the data. However, although the fit looks reasonable, there is still systematic deviation from the data of the model’s predictions. This deviation suggests that maybe a constant offset is needed in addition to the log-normal distribution. This can be implemented in <code>brms</code> by replacing the family specification <code>family = lognormal()</code> with the shifted version <code>family = shifted_lognormal()</code>, and motivates another round of model validation <span class="citation">(see exercise <a href="ch-custom.html#exr:shiftedlogn">12.1</a>, and also chapter <a href="ch-lognormalrace.html#ch-lognormalrace">20</a> which deals with a log-normal race mode using Stan, and see Nicenboim, Logačev, et al. <a href="#ref-NicenboimEtAl2016Frontiersb">2016</a>; Rouder <a href="#ref-Rouder2005">2005</a> for a discussion about shifted log-normal models)</span>.</p>
<p>Next, for the other summary statistics, <!--we transform all of these to log space, by taking the log of reading times before computing the summary statistics. We do this as the model parameters are operating in log space (we use a log-normal distribution). However, alternatively, investigating model parameters without log-transformation is also an option [see @schad2020toward].-->
we first look at the distribution of means. The posterior predictive means capture the mean reading time in the observed data (i.e., the vertical line in Figure <a href="ch-workflow.html#fig:FigPost">7.12</a>b); the data is not perfectly captured, but still in the distribution of the model. For the standard deviation we can see that the model posterior assumes too little posterior variation and that the model thus does not capture the standard deviation of the data well (Figure <a href="ch-workflow.html#fig:FigPost">7.12</a>d). Figure <a href="ch-workflow.html#fig:FigPost">7.12</a>c shows the effect size of object minus subject relative sentences predicted by model (histogram) and observed in the data (vertical line). Here, posterior model predictions for the effect are in line with the empirical data. However, again, the model is showing mostly smaller effect sizes than the data do. For the biggest effect among all subjects (Figure <a href="ch-workflow.html#fig:FigPost">7.12</a>e) the model captures the data reasonably well. For the standard deviation of the effect across subjects (Figure <a href="ch-workflow.html#fig:FigPost">7.12</a>f), the variation in the model is again a bit too small considering the variation in the data. Potentially, the lack of agreement between data and posterior predictive distributions might be due to the mismatch between the true distribution of the data and the log-normal likelihood that we assumed. This could be checked by running the model again and using a shifted log-normal instead of a log-normal likelihood.</p>
</div>
</div>
<div id="summary-6" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.4</span> Summary<a href="ch-workflow.html#summary-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we have introduced key questions to ask about a model and the inference process as discussed by <span class="citation">Betancourt (<a href="#ref-Betancourt:2018aa">2018</a>)</span> and by <span class="citation">Schad, Betancourt, and Vasishth (<a href="#ref-schad2019towardarXiv">2019</a>)</span>, and have applied this to a data set from an experiment involving a typical repeated measures experimental design used in cognitive psychology and psycholinguistics. Prior predictive checks using analyses of simulated prior data suggest that, compared to previous applications in reading experiments <span class="citation">(e.g., Nicenboim and Vasishth <a href="#ref-nicenboimModelsRetrievalSentence2018">2018</a>)</span>, far more informative priors can and should be used. We demonstrated that including such additional domain knowledge into the priors leads to more plausible expected data. Moreover, incorporating more informative priors should also speed up the sampling process. These more informative priors, however, may not alter posterior inferences much for the present design. Posterior predictive checks showed weak support for our statistical model, as the model only partially successfully recovered the tested summary statistics. This may reflect the mis-fit of the likelihood, i.e., that the data may be better explained by a shifted log-normal distribution rather than a log-normal distribution. For inference on whether reading times differ between Chinese object versus subject relative sentences, a Bayes factor analysis would be needed to compare a null model assuming no effect to an alternative model assuming a difference between object versus subject relative sentences. See <span class="citation">Vasishth, Yadav, et al. (<a href="#ref-SampleSizeCBB2021">2022</a>)</span> for more discussion.</p>
<p>In summary, this analysis provides an example and tutorial for using a principled Bayesian workflow <span class="citation">(Betancourt <a href="#ref-Betancourt:2018aa">2018</a>; Schad, Betancourt, and Vasishth <a href="#ref-schad2019towardarXiv">2019</a>)</span> in cognitive science experiments. The workflow reveals useful information about which (weakly informative) priors to use, and performs checks of the used inference procedures and the statistical model. The workflow provides a robust foundation for using a statistical model to answer scientific questions, and will be useful for researchers developing analysis plans as part of pre-registrations, registered reports, or simply as preparatory design analyses prior to conducting an experiment.</p>
</div>
<div id="further-reading-4" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.5</span> Further reading<a href="ch-workflow.html#further-reading-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some important articles relating to developing a principled Bayesian workflow are by <span class="citation">Betancourt (<a href="#ref-Betancourt:2018aa">2018</a>)</span>, <span class="citation">Gabry et al. (<a href="#ref-Gabry:2017aa">2017</a>)</span>, <span class="citation">Gelman et al. (<a href="#ref-gelman2020bayesian">2020</a>)</span>, and <span class="citation">Talts et al. (<a href="#ref-talts2018validating">2018</a>)</span>. The <code>stantargets</code> R package provides tools for a systematic, efficient, and reproducible workflow <span class="citation">(Landau <a href="#ref-stantargets">2021</a>)</span>. Also recommended is the article on reproducible workflows by <span class="citation">Wilson et al. (<a href="#ref-wilson2017good">2017</a>)</span>.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-barr2013">
<p>Barr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013. “Random Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.” <em>Journal of Memory and Language</em> 68 (3). Elsevier: 255–78.</p>
</div>
<div id="ref-Betancourt:2018aa">
<p>Betancourt, Michael J. 2018. “Towards a Principled Bayesian Workflow.” <a href="https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html" class="uri">https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html</a>.</p>
</div>
<div id="ref-box1979robustness">
<p>Box, George EP. 1979. “Robustness in the Strategy of Scientific Model Building.” In <em>Robustness in Statistics</em>, 201–36. Elsevier.</p>
</div>
<div id="ref-chambers2019seven">
<p>Chambers, Chris. 2019. <em>The Seven Deadly Sins of Psychology: A Manifesto for Reforming the Culture of Scientific Practice</em>. Princeton University Press.</p>
</div>
<div id="ref-Gabry:2017aa">
<p>Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael J. Betancourt, and Andrew Gelman. 2017. “Visualization in Bayesian Workflow.” <em>arXiv Preprint arXiv:1709.01449</em>.</p>
</div>
<div id="ref-gelman2020bayesian">
<p>Gelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” <em>arXiv Preprint arXiv:2011.01808</em>.</p>
</div>
<div id="ref-gibsonwu">
<p>Gibson, Edward, and H-H Iris Wu. 2013. “Processing Chinese Relative Clauses in Context.” <em>Language and Cognitive Processes</em> 28 (1-2). Taylor &amp; Francis: 125–55.</p>
</div>
<div id="ref-stantargets">
<p>Landau, William Michael. 2021. “The Stantargets R Package: A Workflow Framework for Efficient Reproducible Stan-Powered Bayesian Data Analysis Pipelines.” <em>Journal of Open Source Software</em> 6 (60): 3193. <a href="https://doi.org/10.21105/joss.03193" class="uri">https://doi.org/10.21105/joss.03193</a>.</p>
</div>
<div id="ref-Lewandowski:2009aa">
<p>Lewandowski, Daniel, Dorota Kurowicka, and Harry Joe. 2009. “Generating Random Correlation Matrices Based on Vines and Extended Onion Method.” <em>Journal of Multivariate Analysis</em> 100 (9): 1989–2001.</p>
</div>
<div id="ref-NicenboimEtAl2016Frontiersb">
<p>Nicenboim, Bruno, Pavel Logačev, Carolina Gattei, and Shravan Vasishth. 2016. “When High-Capacity Readers Slow down and Low-Capacity Readers Speed up: Working Memory and Locality Effects.” <em>Frontiers in Psychology</em> 7 (280). <a href="https://doi.org/10.3389/fpsyg.2016.00280" class="uri">https://doi.org/10.3389/fpsyg.2016.00280</a>.</p>
</div>
<div id="ref-nicenboimModelsRetrievalSentence2018">
<p>Nicenboim, Bruno, and Shravan Vasishth. 2018. “Models of Retrieval in Sentence Comprehension: A Computational Evaluation Using Bayesian Hierarchical Modeling.” <em>Journal of Memory and Language</em> 99: 1–34. <a href="https://doi.org/10.1016/j.jml.2017.08.004" class="uri">https://doi.org/10.1016/j.jml.2017.08.004</a>.</p>
</div>
<div id="ref-Paape:2017aa">
<p>Paape, Dario, Bruno Nicenboim, and Shravan Vasishth. 2017. “Does Antecedent Complexity Affect Ellipsis Processing? An Empirical Investigation.” <em>Glossa: A Journal of General Linguistics</em> 2 (1).</p>
</div>
<div id="ref-Rouder2005">
<p>Rouder, Jeffrey N. 2005. “Are Unshifted Distributional Models Appropriate for Response Time?” <em>Psychometrika</em> 70 (2). Springer Science + Business Media: 377–81. <a href="https://doi.org/10.1007/s11336-005-1297-7" class="uri">https://doi.org/10.1007/s11336-005-1297-7</a>.</p>
</div>
<div id="ref-schad2020toward">
<p>Schad, Daniel J., Michael J. Betancourt, and Shravan Vasishth. 2020. “Toward a Principled Bayesian Workflow in Cognitive Science.” <em>Psychological Methods</em> 26 (1). American Psychological Association: 103–26.</p>
</div>
<div id="ref-schad2019towardarXiv">
<p>Schad, Daniel J., Michael Betancourt, and Shravan Vasishth. 2019. “Toward a Principled Bayesian Workflow in Cognitive Science.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1904.12765" class="uri">https://doi.org/10.48550/ARXIV.1904.12765</a>.</p>
</div>
<div id="ref-schad2020capitalize">
<p>Schad, Daniel J., Shravan Vasishth, Sven Hohenstein, and Reinhold Kliegl. 2019. “How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.” <em>Journal of Memory and Language</em> 110. <a href="https://doi.org/10.1016/j.jml.2019.104038" class="uri">https://doi.org/10.1016/j.jml.2019.104038</a>.</p> 2020. “How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.” <em>Journal of Memory and Language</em> 110. Elsevier: 104038.</p>
</div>
<div id="ref-szollosi2019preregistration">
<p>Szollosi, Aba, David Kellen, Danielle J Navarro, Richard Shiffrin, Iris van Rooij, Trisha Van Zandt, and Chris Donkin. 2020. “Is Preregistration Worthwhile?” <em>Trends in Cognitive Sciences</em> 24 (2). Elsevier: 94–95.</p>
</div>
<div id="ref-talts2018validating">
<p>Talts, Sean, Michael J. Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2018. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” <em>arXiv Preprint arXiv:1804.06788</em>.</p>
</div>
<div id="ref-Vasishth2018aa">
<p>Vasishth, Shravan, Daniela Mertzen, Lena A Jäger, and Andrew Gelman. 2018b. “The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability.” <em>Journal of Memory and Language</em> 103: 151–75.</p>
</div>
<div id="ref-SampleSizeCBB2021">
<p>Vasishth, Shravan, Himanshu Yadav, Daniel J. Schad, and Bruno Nicenboim. 2022. “Sample Size Determination for Bayesian Hierarchical Models Commonly Used in Psycholinguistics.” <em>Computational Brain and Behavior</em>.</p>
</div>
<div id="ref-wilson2017good">
<p>Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K Teal. 2017. “Good Enough Practices in Scientific Computing.” <em>PLoS Computational Biology</em> 13 (6). Public Library of Science San Francisco, CA USA: e1005510.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="25">
<li id="fn25"><p>In frequentist methods (such as implemented in the lme4 package in the lmer program), this problem manifests itself as problems with convergence of the optimizer, which indicates that the likelihood is too flat and that the parameter estimates are not constrained by the data.<a href="ch-workflow.html#fnref25" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-priors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-contr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
