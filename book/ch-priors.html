<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 The Art and Science of Prior Elicitation | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 The Art and Science of Prior Elicitation | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 The Art and Science of Prior Elicitation | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2023-02-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-hierarchical.html"/>
<link rel="next" href="ch-workflow.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b> Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b> Bayesian Regression Models using Stan: brms</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b> Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b> Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a><ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
<li class="chapter" data-level="3.11" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect finger tapping times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b> No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a><ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#model-building"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#exemplary-data-analysis"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-workflow.html"><a href="ch-workflow.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-workflow.html"><a href="ch-workflow.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a><ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-7"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-9"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparameterization</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-10"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a><ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b> Validation of a computed posterior distribution</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#simulation-based-calibration-revealing-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> Simulation-based calibration revealing a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-and-limitation-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues and limitation of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-re-implementing-the-exponential-distribution-manually"><i class="fa fa-check"></i><b>12.3</b> Another custom distribution: Re-implementing the exponential distribution manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-11"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-12"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a><ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a><ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a><ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a><ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="15.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>15.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#summary-13"><i class="fa fa-check"></i><b>15.6</b> Summary</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.7</b> Further reading</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a><ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#issues-with-cross-validation"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a><ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-14"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a><ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-15"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
<li class="chapter" data-level="18.5" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a><ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a><ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-16"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a><ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a><ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-17"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-priors" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 6</span> The Art and Science of Prior Elicitation<a href="ch-priors.html#ch-priors" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Nothing strikes fear into the heart of the newcomer to Bayesian methods more than the idea of specifying priors for the parameters in a model. On the face of it, this concern seems like a valid one; how can one know what the plausible parameter values are in a model before one has even seen the data?<br />
In reality, this worry is purely a consequence of the way we are normally taught to carry out data analysis, especially in areas like psychology and linguistics. Model fitting is considered to be a black-box activity, with the primary concern being whether the effect of interest is “significant” or “non-significant.” As a consequence of the training that we receive, we learn to focus on one thing (the <span class="math inline">\(p\)</span>-value) and we learn to ignore the estimates that we obtain from the model; it becomes irrelevant whether the effect of interest has a mean value of 500 ms (in a reading study, say) or 10 ms; all that matters is whether it is a significant effect or not. In fact, the way many scientists summarize the literature in their field is by classifying studies into two bins: significant and non-significant. There are obvious problems with this classification method; for example, <span class="math inline">\(p=0.051\)</span> might be counted as “marginally” significant, but <span class="math inline">\(p=0.049\)</span> is never counted as marginally non-significant. But there will usually not be any important difference between these two borderline values.
Real-life examples of such a binary classification approach are seen in <span class="citation">Phillips, Wagers, and Lau (<a href="#ref-phillips2011grammatical">2011</a>)</span> and <span class="citation">Hammerly, Staub, and Dillon (<a href="#ref-hammerly2019grammaticality">2019</a>)</span>. Because the focus is on significance, we never develop a sense of what the estimates of an effect are likely to be in a future study. This is why, when faced with a prior-distribution specification problem, we are misled into feeling like we know nothing about the quantitative estimates relating to a problem we are studying.</p>
<p>Prior specification has a lot in common with something that physicists call a Fermi problem. As <span class="citation">Von Baeyer (<a href="#ref-von1988fermi">1988</a>)</span> describes it: “A Fermi problem has a characteristic profile: Upon first hearing it, one doesn’t have even the remotest notion what the answer might be. And one feels certain that too little information exists to find a solution. Yet, when the problem is broken down into subproblems, each one answerable without the help of experts or reference books, an estimate can be made ”. Fermi problems in the physics context are situations where one needs ballpark (approximate) estimates of physical quantities in order to proceed with a calculation. The name comes from a physicist, Enrico Fermi; he developed the ability to carry out fairly accurate back-of-the-envelope calculations when working out approximate numerical values needed for a particular computation. <span class="citation">Von Baeyer (<a href="#ref-von1988fermi">1988</a>)</span> puts it well: “Prudent physicists—those who want to avoid false leads and dead ends—operate according to a long-standing principle: Never start a lengthy calculation until you know the range of values within which the answer is likely to fall (and, equally important, the range within which the answer is unlikely to fall).” As in physics, so in data analysis: as Bayesians, we need to acquire the ability to work out plausible ranges of values for parameters. This is a learnable skill, and improves with practice. With time and practice, we can learn to become prudent data analysts.</p>
<p>As <span class="citation">Spiegelhalter, Abrams, and Myles (<a href="#ref-spiegelhalter2004bayesian">2004</a>)</span> point out, there is no one “correct” prior distribution. One consequence of this fact is that a good Bayesian analysis always takes a range of prior specifications into account; this is called a sensitivity analysis. We have already seen examples of this, but more examples will be provided in this and later chapters.</p>
<p>Prior specification requires the estimation of probabilities. Human beings are not good at estimating probabilities, because they are susceptible to several kinds of biases <span class="citation">(Kadane and Wolfson <a href="#ref-kadane1998experiences">1998</a>; Spiegelhalter, Abrams, and Myles <a href="#ref-spiegelhalter2004bayesian">2004</a>)</span>. We list the most important ones that are relevant to cognitive science applications:</p>
<ul>
<li>Availability bias: Events that are more salient to the researcher are given higher probability, and events that are less salient are given lower probability.</li>
<li>Adjustment and anchoring bias: The initial assessment of the probability of an event can influence one’s subsequent judgements. An example is credible intervals: a researcher’s estimate of the credible interval will tend to be influenced by their initial assessment.</li>
<li>Overconfidence: When eliciting credible intervals from oneself, there is a tendency to specify too tight an interval.</li>
<li>Hindsight bias: If one relies on the data to come up with a prior for the analysis of that very same data set, one’s assessment is likely to be biased.</li>
</ul>
<p>Although training can improve the natural tendency to be biased in these different ways, one must recognize that bias is inevitable when eliciting priors, either from oneself or from other experts; it follows that one should always define “a community of priors” <span class="citation">(Kass and Greenhouse <a href="#ref-kass1989investigating">1989</a>)</span>: one should consider the effect of informed as well as skeptical or agnostic (uninformative) priors on the posterior distribution of interest.
Incidentally, bias is not unique to Bayesian statistics; the same problems arise in frequentist data analysis. Even in frequentist analyses, the researcher always interprets the data in the light of their prior beliefs; the data never really “speak for themselves.” For example, the researcher might remove ``outliers’’ based on a belief that certain values are implausible; or the researcher will choose a particular likelihood based on their belief about the underlying generative process. All these are subjective decisions made by the researcher, and can dramatically impact the outcome of the analyses.</p>
<p>The great advantage that Bayesian methods have is that they allow us to formally take a range of (competing) prior beliefs into account when interpreting the data. We illustrate this point in the present chapter.</p>
<div id="sec-simpleexamplepriors" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.1</span> Eliciting priors from oneself for a self-paced reading study: A simple example<a href="ch-priors.html#sec-simpleexamplepriors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In section <a href="ch-compbda.html#sec-revisit">3.5</a>, we have already encountered a sensitivity analysis; there, several priors were used to investigate how the posterior is affected. Here is another example of a sensitivity analysis; the problem here is how to elicit priors from oneself for a particular research problem.</p>
<div id="an-example-english-relative-clauses" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.1.1</span> An example: English relative clauses<a href="ch-priors.html#an-example-english-relative-clauses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will work out priors from first principles for a commonly-used experiment design in psycholinguistics. As an example, consider English subject vs. object relative clause processing differences. Relative clauses are sentences like (1a) and (1b):</p>
<p>(1a) The <em>reporter</em> [who the photographer <em>sent</em> to the editor] was hoping for a good story. (ORC)</p>
<p>(1b) The <em>reporter</em> [who <em>sent</em> the photographer to the editor] was hoping for a good story. (SRC)</p>
<p>Sentence (1a) is an object relative clause (ORC): the noun <em>reporter</em> is modified by a relative clause (demarcated in square brackets), and the noun <em>reporter</em> is the object of the verb <em>sent</em>.
Sentence (1b) is a subject relative clause (SRC): the noun <em>reporter</em> is modified by a relative clause (demarcated in square brackets), but this time the noun <em>reporter</em> is the subject of the verb <em>sent</em>. Many theories in sentence processing predict that the reading time at the verb <em>sent</em> will be shorter in English subject vs. object relatives; one explanation is that the dependency distance between <em>reporter</em> and <em>sent</em> is shorter in subject vs. object relatives <span class="citation">(Grodner and Gibson <a href="#ref-grodner">2005</a>)</span>.</p>
<p>The experimental method we consider here is self-paced reading.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a> The self-paced reading method is commonly used in psycholinguistics as a cheaper and faster substitute to eyetracking during reading. The subject is seated in front of a computer screen and is initially shown a series of broken lines that mask words from a complete sentence. The subject then unmasks the first word (or phrase) by pressing the space bar. Upon pressing the space bar again, the second word/phrase is unmasked and the first word/phrase is masked again; see Figure <a href="ch-priors.html#fig:SPR-tikz">6.1</a>. The time in milliseconds that elapses between these two space-bar presses counts as the reading time for the first word/phrase. In this way, the reading time for each successive word/phrase in the sentence is recorded. Usually, at the end of each trial, the subject is also asked a yes/no question about the sentence. This is intended to ensure that the subject is adequately attending to the meaning of the sentence.</p>

<div class="figure"><span style="display:block;" id="fig:SPR-tikz"></span>
<img src="bookdown_files/figure-html/SPR-tikz-1.svg" alt="A moving window self-paced reading task for the sentence “The king is dead.” Words are unmasked one by one after each press of the space bar." width="672" />
<p class="caption">
FIGURE 6.1: A moving window self-paced reading task for the sentence “The king is dead.” Words are unmasked one by one after each press of the space bar.
</p>
</div>
<p>A classic example of self-paced reading data appeared in Exercise <a href="ch-hierarchical.html#exr:hierarchical-logn">5.2</a>. A hierarchical model that we could fit to such data would be the following. In chapter <a href="ch-hierarchical.html#ch-hierarchical">5</a>, we showed that for reading-time data, the log-normal likelihood is generally a better choice than a normal likelihood. In the present chapter, in order to make it easier for the reader to get started with thinking about priors, we use the normal likelihood instead of the log-normal. In real-life data analysis, the normal likelihood would be a very poor choice for reading-time data.</p>
<p>The model below has varying intercepts and varying slopes for subjects and for items, but assumes no correlation between the varying intercepts and slopes. The correlation is removed in order to compare the posteriors to the estimates from the corresponding frequentist <code>lme4</code> model. In the model shown below, we use “default” priors that the <code>brm</code> function assumes for all the parameters. We are only using default priors here as a starting point; in practice, we will <strong>never</strong> use default priors for a reported analysis. In the model output below, for brevity we will only display the summary of the posterior distribution for the slope parameter, which represents the difference between the two condition means.</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb328-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_gg05_rc&quot;</span>)</a>
<a class="sourceLine" id="cb328-2" data-line-number="2">df_gg05_rc &lt;-<span class="st"> </span>df_gg05_rc <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb328-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cond =</span> <span class="kw">if_else</span>(condition <span class="op">==</span><span class="st"> &quot;objgap&quot;</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb328-4" data-line-number="4">fit_gg05 &lt;-<span class="st"> </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_cond <span class="op">||</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb328-5" data-line-number="5"><span class="st">                  </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_cond <span class="op">||</span><span class="st"> </span>item), df_gg05_rc)</a></code></pre></div>
<div class="sourceCode" id="cb329"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb329-1" data-line-number="1">(default_b &lt;-<span class="st"> </span><span class="kw">posterior_summary</span>(fit_gg05,</a>
<a class="sourceLine" id="cb329-2" data-line-number="2">                                <span class="dt">variable =</span> <span class="st">&quot;b_c_cond&quot;</span>))</a></code></pre></div>
<pre><code>##          Estimate Est.Error Q2.5 Q97.5
## b_c_cond      103      35.9 34.2   173</code></pre>
<p>The estimates from this model are remarkably similar to those from a frequentist linear mixed model <span class="citation">(Bates, Mächler, et al. <a href="#ref-lme4">2015</a><a href="#ref-lme4">a</a>)</span>:</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" data-line-number="1">fit_lmer &lt;-<span class="st"> </span><span class="kw">lmer</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_cond <span class="op">||</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb331-2" data-line-number="2"><span class="st">                   </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_cond <span class="op">||</span><span class="st"> </span>item), df_gg05_rc)</a>
<a class="sourceLine" id="cb331-3" data-line-number="3">b &lt;-<span class="st"> </span><span class="kw">summary</span>(fit_lmer)<span class="op">$</span>coefficients[<span class="st">&quot;c_cond&quot;</span>, <span class="st">&quot;Estimate&quot;</span>]</a>
<a class="sourceLine" id="cb331-4" data-line-number="4">SE &lt;-<span class="st"> </span><span class="kw">summary</span>(fit_lmer)<span class="op">$</span>coefficients[<span class="st">&quot;c_cond&quot;</span>, <span class="st">&quot;Std. Error&quot;</span>]</a>
<a class="sourceLine" id="cb331-5" data-line-number="5"><span class="co">## estimate of the slope and</span></a>
<a class="sourceLine" id="cb331-6" data-line-number="6"><span class="co">## lower and upper bounds of the 95% CI:</span></a>
<a class="sourceLine" id="cb331-7" data-line-number="7">(lmer_b &lt;-<span class="st"> </span><span class="kw">c</span>(b, b <span class="op">-</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>SE), b <span class="op">+</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>SE)))</a></code></pre></div>
<pre><code>## [1] 102.3  29.9 174.7</code></pre>
<p>The similarity between the estimates from the Bayesian and frequentist models is due to the fact that default priors, being relatively uninformative, don’t influence the posterior much. This leads to the likelihood dominating in determining the posteriors. In general, such uninformative priors on the parameters will show a similar lack of influence on the posterior <span class="citation">(Spiegelhalter, Abrams, and Myles <a href="#ref-spiegelhalter2004bayesian">2004</a>)</span>. We can quickly establish this in the above example by using another uninformative prior:</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb333-1" data-line-number="1">fit_gg05_unif &lt;-<span class="st"> </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_cond <span class="op">||</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb333-2" data-line-number="2"><span class="st">                       </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_cond <span class="op">||</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb333-3" data-line-number="3">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb333-4" data-line-number="4">    <span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="op">-</span><span class="dv">2000</span>, <span class="dv">2000</span>), <span class="dt">class =</span> Intercept,</a>
<a class="sourceLine" id="cb333-5" data-line-number="5">          <span class="dt">lb =</span> <span class="dv">-2000</span>, <span class="dt">ub =</span> <span class="dv">2000</span>),</a>
<a class="sourceLine" id="cb333-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="op">-</span><span class="dv">2000</span>, <span class="dv">2000</span>), <span class="dt">class =</span> b,</a>
<a class="sourceLine" id="cb333-7" data-line-number="7">          <span class="dt">lb =</span> <span class="dv">-2000</span>, <span class="dt">ub =</span> <span class="dv">2000</span>),</a>
<a class="sourceLine" id="cb333-8" data-line-number="8">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">500</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb333-9" data-line-number="9">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">500</span>), <span class="dt">class =</span> sigma)</a>
<a class="sourceLine" id="cb333-10" data-line-number="10">  ), df_gg05_rc</a>
<a class="sourceLine" id="cb333-11" data-line-number="11">  )</a></code></pre></div>
<div class="sourceCode" id="cb334"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb334-1" data-line-number="1">(uniform_b &lt;-<span class="st"> </span><span class="kw">posterior_summary</span>(fit_gg05_unif, </a>
<a class="sourceLine" id="cb334-2" data-line-number="2">                                <span class="dt">variable =</span> <span class="kw">c</span>(<span class="st">&quot;b_c_cond&quot;</span>)))</a></code></pre></div>
<pre><code>##          Estimate Est.Error Q2.5 Q97.5
## b_c_cond      102      38.7 23.1   178</code></pre>
<p>As shown in Table <a href="ch-priors.html#tab:slopesmeansCIs">6.1</a>, the means of the posteriors from this versus the other two model estimates shown above all look very similar.</p>
<table>
<caption>
<span id="tab:slopesmeansCIs">TABLE 6.1: </span>Estimates of the mean difference (with 95% confidence/credible intervals) between two conditions in a hierarchical model of English relative clause data from Grodner and Gibson, 2005, using (a) the frequentist hierarchical model, (b) a Bayesian model using default priors from the <code>brm</code> function, and (c) a Bayesian model with uniform priors.
</caption>
<thead>
<tr>
<th style="text-align:left;">
model
</th>
<th style="text-align:left;">
mean
</th>
<th style="text-align:left;">
lower
</th>
<th style="text-align:left;">
upper
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Frequentist
</td>
<td style="text-align:left;">
<span class="math inline">\(102\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(30\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(175\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Default prior
</td>
<td style="text-align:left;">
<span class="math inline">\(103\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(34\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(173\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Uniform
</td>
<td style="text-align:left;">
<span class="math inline">\(102\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(23\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(178\)</span>
</td>
</tr>
</tbody>
</table>
<p>It is tempting for the newcomer to Bayesian statistics to conclude from Table <a href="ch-priors.html#tab:slopesmeansCIs">6.1</a> that default priors used in <code>brms</code>, or uniform priors, are good enough for fitting models. This conclusion would in general be incorrect. There are many reasons why a sensitivity analysis–which includes regularizing, relatively informative priors–is necessary in Bayesian modeling. First, relatively informative, regularizing priors must be considered in many cases to avoid convergence problems (an example is finite mixture models, presented in chapter <a href="ch-mixture.html#ch-mixture">19</a>). In fact, in many cases the frequentist model fit in <code>lme4</code> will return estimates–such as <span class="math inline">\(\pm 1\)</span> correlation estimates between varying intercepts and varying slopes–that are actually represent convergence failures <span class="citation">(Bates, Kliegl, et al. <a href="#ref-BatesEtAlParsimonious">2015</a>; Matuschek et al. <a href="#ref-hannesBEAP">2017</a>)</span>. In Bayesian models, unless we use regularizing priors that are at least mildly informative, we will generally face similar convergence problems. Second, when computing Bayes factors, sensitivity analyses using increasingly informative priors is vital; see chapter <a href="ch-bf.html#ch-bf">15</a> for extensive discussion of this point. Third, one of the greatest advantages of Bayesian models is that one can formally take into account conflicting or competing prior beliefs in the model, by eliciting informative priors from competing experts. Although such a use of informative priors is still rare in cognitive science, it can be of great value when trying to interpret a statistical analysis.</p>
<p>Given the importance of regularizing, informative priors,
we consider next some informative priors that we could use in the given model. We unpack the process by which we could work these priors out from existing information in the literature.</p>
<p>Initially, when trying to work out some alternative priors for some parameters of interest, we might think that we know absolutely nothing about the seven parameters in this model. But, as in Fermi problems, we actually know more than we realize.</p>
<p>Let’s think about the parameters in the relative clause example one by one. For ease of exposition, we begin by writing out the model in mathematical form. <span class="math inline">\(n\)</span> is the row id in the data-frame. The variable <code>c_cond</code> is a sum-coded (<span class="math inline">\(\pm 0.5\)</span>) predictor.</p>
<p><span class="math display">\[\begin{equation}
RT_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + w_{item[n],1} + c\_cond_n \cdot (\beta+ u_{subj[n],2}+w_{item[n],2}),\sigma)
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  u_1 &amp;\sim \mathit{Normal}(0,\tau_{u_1})\\
 u_2 &amp;\sim \mathit{Normal}(0,\tau_{u_2})\\
 w_1 &amp;\sim \mathit{Normal}(0,\tau_{w_1})\\
 w_2 &amp;\sim \mathit{Normal}(0,\tau_{w_2})
 \end{aligned}
 \end{equation}\]</span></p>
<p>The parameters that we need to define priors for are the following:
<span class="math inline">\(\alpha, \beta, \tau_{u_1}, \tau_{u_2}, \tau_{w_1}, \tau_{w_2}, \sigma\)</span>.</p>
</div>
<div id="eliciting-a-prior-for-the-intercept" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.1.2</span> Eliciting a prior for the intercept<a href="ch-priors.html#eliciting-a-prior-for-the-intercept" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will proceed from first principles. Let’s begin with the intercept, <span class="math inline">\(\alpha\)</span>; under the sum-contrast coding used here, it represents the grand mean reading time in the data set.</p>
<p>Ask yourself: What is the absolute minimum possible reading time? The answer is 0 ms; reading time cannot be negative. You have already eliminated half the real-number line as impossible values! Thus, one cannot really say that one knows <em>nothing</em> about the plausible values of mean reading times. Having eliminated half the real-number line, now ask yourself: what is a reasonable upper bound on reading time for an English ditransitive verb? Even after taking into account variations in word length and frequency, one minute (<span class="math inline">\(60\)</span> seconds) seems like too long; even <span class="math inline">\(30\)</span> seconds seems unreasonably long to spend on a single word. As a first attempt at an approximation, somewhere between <span class="math inline">\(2500\)</span> and <span class="math inline">\(3000\)</span> ms might constitute a reasonable upper bound, with 3000 ms being less likely than <span class="math inline">\(2500\)</span> ms.</p>
<p>Now consider what an approximate average reading time for a verb might be. One can arrive at such a ballpark number by asking oneself how fast one can read an abstract that has, say, <span class="math inline">\(500\)</span> words in it. Suppose that we estimate that we can read <span class="math inline">\(500\)</span> words in <span class="math inline">\(120\)</span> seconds (two minutes). Then, <span class="math inline">\(120/500=0.24\)</span> seconds is the time we would spend per word on average; this is <span class="math inline">\(240\)</span> ms per word. Maybe two minutes for <span class="math inline">\(500\)</span> words was too optimistic? Let’s adjust the mean to <span class="math inline">\(300\)</span> ms, instead of <span class="math inline">\(240\)</span> ms. Such intuition-based judgments can be a valuable starting point for an analysis, as Fermi showed repeatedly in his work <span class="citation">(Von Baeyer <a href="#ref-von1988fermi">1988</a>)</span>. If one is uncomfortable consulting one’s intuition about average reading times, or even as a sanity check to independently validate one’s own intuitions, one can look up a review article on reading that gives empirical estimates <span class="citation">(e.g., Rayner <a href="#ref-rayner1998emr">1998</a>)</span>.</p>
<p>One could express the above guesses as a normal distribution truncated at <span class="math inline">\(0\)</span> ms on the ms scale, with mean <span class="math inline">\(300\)</span> ms and standard deviation <span class="math inline">\(1000\)</span> ms. An essential step in such an estimation procedure is to plot one’s assumed prior distribution graphically to see if it seems reasonable:
Figure <a href="ch-priors.html#fig:initialguess">6.2</a> shows a graphical summary of this prior.</p>
<div class="figure"><span style="display:block;" id="fig:initialguess"></span>
<img src="bookdown_files/figure-html/initialguess-1.svg" alt="A truncated normal distribution representing a prior distribution on mean reading times." width="672" />
<p class="caption">
FIGURE 6.2: A truncated normal distribution representing a prior distribution on mean reading times.
</p>
</div>
<p>Once we plot the prior, one might conclude that the prior distribution is a bit too widely spread out to represent mean reading time per word. But for estimating the posterior distribution, it will rarely be harmful to allow a broader range of values than we strictly consider plausible (the situation is different when it comes to Bayes factors analyses, as we will see later—there, widely spread out priors for a parameter of interest can have a dramatic impact on the Bayes factor test for whether that parameter is zero or not).</p>
<p>Another way to obtain a better feel for what plausible distributions of word reading times might be to just plot some existing data from published work. Figure <a href="ch-priors.html#fig:rtdistrns">6.3</a> shows the distribution of mean reading times from ten published studies.</p>
<div class="figure"><span style="display:block;" id="fig:rtdistrns"></span>
<img src="bookdown_files/figure-html/rtdistrns-1.svg" alt="The distribution of mean reading times from ten self-paced reading studies." width="672" />
<p class="caption">
FIGURE 6.3: The distribution of mean reading times from ten self-paced reading studies.
</p>
</div>
<p>Although our truncated normal distribution, <span class="math inline">\(\mathit{Normal}_+(300,1000)\)</span>, seems like a pretty wild guess, it actually is not terribly unreasonable given what we observe in these ten published self-paced reading studies. As shown in Figure <a href="ch-priors.html#fig:rtdistrns">6.3</a>, the distribution of mean reading times in these different self-paced reading studies from different languages (English, Persian, Dutch, Hindi, German, Spanish) fall within the prior distribution. The means range from a minimum value of 464 ms and a maximum value of 751 ms. These values easily lie within the 95% credible interval for a <span class="math inline">\(\mathit{Normal}_+(300,1000)\)</span>: [40, 2458] ms.
These 10 studies are not about relative clauses; but that doesn’t matter, because we are just trying to come up with a prior distribution on average reading times for a word. We just want an approximate idea of the range of plausible mean reading times.</p>
<p>The above prior specification for the intercept can (and must!) be evaluated in the context of the model using prior predictive checks. We have already encountered prior predictive checks in previous chapters; we will revisit them in detail in chapter <a href="ch-workflow.html#ch-workflow">7</a>. In the above data set on English relative clauses, one could check what the prior on the intercept implies in terms of the data generated by the model (see chapter <a href="ch-hierarchical.html#ch-hierarchical">5</a> for examples). As stressed repeatedly throughout this book, sensitivity analysis is an integral component of Bayesian methodology. A sensitivity analysis should be used to work out what the impact is of a range of priors on the posterior distribution.</p>
</div>
<div id="eliciting-a-prior-for-the-slope" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.1.3</span> Eliciting a prior for the slope<a href="ch-priors.html#eliciting-a-prior-for-the-slope" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having come up with some potential priors for the intercept, consider next the prior specification for the effect of relative clause type on reading time; this is the slope <span class="math inline">\(\beta\)</span> in the model above. Recall that <code>c_cond</code> is <span class="math inline">\(\pm 0.5\)</span> sum coded.</p>
<p>Theory suggests <span class="citation">(see Grodner and Gibson <a href="#ref-grodner">2005</a> for a review)</span> that subject relatives in English should be easier to process than object relatives, at the relative clause verb. This means that a priori, we expect the difference between object and subject relatives to be positive in sign. What would be a reasonable mean for this effect? We can look at previous research to obtain some ballpark estimates.</p>
<p>For example, <span class="citation">Just and Carpenter (<a href="#ref-jc92">1992</a>)</span> carried out a self-paced reading study on English subject and object relatives, and their Figure 2 (p. 130) shows that the difference between the two relative clause types at the relative clause verb ranges from about 10 ms to 100 ms (depending on working memory capacity differences in different groups of subjects). This is already a good starting point, but we can look at some other published data to gain more confidence about the approximate difference between the conditions.</p>
<p>For example, <span class="citation">Reali and Christiansen (<a href="#ref-reali2007">2007</a>)</span> investigated subject and object relatives in four self-paced reading studies; in their design, the noun phrase inside the relative clause was always a pronoun, and they carried out analyses on the verb plus pronoun, not just the verb as in <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span>. We can still use the estimates from this study, because including a pronoun like “I”, “you”, or “they” in a verb region is not going to increase reading times dramatically. The hypothesis in <span class="citation">Reali and Christiansen (<a href="#ref-reali2007">2007</a>)</span> was that because object relatives containing a pronoun occur more frequently in corpora than subject relatives with a pronoun, the relative clause verb should be processed faster in object relatives than subject relatives <span class="citation">(this is the opposite to the prediction for the reading times at the relative clause verb discussed in Grodner and Gibson <a href="#ref-grodner">2005</a>)</span>. The authors report comparisons for the pronoun and relative clause verb taken together (i.e., pronoun+verb in object relatives and verb+pronoun in subject relatives). In experiment 1, they report a <span class="math inline">\(-57\)</span> ms difference between object and subject relatives, with a 95% confidence interval ranging from <span class="math inline">\(-104\)</span> to <span class="math inline">\(-10\)</span> ms. In a second experiment, they report a difference of <span class="math inline">\(-53.5\)</span> ms with a 95% confidence interval ranging from <span class="math inline">\(-79\)</span> to <span class="math inline">\(-28\)</span> ms; in a third experiment, the difference was <span class="math inline">\(-32\)</span> ms [<span class="math inline">\(-48,-16\)</span>]; and in a fourth experiment, <span class="math inline">\(-43\)</span> ms [<span class="math inline">\(-84, -2\)</span>]. This range of values gives us a good ballpark estimate of the magnitude of the effect.</p>
<p>Yet another study involved English relative clauses is by <span class="citation">Fedorenko, Gibson, and Rohde (<a href="#ref-fedorenko2006nature">2006</a>)</span>. In this self-paced reading study, Fedorenko and colleagues compared reading times within the entire relative clause phrase (the relative pronoun and the noun+verb sequence inside the relative clause). Their data show that object relatives are harder to process than subject relatives; the difference in means is <span class="math inline">\(460\)</span> ms, with a confidence interval <span class="math inline">\([299, 621]\)</span> ms. This difference is much larger than in the studies mentioned above, but this is because of the long region of interest considered—it is well-known that the longer the reading/reaction time, the larger the standard deviation and therefore the larger the potential difference between means <span class="citation">(Wagenmakers and Brown <a href="#ref-wagenmakers2007linear">2007</a>)</span>.</p>
<p>One can also look at adjacent, related phenomena in sentence processing to get a feel for what the relative clause processing time difference should be. Research on similarity-based interference is closely related to relative clause processing differences: in both types of phenomenon, the assumption is that intervening nouns can increase processing difficulty. So let’s look at some reading studies on similarity-based interference.</p>
<p>In a recent study <span class="citation">(Jäger, Engelmann, and Vasishth <a href="#ref-JaegerEngelmannVasishth2017">2017</a>)</span>, we investigated the estimates from about 80 reading studies on interference. Interference here refers to the difficulty experienced by the comprehender during sentence comprehension (e.g., in reading studies) when they need to retrieve a particular word from working memory but other words with similar features hinder retrieval. The meta-analysis in <span class="citation">Jäger, Engelmann, and Vasishth (<a href="#ref-JaegerEngelmannVasishth2017">2017</a>)</span> suggests that the effect sizes for interference effects range from at most <span class="math inline">\(-50\)</span> to <span class="math inline">\(50\)</span> ms, depending on the phenomenon <span class="citation">(some kinds of interference cause speed-ups, others cause slow-downs; see the discussion in Engelmann, Jäger, and Vasishth <a href="#ref-EngelmannJaegerVasishth2019">2020</a>)</span>.</p>
<p>Given that the <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> design can be seen as falling within the broader class of interference effects <span class="citation">(Lewis and Vasishth <a href="#ref-lewisvasishth:cogsci05">2005</a>; Vasishth et al. <a href="#ref-VasishthEtAlTiCS2019">2019</a>; Vasishth and Engelmann <a href="#ref-VasishthEngelmann2022">2022</a>)</span>, it is reasonable to choose informative priors that reflect this observed range of interference effects in the literature.</p>
<p>The above discussion gives us some empirical basis for assuming that the object minus subject relative clause difference in the <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> study on English could range from 10 to at most 100 ms or so. Although we expect the effect to be positive, perhaps we don’t want to pre-judge this before we see the data. For this reason, we could decide on a <span class="math inline">\(\mathit{Normal}(0,50)\)</span> prior on the slope parameter in the model. This prior, which implies that we are 95% certain that the range of values lies between <span class="math inline">\(-100\)</span> and <span class="math inline">\(+100\)</span> ms. This prior is specifically for the millisecond scale, and specifically for the case where the critical region is one word (the relative clause verb in English).</p>
<p>In this particular example, it makes sense to assume that large effects like 100 ms are unlikely; this is so even if we do occasionally see estimates that are even higher than 100 ms in published data. For example, in <span class="citation">Gordon, Hendrick, and Johnson (<a href="#ref-gordon01">2001</a>)</span>, their experiments 1-4 have very large OR-SR differences at the relative clause verb: <span class="math inline">\(450\)</span> ms, <span class="math inline">\(250\)</span> ms, <span class="math inline">\(500\)</span> ms, and <span class="math inline">\(200\)</span> ms, respectively, with an approximate SE of <span class="math inline">\(50\)</span> ms. The number of subjects in the four experiments were 44, 48, 48, and 68, respectively. Given the other estimates mentioned above, we would be unwilling to take such large effects seriously because a major reason for observing overly large estimates in a one-word region of interest would be publication bias coupled with Type M error <span class="citation">(Gelman and Carlin <a href="#ref-gelmancarlin">2014</a>)</span>. Published studies in psycholinguistics are often underpowered, which leads to exaggerated estimates being published (Type M error). Because big-news effects are encouraged in major journals, overestimates tend to get published preferentially.<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a> In recent work <span class="citation">(Vasishth, Yadav, et al. <a href="#ref-SampleSizeCBB2021">2022</a>)</span>, we have shown that even under the optimistic assumption that effect size is approximately <span class="math inline">\(50\)</span> ms, achieving 80% power in English relative clause studies would require at least 120 subjects (if one takes the uncertainty of the effect estimate into account, many more subjects would be needed).</p>
<p>Of course, if our experiment is designed so that the critical region constitutes several words, as in the <span class="citation">Fedorenko, Gibson, and Rohde (<a href="#ref-fedorenko2006nature">2006</a>)</span> study, then one would have to choose a prior with a larger mean and standard deviation.</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:scaleprior" class="theorem"><strong>Box 6.1  </strong></span><strong>The scale of the parameter must be taken into account when eliciting a prior</strong></p>
</div>
<p>A related, important issue to consider when defining priors is the scale in which the parameter is defined. For example, if we were analyzing the <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> experiment using the log-normal likelihood, then the intercept and slope are on the log millisecond scale. A uniform prior on the intercept and slope parameter imply rather strange priors on the millisecond scale. For example, suppose we assume that the intercept on the log ms scale has priors <span class="math inline">\(\mathit{Normal}(0,10)\)</span> and the slope has a prior <span class="math inline">\(\mathit{Normal}(0,1)\)</span>. In the millisecond scale, the priors on the intercept and slope imply a very broad range of reading time differences between the two conditions, ranging from a very large negative value to a very large positive value, which obviously makes little sense:</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb336-1" data-line-number="1">intercept &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100000</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb336-2" data-line-number="2">slope &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100000</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb336-3" data-line-number="3">effect &lt;-<span class="st"> </span><span class="kw">exp</span>(intercept <span class="op">+</span><span class="st"> </span>slope <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">-</span></a>
<a class="sourceLine" id="cb336-4" data-line-number="4"><span class="st">  </span><span class="kw">exp</span>(intercept <span class="op">-</span><span class="st"> </span>slope <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb336-5" data-line-number="5"><span class="kw">quantile</span>(effect, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</a></code></pre></div>
<pre><code>##     2.5%    97.5% 
## -8751704  7399342</code></pre>
<p>In this connection, it may be useful to revisit the discussion in section <a href="ch-reg.html#sec-priorslogisticregression">4.3.2</a>, where we discussed the effect of prior specification on the log-odds scale and what that implies on the probability scale.</p>
</div>


<div class="extra">
<div class="theorem">
<p><span id="thm:cromwell" class="theorem"><strong>Box 6.2  </strong></span><strong>Cromwell’s rule</strong></p>
</div>
<p>A frequently asked question from newcomers to Bayes is: what if I define a too restricted prior? Wouldn’t that bias the posterior distribution? This concern is also raised quite often by critics of Bayesian methods. The key point here is that a good Bayesian analysis always involves a sensitivity analysis, and also includes prior and posterior predictive checks under different priors. One should reject the priors that make no sense in the particular research problem we are working on, or which unreasonably bias the posterior. As one gains experience with Bayesian modeling, these concerns will recede as we come to understand how useful and important priors are for interpreting the data. Chapter <a href="ch-workflow.html#ch-workflow">7</a> will elaborate on developing a sensible workflow for understanding and interpreting the results of a Bayesian analysis.</p>
<p>As an extreme example of an overly specific prior, if one were to define a <span class="math inline">\(\mathit{Normal}(0,10)\)</span> prior for the <span class="math inline">\(\alpha\)</span> and/or <span class="math inline">\(\beta\)</span> parameters on the millisecond scale for the <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> example above; that would definitely bias the posterior for the parameters. Let’s check this. Try running this code (the output of the code is suppressed here to conserve space). In this model, the correlation between the varying intercepts and varying slopes for subjects and for items are not included; this is only done in order to keep the model simple.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb338-1" data-line-number="1">restrictive_priors &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb338-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb338-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb338-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">500</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb338-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">500</span>), <span class="dt">class =</span> sigma)</a>
<a class="sourceLine" id="cb338-6" data-line-number="6">)</a>
<a class="sourceLine" id="cb338-7" data-line-number="7"></a>
<a class="sourceLine" id="cb338-8" data-line-number="8">fit_restrictive &lt;-<span class="st"> </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">||</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb338-9" data-line-number="9"><span class="st">                         </span>(c_cond <span class="op">||</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb338-10" data-line-number="10">                       <span class="dt">prior =</span> restrictive_priors,</a>
<a class="sourceLine" id="cb338-11" data-line-number="11">                    <span class="co"># Increase the iterations to avoid warnings</span></a>
<a class="sourceLine" id="cb338-12" data-line-number="12">                       <span class="dt">iter =</span> <span class="dv">4000</span>,</a>
<a class="sourceLine" id="cb338-13" data-line-number="13">                       df_gg05_rc</a>
<a class="sourceLine" id="cb338-14" data-line-number="14">                       )</a></code></pre></div>
<div class="sourceCode" id="cb339"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" data-line-number="1"><span class="kw">summary</span>(fit_restrictive)</a></code></pre></div>
<p>If you run the above code, you will see that the overly specific (and extremely unreasonable) priors on the intercept and slope will dominate in determining the posterior; such priors obviously make no sense. If there is ever any doubt about the implications of a prior, prior and posterior predictive checks should be used to investigate the implications.</p>
<p>Here, an important Bayesian principle is Cromwell’s rule <span class="citation">(Lindley <a href="#ref-lindley1991">1991</a>; Jackman <a href="#ref-jackman2009bayesian">2009</a>)</span>: we should generally allow for some uncertainty in our priors. A prior like <span class="math inline">\(\mathit{Normal}(0,10)\)</span> or <span class="math inline">\(Normal_{+}(0,10)\)</span> on the millisecond scale is clearly overly restrictive given what we’ve established about plausible values of the relative clause effect from existing data. A more reasonable but still quite tight prior would be <span class="math inline">\(\mathit{Normal}(0,50)\)</span>. In the spirit of Cromwell’s rule, just to be conservative, we can consider allowing (in a sensitivity analysis) larger possible effect sizes by adopting a prior such as <span class="math inline">\(\mathit{Normal}(0,75)\)</span>, and we allow the effect to be negative, even if theory suggests otherwise.</p>
<p>Although there are no fixed rules for deciding on a prior, a sensitivity analysis will quickly establish whether the prior or priors chosen are biasing the posterior. One critical thing to remember related to Cromwell’s rule is that if we categorically rule out a range of values a priori for a parameter by giving that range a probability of <span class="math inline">\(0\)</span>, the posterior will also never include that range of values, no matter what the data show. For example, in the <span class="citation">Reali and Christiansen (<a href="#ref-reali2007">2007</a>)</span> experiments, if we had used a truncated prior like <span class="math inline">\(\mathit{Normal}_{+}(0,50)\)</span>, the posterior can never show the observed negative sign on the effects as reported in the paper. As a general rule, therefore, one should allow the effect to vary in both directions, positive and negative. Sometimes unidirectional priors are justified; in those cases, it is of course legitimate to use them. An example is the prior on standard deviations (which cannot be negative).</p>
</div>

</div>
<div id="sec-varcomppriors" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.1.4</span> Eliciting priors for the variance components<a href="ch-priors.html#sec-varcomppriors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having defined the priors for the intercept and the slope, we are left with prior specifications for the variance component parameters. At least in psycholinguistics, the residual standard deviation is usually the largest source of variance; the by-subject intercepts’ standard deviation is usually the next-largest value, and if experimental items are designed to have minimal variance, then these are usually the smallest components. Here again, we can look at some previous data to get a sense of what the priors should look like.</p>
<p>For example, we could use the estimates for the variance components from existing studies. Figure <a href="ch-priors.html#fig:vcs">6.4</a> shows the empirical distributions from 10 published studies. There are four classes of variance component: the subject and item intercept standard deviations, the standard deviations of slopes, and the standard deviations of the residuals. In each case, we can compute the estimated means and standard deviations of each type of variance component, and then use these to define normal distributions truncated at <span class="math inline">\(0\)</span>. The empirically estimated distributions of the variance components are shown in Figure <a href="ch-priors.html#fig:vcs">6.4</a>. The estimated means and standard deviations of each type of variance component are as follows:</p>
<ul>
<li>Subject intercept SDs: estimated mean: 165, estimated standard deviation (sd): 55.</li>
<li>Item intercept SDs: mean: 49, sd: 52.</li>
<li>Slope SDs: mean 39, sd: 58.</li>
<li>Residual SDs: mean: 392, sd: 140.</li>
</ul>
<p>The largest standard deviations are those from the item intercepts and the residual standard deviation, so these are the ones we will focus on. We can and should orient our prior for the group-level (also known as random effects) variance components to subsume these larger values.</p>

<div class="figure"><span style="display:block;" id="fig:vcs"></span>
<img src="bookdown_files/figure-html/vcs-1.svg" alt="Histograms of empirical distributions of the different variance components from ten publishes studies. The y-axis shows counts rather than density in order to make it clear that we are working with only a few data sets." width="672" />
<p class="caption">
FIGURE 6.4: Histograms of empirical distributions of the different variance components from ten publishes studies. The y-axis shows counts rather than density in order to make it clear that we are working with only a few data sets.
</p>
</div>
<p>We can now use the equations <a href="ch-reg.html#eq:meantrunc">(4.2)</a> and <a href="ch-reg.html#eq:vartrunc">(4.3)</a> shown in Box <a href="ch-reg.html#thm:truncation">4.1</a> to work out the means and standard deviations of a corresponding truncated normal distribution. As an example, we could assume a prior distribution truncated at <span class="math inline">\(0\)</span> from below, and at <span class="math inline">\(1000\)</span> ms from above. That is, <span class="math inline">\(a=0\)</span>, and <span class="math inline">\(b=1000\)</span>.</p>
<p>We can write a function that takes the estimated means and standard deviations, and returns the mean and standard deviation of the corresponding truncated distribution (see Box <a href="ch-reg.html#thm:truncation">4.1</a>).</p>
<p>The largest variance component among the group-level effects (that is, all variance components other than the residual standard deviation) is the by-subjects intercept. One can compute the mean and standard deviation of of the truncated distribution that would generate the observed mean and standard deviation of the item-level estimates:</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb340-1" data-line-number="1"><span class="co"># Subject intercept SDs: </span></a>
<a class="sourceLine" id="cb340-2" data-line-number="2"><span class="kw">compute_meansd_parent</span>(<span class="dt">mean_trunc =</span> intsubjmean, <span class="dt">sd_trunc =</span> intsubjsd)</a></code></pre></div>
<pre><code>## [1] &quot;location:  165 scale:  55&quot;</code></pre>
<p>The corresponding truncated distribution is shown in Figure <a href="ch-priors.html#fig:informedintsd">6.5</a>.</p>
<div class="figure"><span style="display:block;" id="fig:informedintsd"></span>
<img src="bookdown_files/figure-html/informedintsd-1.svg" alt="A truncated normal distribution (with location 165 and scale 55) representing an empirically derived prior distribution for the parameter for the by-subjects intercept adjustment in a hierarchical model." width="672" />
<p class="caption">
FIGURE 6.5: A truncated normal distribution (with location 165 and scale 55) representing an empirically derived prior distribution for the parameter for the by-subjects intercept adjustment in a hierarchical model.
</p>
</div>
<p>The prior shown in Figure <a href="ch-priors.html#fig:informedintsd">6.5</a> looks a bit too restrictive; it could well happen that in a future study the by-subject intercept standard deviation is closer to 500 ms. Taking Cromwell’s rule into account, one could widen the scale parameter of the truncated normal to, say 200. The result is Figure <a href="ch-priors.html#fig:informedintsd2">6.6</a>.</p>
<div class="figure"><span style="display:block;" id="fig:informedintsd2"></span>
<img src="bookdown_files/figure-html/informedintsd2-1.svg" alt="A truncated normal distribution (with location 165 and scale 200) representing an empirically derived prior distribution for the parameter for the by-subjects intercept adjustment in a hierarchical model taking Cromwell’s rule into account." width="672" />
<p class="caption">
FIGURE 6.6: A truncated normal distribution (with location 165 and scale 200) representing an empirically derived prior distribution for the parameter for the by-subjects intercept adjustment in a hierarchical model taking Cromwell’s rule into account.
</p>
</div>
<p>Figure <a href="ch-priors.html#fig:informedintsd2">6.6</a> does not look too unreasonable as an informative prior for this variance component. This prior will also serve us well for all the other group-level effects (the random intercept for items, and the random slopes for subject and item), which will have smaller values.</p>
<!--
The means for the by-item intercepts and for the (subject and item) slopes have relatively low values for the mean and standard deviations for the corresponding truncated distributions:


```{.r .fold-show}
# Item intercept SDs: 
compute_meansd_parent(mean_trunc = intitemmean, sd_trunc = intitemsd)
```

```
## [1] "location:  0 scale:  134"
```

```{.r .fold-show}
#Slope SDs: 
compute_meansd_parent(mean_trunc = slopemean, sd_trunc = slopesd)
```

```
## [1] "location:  0 scale:  188"
```

We can simply use the wider truncated distribution shown in Figure \@ref(fig:informedintsd2).
-->
<p>Finally, the prior for the residual standard deviation is going to have to allow a broader range of wider values:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" data-line-number="1"><span class="kw">compute_meansd_parent</span>(<span class="dt">mean_trunc =</span> resmean, <span class="dt">sd_trunc =</span> ressd)</a></code></pre></div>
<pre><code>## [1] &quot;location:  391 scale:  142&quot;</code></pre>
<p>Figure <a href="ch-priors.html#fig:informedintsd3">6.7</a> shows a plausible informative prior derived from the empirical estimates.</p>
<div class="figure"><span style="display:block;" id="fig:informedintsd3"></span>
<img src="bookdown_files/figure-html/informedintsd3-1.svg" alt="A truncated normal distribution representing an empirically derived prior distribution for the parameter for the residual standard deviation in a hierarchical model." width="672" />
<p class="caption">
FIGURE 6.7: A truncated normal distribution representing an empirically derived prior distribution for the parameter for the residual standard deviation in a hierarchical model.
</p>
</div>
<p>We stress again that Cromwell’s rule should generally be kept in mind—it’s usually better to have a little bit more uncertainty than warranted than too tight a prior. An overly tight prior will ensure that the posterior is entirely driven by the prior. Again, prior predictive checks should be an integral part of the process of establishing a sensible set of priors for the variance components. This point about prior predictive checks will be elaborated on with examples in chapter <a href="ch-workflow.html#ch-workflow">7</a>.</p>
<p>We now apply the relatively informative priors we came up with above to analyze the <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> data. Applying Cromwell’s rule, we allow for a bit more uncertainty than our existing empirical data suggest.</p>
<p>Specifically, we could choose the following informative priors for the <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> data:</p>
<ul>
<li>The intercept: <span class="math inline">\(\alpha \sim \mathit{Normal}(500,100)\)</span></li>
<li>The slope: <span class="math inline">\(\beta \sim \mathit{Normal}(50,50)\)</span></li>
<li>All variance components: <span class="math inline">\(\sigma_u, \sigma_w \sim \mathit{Normal}_{+}(165,200)\)</span></li>
<li>The residual standard deviation: <span class="math inline">\(\sigma \sim \mathit{Normal}_{+}(391,200)\)</span></li>
</ul>
<p>The first step is to check whether the prior predictive distribution makes sense. Figure <a href="ch-priors.html#fig:priorpredgrodner">6.8</a> shows that the prior predictive distributions are not too implausible, although they could be improved further. One big problem is the normal distribution assumed in the model; a log-normal distribution captures the shape of the distribution of the <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> data better than a normal distribution. The discrepancy between the <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> data and our prior predictive distribution implies that we might be using the wrong likelihood. Another problem is that the reading times in the prior predictive distribution can be negative—this is also a consequence of our using the wrong likelihood. As an exercise, fit a model with a log-normal likelihood and informative priors based on previous data. When using a log-normal likelihood, the prior for the slope parameter obviously has to be on the log scale. Therefore, we will need to define an informative prior on the log scale for the slope parameter. For example, consider the following prior on the slope: <span class="math inline">\(\mathit{Normal}(0.12, 0.04)\)</span>. Here is how to interpret this on the millisecond scale: Assuming a mean reading time of 6 log ms, this prior roughly corresponds to an effect size on the millisecond scale that has a 95 credible interval ranging from <span class="math inline">\(16\)</span> ms to <span class="math inline">\(81\)</span> ms. Review <a href="ch-compbda.html#sec-lognormal">3.7.2</a> if you have forgotten how this transformation was done.</p>
<p>For now, because our running example uses a normal likelihood on reading times in milliseconds, we can retain these priors.</p>

<div class="figure"><span style="display:block;" id="fig:priorpredgrodner"></span>
<img src="bookdown_files/figure-html/priorpredgrodner-1.svg" alt="Prior predictive distributions from the model (using a normal likelihood) to be used for the Grodner and Gibson data analysis. The panels show eight prior predictive distributions." width="672" />
<p class="caption">
FIGURE 6.8: Prior predictive distributions from the model (using a normal likelihood) to be used for the Grodner and Gibson data analysis. The panels show eight prior predictive distributions.
</p>
</div>
<p>The sensitivity analysis could then be displayed, showing the posteriors under different prior settings. Figures <a href="ch-priors.html#fig:uniformpriorENRC">6.9</a> and <a href="ch-priors.html#fig:infpriorENRC">6.10</a> show the posteriors under two distinct sets of priors.</p>

<div class="figure"><span style="display:block;" id="fig:uniformpriorENRC"></span>
<img src="bookdown_files/figure-html/uniformpriorENRC-1.svg" alt="Posterior distributions of parameters for the English relative clause data, using uniform priors (\(\mathit{Uniform}(0,2000)\)) on the intercept and slope." width="672" />
<p class="caption">
FIGURE 6.9: Posterior distributions of parameters for the English relative clause data, using uniform priors (<span class="math inline">\(\mathit{Uniform}(0,2000)\)</span>) on the intercept and slope.
</p>
</div>

<div class="figure"><span style="display:block;" id="fig:infpriorENRC"></span>
<img src="bookdown_files/figure-html/infpriorENRC-1.svg" alt="Posterior distributions of parameters for the English relative clause data, using relatively informative priors on the intercept and slope." width="672" />
<p class="caption">
FIGURE 6.10: Posterior distributions of parameters for the English relative clause data, using relatively informative priors on the intercept and slope.
</p>
</div>
<p>What can one do if one doesn’t know absolutely anything about one’s research problem? An example is the power posing data that we encountered in Chapter <a href="ch-reg.html#ch-reg">4</a>, in an exercise in section <a href="ch-reg.html#sec-LMexercises">4.6</a>. Here, we investigated the change in testosterone levels after the subject was either asked to adopt a high power pose or a low power pose (a between-subjects design). Not being experts in this domain, we may find ourselves stumped for priors. In such a situation, it could be defensible to use uninformative priors like <span class="math inline">\(\mathit{Cauchy}(0,2.5)\)</span>, at least initially. However, as discussed in a later chapter, if one is committed to doing a Bayes factor analysis, then we are obliged to think carefully about plausible a priori values of the effect. This would require consulting one or more experts or reading the literature on the topic to obtain ballpark estimates. An exercise at the end of this chapter will elaborate on this idea. We turn next to the topic of eliciting priors from experts.</p>
</div>
</div>
<div id="eliciting-priors-from-experts" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.2</span> Eliciting priors from experts<a href="ch-priors.html#eliciting-priors-from-experts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It can happen that one is working on a research problem where either our own prior knowledge is lacking, or we need to incorporate a range of competing prior beliefs into the analysis. In such situations, it becomes important to elicit priors from experts other than oneself. Although informal elicitation can be a perfectly legitimate approach, there does exist a well-developed methodology for systematically eliciting priors in Bayesian statistics <span class="citation">(O’Hagan et al. <a href="#ref-ohagan2006uncertain">2006</a>)</span>.</p>
<p>The particular method developed by O’Hagan and colleagues comes with an R package called <code>SHELF</code>, which stands for the Sheffield Elicitation Framework; the method was developed by statisticians at the University of Sheffield, UK. <code>SHELF</code> is available from <a href="http://www.tonyohagan.co.uk/shelf/" class="uri">http://www.tonyohagan.co.uk/shelf/</a>.
This framework comes with a detailed set of instructions and a fixed procedure for eliciting distributions. It also provides detailed guidance on documenting the elicitation process, thereby allowing a full record of the elicitation process to be created. Creating such a record is important because the elicitation procedure needs to be transparent to a third party reading the final report on the data analysis.</p>
<p>The SHELF procedure works as follows. There is a facilitator and an expert (or a group of experts; we will consider the single expert case here, but one can easily extend the approach to multiple experts).</p>
<ul>
<li>A pre-elicitation form is filled out by the facilitator in consultation with the expert. This form sets the stage for the elicitation exercise and records some background information, such as the nature of the expertise of the assessor.</li>
<li>Then, an elicitation method is chosen. Simple methods are the most effective. One good approach is the quartile method. The expert first decides on a lower and upper limit of possible values for the quantity to be estimated. Because the lower and upper bounds are elicited before the median, this minimizes the effects of the “anchoring and adjustment heuristic” <span class="citation">(O’Hagan et al. <a href="#ref-ohagan2006uncertain">2006</a>)</span>, whereby experts tend to anchor their subsequent estimates of quartiles based on their first judgement of the median. Following this, a median value is decided on, and lower and upper quartiles are elicited. The SHELF package has functions to display these quartiles graphically, allowing the expert to adjust them at this stage if necessary. It is important for the expert to confirm that, in their judgement, the four partitioned regions that result have equal probability.</li>
<li>The elicited distribution is then displayed as a density plot (several choices of probability density functions are available, but we will usually use the normal or the truncated normal in this chapter); this graphical summary serves to give feedback to the expert. The parameters of the distribution are also displayed. Once the expert agrees to the final density, the parameters can be considered the expert’s judgement regarding the prior distribution of the bias. One can consult multiple experts and either combine their judgements into one prior, or consider each expert’s prior separately in a sensitivity analysis.</li>
</ul>
<p>When eliciting priors from more than one expert, one can elicit the priors separately and then use the priors separately in a sensitivity analysis. This approach takes each individual expert’s opinion in interpreting the data and can be a valuable sensitivity analysis <span class="citation">(for an example from psycholinguistics, see the discussion surrounding Table 2.2 on p. 47 in Vasishth and Engelmann <a href="#ref-VasishthEngelmann2022">2022</a>)</span>. Alternatively, one can pool the priors together <span class="citation">(see Spiegelhalter, Abrams, and Myles <a href="#ref-spiegelhalter2004bayesian">2004</a> for discussion)</span> and create a single consensus prior; this would amount to an average of the differing opinions about prior distributions. A third approach is to elicit a consensus prior by bringing all the experts together and eliciting a prior from the group in a single setting. Of course, these approaches are not mutually exclusive. One of the hallmark properties of Bayesian analysis is that the posterior distribution of the parameter of interest can be investigated in light of differing prior beliefs and the data (and of course the model). Box <a href="ch-priors.html#thm:shelfexample">6.3</a> illustrates a simple elicitation procedure involving two experts; the example is adapted from the <code>SHELF</code> package’s vignette.</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:shelfexample" class="theorem"><strong>Box 6.3  </strong></span><strong>Example: prior elicitiation using SHELF</strong></p>
</div>
<p>An example of prior elicitation using SHELF is shown below. This example is adapted from the SHELF vignette.</p>
<p>Suppose that two experts are consulted separately. The question asked of the experts is what they think that a probability parameter <span class="math inline">\(X\)</span> has as plausible values. The parameter <span class="math inline">\(X\)</span> can be seen as a percentage; so, it ranges from 0 to 100.</p>
<p><strong>Step 1</strong>: Elicit quartiles and median from each expert.</p>
<ul>
<li>Expert A states that <span class="math inline">\(P(X&lt;30)=0.25, P(X&lt;40)=0.5, P(X&lt;50)=0.75\)</span>.</li>
<li>Expert B states that <span class="math inline">\(P(X&lt;20)=0.25, P(X&lt;25)=0.5, P(X&lt;35)=0.75\)</span>.</li>
</ul>
<p><strong>Step 2</strong>: Fit the implied distributions for each expert’s judgements and plot the distributions, along with a pooled distribution (the linear pool in the figure).</p>

<div class="sourceCode" id="cb344"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb344-1" data-line-number="1"><span class="kw">library</span>(SHELF)</a>
<a class="sourceLine" id="cb344-2" data-line-number="2">elicited &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">20</span>, <span class="fl">0.25</span>,</a>
<a class="sourceLine" id="cb344-3" data-line-number="3">                     <span class="dv">40</span>, <span class="dv">25</span>, <span class="fl">0.5</span>,</a>
<a class="sourceLine" id="cb344-4" data-line-number="4">                     <span class="dv">50</span>, <span class="dv">35</span>, <span class="fl">0.75</span>),</a>
<a class="sourceLine" id="cb344-5" data-line-number="5">                   <span class="dt">nrow =</span> <span class="dv">3</span>, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb344-6" data-line-number="6">dist_2expr &lt;-<span class="st"> </span><span class="kw">fitdist</span>(<span class="dt">vals =</span> elicited[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],</a>
<a class="sourceLine" id="cb344-7" data-line-number="7">                      <span class="dt">probs =</span> elicited[, <span class="dv">3</span>],</a>
<a class="sourceLine" id="cb344-8" data-line-number="8">                      <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb344-9" data-line-number="9"><span class="kw">plotfit</span>(dist_2expr, <span class="dt">lp =</span> <span class="ot">TRUE</span>, <span class="dt">returnPlot =</span> <span class="ot">TRUE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb344-10" data-line-number="10"><span class="st">  </span><span class="kw">scale_color_grey</span>()</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:shelfplot1"></span>
<img src="bookdown_files/figure-html/shelfplot1-1.svg" alt="Visualizing priors elicited from two experts for a parameter \(X\) representing a percentage ranging from 0 to 100." width="672"  />
<p class="caption">
FIGURE 6.11: Visualizing priors elicited from two experts for a parameter <span class="math inline">\(X\)</span> representing a percentage ranging from 0 to 100.
</p>
</div>
<p><strong>Step 3</strong>: Then bring the two experts together and elicit a consensus distribution.</p>
<p>Suppose that the experts agree that <span class="math inline">\(P(X&lt;25)=0.25, P(X&lt;30)=0.5, P(X&lt;40)=0.75\)</span>. The consensus distribution is then:</p>

<div class="sourceCode" id="cb345"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1">elicited &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">25</span>, <span class="fl">0.25</span>,</a>
<a class="sourceLine" id="cb345-2" data-line-number="2">                     <span class="dv">30</span>, <span class="fl">0.5</span>,</a>
<a class="sourceLine" id="cb345-3" data-line-number="3">                     <span class="dv">40</span>, <span class="fl">0.75</span>),</a>
<a class="sourceLine" id="cb345-4" data-line-number="4">                   <span class="dt">nrow =</span> <span class="dv">3</span>, <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb345-5" data-line-number="5">dist_cons &lt;-<span class="st"> </span><span class="kw">fitdist</span>(<span class="dt">vals =</span> elicited[,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb345-6" data-line-number="6">                     <span class="dt">probs =</span> elicited[,<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb345-7" data-line-number="7">                     <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb345-8" data-line-number="8"><span class="kw">plotfit</span>(dist_cons, <span class="dt">ql =</span> <span class="fl">0.05</span>, <span class="dt">qu =</span> <span class="fl">0.95</span>, <span class="dt">returnPlot =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:shelfplot2"></span>
<img src="bookdown_files/figure-html/shelfplot2-1.svg" alt="Visualizing a consensus prior from two experts for a parameter \(X\) representing a percentage ranging from 0 to 100." width="672"  />
<p class="caption">
FIGURE 6.12: Visualizing a consensus prior from two experts for a parameter <span class="math inline">\(X\)</span> representing a percentage ranging from 0 to 100.
</p>
</div>
<p><strong>Step 4</strong>: Give feedback to the experts by showing them the 5th and
95th percentiles, and check that these bounds match their beliefs. If not, then repeat the above steps.</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb346-1" data-line-number="1"><span class="kw">feedback</span>(dist_cons, <span class="dt">quantiles =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</a></code></pre></div>
<pre><code>## $fitted.quantiles
##      normal     t gamma lognormal logt beta hist
## 0.05   12.5  7.49  16.2      17.3 14.8 15.2    5
## 0.95   50.4 55.10  53.2      55.3 64.1 51.2   88
##      mirrorgamma mirrorlognormal mirrorlogt
## 0.05        10.5            9.18       2.08
## 0.95        49.1           48.60      52.10
## 
## $fitted.probabilities
##    elicited normal     t gamma lognormal  logt  beta hist
## 25     0.25  0.288 0.289 0.279     0.274 0.275 0.283 0.25
## 30     0.50  0.451 0.453 0.461     0.466 0.469 0.456 0.50
## 40     0.75  0.772 0.774 0.769     0.767 0.768 0.770 0.75
##    mirrorgamma mirrorlognormal mirrorlogt
## 25       0.292           0.295      0.296
## 30       0.446           0.444      0.447
## 40       0.772           0.773      0.775</code></pre>
</div>

</div>
<div id="deriving-priors-from-meta-analyses" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.3</span> Deriving priors from meta-analyses<a href="ch-priors.html#deriving-priors-from-meta-analyses" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Meta-analysis has been used widely in clinical research <span class="citation">(Higgins and Green <a href="#ref-cochrane">2008</a>; Sutton et al. <a href="#ref-sutton2012evidence">2012</a>; DerSimonian and Laird <a href="#ref-dersimonian1986meta">1986</a>; Normand <a href="#ref-normand1999tutorial">1999</a>)</span>, but it is used relatively rarely in psychology and linguistics. Random-effects meta-analysis (discussed in a later chapter in detail) is an especially useful tool in cognitive science.</p>
<p>Meta-analysis is not a magic bullet; this is because of publication bias—usually only (supposedly) newsworthy results are published, leading to a skewed picture of the effects. As a consequence, meta-analysis will yield biased estimates; but they can still tell us something about what we know so far from published studies, if only that the studies are too noisy to be interpretable. Nevertheless, some prior information is better than no information. As long as one remains aware of the limitations of meta-analysis, one can still use them effectively to study one’s research questions.</p>
<p>We begin with observed effects <span class="math inline">\(y_n\)</span> (e.g., estimated difference between two conditions) and their estimated standard errors (SEs); the SEs serve as an indication of the precision of the estimate, with larger SEs implying a low-precision estimate. Once we have collected the observed estimates (e.g., from published studies), we can define an assumed underlying generative process whereby each study <span class="math inline">\(n=1,\dots,N\)</span> has an unknown true mean <span class="math inline">\(\zeta_n\)</span>:</p>
<p><span class="math inline">\(y_n \sim \mathit{Normal}(\zeta_n,SE_n)\)</span></p>
<p>A further assumption is that each unknown true mean <span class="math inline">\(\zeta_n\)</span> in each study is generated from a distribution that has some true overall mean <span class="math inline">\(\zeta\)</span>, and standard deviation <span class="math inline">\(\tau\)</span>. The standard deviation <span class="math inline">\(\tau\)</span> reflects between-study variation, which could be due to different subjects being used in each study, different lab protocols, different methods, different languages being studied, etc.</p>
<p><span class="math inline">\(\zeta_n \sim \mathit{Normal}(\zeta,\tau)\)</span></p>
<p>This kind of meta-analysis is actually the familiar hierarchical model we have already encountered in chapter <a href="ch-hierarchical.html#ch-hierarchical">5</a>. As in hierarchical models, hyperpriors have to be defined for <span class="math inline">\(\zeta\)</span> and <span class="math inline">\(\tau\)</span>. A useful application of this kind of meta-analysis is to derive a posterior distribution for <span class="math inline">\(\zeta\)</span> based on the available evidence; this posterior can be used (e.g., with a normal approximation) as a prior for a future study.</p>
<p>A simple example is the published data on Chinese relative clauses; the data are a selection from <span class="citation">Vasishth (<a href="#ref-VasishthMScStatistics">2015</a>)</span>; two estimates for which standard errors could not be computed from the reported statistical summaries have been removed. Table <a href="ch-priors.html#tab:chinesemeans">6.2</a> shows the mean difference between the object and subject relative, along with the standard error, that was derived from published reading studies on Chinese relatives.</p>
<table>
<caption>
<span id="tab:chinesemeans">TABLE 6.2: </span>The difference between object and subject relative clause reading times (effect), along with their standard errors (SE), from different published reading studies on Chinese relative clauses.
</caption>
<thead>
<tr>
<th style="text-align:left;">
study.id
</th>
<th style="text-align:left;">
study
</th>
<th style="text-align:right;">
y
</th>
<th style="text-align:right;">
se
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
Hsiao et al 03
</td>
<td style="text-align:right;">
<span class="math inline">\(50.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(25.0\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
4
</td>
<td style="text-align:left;">
Vas. et al 13, E2
</td>
<td style="text-align:right;">
<span class="math inline">\(82.6\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(41.2\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
Vas. et al 13, E3
</td>
<td style="text-align:right;">
<span class="math inline">\(-109.4\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(54.8\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
Jaeg. et al 15, E1
</td>
<td style="text-align:right;">
<span class="math inline">\(55.6\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(65.1\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
7
</td>
<td style="text-align:left;">
Jaeg. et al 15, E2
</td>
<td style="text-align:right;">
<span class="math inline">\(81.9\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(36.3\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
9
</td>
<td style="text-align:left;">
Wu 09
</td>
<td style="text-align:right;">
<span class="math inline">\(50.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(23.0\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
10
</td>
<td style="text-align:left;">
Qiao et al 11, E1
</td>
<td style="text-align:right;">
<span class="math inline">\(-70.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(42.0\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
11
</td>
<td style="text-align:left;">
Qiao et al 11, E2
</td>
<td style="text-align:right;">
<span class="math inline">\(6.2\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(19.9\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
12
</td>
<td style="text-align:left;">
Lin &amp; Garn. 11, E1
</td>
<td style="text-align:right;">
<span class="math inline">\(-100.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(30.0\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
14
</td>
<td style="text-align:left;">
Chen et al 08
</td>
<td style="text-align:right;">
<span class="math inline">\(75.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(35.5\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
15
</td>
<td style="text-align:left;">
C Lin &amp; Bev. 06
</td>
<td style="text-align:right;">
<span class="math inline">\(100.0\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(80.0\)</span>
</td>
</tr>
</tbody>
</table>
<p>Suppose that we want to do a new study investigating the difference between object and subject relative clauses, and suppose that in the sensitivity analysis, one of the priors we want is an empirically justifiable informative prior. Of course, the sensitivity analysis will also contain uninformative priors; we have seen examples of such priors in the previous chapters.</p>
<p>We can derive an empirically justified prior by conducting a group-level effects meta-analysis. We postpone discussion of how exactly to fit such a model to chapter <a href="ch-remame.html#ch-remame">13</a>; here, we simply report the posterior distribution of the overall effect <span class="math inline">\(\zeta\)</span> based on the prior data, ignoring the details of model fitting.</p>

<div class="figure"><span style="display:block;" id="fig:chinesemetanalysisposterior"></span>
<img src="bookdown_files/figure-html/chinesemetanalysisposterior-1.svg" alt="The posterior distribution of the difference between object and subject relative clause processing in Chinese relative clause data, computed from a random-effects meta-analysis using published Chinese relative clause data from reading studies." width="672" />
<p class="caption">
FIGURE 6.13: The posterior distribution of the difference between object and subject relative clause processing in Chinese relative clause data, computed from a random-effects meta-analysis using published Chinese relative clause data from reading studies.
</p>
</div>
<p>The posterior distribution of <span class="math inline">\(\zeta\)</span> is shown in Figure <a href="ch-priors.html#fig:chinesemetanalysisposterior">6.13</a>. What we can derive from this posterior distribution of <span class="math inline">\(\zeta\)</span> is a normal approximation that represents what we know so far about Chinese relatives, based on the available data. The key here is the word “available”; almost certaintly there exist studies that were inconclusive and were therefore never published. The published record is always biased because of the nature of the publication game in science (only supposedly newsworthy results get published).</p>
<p>The mean of the posterior is <span class="math inline">\(20\)</span> ms, and the width of the 95% credible intervals is <span class="math inline">\(23-2=21\)</span> ms. Since the 95% credible interval has a width that is approximately four times the standard deviation (assuming a normal distribution), we can work out the standard deviation by dividing the width by four: <span class="math inline">\(5.25\)</span>.
Given these estimates, we could use a normal distribution with mean <span class="math inline">\(20\)</span> and standard deviation <span class="math inline">\(5.25\)</span> as an informative prior in a sensitivity analysis. Notice that if the credible interval were to range from <span class="math inline">\(-10\)</span> to <span class="math inline">\(24\)</span> ms, the width would be 34 ms, not 14 (we have noticed that sometimes people are confused by the negative sign).</p>
<p>As an example, we will analyze a data set from <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span> that was not part of the above meta-analysis, and we will use the meta-analysis posterior as an informative prior. First, we load the data and sum-code the predictor:</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb348-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_gibsonwu&quot;</span>)</a>
<a class="sourceLine" id="cb348-2" data-line-number="2">df_gibsonwu &lt;-<span class="st"> </span>df_gibsonwu <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb348-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cond =</span> <span class="kw">if_else</span>(type <span class="op">==</span><span class="st"> &quot;obj-ext&quot;</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="dv">-1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>))</a></code></pre></div>
<p>Because we will now use a log-normal likelihood for the reading time data, we need to work out what the meta-analysis posterior of <span class="math inline">\(\zeta\)</span> corresponds to on the log scale. The grand mean reading time of the <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span> data on the log scale is <span class="math inline">\(6.1\)</span>. In order to arrive at approximately the mean difference of <span class="math inline">\(20\)</span> ms, the log-scale value of the mean difference would be <span class="math inline">\(0.041\)</span>, with a 95% credible interval <span class="math inline">\([-0.079,0.145]\)</span>, which implies a standard deviation of <span class="math inline">\((0.0145-(-0.79))/4=0.2\)</span>. The calculations are shown below (see Box <a href="ch-reg.html#thm:lognormal">4.3</a> in chapter <a href="ch-reg.html#ch-reg">4</a> for an explanation of a model with a log-normal likelihood).</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb349-1" data-line-number="1">int &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">log</span>(df_gibsonwu<span class="op">$</span>rt))</a>
<a class="sourceLine" id="cb349-2" data-line-number="2">b &lt;-<span class="st"> </span><span class="fl">0.041</span></a>
<a class="sourceLine" id="cb349-3" data-line-number="3"><span class="kw">exp</span>(int <span class="op">+</span><span class="st"> </span>b <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">exp</span>(int <span class="op">-</span><span class="st"> </span>b <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 17.6</code></pre>
<div class="sourceCode" id="cb351"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb351-1" data-line-number="1">lower &lt;-<span class="st"> </span><span class="fl">-0.079</span></a>
<a class="sourceLine" id="cb351-2" data-line-number="2"><span class="kw">exp</span>(int <span class="op">+</span><span class="st"> </span>lower <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">exp</span>(int <span class="op">-</span><span class="st"> </span>lower <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] -33.9</code></pre>
<div class="sourceCode" id="cb353"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb353-1" data-line-number="1">upper &lt;-<span class="st"> </span><span class="fl">0.145</span></a>
<a class="sourceLine" id="cb353-2" data-line-number="2"><span class="kw">exp</span>(int <span class="op">+</span><span class="st"> </span>upper <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">exp</span>(int <span class="op">-</span><span class="st"> </span>upper <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 62.2</code></pre>
<p>As always, we will do a sensitivity analysis, using uninformative priors on the slope parameter (<span class="math inline">\(\mathit{Normal}(0,1)\)</span>), as well as the meta-analysis prior.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb355-1" data-line-number="1"><span class="co">## uninformative priors on the parameters of interest</span></a>
<a class="sourceLine" id="cb355-2" data-line-number="2"><span class="co">## and on the variance components:</span></a>
<a class="sourceLine" id="cb355-3" data-line-number="3">fit_gibsonwu_log &lt;-<span class="st"> </span><span class="kw">brm</span>(rt <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span></a>
<a class="sourceLine" id="cb355-4" data-line-number="4"><span class="st">                          </span>(c_cond <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb355-5" data-line-number="5"><span class="st">                          </span>(c_cond <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb355-6" data-line-number="6">                        <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb355-7" data-line-number="7">                        <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb355-8" data-line-number="8">                          <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb355-9" data-line-number="9">                            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb355-10" data-line-number="10">                            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb355-11" data-line-number="11">                            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb355-12" data-line-number="12">                            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb355-13" data-line-number="13">                            <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb355-14" data-line-number="14">                          ),</a>
<a class="sourceLine" id="cb355-15" data-line-number="15">                        <span class="dt">data =</span> df_gibsonwu</a>
<a class="sourceLine" id="cb355-16" data-line-number="16">                        )</a>
<a class="sourceLine" id="cb355-17" data-line-number="17"><span class="co">## meta-analysis priors:</span></a>
<a class="sourceLine" id="cb355-18" data-line-number="18">fit_gibsonwu_ma &lt;-<span class="st"> </span><span class="kw">brm</span>(rt <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span></a>
<a class="sourceLine" id="cb355-19" data-line-number="19"><span class="st">                         </span>(c_cond <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb355-20" data-line-number="20"><span class="st">                         </span>(c_cond <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb355-21" data-line-number="21">                       <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb355-22" data-line-number="22">                       <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb355-23" data-line-number="23">                         <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb355-24" data-line-number="24">                           <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb355-25" data-line-number="25">                           <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="fl">0.041</span>, <span class="fl">0.2</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb355-26" data-line-number="26">                           <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb355-27" data-line-number="27">                           <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb355-28" data-line-number="28">                           <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb355-29" data-line-number="29">                         ),</a>
<a class="sourceLine" id="cb355-30" data-line-number="30">                       <span class="dt">data =</span> df_gibsonwu</a>
<a class="sourceLine" id="cb355-31" data-line-number="31">                       )</a></code></pre></div>
<p>A summary of the posteriors (means and 95% credible intervals) under the <span class="math inline">\(\mathit{Normal}(0,1)\)</span> and the meta-analysis prior is shown in Table <a href="ch-priors.html#tab:mapriors">6.3</a>. In this particular case, the posteriors are not strongly influenced by the two different priors. The differences between the two posteriors are small, but these differences could in principle lead to different outcomes in a Bayes factor analysis.</p>
<table>
<caption>
<span id="tab:mapriors">TABLE 6.3: </span>A summary of the posteriors under a relatively uninformative prior and an informative prior based on a meta-analysis, for the Chinese relative clause data from Gibson and Wu, 2013.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Priors
</th>
<th style="text-align:left;">
Mean
</th>
<th style="text-align:left;">
Lower
</th>
<th style="text-align:left;">
Upper
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0,1)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.07\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.18\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.04\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0.041, 0.2)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.06\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.17\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.04\)</span>
</td>
</tr>
</tbody>
</table>
</div>
<div id="using-previous-experiments-posteriors-as-priors-for-a-new-study" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.4</span> Using previous experiments’ posteriors as priors for a new study<a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a situation where we are attempting to replicate a previous study’s results, we can derive an informative prior for the analysis of the replication attempt by figuring out a prior based on the previous study’s posterior distribution. In the previous chapter, we encountered this in one of the exercises: Given data on Chinese relatives <span class="citation">(Gibson and Wu <a href="#ref-gibsonwu">2013</a>)</span>, we want to replicate the effect with a new data set that has the same design but different subjects. The data from the replication attempt is from <span class="citation">Vasishth et al. (<a href="#ref-VasishthetalPLoSOne2013">2013</a>)</span>.</p>
<p>The first data set from <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span> was analyzed in the previous section using uninformative priors. We can extract the mean and standard deviation of the posterior, and use that to derive an informative prior for the replication attempt.</p>
<p>Now, for the replication study, we can use this posterior (with a normal approximation), if we want to build on what we learned from the original <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span> study. As usual, we will do a sensitivity analysis: one model is fit with an uninformative prior on the parameter of interest, <span class="math inline">\(\mathit{Normal}(0,1)\)</span>, as we did in the preceding section; and another model will be fit with the informative directional prior <span class="math inline">\(\mathit{Normal}(-0.071, 0.209)\)</span>. For good measure, we can also include a model with a prior derived from the meta-analysis in the preceding section (the posterior of the <span class="math inline">\(\zeta\)</span> parameter).</p>
<table>
<caption>
<span id="tab:replicationpriors">TABLE 6.4: </span>A summary of the posteriors under an uninformative prior (Normal(0,1)), a prior based on previous data, and a meta-analysis prior, for data from a replication attempt of Gibson and Wu, 2013.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Priors
</th>
<th style="text-align:left;">
Mean
</th>
<th style="text-align:left;">
Lower
</th>
<th style="text-align:left;">
Upper
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Normal(0,1)
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.08\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.21\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.04\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Normal(-0.07,0.2)
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.08\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.20\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.03\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Normal(0.041, 0.2)
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.07\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(-0.19\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.06\)</span>
</td>
</tr>
</tbody>
</table>
<p>Table <a href="ch-priors.html#tab:replicationpriors">6.4</a> summarizes the different posteriors under the three prior specification. Again, in this case, the differences in the posteriors are small, but in a Bayes factor analysis, the outcomes under these different priors could be different.</p>
</div>
<div id="summary-5" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.5</span> Summary<a href="ch-priors.html#summary-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Working out appropriate priors for one’s research problem is essentially a Fermi problem. One can use several different strategies for working out priors: introspection, a literature review, computing statistics from existing data, conducting a meta-analysis, using posteriors from existing data as priors for a new, closely related study, or formally eliciting priors from domain experts. If a prior specification is too vague, this can lead to slow convergence or convergence problems, and would lead to biased Bayes factors (biased towards the null hypothesis); and if a prior is too informative, this can bias the posterior. This inherent potential for bias in prior specification should be formally investigated using sensitivity analyses (with a collection of uninformative, skeptical, and informative priors of various types), and prior and posterior predictive checks. Although prior specification seems like a daunting task to the beginning student of Bayes, with time and experience one can develop a very well-informed set of priors for one’s research problems.</p>
</div>
<div id="further-reading-3" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.6</span> Further reading<a href="ch-priors.html#further-reading-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For interesting (and amusing) examples of Fermi solutions to questions, see <a href="https://what-if.xkcd.com/84/" class="uri">https://what-if.xkcd.com/84/</a>. Two important books, <span class="citation">Mahajan (<a href="#ref-mahajan2010street">2010</a>)</span> and <span class="citation">Mahajan (<a href="#ref-mahajan2014art">2014</a>)</span>, unpack the art of approximation in mathematics and other disciplines; the approach presented in these books is closely related to the art of Fermi-style approximation. <span class="citation">Levy (<a href="#ref-danlevy">2021</a>)</span> is an important book that develops the analytical skill needed to figure out what your “tacit knowledge” about a particular problem is. <span class="citation">Tetlock and Gardner (<a href="#ref-tetlock">2015</a>)</span> explains how experts deploy existing knowledge to derive probabilistic predictions (predictions that come with a certain amount of uncertainty) about real-world problems—this skill is closely related to prior (self-)elicitation. An excellent presentation of prior elicitation is in <span class="citation">O’Hagan et al. (<a href="#ref-ohagan2006uncertain">2006</a>)</span>. Useful discussions about priors are provided in <span class="citation">Lunn et al. (<a href="#ref-lunn2012bugs">2012</a>)</span>; <span class="citation">Spiegelhalter, Abrams, and Myles (<a href="#ref-spiegelhalter2004bayesian">2004</a>)</span>; <span class="citation">Gelman, Simpson, and Betancourt (<a href="#ref-gelmanPriorCanOften2017">2017</a>)</span>; and <span class="citation">Simpson et al. (<a href="#ref-simpsonPenalisingModelComponent2017">2017</a>)</span>. The Stan website also includes some guidelines: Prior distributions for <code>rstanarm</code> models in <a href="https://mc-stan.org/rstanarm/articles/priors.html" class="uri">https://mc-stan.org/rstanarm/articles/priors.html</a>; and prior choice recommendations in <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations" class="uri">https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations</a>.
<span class="citation">Browne and Draper (<a href="#ref-brownedraper">2006</a>)</span> and <span class="citation">Gelman (<a href="#ref-gelman06">2006</a>)</span> discuss prior specifications in hierarchical models.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-BatesEtAlParsimonious">
<p>Bates, Douglas M, Reinhold Kliegl, Shravan Vasishth, and Harald Baayen. 2015. “Parsimonious Mixed Models.”</p>
</div>
<div id="ref-lme4">
<p>Bates, Douglas M, Martin Mächler, Ben Bolker, and Steve Walker. 2015a. “Fitting Linear Mixed-Effects Models Using lme4.” <em>Journal of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01" class="uri">https://doi.org/10.18637/jss.v067.i01</a>.</p>
</div>
<div id="ref-brownedraper">
<p>Browne, William J, and David Draper. 2006. “A Comparison of Bayesian and Likelihood-Based Methods for Fitting Multilevel Models.” <em>Bayesian Analysis</em> 1 (3). International Society for Bayesian Analysis: 473–514.</p>
</div>
<div id="ref-dersimonian1986meta">
<p>DerSimonian, Rebecca, and Nan Laird. 1986. “Meta-Analysis in Clinical Trials.” <em>Controlled Clinical Trials</em> 7 (3). Elsevier: 177–88.</p>
</div>
<div id="ref-EngelmannJaegerVasishth2019">
<p>Engelmann, Felix, Lena A. Jäger, and Shravan Vasishth. 2020. “The Effect of Prominence and Cue Association in Retrieval Processes: A Computational Account.” <em>Cognitive Science</em> 43 (12): e12800. <a href="https://doi.org/10.1111/cogs.12800" class="uri">https://doi.org/10.1111/cogs.12800</a>.</p>
</div>
<div id="ref-fedorenko2006nature">
<p>Fedorenko, Evelina, Edward Gibson, and Douglas Rohde. 2006. “The Nature of Working Memory Capacity in Sentence Comprehension: Evidence Against Domain-Specific Working Memory Resources.” <em>Journal of Memory and Language</em> 54 (4). Elsevier: 541–53.</p>
</div>
<div id="ref-gelman06">
<p>Gelman, Andrew. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” <em>Bayesian Analysis</em> 1 (3). International Society for Bayesian Analysis: 515–34.</p>
</div>
<div id="ref-gelmancarlin">
<p>Gelman, Andrew, and John B. Carlin. 2014. “Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors.” <em>Perspectives on Psychological Science</em> 9 (6). SAGE Publications: 641–51.</p>
</div>
<div id="ref-gelmanPriorCanOften2017">
<p>Gelman, Andrew, Daniel Simpson, and Michael J. Betancourt. 2017. “The Prior Can Often Only Be Understood in the Context of the Likelihood.” <em>Entropy</em> 19 (10): 555. <a href="https://doi.org/10.3390/e19100555" class="uri">https://doi.org/10.3390/e19100555</a>.</p>
</div>
<div id="ref-gibsonwu">
<p>Gibson, Edward, and H-H Iris Wu. 2013. “Processing Chinese Relative Clauses in Context.” <em>Language and Cognitive Processes</em> 28 (1-2). Taylor &amp; Francis: 125–55.</p>
</div>
<div id="ref-gordon01">
<p>Gordon, P. C., Randall Hendrick, and Marcus Johnson. 2001. “Memory Interference During Language Processing.” <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em> 27(6): 1411–23.</p>
</div>
<div id="ref-grodner">
<p>Grodner, Daniel, and Edward Gibson. 2005. “Consequences of the Serial Nature of Linguistic Input.” <em>Cognitive Science</em> 29: 261–90.</p>
</div>
<div id="ref-hammerly2019grammaticality">
<p>Hammerly, Christopher, Adrian Staub, and Brian Dillon. 2019. “The Grammaticality Asymmetry in Agreement Attraction Reflects Response Bias: Experimental and Modeling Evidence.” <em>Cognitive Psychology</em> 110: 70–104.</p>
</div>
<div id="ref-cochrane">
<p>Higgins, Julian, and Sally Green. 2008. <em>Cochrane Handbook for Systematics Reviews of Interventions</em>. New York: Wiley-Blackwell.</p>
</div>
<div id="ref-jackman2009bayesian">
<p>Jackman, Simon. 2009. <em>Bayesian Analysis for the Social Sciences</em>. Vol. 846. John Wiley &amp; Sons.</p>
</div>
<div id="ref-JaegerEngelmannVasishth2017">
<p>Jäger, Lena A., Felix Engelmann, and Shravan Vasishth. 2017. “Similarity-Based Interference in Sentence Comprehension: Literature review and Bayesian meta-analysis.” <em>Journal of Memory and Language</em> 94: 316–39. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004" class="uri">https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004</a>.</p>
</div>
<div id="ref-JaegerMertzenVanDykeVasishth2019">
<p>Jäger, Lena A., Daniela Mertzen, Julie A. Van Dyke, and Shravan Vasishth. 2020. “Interference Patterns in Subject-Verb Agreement and Reflexives Revisited: A Large-Sample Study.” <em>Journal of Memory and Language</em> 111. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2019.104063" class="uri">https://doi.org/https://doi.org/10.1016/j.jml.2019.104063</a>.</p>
</div>
<div id="ref-jc92">
<p>Just, Marcel A., and Patricia A. Carpenter. 1992. “A Capacity Theory of Comprehension: Individual Differences in Working Memory.” <em>Psychological Review</em> 99(1): 122–49.</p>
</div>
<div id="ref-kadane1998experiences">
<p>Kadane, Joseph, and Lara J Wolfson. 1998. “Experiences in Elicitation: [Read Before the Royal Statistical Society at a Meeting on ‘Elicitation’ on Wednesday, April 16th, 1997, the President, Professor a. F. M. Smith in the Chair].” <em>Journal of the Royal Statistical Society: Series D (the Statistician)</em> 47 (1). Wiley Online Library: 3–19.</p>
</div>
<div id="ref-kass1989investigating">
<p>Kass, Robert E, and Joel B Greenhouse. 1989. “[Investigating Therapies of Potentially Great Benefit: ECMO]: Comment: A Bayesian Perspective.” <em>Statistical Science</em> 4 (4). JSTOR: 310–17.</p>
</div>
<div id="ref-danlevy">
<p>Levy, Dan. 2021. <em>Maxims for Thinking Analytically: The Wisdom of Legendary Harvard Professor Richard Zeckhauser</em>. Dan Levy.</p>
</div>
<div id="ref-lewisvasishth:cogsci05">
<p>Lewis, Richard L., and Shravan Vasishth. 2005. “An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval.” <em>Cognitive Science</em> 29: 1–45.</p>
</div>
<div id="ref-lindley1991">
<p>Lindley, Dennis V. 1991. <em>Making Decisions</em>. Second. John Wiley &amp; Sons.</p>
</div>
<div id="ref-lunn2012bugs">
<p>Lunn, David, Chris Jackson, David J Spiegelhalter, Nicky Best, and Andrew Thomas. 2012. <em>The BUGS Book: A Practical Introduction to Bayesian Analysis</em>. Vol. 98. CRC Press.</p>
</div>
<div id="ref-mahajan2010street">
<p>Mahajan, Sanjoy. 2010. <em>Street-Fighting Mathematics: The Art of Educated Guessing and Opportunistic Problem Solving</em>. Cambridge, MA: The MIT Press.</p>
</div>
<div id="ref-mahajan2014art">
<p>Mahajan, Sanjoy. 2014. <em>The Art of Insight in Science and Engineering: Mastering Complexity</em>. Cambridge, MA: The MIT Press.</p>
</div>
<div id="ref-hannesBEAP">
<p>Matuschek, Hannes, Reinhold Kliegl, Shravan Vasishth, R. Harald Baayen, and Douglas M Bates. 2017. “Balancing Type I Error and Power in Linear Mixed Models.” <em>Journal of Memory and Language</em> 94: 305–15. <a href="https://doi.org/10.1016/j.jml.2017.01.001" class="uri">https://doi.org/10.1016/j.jml.2017.01.001</a>.</p>
</div>
<div id="ref-NicenboimEtAlCogSci2018">
<p>Nicenboim, Bruno, Shravan Vasishth, Felix Engelmann, and Katja Suckow. 2018. “Exploratory and Confirmatory Analyses in Sentence Processing: A case study of number interference in German.” <em>Cognitive Science</em> 42 (S4). <a href="https://doi.org/10.1111/cogs.12589" class="uri">https://doi.org/10.1111/cogs.12589</a>.</p>
</div>
<div id="ref-normand1999tutorial">
<p>Normand, S.L.T. 1999. “Tutorial in Biostatistics Meta-Analysis: Formulating, Evaluating, Combining, and Reporting.” <em>Statistics in Medicine</em> 18 (3): 321–59.</p>
</div>
<div id="ref-ohagan2006uncertain">
<p>O’Hagan, Anthony, Caitlin E Buck, Alireza Daneshkhah, J Richard Eiser, Paul H Garthwaite, David J Jenkinson, Jeremy E Oakley, and Tim Rakow. 2006. <em>Uncertain Judgements: Eliciting Experts’ Probabilities</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-phillips2011grammatical">
<p>Phillips, Colin, Matthew W. Wagers, and Ellen F. Lau. 2011. “Grammatical Illusions and Selective Fallibility in Real-Time Language Comprehension.” In <em>Experiments at the Interfaces</em>, 37:147–80. Emerald Bingley, UK.</p>
</div>
<div id="ref-rayner1998emr">
<p>Rayner, K. 1998. “Eye movements in reading and information processing: 20 years of research.” <em>Psychological Bulletin</em> 124 (3): 372–422.</p>
</div>
<div id="ref-reali2007">
<p>Reali, Florencia, and Morten H Christiansen. 2007. “Processing of Relative Clauses Is Made Easier by Frequency of Occurrence.” <em>Journal of Memory and Language</em> 57 (1). Elsevier: 1–23.</p>
</div>
<div id="ref-simpsonPenalisingModelComponent2017">
<p>Simpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G. Martins, and Sigrunn H. Sørbye. 2017. “Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors.” <em>Statistical Science</em> 32 (1): 1–28. <a href="https://doi.org/10.1214/16-STS576" class="uri">https://doi.org/10.1214/16-STS576</a>.</p>
</div>
<div id="ref-spiegelhalter2004bayesian">
<p>Spiegelhalter, David J, Keith R Abrams, and Jonathan P Myles. 2004. <em>Bayesian Approaches to Clinical Trials and Health-Care Evaluation</em>. Vol. 13. John Wiley &amp; Sons.</p>
</div>
<div id="ref-sutton2012evidence">
<p>Sutton, Alexander J, Nicky J Welton, Nicola Cooper, Keith R Abrams, and AE Ades. 2012. <em>Evidence Synthesis for Decision Making in Healthcare</em>. Vol. 132. John Wiley &amp; Sons.</p>
</div>
<div id="ref-tetlock">
<p>Tetlock, Philip, and Dan Gardner. 2015. <em>Superforecasting: The Art and Science of Prediction</em>. Crown Publishers.</p>
</div>
<div id="ref-VasishthMScStatistics">
<p>Vasishth, Shravan. 2015. “A Meta-Analysis of Relative Clause Processing in MANDARIN CHINESE Using Bias Modelling.” Master’s thesis, Sheffield, UK: School of Mathematics; Statistics, University of Sheffield. <a href="http://www.ling.uni-potsdam.de/~vasishth/pdfs/VasishthMScStatistics.pdf" class="uri">http://www.ling.uni-potsdam.de/~vasishth/pdfs/VasishthMScStatistics.pdf</a>.</p>
</div>
<div id="ref-VasishthetalPLoSOne2013">
<p>Vasishth, Shravan, Zhong Chen, Qiang Li, and Gueilan Guo. 2013. “Processing Chinese Relative Clauses: Evidence for the Subject-Relative Advantage.” <em>PLoS ONE</em> 8 (10). Public Library of Science: 1–14.</p>
</div>
<div id="ref-VasishthEngelmann2022">
<p>Vasishth, Shravan, and Felix Engelmann. 2022. <em>Sentence Comprehension as a Cognitive Process: A Computational Approach</em>. Cambridge, UK: Cambridge University Press. <a href="https://books.google.de/books?id=6KZKzgEACAAJ" class="uri">https://books.google.de/books?id=6KZKzgEACAAJ</a>.</p>
</div>
<div id="ref-VasishthMertzenJaegerGelman2018">
<p>Vasishth, Shravan, Daniela Mertzen, Lena A. Jäger, and Andrew Gelman. 2018a. “The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability.” <em>Journal of Memory and Language</em> 103: 151–75. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2018.07.004" class="uri">https://doi.org/https://doi.org/10.1016/j.jml.2018.07.004</a>.</p>
</div>
<div id="ref-VasishthEtAlTiCS2019">
<p>Vasishth, Shravan, Bruno Nicenboim, Felix Engelmann, and Frank Burchert. 2019. “Computational Models of Retrieval Processes in Sentence Processing.” <em>Trends in Cognitive Sciences</em> 23: 968–82. <a href="https://doi.org/https://doi.org/10.1016/j.tics.2019.09.003" class="uri">https://doi.org/https://doi.org/10.1016/j.tics.2019.09.003</a>.</p>
</div>
<div id="ref-SampleSizeCBB2021">
<p>Vasishth, Shravan, Himanshu Yadav, Daniel J. Schad, and Bruno Nicenboim. 2022. “Sample Size Determination for Bayesian Hierarchical Models Commonly Used in Psycholinguistics.” <em>Computational Brain and Behavior</em>.</p>
</div>
<div id="ref-von1988fermi">
<p>Von Baeyer, Hans Christian. 1988. “How Fermi Would Have Fixed It.” <em>The Sciences</em> 28 (5). Blackwell Publishing Ltd Oxford, UK: 2–4.</p>
</div>
<div id="ref-wagenmakers2007linear">
<p>Wagenmakers, Eric-Jan, and Scott Brown. 2007. “On the Linear Relation Between the Mean and the Standard Deviation of a Response Time Distribution.” <em>Psychological Review</em> 114 (3). American Psychological Association: 830.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="23">
<li id="fn23"><p>This discussion reuses text from <span class="citation">Vasishth, Yadav, et al. (<a href="#ref-SampleSizeCBB2021">2022</a>)</span>.<a href="ch-priors.html#fnref23" class="footnote-back">↩</a></p></li>
<li id="fn24"><p>See <span class="citation">Vasishth et al. (<a href="#ref-VasishthetalPLoSOne2013">2013</a>)</span>;<span class="citation">Jäger et al. (<a href="#ref-JaegerMertzenVanDykeVasishth2019">2020</a>)</span>;<span class="citation">Nicenboim, Vasishth, et al. (<a href="#ref-NicenboimEtAlCogSci2018">2018</a>)</span>;<span class="citation">Vasishth, Mertzen, Jäger, et al. (<a href="#ref-VasishthMertzenJaegerGelman2018">2018</a><a href="#ref-VasishthMertzenJaegerGelman2018">a</a>)</span> for detailed discussion of this point in the context of psycholinguistics.<a href="ch-priors.html#fnref24" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-hierarchical.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-workflow.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
