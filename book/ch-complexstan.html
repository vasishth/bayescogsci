<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Complex models and reparametrization | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Complex models and reparametrization | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Complex models and reparametrization | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2022-09-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-introstan.html"/>
<link rel="next" href="ch-custom.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-anyway"><i class="fa fa-check"></i>Why read this book anyway?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b> Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b> Bayesian Regression Models using Stan: brms</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b> Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.6.1" data-path="ch-compbda.html"><a href="ch-compbda.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.6.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.6.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.6.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.6.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.6.3</b> Re-fitting a single subject pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.7</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.9</b> Further reading</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect response times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b> No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a><ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#model-building"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#exemplary-data-analysis"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-workflow.html"><a href="ch-workflow.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-workflow.html"><a href="ch-workflow.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a><ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-7"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-9"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-10"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a><ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b> Validation of a computed posterior distribution</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#simulation-based-calibration-revealing-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> Simulation-based calibration revealing a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-and-limitation-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues and limitation of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#a-custom-distribution-re-implementing-the-exponential-distribution-manually"><i class="fa fa-check"></i><b>12.3</b> A custom distribution: Re-implementing the exponential distribution manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-11"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Other useful models</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-12"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a><ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a><ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a><ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a><ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#summary-13"><i class="fa fa-check"></i><b>15.6</b> Summary</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.7</b> Further reading</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a><ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#issues-with-cross-validation"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a><ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-14"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a><ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.3</b> Further reading</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a><ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a><ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a><ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a><ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-complexstan" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 11</span> Complex models and reparametrization<a href="ch-complexstan.html#ch-complexstan" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Now that we know how to fit simple regression models using Stan syntax, we can now turn to more complex cases, such as the hierarchical models we fit in <code>brms</code> in chapter <a href="ch-hierarchical.html#ch-hierarchical">5</a>. Fitting such models in Stan allows us a great deal of flexibility. In exchange, however, we need to think on how exactly we code the model. In some cases, two pieces of computer code that are mathematically equivalent might behave differently due to the computer’s limitations; in this chapter, we will learn some of the more common techniques needed to optimize the model’s behavior. In particular, we will learn how to deal with convergence problems using what is called the non-centered reparameterization. <!-- Another very important topic we cover here is defining priors for the correlation parameters in the variance-covariance matrix of the random effects.  --></p>
<div id="sec-hierstan" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.1</span> Hierarchical models with Stan<a href="ch-complexstan.html#sec-hierstan" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the following sections, we will revisit and expand on some of the examples from chapter <a href="ch-hierarchical.html#ch-hierarchical">5</a>.</p>
<div id="varying-intercept-model-with-stan" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.1.1</span> Varying intercept model with Stan<a href="ch-complexstan.html#varying-intercept-model-with-stan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that in section <a href="ch-hierarchical.html#sec-N400hierarchical">5.2</a> we fit models to investigate the effect of cloze probability on EEG averages in the N400 spatiotemporal time window. For our first model, we’ll make the (implausible) assumption that only the average signal varies across subjects, but all subjects share the same effect of cloze probability. This means that the likelihood incorporates the assumption that the intercept, <span class="math inline">\(\alpha\)</span>, is adjusted with the term <span class="math inline">\(u_i\)</span> for each subject.</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n]} + c\_cloze_n \cdot \beta,\sigma)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp;\sim \mathit{Normal}(0,10)\\
 \beta  &amp;\sim \mathit{Normal}(0,10)\\
 u &amp;\sim \mathit{Normal}(0,\tau_u)\\
 \tau_{u} &amp;\sim \mathit{Normal}_+(0,20) \\
 \sigma  &amp;\sim \mathit{Normal}_+(0,50)
 \end{aligned}
 \end{equation}\]</span></p>
<p>Here <span class="math inline">\(n\)</span> represents each observation, the <span class="math inline">\(n\)</span>th row in the data frame and <span class="math inline">\(subj[n]\)</span> is the subject that corresponds to observation <span class="math inline">\(n\)</span>. We present the mathematical notation of the likelihood with “multiple indexing”: the index of <span class="math inline">\(u\)</span> is provided by the vector <span class="math inline">\(subj\)</span>.</p>
<p>Before we discuss the Stan implementation, let’s see what the location of the normal likelihood, the vector <span class="math inline">\(\mu\)</span>, looks like. There are in total 2863 observations; that means that <span class="math inline">\(\boldsymbol{\mu}=\{\mu_1,\mu_2, \ldots, \mu_{2863}\}\)</span>. We have 37 subjects which means that <span class="math inline">\(\boldsymbol{u}=\{u_1,u_2, \ldots, u_{37}\}\)</span>. The following equation shows that the use of multiple indexing allows us to have a vector of adjustments with only 37 different elements, with a total length of 2863. In the equation below, the multiplication operator <span class="math inline">\(\circ\)</span> is the Hadamard product: when we write <span class="math inline">\(X\circ B\)</span>, both <span class="math inline">\(X\)</span> and <span class="math inline">\(B\)</span> have the same dimensions <span class="math inline">\(m\times n\)</span>, and each cell in location <span class="math inline">\([i,j]\)</span> (where <span class="math inline">\(i=1,\dots,m\)</span>, and <span class="math inline">\(j=1,\dots,n\)</span>) in <span class="math inline">\(X\)</span> and <span class="math inline">\(B\)</span> are multiplied together to give a matrix that also has dimensions <span class="math inline">\(m\times n\)</span>.</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
    \boldsymbol{\mu} &amp;=
    \begin{bmatrix}
        \mu_1 \\
        \mu_2 \\
        \ldots \\
        \mu_{101} \\
        \mu_{102} \\
        \ldots \\
        \mu_{215} \\
        \mu_{216} \\
        \mu_{217} \\
        \ldots \\
        \mu_{1000} \\
        \ldots \\
        \mu_{2863}
    \end{bmatrix}
=
    \begin{bmatrix}
        \alpha \\
        \alpha \\
        \ldots \\
        \alpha \\
        \alpha \\
        \ldots \\
        \alpha \\
        \alpha \\
        \alpha \\
        \ldots \\
        \alpha \\
        \ldots \\
        \alpha
    \end{bmatrix}
+
\begin{bmatrix}
u_{subj[1]} \\
u_{subj[2]} \\
\ldots \\
u_{subj[101]} \\
u_{subj[102]} \\
\ldots \\
u_{subj[215]} \\
u_{subj[216]} \\
u_{subj[217]} \\
\ldots \\
u_{subj[1000]} \\
\ldots \\
u_{i[2863]}
\end{bmatrix}
+
\begin{bmatrix}
ccloze_1 \\
ccloze_2 \\
\ldots \\
ccloze_{101} \\
ccloze_{102} \\
\ldots \\
ccloze_{215} \\
ccloze_{216} \\
ccloze_{217} \\
\ldots \\
ccloze_{1000} \\
\ldots \\
ccloze_{2863}
\end{bmatrix}
\circ
\begin{bmatrix}
\beta \\
\beta \\
\ldots \\
\beta \\
\beta \\
\ldots \\
\beta \\
\beta \\
\beta \\
\ldots \\
\beta \\
\ldots \\
\beta
\end{bmatrix} \\
&amp; =
\begin{bmatrix}
\alpha \\
\alpha \\
\ldots \\
\alpha \\
\alpha \\
\ldots \\
\alpha \\
\alpha \\
\alpha \\
\ldots \\
\alpha \\
\ldots \\
\alpha
\end{bmatrix}
+
\begin{bmatrix}
u_{1} \\
u_{1} \\
\ldots \\
u_{2} \\
u_{2} \\
\ldots \\
u_{3} \\
u_{3} \\
u_{3} \\
\ldots \\
u_{13} \\
\ldots \\
u_{37 }
\end{bmatrix}
+
\begin{bmatrix}
{-0.476} \\
{-0.446} \\
\ldots \\
{-0.206} \\
{0.494} \\
\ldots \\
{-0.136} \\
{0.094} \\
{0.294} \\
\ldots \\
{0.524} \\
\ldots \\
{0.494 }
\end{bmatrix}
\circ
\begin{bmatrix}
\beta \\
\beta \\
\ldots \\
\beta \\
\beta \\
\ldots \\
\beta \\
\beta \\
\beta \\
\ldots \\
\beta \\
\ldots \\
\beta
\end{bmatrix}
\end{aligned}
\end{equation}\]</span></p>
<p>In this model each subject has their own intercept adjustment <span class="math inline">\(u_i\)</span>, with <span class="math inline">\(i\)</span> indexing the subjects. If <span class="math inline">\(u_i\)</span> is positive, the subject has a more positive EEG signal than the average over all the subjects; if <span class="math inline">\(u_i\)</span> is negative, then the subject has a more negative EEG signal than the average; and if <span class="math inline">\(u_i\)</span> is <span class="math inline">\(0\)</span>, then the subject has the same EEG signal as the average. As we discussed in section <a href="ch-hierarchical.html#sec-uncorrelated">5.2.3</a>, since we are estimating <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(u\)</span> at the same time and we assume that the average of the <span class="math inline">\(u\)</span>’s is <span class="math inline">\(0\)</span> (since it is assumed to be normally distributed with mean of <span class="math inline">\(0\)</span>), whatever the subjects have in common “goes” to <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(u\)</span> only “absorbs” the differences between subjects through the variance component <span class="math inline">\(\tau_u\)</span>.</p>
<p>We implement this in Stan in the <code>hierarchical1.stan</code>:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  // The following line creates an array of integers;
  array[N] int&lt;lower = 1, upper = N_subj&gt; subj;
}
parameters {
  real&lt;lower = 0&gt; sigma;
  real&lt;lower = 0&gt;  tau_u;
  real alpha;
  real beta;
  vector[N_subj] u;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u | 0, 20)  -
    normal_lccdf(0 | 0, 20);
  target += normal_lpdf(u | 0, tau_u);
  target += normal_lpdf(signal | alpha + u[subj] +
                        c_cloze * beta, sigma);
}
</code></pre>
<p>In the previous Stan code we use <code>array [N] int&lt;lower = 1, upper = N_subj&gt; subj;</code> to define a one-dimensional <em>array</em> of <code>N</code> elements that contains integers (bounded between 1 and <code>N_subj</code>). As we explain in Box <a href="ch-introstan.html#thm:stancontainers">10.4</a>, the difference between vectors and one-dimensional arrays is that vectors can only contain real numbers and can be used with matrix algebra functions, and arrays can contain any type but can’t be used in matrix algebra. We use <code>normal_lpdf</code> rather than <code>normal_glm_lpdf</code> since at the moment there is no efficient likelihood implementation of hierarchical generalized linear models.</p>
<p>The following code centers the predictor cloze and stores the data required by the Stan model in a list. Because we are using <code>subj</code> as a vector of indices, we need to be careful to have integers starting from <code>1</code> and ending in <code>N_subj</code> without skipping any number (but the order won’t matter).<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a></p>
<div class="sourceCode" id="cb700"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb700-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_eeg&quot;</span>)</a>
<a class="sourceLine" id="cb700-2" data-line-number="2">df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb700-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cloze =</span> cloze <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cloze))</a>
<a class="sourceLine" id="cb700-4" data-line-number="4">ls_eeg &lt;-<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb700-5" data-line-number="5">  <span class="dt">N =</span> <span class="kw">nrow</span>(df_eeg),</a>
<a class="sourceLine" id="cb700-6" data-line-number="6">  <span class="dt">signal =</span> df_eeg<span class="op">$</span>n400,</a>
<a class="sourceLine" id="cb700-7" data-line-number="7">  <span class="dt">c_cloze =</span> df_eeg<span class="op">$</span>c_cloze,</a>
<a class="sourceLine" id="cb700-8" data-line-number="8">  <span class="dt">subj =</span> df_eeg<span class="op">$</span>subj,</a>
<a class="sourceLine" id="cb700-9" data-line-number="9">  <span class="dt">N_subj =</span> <span class="kw">max</span>(df_eeg<span class="op">$</span>subj)</a>
<a class="sourceLine" id="cb700-10" data-line-number="10">)</a></code></pre></div>
<p>Fit the model:</p>
<div class="sourceCode" id="cb701"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb701-1" data-line-number="1">hierarchical1 &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb701-2" data-line-number="2">  <span class="st">&quot;hierarchical1.stan&quot;</span>,</a>
<a class="sourceLine" id="cb701-3" data-line-number="3">  <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span></a>
<a class="sourceLine" id="cb701-4" data-line-number="4">)</a>
<a class="sourceLine" id="cb701-5" data-line-number="5">fit_eeg1 &lt;-<span class="st"> </span><span class="kw">stan</span>(hierarchical1, <span class="dt">data =</span> ls_eeg)</a></code></pre></div>
<p>Summary of the model:</p>
<div class="sourceCode" id="cb702"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb702-1" data-line-number="1"><span class="kw">print</span>(fit_eeg1, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<pre><code>##        mean  2.5% 97.5% n_eff Rhat
## alpha  3.66  2.85  4.51  1320 1.01
## beta   2.31  1.28  3.35  4847 1.00
## sigma 11.64 11.34 11.93  5633 1.00
## tau_u  2.16  1.53  2.96  2413 1.00</code></pre>
</div>
<div id="sec-uncorrstan" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.1.2</span> Uncorrelated varying intercept and slopes model with Stan<a href="ch-complexstan.html#sec-uncorrstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the following model, we relax the strong assumption that every subject will be affected equally by the manipulation. For ease of exposition, we start by assuming that the adjustments for the intercept and slope are not correlated, as we did in section <a href="ch-hierarchical.html#sec-uncorrelated">5.2.3</a>.</p>
<p><span class="math display" id="eq:uncorrstanlik">\[\begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + c\_cloze_n \cdot (\beta+ u_{subj[n],2}),\sigma)
\tag{11.1}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:uncorrstanpriors">\[\begin{equation}
 \begin{aligned}
 \alpha &amp;\sim \mathit{Normal}(0,10)\\
 \beta  &amp;\sim \mathit{Normal}(0,10)\\
 u_1 &amp;\sim \mathit{Normal}(0,\tau_{u_1})\\
 u_2 &amp;\sim \mathit{Normal}(0,\tau_{u_2})\\
 \tau_{u_1} &amp;\sim \mathit{Normal}_+(0,20) \\
 \tau_{u_2} &amp;\sim \mathit{Normal}_+(0,20) \\
 \sigma  &amp;\sim \mathit{Normal}_+(0,50)
 \end{aligned}
\tag{11.2}
\end{equation}\]</span></p>
<p>We implement this in Stan in <code>hierarchical2.stan</code>:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  array[N] int&lt;lower = 1, upper = N_subj&gt; subj;
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;
  real alpha;
  real beta;
  matrix[N_subj, 2] u;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u[1] | 0, 20)  -
    normal_lccdf(0 | 0, 20);
  target += normal_lpdf(tau_u[2] | 0, 20)  -
    normal_lccdf(0 | 0, 20);
  target += normal_lpdf(u[, 1]| 0, tau_u[1]);
  target += normal_lpdf(u[, 2]| 0, tau_u[2]);
  target += normal_lpdf(signal | alpha + u[subj, 1] +
                        c_cloze .* (beta + u[subj, 2]), sigma);
}
</code></pre>
<p>In the previous model, we assign the same prior distribution to both <code>tau_u[1]</code> and <code>tau_u[2]</code>, and thus in principle we could have written the two statements in one (we multiply by 2 because there are two PDFs that need to be corrected for the truncation):</p>
<pre><code>  target += normal_lpdf(tau_u | 0, 20)  - 
    2 * normal_lccdf(0 | 0, 20);</code></pre>
<p>Fit the model as follows:</p>
<div class="sourceCode" id="cb706"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb706-1" data-line-number="1">hierarchical2 &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb706-2" data-line-number="2">  <span class="st">&quot;hierarchical2.stan&quot;</span>,</a>
<a class="sourceLine" id="cb706-3" data-line-number="3">  <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span></a>
<a class="sourceLine" id="cb706-4" data-line-number="4">)</a>
<a class="sourceLine" id="cb706-5" data-line-number="5">fit_eeg2 &lt;-<span class="st"> </span><span class="kw">stan</span>(hierarchical2, <span class="dt">data =</span> ls_eeg)</a></code></pre></div>
<pre><code>## Warning: There were 1 divergent transitions after warmup. See
## https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
## to find out why this is a problem and how to eliminate them.</code></pre>
<pre><code>## Warning: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See
## https://mc-stan.org/misc/warnings.html#bfmi-low</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>We see that there are warnings. As we increase the complexity and the number of parameters, the sampler has a harder time exploring the parameter space.</p>
<p>Show the summary of the model below:</p>
<div class="sourceCode" id="cb712"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb712-1" data-line-number="1"><span class="kw">print</span>(fit_eeg2, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</a></code></pre></div>
<pre><code>##           mean  2.5% 97.5% n_eff Rhat
## alpha     3.65  2.80  4.47  1139 1.00
## beta      2.31  1.13  3.54  2693 1.00
## tau_u[1]  2.17  1.54  3.01  1776 1.00
## tau_u[2]  1.68  0.21  3.48   171 1.03
## sigma    11.62 11.33 11.93  4255 1.00</code></pre>
<p>We see that <code>tau_u[2]</code> has a low number of effective samples (<code>n_eff</code>).</p>
<p>Plot the traceplots displayed in Figure <a href="ch-complexstan.html#fig:traceploteeg2">11.1</a>:</p>

<div class="sourceCode" id="cb714"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb714-1" data-line-number="1"><span class="kw">traceplot</span>(fit_eeg2, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceploteeg2"></span>
<img src="bookdown_files/figure-html/traceploteeg2-1.svg" alt="Traceplots of alpha, beta, tau_u, and sigma from the model fit_eeg2." width="672" />
<p class="caption">
FIGURE 11.1: Traceplots of <code>alpha</code>, <code>beta</code>, <code>tau_u</code>, and <code>sigma</code> from the model <code>fit_eeg2</code>.
</p>
</div>
<p>Figure <a href="ch-complexstan.html#fig:traceploteeg2">11.1</a> shows that the chains of the parameter <code>tau_u[2]</code> are not mixing properly. This parameter is specially problematic because there are not enough data from each subject to estimate this parameter accurately, its estimated mean is quite small (in comparison with <code>sigma</code>), it’s bounded by zero, and there is a dependency between this parameter and <code>u[, 2]</code>. This makes the exploration by the sampler quite hard.</p>
<p>Pairs plots can be useful to uncover pathologies in the sampling, since we can visualize correlations between samples, which are in general problematic. The following code creates a pair plot where we see the samples of <code>tau_u[2]</code> against some of the adjustments to the slope <code>u</code>; see Figure <a href="ch-complexstan.html#fig:pairsfunnel">11.2</a>.</p>

<div class="sourceCode" id="cb715"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb715-1" data-line-number="1"><span class="kw">pairs</span>(fit_eeg2, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;u[1,2]&quot;</span>, <span class="st">&quot;u[2,2]&quot;</span>, <span class="st">&quot;u[3,2]&quot;</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pairsfunnel"></span>
<img src="bookdown_files/figure-html/pairsfunnel-1.svg" alt="Pair plots showing a relatively strong correlations (funnel-shaped clouds of samples) between the samples of \(\tau_2\) and some of the by-subject adjustments to the slope." width="672" />
<p class="caption">
FIGURE 11.2: Pair plots showing a relatively strong correlations (funnel-shaped clouds of samples) between the samples of <span class="math inline">\(\tau_2\)</span> and some of the by-subject adjustments to the slope.
</p>
</div>
<p>Compare with <code>tau_u[1]</code> plotted against the by-subject adjustments to the intercept. In Figure <a href="ch-complexstan.html#fig:pairsblobs">11.3</a>, instead of funnels we see blobs, indicating no strong correlation between the parameters.</p>

<div class="sourceCode" id="cb716"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb716-1" data-line-number="1"><span class="kw">pairs</span>(fit_eeg2, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;tau_u[1]&quot;</span>, <span class="st">&quot;u[1,1]&quot;</span>, <span class="st">&quot;u[2,1]&quot;</span>, <span class="st">&quot;u[3,1]&quot;</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pairsblobs"></span>
<img src="bookdown_files/figure-html/pairsblobs-1.svg" alt="Pair plots showing no strong correlation (blob-shaped clouds of samples) between the samples of \(\tau_1\) and some of the by-subject adjustments to the intercept." width="672" />
<p class="caption">
FIGURE 11.3: Pair plots showing no strong correlation (blob-shaped clouds of samples) between the samples of <span class="math inline">\(\tau_1\)</span> and some of the by-subject adjustments to the intercept.
</p>
</div>
<p>In contrast to Figure <a href="ch-complexstan.html#fig:pairsblobs">11.3</a> that shows blob-shaped clouds of samples, Figure <a href="ch-complexstan.html#fig:pairsfunnel">11.2</a> shows a relatively strong correlation between the samples of some of the parameters of the model, in particular <span class="math inline">\(\tau_2\)</span> and the samples of <span class="math inline">\(u_{i,2}\)</span>. This strong correlation is hindering the exploration of the sampler leading to the warnings we saw in Stan.
However, the problem that the sampler faces is, in fact, more serious than what our initial plots show. Stan samples in an <em>unconstrained</em> space where all the parameters can range from minus infinity to infinity, and then transforms back the parameters to the constrained space that we specified where, for example, a standard deviation parameter is restricted to be positive. This means that Stan is actually sampling from an auxiliary parameter equivalent to <code>log(tau_u[2])</code> rather than from <code>tau_u[2]</code>. We can use <code>mcmc_scatter</code> to see the actual funnel; see Figure <a href="ch-complexstan.html#fig:scatterpairs">11.4</a>.</p>

<div class="sourceCode" id="cb717"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb717-1" data-line-number="1"><span class="kw">mcmc_pairs</span>(<span class="kw">as.array</span>(fit_eeg2),</a>
<a class="sourceLine" id="cb717-2" data-line-number="2">  <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;u[1,2]&quot;</span>),</a>
<a class="sourceLine" id="cb717-3" data-line-number="3">  <span class="dt">transform =</span> <span class="kw">list</span>(<span class="st">`</span><span class="dt">tau_u[2]</span><span class="st">`</span> =<span class="st"> &quot;log&quot;</span>)</a>
<a class="sourceLine" id="cb717-4" data-line-number="4">) </a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:scatterpairs"></span>
<img src="bookdown_files/figure-html/scatterpairs-1.svg" alt="Pair plots showing a strong correlation (funnel-shaped clouds of samples) between the samples of \(\tau_2\) and one of the by-subject adjustments to the intercept (\(u_{1,2}\))." width="672" />
<p class="caption">
FIGURE 11.4: Pair plots showing a strong correlation (funnel-shaped clouds of samples) between the samples of <span class="math inline">\(\tau_2\)</span> and one of the by-subject adjustments to the intercept (<span class="math inline">\(u_{1,2}\)</span>).
</p>
</div>
<p>At the neck of the funnel, <code>tau_u[2]</code> is close to zero (and <code>log(tau_u[2])</code> is a negative number) and thus the adjustment <code>u</code> is constrained to be near <span class="math inline">\(0\)</span>. This is a problem because a step size that’s optimized to work well in the broad part of the funnel will fail to work appropriately in the neck of the funnel and vice versa; see also Neal’s funnel <span class="citation">(Neal <a href="#ref-neal2003">2003</a>)</span> and the optimization chapter of Stan’s manual <span class="citation">(Stan Development Team <a href="#ref-Stan2021">2021</a>, sec 22.7)</span>.
There are two options, we might just remove the by-subject varying slope since it’s not giving us much information anyway, or we can alleviate this problem by re-parameterizing the model. In general, this is the trickiest and probably most annoying part of modeling. A model can be theoretically and mathematically sound, but still fail to converge. The best advice to solve this type of problem is to start small with simulated data where we know the true values of the parameters, and increase the complexity of the models gradually. Although in this example the problem was clearly in the parameterization of <code>tau_u[2]</code>, in many cases the biggest hurdle is to identify where the problem lies. Fortunately, the issue with <code>tau_u[2]</code> is a common problem which is easy to solve by using a non-centered parametrization <span class="citation">(Papaspiliopoulos, Roberts, and Sköld <a href="#ref-papaspiliopoulos2007">2007</a>)</span>. The following Box explains the specific re-parametrization we use for the improved version of our Stan code.</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:noncenterparam" class="theorem"><strong>Box 11.1  </strong></span><strong>A simple non-centered re-parametrization</strong></p>
</div>
<p>The sampler can explore the parameter space more easily if its step size is appropriate for all the parameters. This is achieved when there are no strong correlations between parameters. We want to assume the following.</p>
<p><span class="math display">\[\begin{equation}
\mathbf{u}_{2} \sim \mathit{Normal}(0, \tau_{u_2})
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{u}_{2}\)</span> is the column vector of <span class="math inline">\(u_{i,2}\)</span>’s. The index <span class="math inline">\(i\)</span> refers to the subject id.</p>
<p>We can transform <span class="math inline">\(u_2\)</span> to z-scores as follows. This amounts to centering the parameter, so we can call this this centered parameterization.</p>
<p><span class="math display">\[\begin{equation}
\mathbf{z}_{u_2} =\frac{\mathbf{u}_{2} - 0}{\tau_{u_2}}
\end{equation}\]</span></p>
<p>where
<span class="math display">\[\begin{equation}
\mathbf{z}_{u_2} \sim \mathit{Normal}(0, 1)
\end{equation}\]</span></p>
<p>Now <span class="math inline">\(\mathbf{z}_{u_2}\)</span> is easier to sample because it doesn’t depend on other parameters (in particular, it is no longer conditional on <span class="math inline">\(\tau\)</span>) and its scale is <span class="math inline">\(1\)</span>. Once we have sampled this centered parameter, we can derive the actual parameter we care about by carrying out the reverse operation, which is called a non-centered parameterization:</p>
<p><span class="math display">\[\begin{equation}
\mathbf{u}_{2} = \mathbf{z}_{u_2} \cdot \tau_{u_2}
\end{equation}\]</span></p>
<p>A question that might be raised here is whether using a non-centered parametrization is always a good idea. <span class="citation">Betancourt and Girolami (<a href="#ref-betancourt2013hamiltonian">2013</a>)</span> point out that the extremeness of the correlation depends on the amount of data, and the efficacy of the parametrization depends on the relative strength of the data. When there is enough data this parametrization is unnecessary and it may be more efficient to use the centered parameterization. However, cases where there is enough data to render this parametrization useless are also cases where the partial pooling of the hierarchical models isn’t needed in the first place. Although data from conventional lab experiments in psychology, psycholinguistics, and related areas seem to benefit from the non-centered parametrization, the jury is still out for larger data sets with thousands of subjects from crowdsourcing websites.</p>
</div>

<p>From a mathematical point of view, the following model is equivalent to the one described in Equations <a href="ch-complexstan.html#eq:uncorrstanlik">(11.1)</a> and <a href="ch-complexstan.html#eq:uncorrstanpriors">(11.2)</a>. However, as discussed previously, the computational implementation of the “new” model is more efficient. The following model includes the re-parametrization of both adjustments, <span class="math inline">\(u_1\)</span>, and <span class="math inline">\(u_2\)</span>, although the re-parametrization of <span class="math inline">\(u_1\)</span> is not strictly necessary (we didn’t see any problems in the trace plots), it won’t hurt either and the Stan code will be simpler.</p>
<p><span class="math display" id="eq:uncorrstanlik2">\[\begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + c\_cloze_n \cdot (\beta+ u_{subj[n],2}),\sigma)
\tag{11.3}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:uncorrstanpriors2">\[\begin{equation}
 \begin{aligned}
 \alpha &amp;\sim \mathit{Normal}(0,10)\\
 \beta  &amp;\sim \mathit{Normal}(0,10)\\
 z_{u_1} &amp;\sim \mathit{Normal}(0, 1)\\
 z_{u_2} &amp;\sim \mathit{Normal}(0, 1)\\
 \tau_{u_1} &amp;\sim \mathit{Normal}_+(0,20) \\
 \tau_{u_2} &amp;\sim \mathit{Normal}_+(0,20) \\
 \sigma  &amp;\sim \mathit{Normal}_+(0,50)\\
 u_1 &amp;= z_{u_1} \cdot \tau_{u_1}\\
 u_2 &amp;= z_{u_2} \cdot \tau_{u_2}
 \end{aligned}
\tag{11.4}
\end{equation}\]</span></p>
<p>The following Stan code (<code>hierarchical3.stan</code>) uses the previous re-parametrization, and introduces some new Stan functions: <code>to_vector()</code> converts a matrix into a long column vector (in column-major order, that is, concatenating the columns from left to right); and <code>std_normal_lpdf()</code> implements the log PDF of a standard normal distribution, a normal distribution with location 0 and scale 1. This function is just a more efficient version of <code>normal_lpdf(... | 0, 1)</code>. We also introduce a new optional block called <code>transformed parameters</code>, with each iteration of the sampler, the values of the parameters (i.e., <code>alpha</code>, <code>beta</code>, <code>sigma</code>, and <code>z</code>) are available at the <code>transformed parameters</code> block, and we can derive new auxiliary variables based on them. In this case, we use <code>z_u</code> and <code>tau_u</code> to obtain <code>u</code>, that then becomes available in the <code>model</code> block.</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  array[N] int&lt;lower = 1, upper = N_subj&gt; subj;
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;
  real alpha;
  real beta;
  matrix[N_subj, 2] z_u;
}
transformed parameters {
  matrix[N_subj, 2] u;
  u[, 1] = z_u[, 1] * tau_u[1];
  u[, 2] = z_u[, 2] * tau_u[2];
 }
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u[1] | 0, 20)  -
    normal_lccdf(0 | 0, 20);
 target += normal_lpdf(tau_u[2] | 0, 20)  -
    normal_lccdf(0 | 0, 20);
  target += std_normal_lpdf(to_vector(z_u));
  target += normal_lpdf(signal | alpha + u[subj, 1] +
                        c_cloze .* (beta + u[subj, 2]), sigma);
}
</code></pre>
<p>By re-parametrizing the model we can also optimize it more, we can convert the matrix <code>z_u</code> into a long column vector allowing us to use a single call of <code>std_normal_lpdf</code>. Fit the model named <code>hierarchical3.stan</code>.</p>
<div class="sourceCode" id="cb719"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb719-1" data-line-number="1">hierarchical3 &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb719-2" data-line-number="2">  <span class="st">&quot;hierarchical3.stan&quot;</span>,</a>
<a class="sourceLine" id="cb719-3" data-line-number="3">  <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span></a>
<a class="sourceLine" id="cb719-4" data-line-number="4">)</a>
<a class="sourceLine" id="cb719-5" data-line-number="5">fit_eeg3 &lt;-<span class="st"> </span><span class="kw">stan</span>(hierarchical3, <span class="dt">data =</span> ls_eeg) </a></code></pre></div>
<p>Verify that the model worked as expected by printing its summary and traceplots; see Figure <a href="ch-complexstan.html#fig:traceploteeg3">11.5</a>.</p>

<div class="sourceCode" id="cb720"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb720-1" data-line-number="1"><span class="kw">print</span>(fit_eeg3, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</a></code></pre></div>
<pre><code>##           mean  2.5% 97.5% n_eff Rhat
## alpha     3.64  2.83  4.48  1234    1
## beta      2.31  1.06  3.54  3334    1
## tau_u[1]  2.19  1.54  3.01  1226    1
## tau_u[2]  1.76  0.17  3.55  1019    1
## sigma    11.62 11.32 11.92  4924    1</code></pre>
<div class="sourceCode" id="cb722"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb722-1" data-line-number="1"><span class="kw">traceplot</span>(fit_eeg3, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceploteeg3"></span>
<img src="bookdown_files/figure-html/traceploteeg3-1.svg" alt="Traceplots of alpha, beta, tau_u, and sigma from the model fit_eeg3." width="672" />
<p class="caption">
FIGURE 11.5: Traceplots of <code>alpha</code>, <code>beta</code>, <code>tau_u</code>, and <code>sigma</code> from the model <code>fit_eeg3</code>.
</p>
</div>
<p>Although the samples of <code>tau_u[2]</code> are still correlated with the adjustments for the slope, <code>u[,2]</code>, these latter parameters are not the ones explored by the model, the auxiliary parameters, <code>z_u</code>, are the relevant ones for the sampler. The plots <a href="ch-complexstan.html#fig:pairstauu">11.6</a> and <a href="ch-complexstan.html#fig:pairstauz">11.7</a> show that although <code>log(tau_u[2])</code> and <code>u[1,2]</code> are still correlated,
<code>log(tau_u[2])</code> and <code>z_u[1,2]</code> are not.</p>

<div class="sourceCode" id="cb723"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb723-1" data-line-number="1"></a>
<a class="sourceLine" id="cb723-2" data-line-number="2"><span class="kw">mcmc_pairs</span>(<span class="kw">as.array</span>(fit_eeg3),</a>
<a class="sourceLine" id="cb723-3" data-line-number="3">  <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;u[1,2]&quot;</span>),</a>
<a class="sourceLine" id="cb723-4" data-line-number="4">  <span class="dt">transform =</span> <span class="kw">list</span>(<span class="st">`</span><span class="dt">tau_u[2]</span><span class="st">`</span> =<span class="st"> &quot;log&quot;</span>)</a>
<a class="sourceLine" id="cb723-5" data-line-number="5">)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pairstauu"></span>
<img src="bookdown_files/figure-html/pairstauu-1.svg" alt="Pair plots showing a clear correlations (funnel-shaped clouds of samples) between the samples of \(\tau_2\) and some of the by-subject adjustments to the slope." width="672" />
<p class="caption">
FIGURE 11.6: Pair plots showing a clear correlations (funnel-shaped clouds of samples) between the samples of <span class="math inline">\(\tau_2\)</span> and some of the by-subject adjustments to the slope.
</p>
</div>

<div class="sourceCode" id="cb724"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb724-1" data-line-number="1"></a>
<a class="sourceLine" id="cb724-2" data-line-number="2"><span class="kw">mcmc_pairs</span>(<span class="kw">as.array</span>(fit_eeg3),</a>
<a class="sourceLine" id="cb724-3" data-line-number="3">  <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;z_u[1,2]&quot;</span>),</a>
<a class="sourceLine" id="cb724-4" data-line-number="4">  <span class="dt">transform =</span> <span class="kw">list</span>(<span class="st">`</span><span class="dt">tau_u[2]</span><span class="st">`</span> =<span class="st"> &quot;log&quot;</span>)</a>
<a class="sourceLine" id="cb724-5" data-line-number="5">)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pairstauz"></span>
<img src="bookdown_files/figure-html/pairstauz-1.svg" alt="Pair plots showing no clear correlation between the samples of \(\tau_2\) and some of the by-subject auxiliary parameters (z_u) used to build the adjustments to the slope." width="672" />
<p class="caption">
FIGURE 11.7: Pair plots showing no clear correlation between the samples of <span class="math inline">\(\tau_2\)</span> and some of the by-subject auxiliary parameters (<code>z_u</code>) used to build the adjustments to the slope.
</p>
</div>
<!-- Before examine how each subject is being affected by the manipulation as we did with `brms`, we'll fit a correlated intercepts and slopes model. -->
</div>
<div id="sec-corrstan" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.1.3</span> Correlated varying intercept varying slopes model<a href="ch-complexstan.html#sec-corrstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the model with correlated varying intercepts and slopes, the likelihood remains identical to the model without a correlation between group-level intercepts and slopes. Priors and hyperpriors change to reflect the potential correlation between by-subject adjustments to intercepts and slopes:</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + c\_cloze_n \cdot  (\beta + u_{subj[n],2}),\sigma)
\end{equation}\]</span></p>
<p>The correlation is indicated in the priors on the adjustments for vector of by-subject intercepts <span class="math inline">\(u_{1}\)</span> and the vector of by-subject slopes <span class="math inline">\(u_{2}\)</span>.</p>
<ul>
<li>Priors:
<span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp; \sim \mathit{Normal}(0,10) \\
 \beta  &amp; \sim \mathit{Normal}(0,10) \\
  \sigma  &amp;\sim \mathit{Normal}_+(0,50)\\
  {\begin{pmatrix}
  u_{i,1} \\
  u_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix}
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right)
 \end{aligned}
 \end{equation}\]</span></li>
</ul>
<p>where <span class="math inline">\(i = \{1, .., N_{subj} \}\)</span></p>
<p><span class="math display">\[\begin{equation}
\boldsymbol{\Sigma_u} = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_2} &amp; \tau_{u_2}^2
\end{pmatrix}}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim \mathit{Normal}_+(0,20)\\
\tau_{u_2} &amp;\sim \mathit{Normal}_+(0,20)\\
\rho_u &amp;\sim \mathit{LKJcorr}(2) 
\end{aligned}
\end{equation}\]</span></p>
<p>As a first attempt, we write this model following the mathematical notation as closely as possible. We’ll see that this will be problematic in terms of efficient sampling and convergence. In this Stan model (<code>hierarchical_corr.stan</code>), we use some new functions and types:</p>
<ul>
<li><code>corr_matrix[n] M;</code> defines a (square) matrix of <span class="math inline">\(n\)</span> rows and <span class="math inline">\(n\)</span> columns called <span class="math inline">\(M\)</span>, symmetrical around a diagonal of ones.</li>
<li><code>rep_row_vector(X, n)</code> creates a row vector with <span class="math inline">\(n\)</span> columns filled with <span class="math inline">\(X\)</span>.</li>
<li><code>quad_form_diag(M, V)</code> creates a <em>quadratic form</em> using the column vector <span class="math inline">\(V\)</span> as a diagonal matrix (a matrix with all zeros except for its diagonal), this function corresponds in Stan to: <code>diag_matrix(V) * M * diag_matrix(V)</code> and in R to <code>diag(V) %*% M %*% diag(V)</code>. This
computes a variance-covariance matrix from the vector of standard deviations and the correlation matrix (recall the generation of multivariate data in section <a href="ch-intro.html#sec-generatebivariatedata">1.6.3</a>).</li>
</ul>
<pre class="stan fold-show"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  array[N] int&lt;lower = 1, upper = N_subj&gt; subj;
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;
  real alpha;
  real beta;
  matrix[N_subj, 2] u;
  corr_matrix[2] rho_u;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u[1] | 0, 20)  -
    normal_lccdf(0 | 0, 20);
 target += normal_lpdf(tau_u[2] | 0, 20)  -
    normal_lccdf(0 | 0, 20);
  target += lkj_corr_lpdf(rho_u | 2);
  for(i in 1:N_subj)
    target +=  multi_normal_lpdf(u[i,] |
                                 rep_row_vector(0, 2),
                                 quad_form_diag(rho_u, tau_u));
  target += normal_lpdf(signal | alpha + u[subj, 1] +
                        c_cloze .* (beta + u[subj, 2]), sigma);
}</code></pre>
<!-- diag_matrix(vector x) -->
<!-- The diagonal matrix with diagonal x -->
<p>Problematic aspects of the first model presented in section <a href="ch-complexstan.html#sec-uncorrstan">11.1.2</a> (before the reparameterization), that is, dependencies between parameters, are also present here. Fit the model as follows:</p>
<div class="sourceCode" id="cb726"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb726-1" data-line-number="1">hierarchical_corr &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb726-2" data-line-number="2">  <span class="st">&quot;hierarchical_corr.stan&quot;</span>,</a>
<a class="sourceLine" id="cb726-3" data-line-number="3">  <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span></a>
<a class="sourceLine" id="cb726-4" data-line-number="4">)</a>
<a class="sourceLine" id="cb726-5" data-line-number="5">fit_eeg_corr &lt;-<span class="st"> </span><span class="kw">stan</span>(hierarchical_corr, <span class="dt">data =</span> ls_eeg)</a></code></pre></div>
<pre><code>## Warning: There were 2 divergent transitions after warmup. See
## https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
## to find out why this is a problem and how to eliminate them.</code></pre>
<pre><code>## Warning: There were 2 chains where the estimated Bayesian Fraction of Missing Information was low. See
## https://mc-stan.org/misc/warnings.html#bfmi-low</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>As we expected, there are warnings and bad mixing of the chains for <code>tau_u[2]</code>; see also the traceplots in Figure <a href="ch-complexstan.html#fig:traceploteegcorr">11.8</a>.</p>
<div class="sourceCode" id="cb732"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb732-1" data-line-number="1"><span class="kw">print</span>(fit_eeg_corr, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</a></code></pre></div>
<pre><code>##           mean  2.5% 97.5% n_eff Rhat
## alpha     3.65  2.80  4.51  1617 1.00
## beta      2.35  1.15  3.55  3889 1.00
## tau_u[1]  2.18  1.53  3.00  2563 1.00
## tau_u[2]  1.72  0.31  3.51   162 1.02
## sigma    11.62 11.33 11.92  5967 1.00</code></pre>

<div class="sourceCode" id="cb734"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb734-1" data-line-number="1"></a>
<a class="sourceLine" id="cb734-2" data-line-number="2"><span class="kw">traceplot</span>(fit_eeg_corr, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceploteegcorr"></span>
<img src="bookdown_files/figure-html/traceploteegcorr-1.svg" alt="Traceplots of alpha, beta, tau_u, and sigma from the model fit_eeg_corr." width="672" />
<p class="caption">
FIGURE 11.8: Traceplots of <code>alpha</code>, <code>beta</code>, <code>tau_u</code>, and <code>sigma</code> from the model <code>fit_eeg_corr</code>.
</p>
</div>
<p>The problem (which can also be discovered in a pairs plot) is the same one that we saw before: There is a strong correlation between <code>tau_u[2]</code> (in fact, <code>log(tau_u[2])</code>, which is the parameter dimension that the sampler considers) and <code>u</code>, creating a funnel.</p>
<p>The solution to this problem is the reparametrization of this model. The non-centered parametrization for this type of model is called the Cholesky factorization. The mathematics and the intuition behind this parametrization are explained in Box <a href="ch-complexstan.html#thm:cholesky">11.2</a>.</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:cholesky" class="theorem"><strong>Box 11.2  </strong></span><strong>Cholesky factorization</strong></p>
</div>
<p>First, some definitions that we will need below. A matrix is square if the number of rows and columns is identical. A square matrix <span class="math inline">\(A\)</span> is symmetric if <span class="math inline">\(A^T = A\)</span>, i.e., if transposing the matrix gives the matrix back. Suppose that <span class="math inline">\(A\)</span> is a known matrix with real numbers. If <span class="math inline">\(\boldsymbol{x}\)</span> is a vector of variables with length <span class="math inline">\(p\)</span> (a <span class="math inline">\(p\times 1\)</span> matrix), then <span class="math inline">\(x^T A x\)</span> is called a quadratic form in <span class="math inline">\(x\)</span> (<span class="math inline">\(x^T A x\)</span> will be a scalar, <span class="math inline">\(1\times 1\)</span>). If <span class="math inline">\(x^TAx&gt;0\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(A\)</span> is a positive definite matrix. If <span class="math inline">\(x^TAx\geq 0\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(A\)</span> is positive semi-definite.</p>
<p>We encountered correlation matrices first in section <a href="ch-intro.html#sec-generatebivariatedata">1.6.3</a>. A correlation matrix is always symmetric, has ones along the diagonal, and real values ranging between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span> on the off-diagonals. Given a correlation matrix <span class="math inline">\(\boldsymbol{\rho_u}\)</span> that is positive definite or semi-definite, we can decompose it into a lower triangular matrix <span class="math inline">\(\mathbf{L_u}\)</span> such that <span class="math inline">\(\mathbf{L_u}\mathbf{L_u}^T=\boldsymbol{\rho_u}\)</span>. The matrix <span class="math inline">\(\mathbf{L_u}\)</span> is called the Cholesky factor of <span class="math inline">\(\boldsymbol{\rho_u}\)</span>. Intuitively, you can think of <span class="math inline">\(L_u\)</span> as the matrix equivalent of the square root of <span class="math inline">\(\boldsymbol{\rho_u}\)</span>. More details on the Cholesky factorization can be found in <span class="citation">Gentle (<a href="#ref-Gentle">2007</a>)</span>.</p>
<p><span class="math display">\[\begin{equation}
\mathbf{L_u}  =
{\begin{pmatrix} 
l_{11} &amp; 0 \\ 
l_{21}  &amp; l_{22}
\end{pmatrix}}
\end{equation}\]</span></p>
<p>For a model without a correlation between adjustments for the intercept and slope, we assumed that adjustments <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span> were generated by two independent normal distributions. But now we want those adjustments to be correlated. We can use the Cholesky factorization to generate correlated random variables in the following way.</p>
<ol style="list-style-type: decimal">
<li>Generate uncorrelated vectors, <span class="math inline">\(z_{u_1}\)</span> and <span class="math inline">\(z_{u_2}\)</span>, for each vector of
adjustments <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span>, as sampled from <span class="math inline">\(\mathit{Normal}(0,1)\)</span>:</li>
</ol>
<p><span class="math display">\[z_{u_1} \sim \mathit{Normal}(0,1)\]</span>
<span class="math display">\[z_{u_2} \sim \mathit{Normal}(0,1)\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>By multiplying the Cholesky factor by our <span class="math inline">\(z\)</span>’s, generate a matrix that contains two row vectors of correlated variables (with standard deviation of <span class="math inline">\(1\)</span>).</li>
</ol>
<p><span class="math display">\[
  \mathbf{L_u}\cdot \mathbf{z_u}  =
  {\begin{pmatrix} 
  l_{11} &amp; 0 \\ 
  l_{21}  &amp; l_{22}
  \end{pmatrix}}
  {\begin{pmatrix}
  z_{u_{1,subj=1}} &amp; z_{u_{1,subj=2}} &amp; ... &amp; z_{u_{1,subj=N_{subj}}} \\
  z_{u_{2,subj=1}} &amp; z_{u_{2,subj=2}} &amp; ... &amp; z_{u_{2,subj=N_{subj}}}
  \end{pmatrix}}
  \]</span></p>
<p><span class="math display">\[
  \mathbf{L_u}\cdot \mathbf{z_u}  =
  {\begin{pmatrix}
  l_{11} \cdot z_{u_{1,1}} + 0 \cdot z_{u_{2,1}} &amp;   ... &amp; l_{11} \cdot z_{u_{1,N_{subj}}} \\
  l_{21} \cdot z_{u_{1,1}} + l_{22} \cdot z_{u_{2,1}} &amp; ... &amp; l_{11} \cdot z_{u_{1,N_{subj}}} + l_{22} \cdot z_{u_{2,N_{subj}}}
  \end{pmatrix}}
  \]</span></p>
<p>A very informal explanation of why this works is that we are making the
variable that corresponds to the slope to be a function of a scaled version of
the intercept.</p>
<ol start="3" style="list-style-type: decimal">
<li>The last step is to scale the previous matrix to the desired standard deviation. We define the diagonalized matrix <span class="math inline">\(diag\_matrix(\tau_u)\)</span> as before:</li>
</ol>
<p><span class="math display">\[
  {\begin{pmatrix} 
  \tau_{u_1} &amp; 0 \\ 
  0  &amp; \tau_{u_2}
  \end{pmatrix}}
  \]</span></p>
<p>Then pre-multiply it by the correlated variables with SD of 1 from before:</p>
<p><span class="math display">\[\mathbf{u} = diag\_matrix(\tau_u) \cdot \mathbf{L_u}\cdot \mathbf{z_u} = \]</span></p>
<p><span class="math display">\[ 
  {\begin{pmatrix} 
  \tau_{u_1} &amp; 0 \\ 
  0  &amp; \tau_{u_2}
  \end{pmatrix}}
  {\begin{pmatrix}
  l_{11} \cdot z_{u_{1,1}}  &amp; ...  \\
  l_{21} \cdot z_{u_{1,1}} + l_{22} \cdot z_{u_{2,1}} &amp; ... 
  \end{pmatrix}}
  \]</span></p>
<p><span class="math display">\[ 
  {\begin{pmatrix}
  \tau_{u_1} \cdot l_{11} \cdot z_{u_{1,1}}  &amp; \tau_{u_1} \cdot l_{11} \cdot  z_{u_{1,2}} &amp; ...  \\
  \tau_{u_2} \cdot (l_{21} \cdot z_{u_{1,1}} + l_{22} \cdot z_{u_{2,1}}) &amp; \tau_{u_2} \cdot (l_{21} \cdot  z_{u_{1,2}} + l_{22} \cdot z_{u_{2,2}}) &amp; ... 
  \end{pmatrix}}
  \]</span></p>
<p><strong>It might be helpful to see how one would implement this in R:</strong></p>
<p>Let’s assume a correlation of <span class="math inline">\(0.8\)</span>.</p>
<div class="sourceCode" id="cb735"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb735-1" data-line-number="1">rho &lt;-<span class="st"> </span><span class="fl">.8</span></a>
<a class="sourceLine" id="cb735-2" data-line-number="2"><span class="co"># Correlation matrix</span></a>
<a class="sourceLine" id="cb735-3" data-line-number="3">(rho_u &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, rho, rho, <span class="dv">1</span>), <span class="dt">ncol =</span> <span class="dv">2</span>))</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  1.0  0.8
## [2,]  0.8  1.0</code></pre>
<div class="sourceCode" id="cb737"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb737-1" data-line-number="1"><span class="co"># Cholesky factor:</span></a>
<a class="sourceLine" id="cb737-2" data-line-number="2"><span class="co"># (Transpose it so that it looks the same as in Stan)</span></a>
<a class="sourceLine" id="cb737-3" data-line-number="3">(L_u &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">chol</span>(rho_u)))</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  1.0  0.0
## [2,]  0.8  0.6</code></pre>
<div class="sourceCode" id="cb739"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb739-1" data-line-number="1"><span class="co"># Verify that we recover rho_u,</span></a>
<a class="sourceLine" id="cb739-2" data-line-number="2"><span class="co"># Recall that %*% indicates matrix multiplication</span></a>
<a class="sourceLine" id="cb739-3" data-line-number="3">L_u <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(L_u)</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  1.0  0.8
## [2,]  0.8  1.0</code></pre>
<ol style="list-style-type: decimal">
<li>Generate uncorrelated z from a standard normal distribution assuming only 10 subjects.</li>
</ol>
<div class="sourceCode" id="cb741"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb741-1" data-line-number="1">N_subj &lt;-<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb741-2" data-line-number="2">(z_u1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_subj, <span class="dv">0</span>, <span class="dv">1</span>))</a></code></pre></div>
<pre><code>##  [1] -2.5082 -0.1676  0.0382 -1.0596  0.3854 -1.9671  0.9550
##  [8] -1.6634 -2.2027 -0.7636</code></pre>
<div class="sourceCode" id="cb743"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb743-1" data-line-number="1">(z_u2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_subj, <span class="dv">0</span>, <span class="dv">1</span>))</a></code></pre></div>
<pre><code>##  [1]  0.1621 -0.6516 -0.5593  0.3332 -1.0589 -0.0858  0.4979
##  [8]  1.6340  0.4795  1.7148</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Create matrix of correlated parameters.</li>
</ol>
<p>First, create a matrix with the uncorrelated parameters:</p>
<div class="sourceCode" id="cb745"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb745-1" data-line-number="1"><span class="co"># matrix z_u</span></a>
<a class="sourceLine" id="cb745-2" data-line-number="2">(z_u &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(z_u1, z_u2), <span class="dt">ncol =</span> N_subj, <span class="dt">byrow =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##        [,1]   [,2]    [,3]   [,4]   [,5]    [,6]  [,7]
## [1,] -2.508 -0.168  0.0382 -1.060  0.385 -1.9671 0.955
## [2,]  0.162 -0.652 -0.5593  0.333 -1.059 -0.0858 0.498
##       [,8]   [,9]  [,10]
## [1,] -1.66 -2.203 -0.764
## [2,]  1.63  0.479  1.715</code></pre>
<p>Then, generate correlated parameters by pre-multiplying the <span class="math inline">\(\mathbf{z}_u\)</span> matrix with <span class="math inline">\(\mathbf{L}_u\)</span>.</p>
<div class="sourceCode" id="cb747"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb747-1" data-line-number="1">L_u <span class="op">%*%</span><span class="st"> </span>z_u</a></code></pre></div>
<pre><code>##       [,1]   [,2]    [,3]   [,4]   [,5]  [,6]  [,7]  [,8]  [,9]
## [1,] -2.51 -0.168  0.0382 -1.060  0.385 -1.97 0.955 -1.66 -2.20
## [2,] -1.91 -0.525 -0.3050 -0.648 -0.327 -1.63 1.063 -0.35 -1.47
##       [,10]
## [1,] -0.764
## [2,]  0.418</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Use the following diagonal matrix to scale the z_u.</li>
</ol>
<div class="sourceCode" id="cb749"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb749-1" data-line-number="1">tau_u1 &lt;-<span class="st"> </span><span class="fl">.2</span></a>
<a class="sourceLine" id="cb749-2" data-line-number="2">tau_u2 &lt;-<span class="st"> </span><span class="fl">.01</span></a>
<a class="sourceLine" id="cb749-3" data-line-number="3">(diag_matrix_tau &lt;-<span class="st"> </span><span class="kw">diag</span>(<span class="kw">c</span>(tau_u1, tau_u2)))</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  0.2 0.00
## [2,]  0.0 0.01</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Finally, generate the adjustments for each subject u:</li>
</ol>
<div class="sourceCode" id="cb751"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb751-1" data-line-number="1">(u &lt;-<span class="st"> </span>diag_matrix_tau <span class="op">%*%</span><span class="st"> </span>L_u <span class="op">%*%</span><span class="st"> </span>z_u)</a></code></pre></div>
<pre><code>##         [,1]     [,2]     [,3]     [,4]     [,5]    [,6]
## [1,] -0.5016 -0.03352  0.00764 -0.21192  0.07709 -0.3934
## [2,] -0.0191 -0.00525 -0.00305 -0.00648 -0.00327 -0.0163
##        [,7]    [,8]    [,9]    [,10]
## [1,] 0.1910 -0.3327 -0.4405 -0.15271
## [2,] 0.0106 -0.0035 -0.0147  0.00418</code></pre>
<div class="sourceCode" id="cb753"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb753-1" data-line-number="1"><span class="co"># The rows are correlated ~.8</span></a>
<a class="sourceLine" id="cb753-2" data-line-number="2"><span class="kw">cor</span>(u[<span class="dv">1</span>, ], u[<span class="dv">2</span>, ])</a></code></pre></div>
<pre><code>## [1] 0.828</code></pre>
<div class="sourceCode" id="cb755"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb755-1" data-line-number="1"><span class="co"># The variance components can be recovered as well:</span></a>
<a class="sourceLine" id="cb755-2" data-line-number="2"><span class="kw">sd</span>(u[<span class="dv">1</span>, ])</a></code></pre></div>
<pre><code>## [1] 0.236</code></pre>
<div class="sourceCode" id="cb757"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb757-1" data-line-number="1"><span class="kw">sd</span>(u[<span class="dv">2</span>, ])</a></code></pre></div>
<pre><code>## [1] 0.00916</code></pre>
</div>
<p>The reparameterization of the model, which allows for a correlation between adjustments for the intercepts and slopes, in <code>hierarchical_corr2.stan</code> is shown below.
The code implements the following new types and functions:</p>
<ul>
<li><code>cholesky_factor_corr[2] L_u</code>, which defines <code>L_u</code> as a lower triangular (<span class="math inline">\(2 \times 2\)</span>)
matrix which has to be the Cholesky factor of a correlation.</li>
<li><code>diag_pre_multiply(tau_u,L_u)</code> which makes a diagonal matrix out of
the vector <code>tau_u</code> and multiplies it by <code>L_u</code>.</li>
<li><code>to_vector(z_u)</code> makes a long vector out the matrix <code>z_u</code>.</li>
<li><code>L_u ~ lkj_corr_cholesky(2)</code> is the Cholesky factor associated with the LKJ
correlation distribution. It implies that <code>L_u * L_u' = rho_u ~ lkj_corr(2.0)</code>. The symbol <code>'</code> indicates transposition (in R, this corresponds to the function <code>t()</code>).</li>
</ul>
<pre class="stan fold-show"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_cloze;
  array[N] int&lt;lower = 1, upper = N_subj&gt; subj;
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;
  real alpha;
  real beta;
  matrix[2, N_subj] z_u;
  cholesky_factor_corr[2] L_u;
}
transformed parameters {
  matrix[N_subj, 2] u;
  u = (diag_pre_multiply(tau_u, L_u) * z_u)&#39;;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u[1] | 0, 20)  -
    normal_lccdf(0 | 0, 20);
 target += normal_lpdf(tau_u[2] | 0, 20)  -
    normal_lccdf(0 | 0, 20);
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += std_normal_lpdf(to_vector(z_u));
  target += normal_lpdf(signal | alpha + u[subj, 1] +
                        c_cloze .* (beta + u[subj, 2]), sigma);
}
generated quantities {
  corr_matrix[2] rho_u= L_u * L_u&#39;;
  vector[N_subj] effect_by_subj;
  for(i in 1:N_subj)
    effect_by_subj[i] = beta + u[i, 2];
}</code></pre>
<p>In this Stan model, we also created an <code>effect_by_subject</code> in the generated quantities. This would allow us to plot or to summarize by-subject effects of cloze probability.</p>
<p>One can recover the correlation parameter by adding in the <code>generated quantities</code> section a <span class="math inline">\(2 \times 2\)</span> matrix <code>rho_u</code>, defined as <code>rho_u = L_u * L_u';</code>.</p>
<p>Fit the new model:</p>
<div class="sourceCode" id="cb760"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb760-1" data-line-number="1">hierarchical_corr2 &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb760-2" data-line-number="2">  <span class="st">&quot;hierarchical_corr2.stan&quot;</span>,</a>
<a class="sourceLine" id="cb760-3" data-line-number="3">  <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span></a>
<a class="sourceLine" id="cb760-4" data-line-number="4">)</a>
<a class="sourceLine" id="cb760-5" data-line-number="5">fit_eeg_corr2 &lt;-<span class="st"> </span><span class="kw">stan</span>(hierarchical_corr2, <span class="dt">data =</span> ls_eeg)</a></code></pre></div>
<!-- Unfortunately `rstan` version 2.21.1 throws spurious warnings here. This is because the Cholesky matrix has elements which are always zero or one, and thus the variance within and between changes, and thus Rhat, is not defined.^[The  lightweight interface `cmdstanr` version 0.0.0.9008 for cmdstan version 2.24.0 is not throwing this warning though.] However, the parameters of the model have an appropriate number of effective sample size,  Rhat, and the chains are mixing well. -->
<p>The Cholesky matrix has elements which are always zero or one, and thus the variance within and between chains (and therefore Rhat) are not defined. However, the rest of the parameters of the model have an appropriate number of effective sample size (more than 10% of the total number of post-warmup samples), Rhats are close to one, and the chains are mixing well; see also the traceplots in Figure <a href="ch-complexstan.html#fig:traceploteegcorr2">11.9</a>.</p>
<div class="sourceCode" id="cb761"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb761-1" data-line-number="1"><span class="kw">print</span>(fit_eeg_corr2,</a>
<a class="sourceLine" id="cb761-2" data-line-number="2">  <span class="dt">pars =</span></a>
<a class="sourceLine" id="cb761-3" data-line-number="3">    <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;rho_u&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;L_u&quot;</span>))</a></code></pre></div>
<pre><code>##             mean  2.5% 97.5% n_eff Rhat
## alpha       3.65  2.78  4.43  1282    1
## beta        2.33  1.12  3.58  4331    1
## tau_u[1]    2.18  1.52  2.96  1563    1
## tau_u[2]    1.65  0.10  3.48  1095    1
## rho_u[1,1]  1.00  1.00  1.00   NaN  NaN
## rho_u[1,2]  0.16 -0.57  0.75  3535    1
## rho_u[2,1]  0.16 -0.57  0.75  3535    1
## rho_u[2,2]  1.00  1.00  1.00   133    1
## sigma      11.62 11.31 11.94  5090    1
## L_u[1,1]    1.00  1.00  1.00   NaN  NaN
## L_u[1,2]    0.00  0.00  0.00   NaN  NaN
## L_u[2,1]    0.16 -0.57  0.75  3535    1
## L_u[2,2]    0.92  0.64  1.00  2041    1</code></pre>

<div class="sourceCode" id="cb763"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb763-1" data-line-number="1"></a>
<a class="sourceLine" id="cb763-2" data-line-number="2"><span class="kw">traceplot</span>(fit_eeg_corr2,</a>
<a class="sourceLine" id="cb763-3" data-line-number="3">  <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;L_u&quot;</span>, <span class="st">&quot;sigma&quot;</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceploteegcorr2"></span>
<img src="bookdown_files/figure-html/traceploteegcorr2-1.svg" alt="Traceplots of alpha, beta, tau_u, and sigma from the model fit_eeg_corr." width="672" />
<p class="caption">
FIGURE 11.9: Traceplots of <code>alpha</code>, <code>beta</code>, <code>tau_u</code>, and <code>sigma</code> from the model <code>fit_eeg_corr</code>.
</p>
</div>
<p>Is there a correlation between the by-subject intercept and slope?</p>
<p>Let’s visualize some of the posteriors in Figure <a href="ch-complexstan.html#fig:posteegcorr2">11.10</a> with the following code:</p>

<div class="sourceCode" id="cb764"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb764-1" data-line-number="1"><span class="kw">mcmc_hist</span>(<span class="kw">as.data.frame</span>(fit_eeg_corr2),</a>
<a class="sourceLine" id="cb764-2" data-line-number="2">  <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;beta&quot;</span>, <span class="st">&quot;rho_u[1,2]&quot;</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:posteegcorr2"></span>
<img src="bookdown_files/figure-html/posteegcorr2-1.svg" alt="Histograms of the samples of the posteriors of beta and rho_u[1,2] from the model fit_eeg_corr2." width="672" />
<p class="caption">
FIGURE 11.10: Histograms of the samples of the posteriors of <code>beta</code> and <code>rho_u[1,2]</code> from the model <code>fit_eeg_corr2</code>.
</p>
</div>
<p>The posterior distribution of the correlation parameter shows that one can’t really know whether the by-subject intercepts and slopes are correlated. The broad spread of the posterior indicates that we don’t have enough data to estimate this parameter accurately: the posterior ranges from <span class="math inline">\(-1\)</span> to <span class="math inline">\(1\)</span>, and is basically just reflecting the prior specification (the LKJcorr prior with parameter <span class="math inline">\(\eta = 2\)</span>).</p>
</div>
<div id="sec-crosscorrstan" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.1.4</span> By-subject and by-items correlated varying intercept varying slopes model<a href="ch-complexstan.html#sec-crosscorrstan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We extend the previous model by adding by-items intercepts and slopes, and priors and hyperpriors that reflect the potential correlation between by-items adjustments to intercepts and slopes:</p>
<p><span class="math display">\[\begin{multline}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n], 1} + w_{item[n], 1} + \\
  c\_cloze_n \cdot  (\beta + u_{subj[n],2} + w_{item[n], 2}),\sigma)
\end{multline}\]</span></p>
<p>The correlation is indicated in the priors on the adjustments for the vectors representing the varying intercepts <span class="math inline">\(u_{1}\)</span> and varying slopes <span class="math inline">\(u_{2}\)</span> for subjects, and the varying intercepts <span class="math inline">\(w_{1}\)</span> and varying slopes <span class="math inline">\(w_{2}\)</span> for items.</p>
<ul>
<li>Priors:
<span class="math display">\[\begin{equation}
 \begin{aligned} 
 \alpha &amp; \sim \mathit{Normal}(0,10) \\
 \beta  &amp; \sim \mathit{Normal}(0,10) \\
  \sigma  &amp;\sim \mathit{Normal}_+(0, 50)\\
    {\begin{pmatrix}
  u_{i,1} \\
  u_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right) \\
   {\begin{pmatrix}
  w_{i,1} \\
  w_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_w} \right)
 \end{aligned}
 \end{equation}\]</span></li>
</ul>
<p><span class="math display">\[\begin{equation}
\boldsymbol{\Sigma_u} = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_1} &amp; \tau_{u_2}^2
\end{pmatrix}}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\boldsymbol{\Sigma_w} = 
{\begin{pmatrix} 
\tau_{w_1}^2 &amp; \rho_w \tau_{w_1} \tau_{w_2} \\ 
\rho_w \tau_{w_1} \tau_{w_1} &amp; \tau_{w_2}^2
\end{pmatrix}}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim \mathit{Normal}_+(0,20)\\
\tau_{u_2} &amp;\sim \mathit{Normal}_+(0,20)\\
\rho_u &amp;\sim \mathit{LKJcorr}(2) 
\end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{w_1} &amp;\sim \mathit{Normal}_+(0,20)\\
\tau_{w_2} &amp;\sim \mathit{Normal}_+(0,20)\\
\rho_w &amp;\sim \mathit{LKJcorr}(2) 
\end{aligned}
\end{equation}\]</span></p>
<p>The translation to Stan (<code>hierarchical_corr_by.stan</code>) looks as follows:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower=1&gt; N;
  vector[N] signal;
  int&lt;lower = 1&gt; N_subj;
  int&lt;lower = 1&gt; N_item;
  vector[N] c_cloze;
  array[N] int&lt;lower = 1, upper = N_subj&gt; subj;
  array[N] int&lt;lower = 1, upper = N_item&gt; item;
}
parameters {
  real&lt;lower = 0&gt; sigma;
  vector&lt;lower = 0&gt;[2]  tau_u;
  vector&lt;lower = 0&gt;[2]  tau_w;
  real alpha;
  real beta;
  matrix[2, N_subj] z_u;
  matrix[2, N_item] z_w;
  cholesky_factor_corr[2] L_u;
  cholesky_factor_corr[2] L_w;
}
transformed parameters {
  matrix[N_subj, 2] u;
  matrix[N_item, 2] w;
  u = (diag_pre_multiply(tau_u, L_u) * z_u)&#39;;
  w = (diag_pre_multiply(tau_w, L_w) * z_w)&#39;;
}
model {
  target += normal_lpdf(alpha| 0,10);
  target += normal_lpdf(beta | 0,10);
  target += normal_lpdf(sigma | 0, 50)  -
    normal_lccdf(0 | 0, 50);
  target += normal_lpdf(tau_u | 0, 20) -
    2 * normal_lccdf(0 | 0, 20);
 target += normal_lpdf(tau_w | 0, 20) -
    2* normal_lccdf(0 | 0, 20);
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += lkj_corr_cholesky_lpdf(L_w | 2);
  target += std_normal_lpdf(to_vector(z_u));
  target += std_normal_lpdf(to_vector(z_w));
  target += normal_lpdf(signal | alpha + u[subj, 1] + w[item, 1]+
                        c_cloze .* (beta + u[subj, 2] + w[item, 2]), sigma);
}
generated quantities {
  corr_matrix[2] rho_u = L_u * L_u&#39;;
  corr_matrix[2] rho_w = L_w * L_w&#39;;
}</code></pre>
<p>Add item as a number to the data and store it in a list:</p>
<div class="sourceCode" id="cb766"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb766-1" data-line-number="1">df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb766-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">item =</span> <span class="kw">as.numeric</span>(<span class="kw">as.factor</span>(item)))</a>
<a class="sourceLine" id="cb766-3" data-line-number="3">ls_eeg &lt;-<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb766-4" data-line-number="4">  <span class="dt">N =</span> <span class="kw">nrow</span>(df_eeg),</a>
<a class="sourceLine" id="cb766-5" data-line-number="5">  <span class="dt">signal =</span> df_eeg<span class="op">$</span>n400,</a>
<a class="sourceLine" id="cb766-6" data-line-number="6">  <span class="dt">c_cloze =</span> df_eeg<span class="op">$</span>c_cloze,</a>
<a class="sourceLine" id="cb766-7" data-line-number="7">  <span class="dt">subj =</span> df_eeg<span class="op">$</span>subj,</a>
<a class="sourceLine" id="cb766-8" data-line-number="8">  <span class="dt">item =</span> df_eeg<span class="op">$</span>item,</a>
<a class="sourceLine" id="cb766-9" data-line-number="9">  <span class="dt">N_subj =</span> <span class="kw">max</span>(df_eeg<span class="op">$</span>subj),</a>
<a class="sourceLine" id="cb766-10" data-line-number="10">  <span class="dt">N_item =</span> <span class="kw">max</span>(df_eeg<span class="op">$</span>item))</a></code></pre></div>
<p>Fit the model:</p>
<div class="sourceCode" id="cb767"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb767-1" data-line-number="1">hierarchical_corr_by &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb767-2" data-line-number="2">  <span class="st">&quot;hierarchical_corr_by.stan&quot;</span>,</a>
<a class="sourceLine" id="cb767-3" data-line-number="3">  <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</a>
<a class="sourceLine" id="cb767-4" data-line-number="4">fit_eeg_corr_by &lt;-<span class="st"> </span><span class="kw">stan</span>(hierarchical_corr_by, <span class="dt">data =</span> ls_eeg)</a></code></pre></div>
<p>Print the summary:</p>
<div class="sourceCode" id="cb768"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb768-1" data-line-number="1"><span class="kw">print</span>(fit_eeg_corr_by,</a>
<a class="sourceLine" id="cb768-2" data-line-number="2">  <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;tau_w&quot;</span>, <span class="st">&quot;rho_u&quot;</span>, <span class="st">&quot;rho_w&quot;</span>))</a></code></pre></div>
<pre><code>##             mean  2.5% 97.5% n_eff Rhat
## alpha       3.67  2.79  4.55  1974    1
## beta        2.29  0.90  3.63  4030    1
## sigma      11.49 11.19 11.80  5975    1
## tau_u[1]    2.19  1.55  3.01  1601    1
## tau_u[2]    1.49  0.09  3.34  1067    1
## tau_w[1]    1.51  0.83  2.18  1663    1
## tau_w[2]    2.27  0.26  4.21   935    1
## rho_u[1,1]  1.00  1.00  1.00   NaN  NaN
## rho_u[1,2]  0.14 -0.60  0.77  3872    1
## rho_u[2,1]  0.14 -0.60  0.77  3872    1
## rho_u[2,2]  1.00  1.00  1.00   629    1
## rho_w[1,1]  1.00  1.00  1.00   NaN  NaN
## rho_w[1,2] -0.41 -0.89  0.31  2143    1
## rho_w[2,1] -0.41 -0.89  0.31  2143    1
## rho_w[2,2]  1.00  1.00  1.00   180    1</code></pre>
<p>The correlations of interest are <code>rho_u[1,2]</code> and <code>rho_w[1,2]</code>; the summary above shows that the data are far too sparse to get tight estimates of these parameters: both posteriors are widely spread out.</p>
<p>This completes our review of hierarchical models and their implementation in Stan. The importance of coding directly a hierarchical model in Stan rather than using <code>brms</code> is that this increases the flexibility of the type of models that we can fit. In fact, we will see in chapter <a href="ch-cogmod.html#ch-cogmod">17</a>-<a href="ch-lognormalrace.html#ch-lognormalrace">20</a> that the same “machinery” can be used to have hierarchical parameters in cognitive models.</p>
<!-- A hierarchical log-normal model: The Stroop effect + add interaction here -->
</div>
</div>
<div id="summary-10" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.2</span> Summary<a href="ch-complexstan.html#summary-10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we learned to fit the four standard types of hierarchical models that we encountered in earlier chapters:</p>
<ul>
<li>The by-subjects varying intercepts model.</li>
<li>The by-subjects varying intercepts and varying slopes model without any correlation.</li>
<li>The by-subjects varying intercepts and varying slopes model with correlation.</li>
<li>The hierarchical model, with a full variance covariance matrix for both subjects and items.</li>
</ul>
<p>We also learned some important and powerful tools for making the Stan models more efficient at sampling: the non-centered parameterization and the Cholesky factorization. One important takeaway was that if data are sparse, the posteriors will just reflect the priors. We saw examples of this situation when investigating the posteriors of the correlation parameters.</p>
</div>
<div id="further-reading-8" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.3</span> Further reading<a href="ch-complexstan.html#further-reading-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="citation">Gelman and Hill (<a href="#ref-GelmanHill2007">2007</a>)</span> provides a comprehensive introduction to Bayesian hierarchical models, although that edition does not use Stan but rather WinBUGS. <span class="citation">Sorensen, Hohenstein, and Vasishth (<a href="#ref-SorensenVasishthTutorial">2016</a>)</span> is a short tutorial on hierarchical modeling using Stan, especially tailored for psychologists and linguists.</p>
</div>
<div id="exercises-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.4</span> Exercises<a href="ch-complexstan.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- Easier exercises first -->
<!-- RTs in lexical decision task: -->
<!-- https://link.springer.com/article/10.3758/s13428-015-0678-9 -->
<div class="exercise">
<p><span id="exr:stroop" class="exercise"><strong>Exercise 11.1  </strong></span>Log-normal model in Stan</p>
</div>
<p>Refit the Stroop example from section <a href="ch-hierarchical.html#sec-stroop">5.3</a> in Stan (<code>df_stroop</code>).</p>
<p>Assume the following likelihood and priors:</p>
<p><span class="math display">\[\begin{equation}
  rt_n \sim \mathit{LogNormal}(\alpha + u_{subj[n],1}  + c\_cond_n \cdot  (\beta + u_{subj[n],2}), \sigma)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
   \alpha &amp; \sim \mathit{Normal}(6, 1.5) \\
   \beta  &amp; \sim \mathit{Normal}(0, .1) \\
    \sigma  &amp;\sim \mathit{Normal}_+(0, 1)
 \end{aligned}
 \end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim \mathit{Normal}_+(0,1)\\
\tau_{u_2} &amp;\sim \mathit{Normal}_+(0,1)\\
\rho_u &amp;\sim \mathit{LKJcorr}(2) 
\end{aligned}
\end{equation}\]</span></p>
<div class="exercise">
<p><span id="exr:hierarchical-logn-stan" class="exercise"><strong>Exercise 11.2  </strong></span>By subject and by items hierarchical model with a log-normal likelihood.</p>
</div>
<p>We’ll revisit the question “Are subject relatives easier to process than object relatives?” Refit in Stan the exercise <a href="ch-hierarchical.html#exr:hierarchical-logn">5.2</a>.</p>
<div class="exercise">
<p><span id="exr:strooplogis" class="exercise"><strong>Exercise 11.3  </strong></span>A hierarchical logistic regression with Stan.</p>
</div>
<p>We’ll revisit the question “Is there a Stroop effect in accuracy?”. Refit in Stan the exercise <a href="ch-hierarchical.html#exr:strooplogis-brms">5.6</a>.</p>
<div class="exercise">
<p><span id="exr:distr-stan" class="exercise"><strong>Exercise 11.4  </strong></span>Distributional regression model of the effect of cloze probability on the N400</p>
</div>
<p>In section <a href="ch-hierarchical.html#sec-distrmodel">5.2.6</a>, we saw how to fit a distributional regression model. We might want to extend this approach to Stan. Fit the EEG data to a hierarchical model with by-subject and by-items varying intercept and slopes, and in addition assume that the variance component of the model can vary by subject.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  signal_n &amp;\sim \mathit{Normal}(\alpha + u_{subj[n],1} + w_{item[n],1} + \\
  &amp; c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma_n)\\
  \sigma_n &amp;= \exp(\alpha_{\sigma} + u_{\sigma_{subj[n]}})
\end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  \alpha_\alpha &amp;\sim \mathit{Normal}(0,log(50))\\
  u_\sigma &amp;\sim \mathit{Normal}(0, \tau_{u_\sigma}) \\
  \tau_{u_\sigma} &amp;\sim \mathit{Normal}_+(0, 5)
\end{aligned}
\end{equation}\]</span></p>
<p>To fit this model, take into account that <code>sigma</code> is now a vector, and it’s transformed parameter which depends on two parameters: <code>alpha_sigma</code> and the vector with <code>N_subj</code> elements <code>u_sigma</code>. In addition, <code>u_sigma</code> depends on the hyperparameter <code>tau_u_sigma</code> (<span class="math inline">\(\tau_{u_\sigma}\)</span>). (Using non-centered parametrization for <code>u_sigma</code> speeds up the model fit considerably).</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-betancourt2013hamiltonian">
<p>Betancourt, Michael J., and Mark Girolami. 2013. “Hamiltonian Monte Carlo for Hierarchical Models.”</p>
</div>
<div id="ref-GelmanHill2007">
<p>Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge University Press.</p>
</div>
<div id="ref-Gentle">
<p>Gentle, James E. 2007. “Matrix Algebra: Theory, Computations, and Applications in Statistics.” <em>Springer Texts in Statistics</em> 10. New York, NY: Springer.</p>
</div>
<div id="ref-neal2003">
<p>Neal, Radford M. 2003. “Slice Sampling.” <em>Annals of Statistics</em> 31 (3). The Institute of Mathematical Statistics: 705–67. <a href="https://doi.org/10.1214/aos/1056562461" class="uri">https://doi.org/10.1214/aos/1056562461</a>.</p>
</div>
<div id="ref-papaspiliopoulos2007">
<p>Papaspiliopoulos, Omiros, Gareth O. Roberts, and Martin Sköld. 2007. “A General Framework for the Parametrization of Hierarchical Models.” <em>Statistical Science</em> 22 (1). The Institute of Mathematical Statistics: 59–73. <a href="https://doi.org/10.1214/088342307000000014" class="uri">https://doi.org/10.1214/088342307000000014</a>.</p>
</div>
<div id="ref-SorensenVasishthTutorial">
<p>Sorensen, Tanner, Sven Hohenstein, and Shravan Vasishth. 2016. “Bayesian Linear Mixed Models Using Stan: A Tutorial for Psychologists, Linguists, and Cognitive Scientists.” <em>Quantitative Methods for Psychology</em> 12 (3): 175–200.</p>
</div>
<div id="ref-Stan2021">
<p>Stan Development Team. 2021. “Stan Modeling Language Users Guide and Reference Manual, Version 2.27.” <a href="https://mc-stan.org" class="uri">https://mc-stan.org</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="38">
<li id="fn38"><p>If this is not the case, we can do <code>as.numeric(as.factor(df_eeg$subj))</code> to transform the data appropriately.<a href="ch-complexstan.html#fnref38" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-introstan.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-custom.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
