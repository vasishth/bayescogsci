<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Bayesian hierarchical models | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Bayesian hierarchical models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Bayesian hierarchical models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2022-09-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-reg.html"/>
<link rel="next" href="ch-priors.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-anyway"><i class="fa fa-check"></i>Why read this book anyway?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b> Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b> Bayesian Regression Models using Stan: brms</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b> Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.6.1" data-path="ch-compbda.html"><a href="ch-compbda.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.6.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.6.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.6.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.6.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.6.3</b> Re-fitting a single subject pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.7</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.9</b> Further reading</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect response times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b> No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a><ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#model-building"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#exemplary-data-analysis"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-workflow.html"><a href="ch-workflow.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-workflow.html"><a href="ch-workflow.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a><ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-7"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-9"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-10"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a><ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b> Validation of a computed posterior distribution</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#simulation-based-calibration-revealing-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> Simulation-based calibration revealing a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-and-limitation-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues and limitation of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#a-custom-distribution-re-implementing-the-exponential-distribution-manually"><i class="fa fa-check"></i><b>12.3</b> A custom distribution: Re-implementing the exponential distribution manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-11"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Other useful models</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-12"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a><ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a><ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a><ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a><ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#summary-13"><i class="fa fa-check"></i><b>15.6</b> Summary</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.7</b> Further reading</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a><ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#issues-with-cross-validation"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a><ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-14"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a><ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.3</b> Further reading</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a><ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a><ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a><ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a><ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-hierarchical" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 5</span> Bayesian hierarchical models<a href="ch-hierarchical.html#ch-hierarchical" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Usually, experimental data in cognitive science contain “clusters”. These are natural groups that contain observations that are more similar within the clusters than between them.
The most common examples of clusters in experimental designs are subjects and experimental items (e.g., words, pictures, objects that are presented to the subjects). These clusters arise because we have multiple (repeated) observations for each subject, and for each item. If we want to incorporate this grouping structure in our analysis, we generally use a hierarchical model <span class="citation">(also called multi-level or a mixed model, Pinheiro and Bates <a href="#ref-pinheirobates">2000</a>)</span>. This kind of clustering and hierarchical modeling arises as a consequence of the idea of <em>exchangeability</em>.</p>
<div id="exchangeability-and-hierarchical-models" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.1</span> Exchangeability and hierarchical models<a href="ch-hierarchical.html#exchangeability-and-hierarchical-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Exchangeability is the Bayesian analog of the phrase “independent and identically distributed” that appears regularly in classical (i.e., frequentist) statistics. Some connections and differences between exchangeability and the frequentist concept of independent and identically distributed (iid) are detailed in Box <a href="ch-hierarchical.html#thm:exch">5.1</a>.</p>
<p>Informally, the idea of exchangeability is as follows. Suppose we assign a numerical index to each of the levels of a group (e.g., to each subject). When the levels are exchangeable, we can reassign the indices arbitrarily and lose no information; that is, the joint distribution will remain the same, because we don’t have any different prior information for each cluster (here, for each subject). In hierarchical models, we treat the levels of the group as exchangeable, and observations within each level in the group as also exchangeable. We generally include predictors at the level of the observations, those are the predictors that correspond to the experimental manipulations (e.g., attentional load, trial number, cloze probability, etc.); and maybe also at the group level, these are predictors that indicate characteristics of the levels in the group (e.g., the working memory capacity score of each subject). Then the conditional distributions given these explanatory variables would be exchangeable; that is, our predictors incorporate all the information that is not exchangeable, and when we factor the predictors out, the observations or units in the group are exchangeable. This is the reason why the item number is an appropriate cluster, but trial number is not: In the first case, if we permute the numbering of the items there is no loss of information because the indexes are exchangeable, all the information about the items is incorporated as predictors in the model. In the second case, the numbering of the trials include information that will be lost if we treat them as exchangeable. For example, consider the case where, as trial numbers increase, subjects get more experienced or fatigued. Even if we are not interested in the specific cluster-level estimates, hierarchical models allow us to generalize to the underlying population (subjects, items) from which the clusters in the sample were drawn. For more on exchangeability, consult the further reading at the end of the chapter.</p>
<p>Exchangeability is important in Bayesian statistics because of a theorem called the Representation Theorem <span class="citation">(de Finetti <a href="#ref-deFinetti">1931</a>)</span>. This theorem states that if a sequence of random variables is exchangeable, then the prior distributions on the parameters in a model are a necessary consequence; priors are not merely an arbitrary addition to the frequentist modeling approach that we are familiar with.</p>
<p>Furthermore, exchangeability has been shown <span class="citation">(Bernardo and Smith <a href="#ref-bernardosmith">2009</a>)</span> to be mathematically equivalent to assuming a hierarchical structure in the model. The argument goes as follows. Suppose that the parameters for each level in a group are <span class="math inline">\(\mu_i\)</span>, where the levels are labeled <span class="math inline">\(i=1,\dots,I\)</span>. An example of groups is subjects. Suppose also that the data <span class="math inline">\(y_n\)</span>, where <span class="math inline">\(n=1,\dots,N\)</span> are observations from these subjects (e.g., pupil size measurements, IQ scores, or any other approximately normally distributed outcome). The data are assumed to be generated as</p>
<p><span class="math display">\[\begin{equation}
y_n \sim \mathit{Normal}(\mu_{subj[n]},\sigma)
\end{equation}\]</span></p>
<p>The notation <span class="math inline">\(subj[n]\)</span>, which roughly follows <span class="citation">Gelman and Hill (<a href="#ref-GelmanHill2007">2007</a>)</span>, identifies the subject index. Suppose that 20 subjects respond 50 times each. If the data are ordered by subject id, then <span class="math inline">\(subj[1]\)</span> to <span class="math inline">\(subj[50]\)</span> corresponds to a subject with id <span class="math inline">\(i=1\)</span>, <span class="math inline">\(subj[51]\)</span> to <span class="math inline">\(subj[100]\)</span> corresponds to a subject with id <span class="math inline">\(i=2\)</span>, and so forth.</p>
<p>We can code this representation in a straightforward way in R:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb246-1" data-line-number="1">N_subj &lt;-<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb246-2" data-line-number="2">N_obs_subj &lt;-<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb246-3" data-line-number="3">N &lt;-<span class="st"> </span>N_subj <span class="op">*</span><span class="st"> </span>N_obs_subj</a>
<a class="sourceLine" id="cb246-4" data-line-number="4">df &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">row =</span> <span class="dv">1</span><span class="op">:</span>N,</a>
<a class="sourceLine" id="cb246-5" data-line-number="5">             <span class="dt">subj =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>N_subj), <span class="dt">each =</span> N_obs_subj))</a>
<a class="sourceLine" id="cb246-6" data-line-number="6">df</a></code></pre></div>
<pre><code>## # A tibble: 1,000 × 2
##     row  subj
##   &lt;int&gt; &lt;int&gt;
## 1     1     1
## 2     2     1
## 3     3     1
## # … with 997 more rows</code></pre>
<div class="sourceCode" id="cb248"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb248-1" data-line-number="1"><span class="co"># Example:</span></a>
<a class="sourceLine" id="cb248-2" data-line-number="2"><span class="kw">c</span>(df<span class="op">$</span>subj[<span class="dv">1</span>], df<span class="op">$</span>subj[<span class="dv">2</span>], df<span class="op">$</span>subj[<span class="dv">51</span>])</a></code></pre></div>
<pre><code>## [1] 1 1 2</code></pre>
<p>If the data <span class="math inline">\(y_n\)</span> are exchangeable, the parameters <span class="math inline">\(\mu_i\)</span> are also exchangeable. The fact that the <span class="math inline">\(\mu_i\)</span> are exchangeable can be shown <span class="citation">(Bernardo and Smith <a href="#ref-bernardosmith">2009</a>)</span> to be mathematically equivalent to assuming that they come from a common distribution, for example:</p>
<p><span class="math display">\[\begin{equation}
\mu_i \sim \mathit{Normal}(\mu,\tau)
\end{equation}\]</span></p>
<p>To make this more concrete, assume some completely arbitrary true values for the parameters, and generate observations based on a hierarchical process in R.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb250-1" data-line-number="1">mu &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb250-2" data-line-number="2">tau &lt;-<span class="st"> </span><span class="dv">15</span></a>
<a class="sourceLine" id="cb250-3" data-line-number="3">sigma &lt;-<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb250-4" data-line-number="4">mu_i &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_subj, mu, tau)</a>
<a class="sourceLine" id="cb250-5" data-line-number="5">df_h &lt;-<span class="st"> </span><span class="kw">mutate</span>(df, <span class="dt">y =</span> <span class="kw">rnorm</span>(N, mu_i[subj], sigma))</a>
<a class="sourceLine" id="cb250-6" data-line-number="6">df_h </a></code></pre></div>
<pre><code>## # A tibble: 1,000 × 3
##     row  subj     y
##   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
## 1     1     1 102. 
## 2     2     1  96.6
## 3     3     1 107. 
## # … with 997 more rows</code></pre>
<p>The parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>, called hyperparameters, are unknown and have prior distributions (hyperpriors) defined for them. This fact leads to a hierarchical relationship between the parameters: there is a common parameter <span class="math inline">\(\mu\)</span> for all the levels of a group, and the parameters <span class="math inline">\(\mu_i\)</span> are assumed to be generated as a function of this common parameter <span class="math inline">\(\mu\)</span>. Here, <span class="math inline">\(\tau\)</span> represents between-group variability, and <span class="math inline">\(\sigma\)</span> represents within-group variability. The three parameters have priors defined for them. The first two priors below are called hyperpriors.</p>
<ul>
<li><span class="math inline">\(p(\mu)\)</span></li>
<li><span class="math inline">\(p(\tau)\)</span></li>
<li><span class="math inline">\(p(\sigma)\)</span></li>
</ul>
<p>In such a model, information about <span class="math inline">\(\mu_i\)</span> comes from two sources:</p>
<ol style="list-style-type: lower-alpha">
<li><p>from each of the observed <span class="math inline">\(y_n\)</span> corresponding to the respective <span class="math inline">\(\mu_{subj[n]}\)</span> parameter, and</p></li>
<li><p>from the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> that led to all the other <span class="math inline">\(y_k\)</span> (where <span class="math inline">\(k\neq n\)</span>) being generated.</p></li>
</ol>
<p>This is illustrated in Figure <a href="ch-hierarchical.html#fig:dags1">5.1</a>.</p>
<p>Fit this model in <code>brms</code> in the following way. <code>Intercept</code> corresponds to <span class="math inline">\(\mu\)</span>, <code>sigma</code> to <span class="math inline">\(\sigma\)</span>, and <code>sd</code> to <span class="math inline">\(\tau\)</span>. For now the prior distributions are arbitrary.</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" data-line-number="1">fit_h &lt;-<span class="st"> </span><span class="kw">brm</span>(y <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>subj), df_h,</a>
<a class="sourceLine" id="cb252-2" data-line-number="2">             <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb252-3" data-line-number="3">               <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">50</span>, <span class="dv">200</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb252-4" data-line-number="4">                 <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb252-5" data-line-number="5">                 <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">10</span>, <span class="dv">20</span>), <span class="dt">class =</span> sd)),</a>
<a class="sourceLine" id="cb252-6" data-line-number="6">             <span class="co"># increase iterations to avoid convergence issues</span></a>
<a class="sourceLine" id="cb252-7" data-line-number="7">             <span class="dt">iter =</span> <span class="dv">4000</span>,</a>
<a class="sourceLine" id="cb252-8" data-line-number="8">             <span class="dt">warmup =</span> <span class="dv">1000</span>)</a></code></pre></div>
<div class="sourceCode" id="cb253"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb253-1" data-line-number="1">fit_h</a></code></pre></div>
<pre><code>## ...
## Group-Level Effects: 
## ~subj (Number of levels: 20) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)    19.55      3.20    14.27    27.13 1.01      650
##               Tail_ESS
## sd(Intercept)     1293
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    99.01      4.50    89.66   108.25 1.01      481      597
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     3.94      0.09     3.77     4.12 1.00     1873     2713
## 
## ...</code></pre>
<p>In this output, <code>Intercept</code> corresponds to the posterior of <span class="math inline">\(\mu\)</span>, <code>sigma</code> to <span class="math inline">\(\sigma\)</span>, and <code>sd(Intercept)</code> to <span class="math inline">\(\tau\)</span>. There is more information in the <code>brms</code> object, we can also get the posteriors for each level of our group. However, rather than estimating <span class="math inline">\(\mu_i\)</span>, <code>brms</code> estimates the adjustments to <span class="math inline">\(\mu\)</span>, <span class="math inline">\(u_i\)</span>, named <code>r_subj[i,Intercept]</code>, so that <span class="math inline">\(\mu_i = \mu + u_i\)</span>. See the code below.</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb255-1" data-line-number="1"><span class="co"># Extract the posterior estimates of u_i</span></a>
<a class="sourceLine" id="cb255-2" data-line-number="2">u_i_post &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_h) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb255-3" data-line-number="3"><span class="st">  </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">&quot;r_subj&quot;</span>))</a>
<a class="sourceLine" id="cb255-4" data-line-number="4"><span class="co"># Extract the posterior estimate of mu</span></a>
<a class="sourceLine" id="cb255-5" data-line-number="5">mu_post &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_h)<span class="op">$</span>b_Intercept </a>
<a class="sourceLine" id="cb255-6" data-line-number="6"><span class="co"># Build the posterior estimate of mu_i</span></a>
<a class="sourceLine" id="cb255-7" data-line-number="7">mu_i_post &lt;-<span class="st"> </span>mu_post <span class="op">+</span><span class="st"> </span>u_i_post</a>
<a class="sourceLine" id="cb255-8" data-line-number="8"><span class="kw">colMeans</span>(mu_i_post) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unname</span>()</a></code></pre></div>
<pre><code>##  [1] 102.5 124.7  79.9 101.7  88.6 104.3 121.9 103.5  88.2  57.2  86.3
## [12] 131.6 118.0 100.5  73.3  94.4  84.5 101.4  97.1 122.3</code></pre>
<div class="sourceCode" id="cb257"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb257-1" data-line-number="1"><span class="co"># Compare with true values</span></a>
<a class="sourceLine" id="cb257-2" data-line-number="2">mu_i</a></code></pre></div>
<pre><code>##  [1] 102.5 125.3  79.8 101.9  88.0 104.5 122.2 103.9  88.0  55.9  86.7
## [12] 130.2 117.7 100.2  72.8  93.9  84.7 100.8  97.1 122.0</code></pre>

<div class="figure"><span style="display:block;" id="fig:dags1"></span>
<img src="bookdown_files/figure-html/dags1-1.png" alt="A directed acyclic graph illustrating a hierarchical model (partial pooling)." width="100%" />
<p class="caption">
FIGURE 5.1: A directed acyclic graph illustrating a hierarchical model (partial pooling).
</p>
</div>
<p>There are two other configurations possible that do not involve this hierarchical structure and which represent two alternative, extreme scenarios.</p>
<p>One of these two configurations is called the <em>complete pooling model</em>, Here, the data <span class="math inline">\(y_n\)</span> are assumed to be generated from a single distribution:</p>
<p><span class="math display">\[\begin{equation}
y_n \sim \mathit{Normal}(\mu,\sigma).
\end{equation}\]</span></p>
<p>This model is an intercept only regression similar to what we saw in chapter <a href="ch-compbda.html#ch-compbda">3</a>.</p>
<p>Generate fake observations in a vector <code>y</code> based on arbitrary true values in R in the following way.</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb259-1" data-line-number="1">sigma &lt;-<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb259-2" data-line-number="2">mu &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb259-3" data-line-number="3">df_cp &lt;-<span class="st"> </span><span class="kw">mutate</span>(df, <span class="dt">y =</span> <span class="kw">rnorm</span>(N, mu, sigma))</a>
<a class="sourceLine" id="cb259-4" data-line-number="4">df_cp</a></code></pre></div>
<pre><code>## # A tibble: 1,000 × 3
##     row  subj     y
##   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
## 1     1     1  97.4
## 2     2     1  95.3
## 3     3     1  96.2
## # … with 997 more rows</code></pre>
<p>Fit it in <code>brms</code>.</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb261-1" data-line-number="1">fit_cp &lt;-<span class="st"> </span><span class="kw">brm</span>(y <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, df_cp,</a>
<a class="sourceLine" id="cb261-2" data-line-number="2">             <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb261-3" data-line-number="3">               <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">50</span>, <span class="dv">200</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb261-4" data-line-number="4">                 <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="dt">class =</span> sigma)))</a></code></pre></div>
<div class="sourceCode" id="cb262"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb262-1" data-line-number="1">fit_cp</a></code></pre></div>
<pre><code>## ...
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   100.07      0.12    99.83   100.32 1.00     3117     2169
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     3.96      0.09     3.79     4.14 1.00     3575     2626
## 
## ...</code></pre>
<p>The configuration of the complete pooling model is illustrated in Figure <a href="ch-hierarchical.html#fig:dags3">5.2</a>.</p>

<div class="figure"><span style="display:block;" id="fig:dags3"></span>
<img src="bookdown_files/figure-html/dags3-1.png" alt="A directed acyclic graph illustrating a complete pooling model." width="100%"  />
<p class="caption">
FIGURE 5.2: A directed acyclic graph illustrating a complete pooling model.
</p>
</div>
<p>The other configuration is called the <em>no pooling model</em>; here, each <span class="math inline">\(y_n\)</span> is assumed to be generated from an independent distribution:</p>
<p><span class="math display">\[\begin{equation}
y_n \sim \mathit{Normal}(\mu_{subj[n]},\sigma)
\end{equation}\]</span></p>
<p>Generate fake observations from the no pooling model in R with arbitrary true values.</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" data-line-number="1">sigma &lt;-<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb264-2" data-line-number="2">mu_i &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">156</span>, <span class="dv">178</span>, <span class="dv">95</span>, <span class="dv">183</span>, <span class="dv">147</span>, <span class="dv">191</span>, <span class="dv">67</span>, <span class="dv">153</span>, <span class="dv">129</span>, <span class="dv">119</span>, <span class="dv">195</span>,</a>
<a class="sourceLine" id="cb264-3" data-line-number="3">          <span class="dv">150</span>, <span class="dv">172</span>, <span class="dv">97</span>, <span class="dv">110</span>, <span class="dv">115</span>, <span class="dv">78</span>, <span class="dv">126</span>, <span class="dv">175</span>, <span class="dv">80</span>)</a>
<a class="sourceLine" id="cb264-4" data-line-number="4">df_np &lt;-<span class="st"> </span><span class="kw">mutate</span>(df, <span class="dt">y =</span> <span class="kw">rnorm</span>(N, mu_i[subj], sigma))</a>
<a class="sourceLine" id="cb264-5" data-line-number="5">df_np</a></code></pre></div>
<pre><code>## # A tibble: 1,000 × 3
##     row  subj     y
##   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
## 1     1     1  152.
## 2     2     1  164.
## 3     3     1  153.
## # … with 997 more rows</code></pre>
<p>Fit it in <code>brms</code>. By using the formula <code>0 + factor(subj)</code>, we remove the common intercept and force the model to estimate one intercept for each level of <code>subj</code>. The column <code>subj</code> is converted to a factor so that <code>brms</code> does not interpret it as a number.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" data-line-number="1">fit_np &lt;-<span class="st"> </span><span class="kw">brm</span>(y <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span><span class="kw">factor</span>(subj), df_np,</a>
<a class="sourceLine" id="cb266-2" data-line-number="2">             <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb266-3" data-line-number="3">               <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">200</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb266-4" data-line-number="4">                 <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="dt">class =</span> sigma)))</a></code></pre></div>
<p>The summary shows now the 20 estimates of <span class="math inline">\(\mu_i\)</span> as <code>b_factorsubj</code> and <span class="math inline">\(\sigma\)</span>. (We ignore <code>lp__</code> and <code>lprior</code>).</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb267-1" data-line-number="1">fit_np <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">posterior_summary</span>()</a></code></pre></div>
<pre><code>##                Estimate Est.Error     Q2.5    Q97.5
## b_factorsubj1    155.88     0.573   154.76   157.01
## b_factorsubj2    178.35     0.558   177.28   179.41
## b_factorsubj3     94.80     0.552    93.73    95.88
## b_factorsubj4    182.87     0.575   181.74   184.01
## b_factorsubj5    146.76     0.569   145.62   147.88
## b_factorsubj6    190.62     0.571   189.51   191.73
## b_factorsubj7     67.10     0.557    66.02    68.17
## b_factorsubj8    152.74     0.566   151.64   153.84
## b_factorsubj9    129.96     0.577   128.82   131.09
## b_factorsubj10   118.53     0.574   117.43   119.64
## b_factorsubj11   195.27     0.581   194.13   196.41
## b_factorsubj12   149.42     0.580   148.31   150.55
## b_factorsubj13   171.45     0.560   170.34   172.54
## b_factorsubj14    96.47     0.571    95.36    97.59
## b_factorsubj15   110.23     0.569   109.10   111.35
## b_factorsubj16   114.90     0.562   113.79   115.98
## b_factorsubj17    77.30     0.556    76.23    78.39
## b_factorsubj18   126.70     0.574   125.58   127.81
## b_factorsubj19   175.21     0.567   174.08   176.34
## b_factorsubj20    80.45     0.577    79.33    81.55
## sigma              4.03     0.095     3.85     4.22
## lprior          -131.52     0.012  -131.54  -131.50
## lp__           -2942.31     3.240 -2949.59 -2936.88</code></pre>
<p>Unlike the hierarchical model, now there is no common distribution that generates the <span class="math inline">\(\mu_i\)</span> parameters. This is illustrated in <a href="ch-hierarchical.html#fig:dags2">5.3</a>.</p>

<div class="figure"><span style="display:block;" id="fig:dags2"></span>
<img src="bookdown_files/figure-html/dags2-1.png" alt="A directed acyclic graph illustrating a no pooling model." width="100%" />
<p class="caption">
FIGURE 5.3: A directed acyclic graph illustrating a no pooling model.
</p>
</div>
<p>The hierarchical model lies between these two extremes and for this reason is sometimes called a <em>partial pooling model</em>. One way that the hierarchical model is often described is that the estimates <span class="math inline">\(\mu_i\)</span> “borrow strength” from the parameter <span class="math inline">\(\mu\)</span> (which represents the grand mean in the above example).</p>
<p>An important practical consequence of partial pooling is the idea of “borrowing strength from the mean”: if we have very sparse data from a particular member of a group (e.g., missing data from a particular subject), the estimate <span class="math inline">\(\mu_i\)</span> of that particular group member <span class="math inline">\(n\)</span> is determined by the parameter <span class="math inline">\(\mu\)</span>. In other words, when the data are sparse for group member <span class="math inline">\(n\)</span>, the posterior estimate <span class="math inline">\(\mu_i\)</span> is determined largely by the prior <span class="math inline">\(p(\mu)\)</span>.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<p>So far we focused on the structure of <span class="math inline">\(\mu\)</span>, the location parameter of the likelihood. We could even have partial pooling, complete pooling or no pooling with respect to <span class="math inline">\(\sigma\)</span>, the scale parameter of the likelihood. More generally, any parameter of a likelihood can have any of these kinds of pooling.</p>
<p>In the coming sections, we will be looking at each of these models with more detail and using realistic examples.</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:exch" class="theorem"><strong>Box 5.1  </strong></span><strong>Finitely exchangeable random variables</strong></p>
</div>
<p>Formally, we say that the random variables <span class="math inline">\(Y_1,\dots,Y_N\)</span> are finitely exchangeable if, for any set of particular outcomes of an experiment <span class="math inline">\(y_1,\dots,y_N\)</span>, the probability <span class="math inline">\(p(y_1,\dots,y_N)\)</span> that we assign to these outcomes is unaffected by permuting the labels given to the variables. In other words, for any permutation <span class="math inline">\(\pi(n)\)</span>, where <span class="math inline">\(n=1,\dots,N\)</span> (
<span class="math inline">\(\pi\)</span> is a function that takes as input the positive integer <span class="math inline">\(n\)</span> and returns another positive integer; e.g., the function takes a subject indexed as 1, and returns index 3), we can reasonably assume that <span class="math inline">\(p(y_1,\dots,y_N)=p(y_{\pi(1)},\dots,y_{\pi(N)})\)</span>. A simple example is a coin tossed twice. Suppose the first coin toss is <span class="math inline">\(Y_1=1\)</span>, a heads, and the second coin toss is <span class="math inline">\(Y_2=0\)</span>, a tails. If we are willing to assume that the probability of getting one heads is unaffected by whether it appears in the first or the second toss, i.e., <span class="math inline">\(p(Y_1=1,Y_2=0)=p(Y_1=0,Y_2=1)\)</span>, then we assume that the indices are exchangeable.</p>
<p>Some important connections and differences between exchangeability and the frequentist concept of independent and identically distributed (iid):</p>
<ul>
<li><p><strong>If the data are exchangeable, they are not necessarily iid</strong>. For example, suppose you have a box with one black ball and two red balls in it. Your task is to repeatedly draw a ball at random. Suppose that in your first draw, you draw one ball and get the black ball. The probability of getting a black ball in the next two draws is now <span class="math inline">\(0\)</span>. However, if in your first draw you had retrieved a red ball, then there is a non-zero probability of drawing a black ball in the next two draws. The outcome in the first draw affects the probability of subsequent draws–they are not independent. But the sequence of random variables is exchangeable. To see this, consider the following: If a red ball is drawn, count it as a <span class="math inline">\(0\)</span>, and if a black ball is drawn, then count it as <span class="math inline">\(1\)</span>. Then, the three possible outcomes and the probabilities are</p>
<ul>
<li><span class="math inline">\(1,0,0\)</span>; <span class="math inline">\(P(X_1=1,X_2=0,X_3=0) = \frac{1}{3} \times 1 \times 1=\frac{1}{3}\)</span></li>
<li><span class="math inline">\(0,1,0\)</span> <span class="math inline">\(P(X_1=0,X_2=1,X_3=0) = \frac{2}{3} \times \frac{1}{2} \times 1=\frac{1}{3}\)</span></li>
<li><span class="math inline">\(0,0,1\)</span> <span class="math inline">\(P(X_1=0,X_2=0,X_3=1) = \frac{2}{3} \times \frac{1}{2} \times 1=\frac{1}{3}\)</span></li>
</ul>
<p>The random variables <span class="math inline">\(X_1,X_2,X_3\)</span> can be permuted and the joint probability distribution (technically, the PMF) is the same in each case.</p></li>
<li><p><strong>If the data are exchangeable, then they are identically distributed</strong>. For example, in the box containing one black ball and two red balls, suppose we count the draw of a black ball as a <span class="math inline">\(1\)</span>, and the draw of a red ball as a <span class="math inline">\(0\)</span>. Then the probability <span class="math inline">\(P(X_1=1)=\frac{1}{3}\)</span> and <span class="math inline">\(P(X_1=0)=\frac{2}{3}\)</span>; this is also true for <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span>. That is, these random variables are identically distributed.</p></li>
<li><p><strong>If the data are iid in the standard frequentist sense, then they are exchangeable</strong>. For example, suppose you have <span class="math inline">\(i=1,\dots,n\)</span> instances of a random variable <span class="math inline">\(X\)</span> whose PDF is <span class="math inline">\(f(x)\)</span>. Suppose also that <span class="math inline">\(X_i\)</span> are iid. The joint PDF (this can be discrete or continuous, i.e., a PMF or PDF) is</p>
<p><span class="math display">\[\begin{equation}
 f_{X_1,\dots,X_n}(x_1,\dots,x_n) = f(x_1) \cdot \dots \cdot f(x_n)
 \end{equation}\]</span></p>
<p>Because the terms on the right-hand side can be permuted, the labels can be permuted on any of the <span class="math inline">\(x_i\)</span>. This means that <span class="math inline">\(X_1,\dots,X_n\)</span> are exchangeable.</p></li>
</ul>
</div>
</div>
<div id="sec-N400hierarchical" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.2</span> A hierarchical model with a normal likelihood: The N400 effect<a href="ch-hierarchical.html#sec-N400hierarchical" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Event-related potentials (ERPs) allow scientists to observe electrophysiological responses in the brain measured by means of electroencephalography (EEG) that are time-locked to a specific event (i.e., the presentation of the stimuli). A very robust ERP effect in the study of language is the N400. Words with low predictability are accompanied by an <em>N400 effect</em> in comparison with high-predictable words, this is a relative negativity that peaks around 300-500 after word onset over central parietal scalp sites <span class="citation">(first reported in Kutas and Hillyard <a href="#ref-kutasReadingSenselessSentences1980">1980</a>, for semantic anomalies, and in <a href="#ref-kutasBrainPotentialsReading1984">1984</a> for low predictable word; for a review, see Kutas and Federmeier <a href="#ref-kutasThirtyYearsCounting2011">2011</a>)</span>. The N400 is illustrated in Figure <a href="ch-hierarchical.html#fig:N400noun">5.4</a>.</p>

<div class="figure"><span style="display:block;" id="fig:N400noun"></span>
<img src="bookdown_files/figure-html/N400noun-1.svg" alt="Typical ERP for the grand average across the N400 spatial window (central parietal electrodes: Cz, CP1, CP2, P3, Pz, P4, POz) for high and low predictability nouns (specifically from the constraining context of the experiment reported in Nicenboim, Vasishth, and Rösler 2020a). The x-axis indicates time in seconds and the y-axis indicates voltage in microvolts (unlike many EEG/ERP plots, the negative polarity is plotted downwards)." width="672" />
<p class="caption">
FIGURE 5.4: Typical ERP for the grand average across the N400 spatial window (central parietal electrodes: Cz, CP1, CP2, P3, Pz, P4, POz) for high and low predictability nouns <span class="citation">(specifically from the constraining context of the experiment reported in Nicenboim, Vasishth, and Rösler <a href="#ref-nicenboim_vasishth_rosler_2020">2020</a><a href="#ref-nicenboim_vasishth_rosler_2020">a</a>)</span>. The x-axis indicates time in seconds and the y-axis indicates voltage in microvolts (unlike many EEG/ERP plots, the negative polarity is plotted downwards).
</p>
</div>
<p>For example, in (1) below, the continuation <em>‘paint’</em> has higher predictability than the continuation <em>‘dog’</em>, and thus we would expect a more negative signal, that is, an N400 effect, in <em>‘dog’</em> in (b) in comparison with <em>‘paint’</em> in (a). It is often the case that predictability is measured with a cloze task (see section <a href="ch-intro.html#sec-binomialcloze">1.4</a>).</p>
<ol style="list-style-type: decimal">
<li>Example from <span class="citation">Kutas and Hillyard (<a href="#ref-kutasBrainPotentialsReading1984">1984</a>)</span>
<ol style="list-style-type: lower-alpha">
<li>Don’t touch the wet paint.</li>
<li>Don’t touch the wet dog.</li>
</ol></li>
</ol>
<p>The EEG data are typically recorded in tens of electrodes every couple of milliseconds, but for our purposes (i.e., for learning about Bayesian hierarchical models), we can safely ignore the complexity of the data. A common way to simplify the high-dimensional EEG data when we are dealing with the N400 is to focus on the average amplitude of the EEG signal at the typical spatio-temporal window of the N400 <span class="citation">(for example, see Frank et al. <a href="#ref-frankERPResponseAmount2015">2015</a>)</span>.</p>
<p>For this example, we are going to focus on the N400 effect for critical nouns from a subset of the data of <span class="citation">Nieuwland et al. (<a href="#ref-nieuwlandLargescaleReplicationStudy2018">2018</a>)</span>. <span class="citation">Nieuwland et al. (<a href="#ref-nieuwlandLargescaleReplicationStudy2018">2018</a>)</span> presented a replication attempt of an original experiment of <span class="citation">DeLong, Urbach, and Kutas (<a href="#ref-delongProbabilisticWordPreactivation2005">2005</a>)</span> with sentences like (2).</p>
<ol start="2" style="list-style-type: decimal">
<li>Example from <span class="citation">DeLong, Urbach, and Kutas (<a href="#ref-delongProbabilisticWordPreactivation2005">2005</a>)</span>
<ol style="list-style-type: lower-alpha">
<li>The day was breezy so the boy went outside to fly a kite.</li>
<li>The day was breezy so the boy went outside to fly an airplane.</li>
</ol></li>
</ol>
<p>We’ll ignore the goal of original experiment <span class="citation">(DeLong, Urbach, and Kutas <a href="#ref-delongProbabilisticWordPreactivation2005">2005</a>)</span>, and its replication <span class="citation">(Nieuwland et al. <a href="#ref-nieuwlandLargescaleReplicationStudy2018">2018</a>)</span>. We are going to focus on the N400 at the final nouns in the experimental stimuli. In example (2), for example, the final noun <em>‘kite’</em> has higher predictability than <em>‘airplane’</em>, and thus we would expect a more negative signal in <em>‘airplane’</em> in (b) in comparison with <em>‘kite’</em> in (a).</p>
<p>To speed up computation, we restrict the data set to the subjects from the Edinburgh lab, using the relevant subset containing the Edinburgh data fro <code>df_eeg</code> in the <code>bcogsci</code> package. Center the cloze probability before using it as a predictor.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb269-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_eeg&quot;</span>)</a>
<a class="sourceLine" id="cb269-2" data-line-number="2">(df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb269-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cloze =</span> cloze <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cloze)))</a></code></pre></div>
<pre><code>## # A tibble: 2,863 × 7
##    subj cloze  item  n400 cloze_ans     N c_cloze
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1     1  0        1  7.08         0    44  -0.476
## 2     1  0.03     2 -0.68         1    44  -0.446
## 3     1  1        3  1.39        44    44   0.524
## # … with 2,860 more rows</code></pre>
<div class="sourceCode" id="cb271"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb271-1" data-line-number="1"><span class="co"># Number of subjects</span></a>
<a class="sourceLine" id="cb271-2" data-line-number="2">df_eeg <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb271-3" data-line-number="3"><span class="st">  </span><span class="kw">distinct</span>(subj) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb271-4" data-line-number="4"><span class="st">  </span><span class="kw">count</span>()</a></code></pre></div>
<pre><code>## # A tibble: 1 × 1
##       n
##   &lt;int&gt;
## 1    37</code></pre>
<p>One convenient aspect of using averages of EEG data is that they are roughly normally distributed. This allows us to use the normal likelihood. Figure <a href="ch-hierarchical.html#fig:histn400">5.5</a> shows the distribution of the data.</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb273-1" data-line-number="1"></a>
<a class="sourceLine" id="cb273-2" data-line-number="2">df_eeg <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(n400)) <span class="op">+</span></a>
<a class="sourceLine" id="cb273-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_histogram</span>(</a>
<a class="sourceLine" id="cb273-4" data-line-number="4">    <span class="dt">binwidth =</span> <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb273-5" data-line-number="5">    <span class="dt">colour =</span> <span class="st">&quot;gray&quot;</span>,</a>
<a class="sourceLine" id="cb273-6" data-line-number="6">    <span class="dt">alpha =</span> <span class="fl">.5</span>,</a>
<a class="sourceLine" id="cb273-7" data-line-number="7">    <span class="kw">aes</span>(<span class="dt">y =</span> ..density..)</a>
<a class="sourceLine" id="cb273-8" data-line-number="8">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb273-9" data-line-number="9"><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb273-10" data-line-number="10">    <span class="dt">mean =</span> <span class="kw">mean</span>(df_eeg<span class="op">$</span>n400),</a>
<a class="sourceLine" id="cb273-11" data-line-number="11">    <span class="dt">sd =</span> <span class="kw">sd</span>(df_eeg<span class="op">$</span>n400)</a>
<a class="sourceLine" id="cb273-12" data-line-number="12">  )) <span class="op">+</span></a>
<a class="sourceLine" id="cb273-13" data-line-number="13"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Average voltage in microvolts for </span></a>
<a class="sourceLine" id="cb273-14" data-line-number="14"><span class="st">       the N400 spatiotemporal window&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:histn400"></span>
<img src="bookdown_files/figure-html/histn400-1.svg" alt="Histogram of the N400 averages for every trial, overlaid is a density plot of a normal distribution." width="672" />
<p class="caption">
FIGURE 5.5: Histogram of the N400 averages for every trial, overlaid is a density plot of a normal distribution.
</p>
</div>
<div id="sec-Mcp" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.1</span> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)<a href="ch-hierarchical.html#sec-Mcp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ll start from the simplest model which is basically the linear regression we encountered in the preceding chapter.</p>
<div id="model-assumptions" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.2.1.1</span> Model assumptions<a href="ch-hierarchical.html#model-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This model, call it <span class="math inline">\(M_{cp}\)</span>, makes the following assumptions.</p>
<ol style="list-style-type: decimal">
<li>The EEG averages for the N400 spatiotemporal window are normally distributed.</li>
<li>Observations are independent.</li>
<li>There is a linear relationship between cloze and the EEG signal for the trial.</li>
</ol>
<p><strong>This model is incorrect for these data due to assumption (2) being violated.</strong></p>
<p>With the last assumption, we are saying that the difference in the average signal when we compare nouns with cloze probability of 0 and 0.1 is the same as the difference in the signal when we compare nouns with cloze values of 0.1 and 0.2 (or 0.9 and 1). This is just an assumption, and it may not necessarily be the case in the actual data. This means that we are going to get a posterior for <span class="math inline">\(\beta\)</span> conditional on the assumption that the linear relationship holds. Even if it approximately holds, we still don’t know how much we deviate from this assumption.</p>
<!-- BN: we never really come back to that, I think -->
<!-- We'll come back to this issue in chapters \@ref(ch-bf)-\@ref(ch-cv) when we deal with model comparison. -->
<p>We can now decide on a likelihood and priors.</p>
</div>
<div id="likelihood-and-priors-1" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.2.1.2</span> Likelihood and priors<a href="ch-hierarchical.html#likelihood-and-priors-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A normal likelihood seems reasonable for these data:</p>
<p><span class="math display" id="eq:Mcp">\[\begin{equation}
   signal_n \sim \mathit{Normal}( \alpha + c\_cloze_n \cdot \beta,\sigma)
  \tag{5.1}
 \end{equation}\]</span></p>
<p>where <span class="math inline">\(n =1, \ldots, N\)</span>, and <span class="math inline">\(signal\)</span> is the dependent variable (average signal in the N400 spatiotemporal window in microvolts). The variable <span class="math inline">\(N\)</span> represents the total number of data points.</p>
<p>As always we need to rely on our previous knowledge and domain expertise to decide on priors. We know that ERPs (signals time-locked to a stimulus) have mean amplitudes of a couple of microvolts: This is easy to see in any plot of the EEG literature. This means that we don’t expect the effect of our manipulation to exceed, say, <span class="math inline">\(10 \mu V\)</span>. As before, a priori we’ll assume that effects can be negative or positive. We can quantify our prior knowledge regarding plausible values of <span class="math inline">\(\beta\)</span> as normally distributed centered at zero with a standard deviation of <span class="math inline">\(10 \mu V\)</span>. (Other values such as <span class="math inline">\(5 \mu V\)</span> would have been also reasonable, since it would entail that 95% of the prior mass probability is between <span class="math inline">\(-10\)</span>$ and <span class="math inline">\(10 \mu V\)</span>.)</p>
<p>If the signal for each ERP is <em>baselined</em>, that is, the mean signal of a time window before the time window of interest is subtracted from the time window of interest, then the mean signal would be relatively close to <span class="math inline">\(0\)</span>. Since we know that the ERPs were baselined in this study, we expect that the grand mean of our signal should be relatively close to zero. Our prior for <span class="math inline">\(\alpha\)</span> is then normally distributed centered in zero with a standard deviation of <span class="math inline">\(10 \mu V\)</span> as well.</p>
<p>The standard deviation of our signal distribution is harder to guess. We know that EEG signals are quite noisy, and that the standard deviation must be higher than zero. Our prior for <span class="math inline">\(\sigma\)</span> is a truncated normal distribution with location zero and scale 50. Recall that since we truncate the distribution, the parameters location and scale do not correspond to the mean and standard deviation of the new distribution; see Box <a href="ch-reg.html#thm:truncation">4.1</a>.</p>
<p>We can draw random samples from this truncated distribution and calculate their mean and standard deviation:</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb274-1" data-line-number="1">samples &lt;-<span class="st"> </span><span class="kw">rtnorm</span>(<span class="dv">20000</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">50</span>, <span class="dt">a =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb274-2" data-line-number="2"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(samples), <span class="dt">sd =</span> <span class="kw">sd</span>(samples))</a></code></pre></div>
<pre><code>## mean   sd 
## 40.1 30.5</code></pre>
<p>So we are essentially saying that we assume a priori that we will find the true standard deviation of the signal in the following interval with 95% probability:</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb276-1" data-line-number="1"><span class="kw">quantile</span>(samples, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">.975</span>))</a></code></pre></div>
<pre><code>##   2.5%  97.5% 
##   1.52 113.81</code></pre>
<div class="sourceCode" id="cb278"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb278-1" data-line-number="1"><span class="co"># Analytically:</span></a>
<a class="sourceLine" id="cb278-2" data-line-number="2"><span class="co"># c(qtnorm(.025, 0, 50, a = 0), qtnorm(.975, 0, 50, a = 0))</span></a></code></pre></div>
<p>To sum up, we are going to use the following priors:</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp;\sim \mathit{Normal}(0,10)\\
 \beta  &amp;\sim \mathit{Normal}(0,10)\\
 \sigma  &amp;\sim \mathit{Normal}_{+}(0,50)
 \end{aligned}
 \end{equation}\]</span></p>
<p>A model such as <span class="math inline">\(M_{cp}\)</span> is sometimes called a <em>fixed-effects</em> model: all the parameters are fixed in the sense that do not vary from subject to subject or from item to item. A similar frequentist model would correspond to fitting a simple linear model using the <code>lm</code> function: <code>lm(n400 ~ 1 + cloze, data = df_eeg)</code>.</p>
<p>We fit this model in <code>brms</code> as follows (the default family is <code>gaussian()</code> so we can omit it). As with the <code>lm</code> function in R, by default an intercept is fitted and thus <code>n400 ~ c_cloze</code> is equivalent to <code>n400 ~ 1 + c_cloze</code>:</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb279-1" data-line-number="1">fit_N400_cp &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze,</a>
<a class="sourceLine" id="cb279-2" data-line-number="2">  <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb279-3" data-line-number="3">    <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb279-4" data-line-number="4">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb279-5" data-line-number="5">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_cloze),</a>
<a class="sourceLine" id="cb279-6" data-line-number="6">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma)</a>
<a class="sourceLine" id="cb279-7" data-line-number="7">    ),</a>
<a class="sourceLine" id="cb279-8" data-line-number="8">  <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb279-9" data-line-number="9">)</a></code></pre></div>
<p>For now, check the summary, and plot the posteriors of the model (Figure <a href="ch-hierarchical.html#fig:figurefitN400cp">5.6</a>).</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb280-1" data-line-number="1">fit_N400_cp</a></code></pre></div>
<pre><code>## ...
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     3.67      0.22     3.26     4.09 1.00     4085     3013
## c_cloze       2.28      0.54     1.20     3.31 1.00     4110     2992
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    11.82      0.16    11.52    12.15 1.00     4200     2630
## 
## ...</code></pre>

<div class="sourceCode" id="cb282"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb282-1" data-line-number="1"><span class="kw">plot</span>(fit_N400_cp)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:figurefitN400cp"></span>
<img src="bookdown_files/figure-html/figurefitN400cp-1.svg" alt="Posterior distributions of the complete pooling model, fit_N400_cp." width="672" />
<p class="caption">
FIGURE 5.6: Posterior distributions of the complete pooling model, <code>fit_N400_cp</code>.
</p>
</div>
</div>
</div>
<div id="no-pooling-model-m_np" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.2</span> No pooling model (<span class="math inline">\(M_{np}\)</span>)<a href="ch-hierarchical.html#no-pooling-model-m_np" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the assumptions of the previous model is clearly wrong: observations are not independent, they are clustered by subject (and also by the specific item, but we’ll ignore this until section <a href="ch-hierarchical.html#sec-mcvivs">5.2.4</a>). It is reasonable to assume that EEG signals are more similar within subjects than between them. The following model assumes that each subject is completely independent from each other.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></p>
<div id="model-assumptions-1" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.2.2.1</span> Model assumptions<a href="ch-hierarchical.html#model-assumptions-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li>EEG averages for the N400 spatio-temporal window are normally distributed.</li>
<li>Every subject’s model is fit independently of the other subjects; the subjects have no parameters in common (an exception is the standard deviation; this is the same for all subjects).</li>
<li>There is a linear relationship between cloze and the EEG signal for the trial.</li>
</ol>
<p>What likelihood and priors can we choose here?</p>
</div>
<div id="likelihood-and-priors-2" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.2.2.2</span> Likelihood and priors<a href="ch-hierarchical.html#likelihood-and-priors-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The likelihood is a normal distribution as before:</p>
<p><span class="math display">\[\begin{equation}
 signal_n \sim \mathit{Normal}( \alpha_{subj[n]} + c\_cloze_n \cdot \beta_{subj[n]},\sigma)
 \end{equation}\]</span></p>
<!-- This model is actually fitting a separate linear model for each subject, with a single standard  deviation $\sigma$ across all subjects.
-->
<p>As before, <span class="math inline">\(n\)</span> represents each observation, that is, the <span class="math inline">\(n\)</span>th row in the data frame, which has <span class="math inline">\(N\)</span> rows, and now the index <span class="math inline">\(i\)</span> identifies the subject. The notation <span class="math inline">\(subj[n]\)</span>, which roughly follows <span class="citation">Gelman and Hill (<a href="#ref-GelmanHill2007">2007</a>)</span>, identifies the subject index; for example, if <span class="math inline">\(subj[10]=3\)</span>, then the <span class="math inline">\(10\)</span>th row of the data frame is from subject <span class="math inline">\(3\)</span>.</p>
<p>We define the priors as follows:</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha_{i} &amp;\sim \mathit{Normal}(0,10)\\
 \beta_{i}  &amp;\sim \mathit{Normal}(0,10)\\
 \sigma  &amp;\sim \mathit{Normal}_+(0,50)
 \end{aligned}
 \end{equation}\]</span></p>
<p>In <code>brms</code>, such a model can be fit by removing the common intercept with the formula <code>n400 ~ 0 + factor(subj) + c_cloze:factor(subj)</code>.</p>
<p>This formula forces the model to estimate one intercept and one slope for <em>each</em> level of <code>subj</code>.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>
The by-subject intercepts are indicated with <code>factor(subj)</code> and the by-subject slopes with <code>c_cloze:factor(subj)</code>. It’s very important to specify that <code>subject</code> should be treated as a factor and not as a number; we don’t assume that subject number 3 will show 3 times more positive (or negative) average signal than subject number 1! The model fits 37 independent intercepts and 37 independent slopes. By setting a prior to <code>class = b</code> and omitting <code>coef</code>, we are essentially setting identical priors to all the intercepts and slopes of the model. The parameters are independent from each other; it is only our previous knowledge (or prior beliefs) about their possible values (encoded in the priors) that is identical. We can set different priors to each intercept and slope, but that will mean setting 74 priors!</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb283-1" data-line-number="1">fit_N400_np &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span><span class="kw">factor</span>(subj) <span class="op">+</span><span class="st"> </span>c_cloze<span class="op">:</span><span class="kw">factor</span>(subj),</a>
<a class="sourceLine" id="cb283-2" data-line-number="2">  <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb283-3" data-line-number="3">    <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb283-4" data-line-number="4">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb283-5" data-line-number="5">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma)</a>
<a class="sourceLine" id="cb283-6" data-line-number="6">    ),</a>
<a class="sourceLine" id="cb283-7" data-line-number="7">  <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb283-8" data-line-number="8">)</a></code></pre></div>
<p>For this model, printing a summary means printing the 75 parameters (<span class="math inline">\(\alpha_{1,...,37}\)</span>, <span class="math inline">\(\beta_{1,...,37}\)</span>, and <span class="math inline">\(\sigma\)</span>). We could do this as always by printing out the model results: just type <code>fit_N400_np</code>.</p>
<p>It may be easier to understand the output of the model by plotting <span class="math inline">\(\beta_{1,..,37}\)</span> using <code>bayesplot</code>. (<code>brms</code> also includes a wrapper for this function called <code>stanplot</code>). We can take a look at the internal names that <code>brms</code> gives to the parameters with <code>variables(fit_N400_np)</code>; they are <code>b_factorsubj</code>, then the subject index and then <code>:c_cloze</code>. The code below changes the subject labels back to their original numerical indices and plots them in Figure <a href="ch-hierarchical.html#fig:nopooling">5.7</a>. The subjects are ordered by the magnitude of their mean effects.</p>
<p>The model <span class="math inline">\(M_{np}\)</span> does not estimate a unique population-level effect; instead, there is a different effect estimated for each subject. However, given the posterior means from each subject, it is still possible to calculate the average of these estimates <span class="math inline">\(\hat\beta_{1,...,I}\)</span>, where <span class="math inline">\(I\)</span> is the total number of subjects:</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb284-1" data-line-number="1"><span class="co"># parameter name of beta by subject:</span></a>
<a class="sourceLine" id="cb284-2" data-line-number="2">ind_effects_np &lt;-<span class="st"> </span><span class="kw">paste0</span>(</a>
<a class="sourceLine" id="cb284-3" data-line-number="3">  <span class="st">&quot;b_factorsubj&quot;</span>,</a>
<a class="sourceLine" id="cb284-4" data-line-number="4">  <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj), <span class="st">&quot;:c_cloze&quot;</span></a>
<a class="sourceLine" id="cb284-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb284-6" data-line-number="6">beta_across_subj &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_N400_np) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb284-7" data-line-number="7"><span class="st">  </span><span class="co">#removes the meta data from the object</span></a>
<a class="sourceLine" id="cb284-8" data-line-number="8"><span class="st">  </span><span class="kw">select</span>(<span class="kw">all_of</span>(ind_effects_np)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb284-9" data-line-number="9"><span class="st">    </span><span class="kw">rowMeans</span>()</a>
<a class="sourceLine" id="cb284-10" data-line-number="10"></a>
<a class="sourceLine" id="cb284-11" data-line-number="11"><span class="co"># Calculate the average of these estimates</span></a>
<a class="sourceLine" id="cb284-12" data-line-number="12">(grand_av_beta &lt;-<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb284-13" data-line-number="13">  <span class="dt">mean =</span> <span class="kw">mean</span>(beta_across_subj),</a>
<a class="sourceLine" id="cb284-14" data-line-number="14">  <span class="dt">lq =</span> <span class="kw">quantile</span>(beta_across_subj, <span class="kw">c</span>(.<span class="dv">025</span>)),</a>
<a class="sourceLine" id="cb284-15" data-line-number="15">  <span class="dt">hq =</span> <span class="kw">quantile</span>(beta_across_subj, <span class="kw">c</span>(.<span class="dv">975</span>))</a>
<a class="sourceLine" id="cb284-16" data-line-number="16">))</a></code></pre></div>
<pre><code>## # A tibble: 1 × 3
##    mean    lq    hq
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  2.15  1.18  3.12</code></pre>
<p>In Figure <a href="ch-hierarchical.html#fig:nopooling">5.7</a>, the 95% credible interval of this overall mean effect is plotted as two vertical lines together with the effect of cloze probability for each subject (ordered by effect size). Here, rather than using a plotting function from <code>brms</code>, we can extract the summary of by-subject effects, reorder them by magnitude, and then plot the summary with a custom plot using <code>ggplot2</code>.</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb286-1" data-line-number="1"><span class="co"># make a table of beta&#39;s by subject</span></a>
<a class="sourceLine" id="cb286-2" data-line-number="2">beta_by_subj &lt;-<span class="st"> </span><span class="kw">posterior_summary</span>(fit_N400_np,</a>
<a class="sourceLine" id="cb286-3" data-line-number="3">  <span class="dt">variable =</span> ind_effects_np</a>
<a class="sourceLine" id="cb286-4" data-line-number="4">) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb286-5" data-line-number="5"><span class="st">  </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb286-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subject =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">n</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb286-7" data-line-number="7"><span class="st">  </span><span class="co">## reorder plot by magnitude of mean:</span></a>
<a class="sourceLine" id="cb286-8" data-line-number="8"><span class="st">  </span><span class="kw">arrange</span>(Estimate) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb286-9" data-line-number="9"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subject =</span> <span class="kw">factor</span>(subject, <span class="dt">levels =</span> subject))</a></code></pre></div>
<p>The code below generates Figure <a href="ch-hierarchical.html#fig:nopooling">5.7</a>.</p>

<div class="sourceCode" id="cb287"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb287-1" data-line-number="1"><span class="kw">ggplot</span>(</a>
<a class="sourceLine" id="cb287-2" data-line-number="2">  beta_by_subj,</a>
<a class="sourceLine" id="cb287-3" data-line-number="3">  <span class="kw">aes</span>(<span class="dt">x =</span> Estimate, <span class="dt">xmin =</span> Q2<span class="fl">.5</span>, <span class="dt">xmax =</span> Q97<span class="fl">.5</span>, <span class="dt">y =</span> subject)</a>
<a class="sourceLine" id="cb287-4" data-line-number="4">) <span class="op">+</span></a>
<a class="sourceLine" id="cb287-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb287-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_errorbarh</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb287-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> grand_av_beta<span class="op">$</span>mean) <span class="op">+</span></a>
<a class="sourceLine" id="cb287-8" data-line-number="8"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> grand_av_beta<span class="op">$</span>lq, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb287-9" data-line-number="9"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> grand_av_beta<span class="op">$</span>hq, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb287-10" data-line-number="10"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;By-subject effect of cloze probability in microvolts&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:nopooling"></span>
<img src="bookdown_files/figure-html/nopooling-1.svg" alt="95% credible intervals of the effect of cloze probability for each subject according to the no pooling model, fit_N400_np. The solid vertical line represents the mean over all the subjects; and the broken vertical lines mark the 95% credible interval for this mean." width="672" />
<p class="caption">
FIGURE 5.7: 95% credible intervals of the effect of cloze probability for each subject according to the no pooling model, <code>fit_N400_np</code>. The solid vertical line represents the mean over all the subjects; and the broken vertical lines mark the 95% credible interval for this mean.
</p>
</div>
</div>
</div>
<div id="sec-uncorrelated" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.3</span> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)<a href="ch-hierarchical.html#sec-uncorrelated" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One major problem with the no-pooling model is that we completely ignore the fact that the subjects were doing the same experiment. We fit each subject’s data ignoring the information available in the other subjects’ data.
The no-pooling model is very likely to <em>overfit</em> the individual subjects’ data; we are likely to ignore the generalities of the data and we may end up overinterpreting noisy estimates from each subject’s data. The model can be modified to explicitly assume that the subjects have an overall effect common to all the subjects, with the individual subjects deviating from this common effect.</p>
<p>In the model that we fit next, we will assume that there is an overall effect that is common to the subjects and, importantly, that all subjects’ parameters originate from one common (normal) distribution. This model specification will result in the estimation of posteriors for each subject being also influenced by what we know about all the subjects together. We begin with a hierarchical model with uncorrelated varying intercepts and slopes.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a></p>
<div id="model-assumptions-2" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.2.3.1</span> Model assumptions<a href="ch-hierarchical.html#model-assumptions-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li>EEG averages for the N400 spatio-temporal window are normally distributed.</li>
<li>Each subject deviates to some extent (this is made precise below) from the grand mean and from the mean effect of predictability. This implies that there is some between-subject variability in the individual-level intercept and slope adjustments by subject.</li>
<li>There is a linear relationship between cloze and the EEG signal.</li>
</ol>
</div>
<div id="likelihood-and-priors-3" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.2.3.2</span> Likelihood and priors<a href="ch-hierarchical.html#likelihood-and-priors-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The likelihood now incorporates the assumption that both the intercept and slope are adjusted by subject.</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + c\_cloze_n \cdot (\beta+ u_{subj[n],2}),\sigma)
 \end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp;\sim \mathit{Normal}(0,10)\\
 \beta  &amp;\sim \mathit{Normal}(0,10)\\
 u_1 &amp;\sim \mathit{Normal}(0,\tau_{u_1})\\
 u_2 &amp;\sim \mathit{Normal}(0,\tau_{u_2})\\
 \tau_{u_1} &amp;\sim \mathit{Normal}_+(0,20) \\
 \tau_{u_2} &amp;\sim \mathit{Normal}_+(0,20) \\
 \sigma  &amp;\sim \mathit{Normal}_+(0,50)
 \end{aligned}
 \end{equation}\]</span></p>
<p>In this model each subject has their own intercept adjustment, <span class="math inline">\(u_{subj,1}\)</span>, and slope adjustment, <span class="math inline">\(u_{subj,2}\)</span>.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> If <span class="math inline">\(u_{subj,1}\)</span> is positive, the subject will have a more positive EEG signal than the grand mean average. If <span class="math inline">\(u_{subj,2}\)</span> is positive, the subject will have a more positive EEG response to a change of one unit in <code>c_cloze</code> than the overall mean effect (i.e., there will be a more positive effect of cloze probability on the N400). The parameters <span class="math inline">\(u\)</span> are sometimes called random effects and thus a model with fixed effects (<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) and random effects is called a mixed model. However, random effects have different meanings in different contexts. To avoid ambiguity, <code>brms</code> calls these random-effects parameters <em>group-level</em> effects. Since we are estimating <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(u\)</span> at the same time and we assume that the average of the <span class="math inline">\(u\)</span>’s is <span class="math inline">\(0\)</span> (since it is assumed to be normally distributed with mean <span class="math inline">\(0\)</span>), what is common between the subjects, the grand mean, is estimated as the intercept <span class="math inline">\(\alpha\)</span>, and the deviations of individual subjects’ means from this grand mean are the adjustments <span class="math inline">\(u_1\)</span>. Similarly, the mean effect of cloze is estimated as <span class="math inline">\(\beta\)</span>, and the deviations of individual subjects’ mean effects of cloze from <span class="math inline">\(\beta\)</span> are the adjustment <span class="math inline">\(u_2\)</span>. The standard deviations of these two adjustment terms, <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span>, respectively, represent between subject variability; see Box <a href="ch-hierarchical.html#thm:hierarchical">5.2</a>.</p>
<p>Thus, the model <span class="math inline">\(M_{v}\)</span> has three <em>standard deviations</em>: <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span>. In statistics, it is conventional to talk about variances (the square of these standard deviations); for this reason, these standard deviations are also (confusingly) called <em>variance components</em>. The variance components <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span> characterize between-subject variability, and the variance component <span class="math inline">\(\sigma\)</span> characterizes within-subject variability.</p>
<p>The by-subject adjustments <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are parameters in the model, and therefore have priors defined on them.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> Parameters that appear in the prior specifications for parameters, such as <span class="math inline">\(\tau_u\)</span>, are often called <em>hyperparameters</em>,<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> and the priors on such hyperparameters are called <em>hyperpriors</em>. Thus, the parameter <span class="math inline">\(u_1\)</span> has <span class="math inline">\(\mathit{Normal}(0,\tau_{u_1})\)</span> as a prior; <span class="math inline">\(\tau_{u_1}\)</span> is a hyperparameter, and the hyperprior on <span class="math inline">\(\tau_{u_1}\)</span> is <span class="math inline">\(\mathit{Normal}(0,20)\)</span>.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a></p>
<p>We know that in general, in EEG experiments, the standard deviations for the by-subject adjustments are smaller than the standard deviation of the observations (which is the within-subjects standard deviation). That is, usually the between-subject variability in the intercepts and slopes is smaller than the within-subjects variability in the data. For this reason, reducing the scale of the truncated normal distribution to <span class="math inline">\(20\)</span> (in comparison to <span class="math inline">\(50\)</span>) seems reasonable for the priors of the <span class="math inline">\(\tau\)</span> parameters. As always, we can do a sensitivity analysis to verify that our priors are reasonably uninformative (if we intended them to be uninformative).</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:hierarchical" class="theorem"><strong>Box 5.2  </strong></span><strong>Some important (and sometimes confusing) points:</strong></p>
</div>
<ul>
<li><p>Why does <span class="math inline">\(u\)</span> have a mean of <span class="math inline">\(0\)</span>?</p>
<p>Because we want <span class="math inline">\(u\)</span> to capture only differences between subjects, we could achieve the same by assuming the following relationship between the likelihood and the intercept and slope:</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 signal_n &amp;\sim \mathit{Normal}(\alpha_{subj[n]} + \beta_{subj[n]} \cdot c\_cloze_n, \sigma)  \\
 \alpha_i &amp;\sim \mathit{Normal}(\alpha,\tau_{u_1})\\
 \beta_i &amp;\sim \mathit{Normal}(\beta,\tau_{u_2})\\
 \end{aligned}
 \end{equation}\]</span></p>
<p>In fact, this is another common way to write the model.</p></li>
<li><p>Why do the adjustments <span class="math inline">\(u\)</span> have a normal distribution?</p></li>
</ul>
<p>Mostly by convention, the adjustments <span class="math inline">\(u\)</span> are assumed to come from a normal distribution. Another reason is that if we don’t know anything about the distribution besides its mean and variance, the normal distribution is the most conservative assumption <span class="citation">(see chapter 9 of McElreath <a href="#ref-mcelreath2015statistical">2020</a>)</span>.</p>
</div>

<p>For now, we are assuming that there is no relationship (no correlation) between the by-subject intercept and slope adjustments <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span>; this lack of correlation is indicated in <code>brms</code> using the double pipe <code>||</code>.<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a> In <code>brms</code>, we need to specify hyperpriors for <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span>; these are called <code>sd</code> in <code>brms</code>, to distinguish these standard deviations from the standard deviation of the residuals <span class="math inline">\(\sigma\)</span>. As with the population-level effects, the by-subjects intercept adjustments are implicitly fit for the group-level effects and thus <code>(c_cloze || subj)</code> is equivalent to <code>(1 + c_cloze || subj)</code>. If we don’t want an intercept we need to explicitly indicate it with <code>(0 + c_cloze || subj)</code> or <code>(-1 + c_cloze || subj)</code>. Such a removal of the intercept is not normally done.</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb288-1" data-line-number="1">prior_v &lt;-</a>
<a class="sourceLine" id="cb288-2" data-line-number="2"><span class="st">  </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb288-3" data-line-number="3">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb288-4" data-line-number="4">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_cloze),</a>
<a class="sourceLine" id="cb288-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb288-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">class =</span> sd, <span class="dt">coef =</span> Intercept, <span class="dt">group =</span> subj),</a>
<a class="sourceLine" id="cb288-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">class =</span> sd, <span class="dt">coef =</span> c_cloze, <span class="dt">group =</span> subj)</a>
<a class="sourceLine" id="cb288-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb288-9" data-line-number="9">fit_N400_v &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">||</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb288-10" data-line-number="10">  <span class="dt">prior =</span> prior_v,</a>
<a class="sourceLine" id="cb288-11" data-line-number="11">  <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb288-12" data-line-number="12">)</a></code></pre></div>
<p>When we print a <code>brms</code> fit, we first see the summaries of the posteriors of the standard deviation of the by-group intercept and slopes, <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span> as <code>sd(Intercept)</code> and <code>sd(c_cloze)</code>, and then, as with previous models, the population-level effects, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> as <code>Intercept</code> and <code>c_cloze</code>, and the scale of the likelihood, <span class="math inline">\(\sigma\)</span>, as <code>sigma</code>. The full summary can be printed out by typing:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb289-1" data-line-number="1">fit_N400_v</a></code></pre></div>
<p>Because the above command will result in some pages of output, it is easier to understand the summary graphically (Figure <a href="ch-hierarchical.html#fig:plotfitN400v">5.8</a>). Rather than the wrapper <code>plot()</code>, we use the original function of the package <code>bayesplot</code>, <code>mcmc_dens()</code>, to only show density plots. We extract the first 5 parameters of the model with <code>variables(fit_N400_v)[1:5]</code>.</p>

<div class="sourceCode" id="cb290"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb290-1" data-line-number="1"><span class="kw">mcmc_dens</span>(fit_N400_v, <span class="dt">pars =</span> <span class="kw">variables</span>(fit_N400_v)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>])</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:plotfitN400v"></span>
<img src="bookdown_files/figure-html/plotfitN400v-1.svg" alt="Posterior distributions of the parameters in the model fit_N400_v." width="672" />
<p class="caption">
FIGURE 5.8: Posterior distributions of the parameters in the model <code>fit_N400_v</code>.
</p>
</div>
<p>Because we estimated how the population-level effect of cloze is adjusted for each subject, we could examine how each subject is being affected by the manipulation. For this we do the following, and we plot it in Figure <a href="ch-hierarchical.html#fig:partialpooling">5.9</a>. These are adjustments, <span class="math inline">\(u_{1,1},u_{1,...},u_{1,37}\)</span>, and not the effect of the manipulation by subject, <span class="math inline">\(\beta + [u_{1,1},u_{1,...},u_{1,37}]\)</span>. The code below produces Figure <a href="ch-hierarchical.html#fig:partialpooling">5.9</a>.</p>

<div class="sourceCode" id="cb291"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" data-line-number="1"><span class="co"># make a table of u_2s</span></a>
<a class="sourceLine" id="cb291-2" data-line-number="2">ind_effects_v &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;r_subj[&quot;</span>, <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj), </a>
<a class="sourceLine" id="cb291-3" data-line-number="3">                        <span class="st">&quot;,c_cloze]&quot;</span>)</a>
<a class="sourceLine" id="cb291-4" data-line-number="4">u_<span class="dv">2</span>_v &lt;-<span class="st"> </span><span class="kw">posterior_summary</span>(fit_N400_v, <span class="dt">variable =</span> ind_effects_v) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb291-5" data-line-number="5"><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb291-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subj =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">n</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb291-7" data-line-number="7"><span class="st">  </span><span class="co">## reorder plot by magnitude of mean:</span></a>
<a class="sourceLine" id="cb291-8" data-line-number="8"><span class="st">  </span><span class="kw">arrange</span>(Estimate) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb291-9" data-line-number="9"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subj =</span> <span class="kw">factor</span>(subj, <span class="dt">levels =</span> subj))</a>
<a class="sourceLine" id="cb291-10" data-line-number="10"><span class="co"># We plot:</span></a>
<a class="sourceLine" id="cb291-11" data-line-number="11"><span class="kw">ggplot</span>(</a>
<a class="sourceLine" id="cb291-12" data-line-number="12">  u_<span class="dv">2</span>_v,</a>
<a class="sourceLine" id="cb291-13" data-line-number="13">  <span class="kw">aes</span>(<span class="dt">x =</span> Estimate, <span class="dt">xmin =</span> Q2<span class="fl">.5</span>, <span class="dt">xmax =</span> Q97<span class="fl">.5</span>, <span class="dt">y =</span> subj)</a>
<a class="sourceLine" id="cb291-14" data-line-number="14">) <span class="op">+</span></a>
<a class="sourceLine" id="cb291-15" data-line-number="15"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb291-16" data-line-number="16"><span class="st">  </span><span class="kw">geom_errorbarh</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb291-17" data-line-number="17"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;By-subject adjustment to the slope in microvolts&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:partialpooling"></span>
<img src="bookdown_files/figure-html/partialpooling-1.svg" alt="95% credible intervals of adjustments to the effect of cloze probability for each subject (\(u_{1,1..37}\)) according to the varying intercept and varying slopes model, fit_N400_v. To obtain the effect of cloze probability for each subject, we would need to add the estimate of \(\beta\) to each adjustment." width="672" />
<p class="caption">
FIGURE 5.9: 95% credible intervals of adjustments to the effect of cloze probability for each subject (<span class="math inline">\(u_{1,1..37}\)</span>) according to the varying intercept and varying slopes model, <code>fit_N400_v</code>. To obtain the effect of cloze probability for each subject, we would need to add the estimate of <span class="math inline">\(\beta\)</span> to each adjustment.
</p>
</div>
<p>There is an important difference between the no-pooling model and the varying intercepts and slopes model we just fit. The no-pooling model fits each individual subject’s intercept and slope independently for each subject. By contrast, the varying intercepts and slopes model takes <em>all</em> the subjects’ data into account in order to compute the fixed effects <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>; and the model “shrinks” <span class="citation">(Pinheiro and Bates <a href="#ref-pinheirobates">2000</a>)</span> the by-subject intercept and slope adjustments towards the fixed effects estimates. In Figure <a href="ch-hierarchical.html#fig:comparison">5.10</a>, we can see the shrinkage of the estimates in the varying intercepts model by comparing them with the estimates of the no pooling model (<span class="math inline">\(M_{np}\)</span>). <!-- The code below produces Figure \@ref(fig:comparison). This code is more involved since it requires us to build a data frame with the by-subject effects ($\beta + u_2$). --></p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb292-1" data-line-number="1"></a>
<a class="sourceLine" id="cb292-2" data-line-number="2"><span class="co"># Extract parameter estimates from the no pooling model:</span></a>
<a class="sourceLine" id="cb292-3" data-line-number="3">par_np &lt;-<span class="st"> </span><span class="kw">posterior_summary</span>(fit_N400_np, <span class="dt">variable =</span> ind_effects_np) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb292-4" data-line-number="4"><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb292-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb292-6" data-line-number="6">    <span class="dt">model =</span> <span class="st">&quot;No pooling&quot;</span>,</a>
<a class="sourceLine" id="cb292-7" data-line-number="7">    <span class="dt">subj =</span> <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj)</a>
<a class="sourceLine" id="cb292-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb292-9" data-line-number="9"><span class="co"># For the hierarchical model, the code is more complicated</span></a>
<a class="sourceLine" id="cb292-10" data-line-number="10"><span class="co"># because we want the effect (beta) + adjustment.</span></a>
<a class="sourceLine" id="cb292-11" data-line-number="11"><span class="co"># Extract the overall group level effect:</span></a>
<a class="sourceLine" id="cb292-12" data-line-number="12">beta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">as_draws_df</span>(fit_N400_v)<span class="op">$</span>b_c_cloze)</a>
<a class="sourceLine" id="cb292-13" data-line-number="13"><span class="co"># Extract the individual adjustments:</span></a>
<a class="sourceLine" id="cb292-14" data-line-number="14">ind_effects_v &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;r_subj[&quot;</span>, <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj), <span class="st">&quot;,c_cloze]&quot;</span>)</a>
<a class="sourceLine" id="cb292-15" data-line-number="15">adjustment &lt;-<span class="st"> </span><span class="kw">as_draws_matrix</span>(fit_N400_v, <span class="dt">variable =</span> ind_effects_v)</a>
<a class="sourceLine" id="cb292-16" data-line-number="16"><span class="co"># Get the by subject effects in a data frame where each adjustment</span></a>
<a class="sourceLine" id="cb292-17" data-line-number="17"><span class="co"># is in each column.</span></a>
<a class="sourceLine" id="cb292-18" data-line-number="18"><span class="co"># Remove all the draws meta data by using as.data.frame</span></a>
<a class="sourceLine" id="cb292-19" data-line-number="19">by_subj_effect &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(beta <span class="op">+</span><span class="st"> </span>adjustment)</a>
<a class="sourceLine" id="cb292-20" data-line-number="20"><span class="co"># Summarize them by getting a table with the mean and the</span></a>
<a class="sourceLine" id="cb292-21" data-line-number="21"><span class="co"># quantiles for each column and then binding them.</span></a>
<a class="sourceLine" id="cb292-22" data-line-number="22">par_h &lt;-<span class="st"> </span><span class="kw">lapply</span>(by_subj_effect, <span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb292-23" data-line-number="23">  <span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb292-24" data-line-number="24">    <span class="dt">Estimate =</span> <span class="kw">mean</span>(x),</a>
<a class="sourceLine" id="cb292-25" data-line-number="25">    <span class="dt">Q2.5 =</span> <span class="kw">quantile</span>(x, <span class="fl">.025</span>),</a>
<a class="sourceLine" id="cb292-26" data-line-number="26">    <span class="dt">Q97.5 =</span> <span class="kw">quantile</span>(x, <span class="fl">.975</span>)</a>
<a class="sourceLine" id="cb292-27" data-line-number="27">  )</a>
<a class="sourceLine" id="cb292-28" data-line-number="28">}) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb292-29" data-line-number="29"><span class="st">  </span><span class="kw">bind_rows</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb292-30" data-line-number="30"><span class="st">  </span><span class="co"># Add a column to identify that the model,</span></a>
<a class="sourceLine" id="cb292-31" data-line-number="31"><span class="st">  </span><span class="co"># and one with the subject labels:</span></a>
<a class="sourceLine" id="cb292-32" data-line-number="32"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb292-33" data-line-number="33">    <span class="dt">model =</span> <span class="st">&quot;Hierarchical&quot;</span>,</a>
<a class="sourceLine" id="cb292-34" data-line-number="34">    <span class="dt">subj =</span> <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj)</a>
<a class="sourceLine" id="cb292-35" data-line-number="35">  )</a>
<a class="sourceLine" id="cb292-36" data-line-number="36"><span class="co"># The mean and 95% CI of both models in one data frame:</span></a>
<a class="sourceLine" id="cb292-37" data-line-number="37">by_subj_df &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(par_h, par_np) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb292-38" data-line-number="38"><span class="st">  </span><span class="kw">arrange</span>(Estimate) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb292-39" data-line-number="39"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subj =</span> <span class="kw">factor</span>(subj, <span class="dt">levels =</span> <span class="kw">unique</span>(.data<span class="op">$</span>subj))) </a></code></pre></div>

<div class="sourceCode" id="cb293"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" data-line-number="1"><span class="kw">ggplot</span>(</a>
<a class="sourceLine" id="cb293-2" data-line-number="2">  by_subj_df,</a>
<a class="sourceLine" id="cb293-3" data-line-number="3">  <span class="kw">aes</span>(</a>
<a class="sourceLine" id="cb293-4" data-line-number="4">    <span class="dt">ymin =</span> Q2<span class="fl">.5</span>, <span class="dt">ymax =</span> Q97<span class="fl">.5</span>, <span class="dt">x =</span> subj, <span class="dt">y =</span> Estimate, <span class="dt">color =</span> model,</a>
<a class="sourceLine" id="cb293-5" data-line-number="5">    <span class="dt">shape =</span> model</a>
<a class="sourceLine" id="cb293-6" data-line-number="6">  )</a>
<a class="sourceLine" id="cb293-7" data-line-number="7">) <span class="op">+</span></a>
<a class="sourceLine" id="cb293-8" data-line-number="8"><span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="dt">position =</span> <span class="kw">position_dodge</span>(<span class="dv">1</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb293-9" data-line-number="9"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position =</span> <span class="kw">position_dodge</span>(<span class="dv">1</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb293-10" data-line-number="10"><span class="st">  </span><span class="co"># We&#39;ll also add the mean and 95% CrI of the overall difference</span></a>
<a class="sourceLine" id="cb293-11" data-line-number="11"><span class="st">  </span><span class="co"># to the plot:</span></a>
<a class="sourceLine" id="cb293-12" data-line-number="12"><span class="st">  </span><span class="kw">geom_hline</span>(</a>
<a class="sourceLine" id="cb293-13" data-line-number="13">    <span class="dt">yintercept =</span></a>
<a class="sourceLine" id="cb293-14" data-line-number="14">      <span class="kw">posterior_summary</span>(fit_N400_v,</a>
<a class="sourceLine" id="cb293-15" data-line-number="15">                        <span class="dt">variable =</span> <span class="st">&quot;b_c_cloze&quot;</span>)[, <span class="st">&quot;Estimate&quot;</span>]</a>
<a class="sourceLine" id="cb293-16" data-line-number="16">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb293-17" data-line-number="17"><span class="st">  </span><span class="kw">geom_hline</span>(</a>
<a class="sourceLine" id="cb293-18" data-line-number="18">    <span class="dt">yintercept =</span></a>
<a class="sourceLine" id="cb293-19" data-line-number="19">      <span class="kw">posterior_summary</span>(fit_N400_v,</a>
<a class="sourceLine" id="cb293-20" data-line-number="20">                        <span class="dt">variable =</span> <span class="st">&quot;b_c_cloze&quot;</span>)[, <span class="st">&quot;Q2.5&quot;</span>],</a>
<a class="sourceLine" id="cb293-21" data-line-number="21">    <span class="dt">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="dt">size =</span> <span class="fl">.5</span></a>
<a class="sourceLine" id="cb293-22" data-line-number="22">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb293-23" data-line-number="23"><span class="st">  </span><span class="kw">geom_hline</span>(</a>
<a class="sourceLine" id="cb293-24" data-line-number="24">    <span class="dt">yintercept =</span></a>
<a class="sourceLine" id="cb293-25" data-line-number="25">      <span class="kw">posterior_summary</span>(fit_N400_v,</a>
<a class="sourceLine" id="cb293-26" data-line-number="26">                        <span class="dt">variable =</span> <span class="st">&quot;b_c_cloze&quot;</span>)[, <span class="st">&quot;Q97.5&quot;</span>],</a>
<a class="sourceLine" id="cb293-27" data-line-number="27">    <span class="dt">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="dt">size =</span> <span class="fl">.5</span></a>
<a class="sourceLine" id="cb293-28" data-line-number="28">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb293-29" data-line-number="29"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;N400 effect of predictability&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb293-30" data-line-number="30"><span class="st">  </span><span class="kw">coord_flip</span>()</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:comparison"></span>
<img src="bookdown_files/figure-html/comparison-1.svg" alt="This plot compares the estimates of the effect of cloze probability for each subject between (i) the no pooling, fit_N400_np and (ii) the varying intercepts and varying slopes, hierarchical, model, fit_N400_v." width="672" />
<p class="caption">
FIGURE 5.10: This plot compares the estimates of the effect of cloze probability for each subject between (i) the no pooling, <code>fit_N400_np</code> and (ii) the varying intercepts and varying slopes, hierarchical, model, <code>fit_N400_v</code>.
</p>
</div>
</div>
</div>
<div id="sec-mcvivs" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.4</span> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)<a href="ch-hierarchical.html#sec-mcvivs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The model <span class="math inline">\(M_{v}\)</span> allowed for differences in intercepts (mean voltage) and slopes (effects of cloze) across subjects, but it has the implicit assumption that these varying intercepts and varying slopes are independent. It is in principle possible that subjects showing more negative voltage may also show stronger effects (or weaker effects). Next, we fit a model that allows a correlation between the intercepts and slopes. We model the correlation between varying intercepts and slopes by defining a variance-covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> between the by-subject varying intercepts and slopes, and by assuming that both adjustments (intercept and slope) come from a multivariate (in this case, a bivariate) normal distribution. <!-- See Box \@ref(thm:vcovmatrix) for a short overview of the essential details regarding the variance-covariance matrix. --></p>
<!-- \Begin{extra} -->
<!-- <div class="extra"> -->
<!-- ```{theorem, vcovmatrix} -->
<!-- **The variance-covariance matrix and the corresponding correlation matrix:** -->
<!-- ``` -->
<!-- The variances in a multivariate distribution will be composed of -->
<!--   - variances for each random variable -->
<!--   - covariances between pairs of random variables, which includes some correlation $\rho$ between pairs of random variables -->
<!-- E.g., for a bivariate distribution with random variables $u_1$ and $u_2$, this information is expressed in a variance-covariance matrix. -->
<!-- \begin{equation} -->
<!-- \Sigma_u -->
<!-- = -->
<!-- \begin{pmatrix} -->
<!-- \tau _{u_1}^2  & \rho_u \tau _{u_1}\tau _{u1}\\ -->
<!-- \rho \tau _{u_1}\tau _{u_2}    & \tau _{u_2}^2\\ -->
<!-- \end{pmatrix} -->
<!-- \end{equation} -->
<!-- the covariance $Cov(u_1,u_2)$ between two variables $X$ and $Y$ is -->
<!-- defined as the product of their correlation $\rho_u$ and their standard -->
<!-- deviations $\tau_{u_1}$ and $\tau_{u_2}$, such that, $Cov(u_1,u_2) = \rho_u -->
<!-- \tau_{u_1} \tau_{u_2}$. -->
<!-- The covariance matrix can be decomposed into a matrix of standard deviations and a correlation matrix. For our example, the correlation matrix looks like this: -->
<!-- \begin{equation} -->
<!-- \mathbf{\rho}_u =  -->
<!-- {\begin{pmatrix}  -->
<!-- 1 & \rho_u  \\  -->
<!-- \rho_u  & 1 -->
<!-- \end{pmatrix}} -->
<!-- \end{equation} -->
<!-- This means that we can decompose the covariance matrix into three parts: -->
<!-- \begin{equation} -->
<!-- \begin{aligned} -->
<!-- \boldsymbol{\Sigma_u}  -->
<!-- &= -->
<!-- {\begin{pmatrix}  -->
<!-- \tau_{u_1} & 0 \\  -->
<!-- 0  & \tau_{u_2} -->
<!-- \end{pmatrix}} -->
<!-- {\begin{pmatrix}  -->
<!-- 1 & \rho_u  \\  -->
<!-- \rho_u  & 1 -->
<!-- \end{pmatrix}} -->
<!-- {\begin{pmatrix}  -->
<!-- \tau_{u_1} & 0 \\  -->
<!-- 0  & \tau_{u_2} -->
<!-- \end{pmatrix}} -->
<!-- \end{aligned} -->
<!-- \end{equation} -->
<!-- The importance of the correlation matrix is that a prior will be defined on the correlation matrix rather than on the individual correlation parameter. One reason for this is generality: in more complex designs, such as $2\times 2\times 2$  factorial experiments, the variance covariance matrix is much larger than in our example in the text, but the proliferation of correlations that result is no problem for `brms` or Stan because we define the prior on the correlation matrix. -->
<!-- </div> -->
<!-- \End{extra} -->
<ul>
<li>In <span class="math inline">\(M_h\)</span>, we model the EEG data with the following assumptions:</li>
</ul>
<ol style="list-style-type: decimal">
<li>EEG averages for the N400 spatio-temporal window are normally distributed.</li>
<li>Some aspects of the mean signal voltage and of the effect of predictability depend on the subject, and these two might be correlated, i.e., we assume group-level intercepts and slopes, and allow a correlation between them by-subject.</li>
<li>There is a linear relationship between cloze and the EEG signal for the trial.</li>
</ol>
<p>The likelihood remains identical to the model <span class="math inline">\(M_v\)</span>, which assumes no correlation between group-level intercepts and slopes (section <a href="ch-hierarchical.html#sec-uncorrelated">5.2.3</a>):</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + c\_cloze_n \cdot  (\beta + u_{subj[n],2}),\sigma)
  \end{equation}\]</span></p>
<p>The correlation is indicated in the priors on the adjustments for intercept <span class="math inline">\(u_{1}\)</span> and slopes <span class="math inline">\(u_{2}\)</span>.</p>
<ul>
<li>Priors:
<span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp; \sim \mathit{Normal}(0,10) \\
 \beta  &amp; \sim \mathit{Normal}(0,10) \\
  \sigma  &amp;\sim \mathit{Normal}_+(0,50)\\
  {\begin{pmatrix}
  u_{i,1} \\
  u_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right)
 \end{aligned}
 \end{equation}\]</span></li>
</ul>
<p>In this model, a bivariate normal distribution generates the varying intercepts and varying slopes <span class="math inline">\(\mathbf{u}\)</span>; this is an <span class="math inline">\(n\times 2\)</span> matrix. The variance-covariance matrix <span class="math inline">\(\boldsymbol{\Sigma_u}\)</span> defines the standard deviations of the varying intercepts and varying slopes, and the correlation between them. Recall from section <a href="ch-intro.html#sec-contbivar">1.6.2</a> that the diagonals of the variance-covariance matrix contain the variances of the correlated random variables, and the off-diagonals contain the covariances. In this example, the covariance <span class="math inline">\(Cov(u_1,u_2)\)</span> between two variables <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> is defined as the product of their correlation <span class="math inline">\(\rho\)</span> and their standard deviations <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span>. In other words, <span class="math inline">\(Cov(u_1,u_2) = \rho_u \tau_{u_1} \tau_{u_2}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\boldsymbol{\Sigma_u} = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_2} &amp; \tau_{u_2}^2
\end{pmatrix}}
\end{equation}\]</span></p>
<p>In order to specify a prior for <span class="math inline">\(\Sigma_u\)</span>, we need priors for the standard deviations, <span class="math inline">\(\tau_{u_1}\)</span>, and <span class="math inline">\(\tau_{u_2}\)</span>, and also for their correlation, <span class="math inline">\(\rho_u\)</span>. We can use the same priors for <span class="math inline">\(\tau\)</span> as before. For the correlation parameter <span class="math inline">\(\rho_u\)</span> (and the correlation matrix more generally), we use the LKJ prior. The basic idea of the LKJ prior on the correlation matrix is that as its parameter, <span class="math inline">\(\eta\)</span> (<em>eta</em>), increases, it will favor correlations closer to zero.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a> At <span class="math inline">\(\eta = 1\)</span>, the LKJ correlation distribution is uninformative (similar to <span class="math inline">\(Beta(1,1)\)</span>), at <span class="math inline">\(\eta &lt; 1\)</span>, it favors extreme correlations (similar to <span class="math inline">\(Beta(a&lt;1,b&lt;1)\)</span>). We set <span class="math inline">\(\eta = 2\)</span> so that we don’t favor extreme correlations, and we still represent our lack of knowledge through the wide spread of the prior between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. Thus, <span class="math inline">\(\eta = 2\)</span> gives us a regularizing, relatively uninformative or mildly informative prior.</p>
<p>Figure <a href="ch-hierarchical.html#fig:lkjviz">5.11</a> shows a visualization of different parametrizations of the LKJ prior.</p>

<div class="figure"><span style="display:block;" id="fig:lkjviz"></span>
<img src="bookdown_files/figure-html/lkjviz-1.svg" alt="Visualization of the LKJ correlation distribution prior with four different values of the \(\eta\) parameter." width="48%" /><img src="bookdown_files/figure-html/lkjviz-2.svg" alt="Visualization of the LKJ correlation distribution prior with four different values of the \(\eta\) parameter." width="48%" /><img src="bookdown_files/figure-html/lkjviz-3.svg" alt="Visualization of the LKJ correlation distribution prior with four different values of the \(\eta\) parameter." width="48%" /><img src="bookdown_files/figure-html/lkjviz-4.svg" alt="Visualization of the LKJ correlation distribution prior with four different values of the \(\eta\) parameter." width="48%" />
<p class="caption">
FIGURE 5.11: Visualization of the LKJ correlation distribution prior with four different values of the <span class="math inline">\(\eta\)</span> parameter.
</p>
</div>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim \mathit{Normal}_+(0,20)\\
\tau_{u_2} &amp;\sim \mathit{Normal}_+(0,20)\\
\rho_u &amp;\sim \mathit{LKJcorr}(2) 
\end{aligned}
\end{equation}\]</span></p>
<p>In our <code>brms</code> model, we allow a correlation between the by-subject intercepts and slopes by using a single pipe <code>|</code> instead of the double pipe <code>||</code> that we used previously. This convention follows that in the frequentist <code>lmer</code> function. As before, the varying intercepts are implicitly fit.</p>
<p>Because we have a new parameter, the correlation <span class="math inline">\(\rho_{u}\)</span>, we need to add a new prior for this correlation; in <code>brms</code>, this is achieved by addition a prior for the parameter type <code>cor</code>.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb294-1" data-line-number="1">prior_h &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb294-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb294-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_cloze),</a>
<a class="sourceLine" id="cb294-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb294-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb294-6" data-line-number="6">    <span class="dt">class =</span> sd, <span class="dt">coef =</span> Intercept,</a>
<a class="sourceLine" id="cb294-7" data-line-number="7">    <span class="dt">group =</span> subj</a>
<a class="sourceLine" id="cb294-8" data-line-number="8">  ),</a>
<a class="sourceLine" id="cb294-9" data-line-number="9">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb294-10" data-line-number="10">    <span class="dt">class =</span> sd, <span class="dt">coef =</span> c_cloze,</a>
<a class="sourceLine" id="cb294-11" data-line-number="11">    <span class="dt">group =</span> subj</a>
<a class="sourceLine" id="cb294-12" data-line-number="12">  ),</a>
<a class="sourceLine" id="cb294-13" data-line-number="13">  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor, <span class="dt">group =</span> subj)</a>
<a class="sourceLine" id="cb294-14" data-line-number="14">)</a>
<a class="sourceLine" id="cb294-15" data-line-number="15">fit_N400_h &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb294-16" data-line-number="16">  <span class="dt">prior =</span> prior_h,</a>
<a class="sourceLine" id="cb294-17" data-line-number="17">  <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb294-18" data-line-number="18">)</a></code></pre></div>
<p>The estimates do not change much in comparison with the varying intercept/slope model, probably because the estimation of the correlation is quite poor (i.e., there is a lot of uncertainty). As before we show the estimates graphically (Figure <a href="ch-hierarchical.html#fig:plotfitN400h">5.12</a>. One can access the complete summary as always with <code>fit_N400_h</code>.</p>

<div class="sourceCode" id="cb295"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" data-line-number="1"><span class="kw">plot</span>(fit_N400_h, <span class="dt">N =</span> <span class="dv">6</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:plotfitN400h"></span>
<img src="bookdown_files/figure-html/plotfitN400h-1.svg" alt="The posteriors of the parameters in the model fit_N400_h." width="672" />
<p class="caption">
FIGURE 5.12: The posteriors of the parameters in the model <code>fit_N400_h</code>.
</p>
</div>
<p>We are now half-way to what is sometimes called the “maximal” hierarchical model <span class="citation">(Barr et al. <a href="#ref-barr2013">2013</a>)</span>. This usually refers to a model with all the by-participant and by-items group-level variance components <em>allowed by the experimental design</em> and <em>a full variance covariance matrix</em> for all the group-level parameters. Not all variance components are allowed by the experimental design: in particular, between-group manipulations cannot have variance components. For example, even if we assume that the working memory capacity of the subjects might affect the N400, we cannot measure how working memory affects the subjects differently.</p>
<p>When we refer to a full variance-covariance matrix, we mean a variance-covariance matrix where all the elements (variances and covariances) are non-zero. In our previous model, for example, the variance-covariance matrix <span class="math inline">\(\boldsymbol{\Sigma_u}\)</span> was full because no element was zero. If we assume no correlation between group-level intercept and slope, it would mean to have zeros in the diagonal of the matrix and this would render the model to be identical to <span class="math inline">\(M_{v}\)</span> defined in section <a href="ch-hierarchical.html#sec-uncorrelated">5.2.3</a>; if we assume that also the bottom right element (<span class="math inline">\(\tau^2\)</span>) is zero, the model would turn into a varying intercept model (in <code>brms</code> formula <code>n400 ~ c_cloze + (1 | subj)</code>); and if we assume that the matrix has only zeros, the model would turn into a complete pooling model, <span class="math inline">\(M_{cp}\)</span>, as defined in section <a href="ch-hierarchical.html#sec-Mcp">5.2.1</a>.</p>
<p>As we will see in section <a href="ch-hierarchical.html#sec-distrmodel">5.2.6</a> and in chapter <a href="ch-workflow.html#ch-workflow">7</a>, “maximal” is a misnomer for Bayesian models, since this mostly refers to limitations of the popular frequentist package for fitting models, <code>lme4</code>.</p>
<p>The next section spells out a model with full variance-covariance matrix for both subjects and items-level effects.</p>
</div>
<div id="sec-sih" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.5</span> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)<a href="ch-hierarchical.html#sec-sih" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our new model, <span class="math inline">\(M_{sih}\)</span> will allow for differences in intercepts (mean voltage) and slopes (effects of predictability) across subjects and across items. In typical Latin square designs, subjects and items are said to be <em>crossed random effects</em>—each subject sees exactly one instance of each item. Here we assume a possible correlation between varying intercepts and slopes by subjects, and another one by items.</p>
<ul>
<li>In <span class="math inline">\(M_{sih}\)</span>, we model the EEG data with the following assumptions:</li>
</ul>
<ol style="list-style-type: decimal">
<li>EEG averages for the N400 spatio-temporal window are normally distributed.</li>
<li>Some aspects of the mean signal voltage and of the effect of predictability depend on the subject, i.e., we assume group-level intercepts, and slopes, and a correlation between them by-subject.</li>
<li>Some aspects of the mean signal voltage and of the effect of predictability depend on the item, i.e., we assume group-level intercepts, and slopes, and a correlation between them by-item.
<!-- 3. The variation in the signal is independent from the subject. --></li>
<li>There is a linear relationship between cloze and the EEG signal for the trial.</li>
</ol>
<ul>
<li>Likelihood:</li>
</ul>
<p><span class="math display">\[\begin{multline}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + w_{item[n],1} + \\ c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma)
  \end{multline}\]</span></p>
<ul>
<li>Priors:
<span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp; \sim \mathit{Normal}(0,10) \\
 \beta  &amp; \sim \mathit{Normal}(0,10) \\
  \sigma  &amp;\sim \mathit{Normal}_+(0,50)\\
  {\begin{pmatrix}
  u_{i,1} \\
  u_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right) \\
   {\begin{pmatrix}
  w_{j,1} \\
  w_{j,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_w} \right) 
 \end{aligned}
 \end{equation}\]</span></li>
</ul>
<p>We have added the index <span class="math inline">\(j\)</span>, which represents each item, as we did with subjects; <span class="math inline">\(item[n]\)</span> indicates the item that corresponds to the observation in the <span class="math inline">\(n\)</span>-th row of the data frame.</p>
<p>We have hyperparameters and hyperpriors as before:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
 \boldsymbol{\Sigma_u} &amp; = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_2} &amp; \tau_{u_2}^2
\end{pmatrix}}\\
 \boldsymbol{\Sigma_w} &amp; = 
{\begin{pmatrix} 
\tau_{w_1}^2 &amp; \rho_w \tau_{w_1} \tau_{w_2} \\ 
\rho_w \tau_{w_1} \tau_{w_2} &amp; \tau_{w_2}^2
\end{pmatrix}}
 \end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim \mathit{Normal}_+(0,20)\\
\tau_{u_2} &amp;\sim \mathit{Normal}_+(0,20)\\
\rho_u &amp;\sim \mathit{LKJcorr}(2) \\
\tau_{w_1} &amp;\sim \mathit{Normal}_+(0,20)\\
\tau_{w_2} &amp;\sim \mathit{Normal}_+(0,20)\\
\rho_w &amp;\sim \mathit{LKJcorr}(2) \\
\end{aligned}
\end{equation}\]</span></p>
<p>We set identical priors for by-items group-level effects as for by-subject group-level effects, but this only because we don’t have any differentiated prior information about subject-level vs. item-level variation. However, bear in mind that the estimation for items is completely independent from the estimation for subjects. Although we wrote many more equations than before, the <code>brms</code> model is quite straightforward to extend:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb296-1" data-line-number="1">prior_sih_full &lt;-</a>
<a class="sourceLine" id="cb296-2" data-line-number="2"><span class="st">  </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb296-3" data-line-number="3">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb296-4" data-line-number="4">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_cloze),</a>
<a class="sourceLine" id="cb296-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb296-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb296-7" data-line-number="7">      <span class="dt">class =</span> sd, <span class="dt">coef =</span> Intercept,</a>
<a class="sourceLine" id="cb296-8" data-line-number="8">      <span class="dt">group =</span> subj</a>
<a class="sourceLine" id="cb296-9" data-line-number="9">    ),</a>
<a class="sourceLine" id="cb296-10" data-line-number="10">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb296-11" data-line-number="11">      <span class="dt">class =</span> sd, <span class="dt">coef =</span> c_cloze,</a>
<a class="sourceLine" id="cb296-12" data-line-number="12">      <span class="dt">group =</span> subj</a>
<a class="sourceLine" id="cb296-13" data-line-number="13">    ),</a>
<a class="sourceLine" id="cb296-14" data-line-number="14">    <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor, <span class="dt">group =</span> subject),</a>
<a class="sourceLine" id="cb296-15" data-line-number="15">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb296-16" data-line-number="16">      <span class="dt">class =</span> sd, <span class="dt">coef =</span> Intercept,</a>
<a class="sourceLine" id="cb296-17" data-line-number="17">      <span class="dt">group =</span> item</a>
<a class="sourceLine" id="cb296-18" data-line-number="18">    ),</a>
<a class="sourceLine" id="cb296-19" data-line-number="19">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb296-20" data-line-number="20">      <span class="dt">class =</span> sd, <span class="dt">coef =</span> c_cloze,</a>
<a class="sourceLine" id="cb296-21" data-line-number="21">      <span class="dt">group =</span> item</a>
<a class="sourceLine" id="cb296-22" data-line-number="22">    ),</a>
<a class="sourceLine" id="cb296-23" data-line-number="23">    <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor, <span class="dt">group =</span> item)</a>
<a class="sourceLine" id="cb296-24" data-line-number="24">  )</a>
<a class="sourceLine" id="cb296-25" data-line-number="25">fit_N400_sih &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb296-26" data-line-number="26"><span class="st">                      </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb296-27" data-line-number="27">                    <span class="dt">prior =</span> prior_sih_full,</a>
<a class="sourceLine" id="cb296-28" data-line-number="28">                    <span class="dt">data =</span> df_eeg)</a></code></pre></div>
<p>We can also simplify the call to <code>brms</code>, when we assign the same priors to the by-subject and by-item parameters:</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb297-1" data-line-number="1">prior_sih &lt;-</a>
<a class="sourceLine" id="cb297-2" data-line-number="2"><span class="st">  </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb297-3" data-line-number="3">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb297-4" data-line-number="4">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb297-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb297-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb297-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb297-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb297-9" data-line-number="9">fit_N400_sih &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb297-10" data-line-number="10"><span class="st">                      </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb297-11" data-line-number="11">                    <span class="dt">prior =</span> prior_sih,</a>
<a class="sourceLine" id="cb297-12" data-line-number="12">                    <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb297-13" data-line-number="13">)</a></code></pre></div>
<p>We have new group-level effects in the summary, but again the estimate of the effect of cloze remains virtually unchanged (Figure <a href="ch-hierarchical.html#fig:fitN400sih">5.13</a>).</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb298-1" data-line-number="1">fit_N400_sih</a></code></pre></div>

<div class="sourceCode" id="cb299"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb299-1" data-line-number="1"><span class="kw">plot</span>(fit_N400_sih, <span class="dt">N =</span> <span class="dv">9</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:fitN400sih"></span>
<img src="bookdown_files/figure-html/fitN400sih-1.svg" alt="The posterior distributions of the parameters in the model fit_N400_sih." width="672" />
<p class="caption">
FIGURE 5.13: The posterior distributions of the parameters in the model fit_N400_sih.
</p>
</div>

<div class="extra">
<div class="theorem">
<p><span id="thm:matrixHierachicalModel" class="theorem"><strong>Box 5.3  </strong></span><strong>The Matrix Formulation of Hierarchical Models (the Laird-Ware form)</strong></p>
</div>
<p>We have been writing linear models as follows; where <span class="math inline">\(n\)</span> refers to the row id in the data frame.</p>
<p><span class="math display">\[\begin{equation}
y_n \sim \mathit{Normal}(\alpha + \beta\cdot x_n)
\end{equation}\]</span></p>
<p>This simple linear model can be re-written as follows:</p>
<p><span class="math display">\[\begin{equation}
y_n = \alpha + \beta\cdot x_n + \varepsilon_n
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\varepsilon_n \sim \mathit{Normal}(0, \sigma)\)</span>.</p>
<p>The model does not change if <span class="math inline">\(\alpha\)</span> is multiplied by <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[\begin{equation}
y_n = \alpha\cdot 1 + \beta\cdot x_n + \varepsilon_n
\end{equation}\]</span></p>
<p>The above is actually <span class="math inline">\(n\)</span> linear equations, and can be written compactly in matrix form:</p>
<p><span class="math display">\[\begin{equation}
{\begin{pmatrix} 
    y_1\\
    y_2\\
    \vdots \\
    y_n\\
   \end{pmatrix}}
 = 
 {\begin{pmatrix} 
    1 &amp; x_1\\
    1 &amp; x_2\\
    \vdots &amp; \vdots \\
    1 &amp; x_n\\
   \end{pmatrix}}
{\begin{pmatrix} 
    \alpha\\
    \beta \\
   \end{pmatrix}}
+
 {\begin{pmatrix} 
    \varepsilon_1 \\
    \varepsilon_2 \\
    \vdots \\
     \varepsilon_n \\
   \end{pmatrix}}   
\end{equation}\]</span></p>
<p>Consider this matrix in the above equation:</p>
<p><span class="math display">\[\begin{equation}
{\begin{pmatrix} 
    1 &amp; x_1\\
    1 &amp; x_2\\
    \vdots &amp; \vdots \\
    1 &amp; x_n\\
   \end{pmatrix}}
\end{equation}\]</span></p>
<p>This matrix is called the model matrix or the design matrix; we will encounter it again in the contrast coding chapters, where it plays a crucial role. If we write the dependent variable <span class="math inline">\(y\)</span> as a <span class="math inline">\(n\times 1\)</span> vector, the above matrix as the matrix <span class="math inline">\(X\)</span> (which has dimensions <span class="math inline">\(n\times 2\)</span>, the intercept and slope parameters as a <span class="math inline">\(2\times 1\)</span> matrix <span class="math inline">\(\zeta\)</span>, and the residual errors as an <span class="math inline">\(n\times 1\)</span> matrix, we can write the linear model very compactly:</p>
<p><span class="math display">\[\begin{equation}
y =  X \zeta + \varepsilon
\end{equation}\]</span></p>
<p>The above matrix formulation of the linear model extends to the hierarchical model very straightforwardly. For example,
consider the model <span class="math inline">\(M_{sih}\)</span> that we just saw above. This model has the following likelihood:</p>
<p><span class="math display">\[\begin{multline}
  signal_n \sim \mathit{Normal}(\alpha + u_{subj[n],1} + w_{item[n],1} \\
  + c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma)
  \end{multline}\]</span></p>
<p>The terms in the location parameter in the Normal likelihood can be re-written in matrix form, just like the linear model above. To see this, consider the fact that the location term</p>
<p><span class="math display">\[\begin{equation}
\alpha + u_{subj[n],1} + w_{item[n],1} + c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2})
\end{equation}\]</span></p>
<p>can be re-written as</p>
<p><span class="math display">\[\begin{multline}
\alpha\cdot 1 + u_{subj[n],1}\cdot 1 + w_{item[n],1}\cdot 1 +\\
\beta \cdot c\_cloze_n + u_{subj[n],2}\cdot c\_cloze_n+ w_{item[n],2}\cdot c\_cloze_n
\end{multline}\]</span></p>
<p>The above equation can in turn be written in matrix form:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
&amp;
{\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}
{\begin{pmatrix} 
    \alpha\\
    \beta \\
   \end{pmatrix}}
+ \\
&amp; {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}
   {\begin{pmatrix} 
    u_{subj[1],1}  &amp; u_{subj[2],1} &amp; \dots &amp; u_{subj[n],1}\\
    u_{subj[1],2} &amp; u_{subj[2],2} &amp;  \dots &amp; u_{subj[n],2}\\
   \end{pmatrix}}
   +\\
&amp; {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}
   {\begin{pmatrix} 
    w_{item[1],1} &amp; w_{item[2],1} &amp; \dots &amp; w_{item[n],1} \\
    w_{item[1],2} &amp; w_{item[2],2} &amp; \dots &amp; w_{item[n],2} \\
   \end{pmatrix}}
 \end{aligned}
\end{equation}\]</span></p>
<p>In this hierarchical model, there are three model matrices:</p>
<ul>
<li>the matrix associated with the intercept <span class="math inline">\(\alpha\)</span> and the slope <span class="math inline">\(\beta\)</span>; below, we call this the matrix <span class="math inline">\(X\)</span>.<br />
</li>
<li>the matrix associated with the by-subject varying intercepts and slopes; call this the matrix <span class="math inline">\(Z_u\)</span>.</li>
<li>the matrix associated with the by-item varying intercepts and slopes; call this the matrix <span class="math inline">\(Z_w\)</span>.</li>
</ul>
<p>The model can now be written very compactly in matrix form by writing these three matrices as follows:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
X = &amp; {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}\\ 
   Z_u = &amp;
   {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}} \\
   Z_w = &amp;
   {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}} \\
\end{aligned}
\end{equation}\]</span></p>
<p>The location part of the model <span class="math inline">\(M_{sih}\)</span> can now be written very compactly:</p>
<p><span class="math display">\[\begin{equation}
X \zeta
+
Z_u z_u
+ 
Z_w z_w
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\zeta\)</span> is a <span class="math inline">\(2\times 1\)</span> matrix containing the intercept <span class="math inline">\(\alpha\)</span> and the slope <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(z_u\)</span> and <span class="math inline">\(z_w\)</span> are the intercept and slope adjustments by subject and by item:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
z_u = &amp;
 {\begin{pmatrix} 
    u_{subj[1],1}  &amp; u_{subj[2],1} &amp; \dots &amp; u_{subj[n],1}\\
    u_{subj[1],2} &amp; u_{subj[2],2} &amp;  \dots &amp; u_{subj[n],2}\\
   \end{pmatrix}}\\
 z_w = &amp; 
    {\begin{pmatrix} 
    w_{item[1],1} &amp; w_{item[2],1} &amp; \dots &amp; w_{item[n],1} \\
    w_{item[1],2} &amp; w_{item[2],2} &amp; \dots &amp; w_{item[n],2} \\
   \end{pmatrix}}\\
\end{aligned}
\end{equation}\]</span></p>
<p>In summary, the hierarchical model has a very general matrix formulation, called the Laird-Ware form <span class="citation">(Laird and Ware <a href="#ref-laird1982random">1982</a>)</span>:</p>
<p><span class="math display">\[\begin{equation}
 signal = X \zeta
+
Z_u z_u
+ 
Z_w z_w +
\varepsilon
 \end{equation}\]</span></p>
<p>The practical relevance of this matrix formulation is that we can define hierarchical models very compactly and efficiently in Stan by expressing the model in terms of the model matrices <span class="citation">(Sorensen, Hohenstein, and Vasishth <a href="#ref-SorensenVasishthTutorial">2016</a>)</span>. As an aside, notice that in the above example, <span class="math inline">\(X=Z_u=Z_w\)</span>; but in principle one could have different model matrices for the fixed vs. random effects.</p>
</div>
</div>
<div id="sec-distrmodel" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.6</span> Beyond the maximal model–Distributional regression models<a href="ch-hierarchical.html#sec-distrmodel" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use posterior predictive checks to verify that our last model can capture the entire signal distribution. This is shown in Figure <a href="ch-hierarchical.html#fig:ppcheckdens">5.14</a></p>

<div class="sourceCode" id="cb300"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb300-1" data-line-number="1"><span class="kw">pp_check</span>(fit_N400_sih, <span class="dt">ndraws =</span> <span class="dv">50</span>, <span class="dt">type =</span> <span class="st">&quot;dens_overlay&quot;</span>) </a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ppcheckdens"></span>
<img src="bookdown_files/figure-html/ppcheckdens-1.svg" alt="Overlay of densities from the posterior predictive distributions of the model fit_N400_sih." width="672" />
<p class="caption">
FIGURE 5.14: Overlay of densities from the posterior predictive distributions of the model <code>fit_N400_sih</code>.
</p>
</div>
<p>However, we know that in ERP studies, large levels of impedance between the recording electrodes and the skin tissue increase the noise in the recordings <span class="citation">(Picton et al. <a href="#ref-picton_etal_2000">2000</a>)</span>. Given that skin tissue is different between subjects, it could be the case that the level of noise varies by subject.
It might be a good idea to verify that our model is good enough for capturing the by-subject variability. The code below produces Figure <a href="ch-hierarchical.html#fig:postpreddensbysubj">5.15</a>.</p>

<div class="sourceCode" id="cb301"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb301-1" data-line-number="1"><span class="kw">ppc_dens_overlay_grouped</span>(df_eeg<span class="op">$</span>n400,</a>
<a class="sourceLine" id="cb301-2" data-line-number="2">  <span class="dt">yrep =</span></a>
<a class="sourceLine" id="cb301-3" data-line-number="3">    <span class="kw">posterior_predict</span>(fit_N400_sih,</a>
<a class="sourceLine" id="cb301-4" data-line-number="4">      <span class="dt">ndraws =</span> <span class="dv">100</span></a>
<a class="sourceLine" id="cb301-5" data-line-number="5">    ),</a>
<a class="sourceLine" id="cb301-6" data-line-number="6">  <span class="dt">group =</span> df_eeg<span class="op">$</span>subj</a>
<a class="sourceLine" id="cb301-7" data-line-number="7">) <span class="op">+</span></a>
<a class="sourceLine" id="cb301-8" data-line-number="8"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Signal in the N400 spatiotemporal window&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postpreddensbysubj"></span>
<img src="bookdown_files/figure-html/postpreddensbysubj-1.svg" alt="The plot shows 100 predicted distributions with the label \(y_rep\) and the distribution of the average signal data with the label \(y\) density plots for the 37 subjects that participated in the experiment." width="672" />
<p class="caption">
FIGURE 5.15: The plot shows 100 predicted distributions with the label <span class="math inline">\(y_rep\)</span> and the distribution of the average signal data with the label <span class="math inline">\(y\)</span> density plots for the 37 subjects that participated in the experiment.
</p>
</div>
<p>Figure <a href="ch-hierarchical.html#fig:postpreddensbysubj">5.15</a> hints that we might be misfitting some subjects: Some of the by-subject observed distributions of the EEG signal averages look much tighter than their corresponding posterior predictive distributions (e.g., subjects 3, 5, 9, 10, 14), whereas some other by-subject observed distributions look wider (e.g., subjects 25, 26, 27). Another approach to examine whether we misfit the by-subject noise level is to plot posterior distributions of the standard deviations and compare them with the observed standard deviation. This is achieved in the following code, which groups the data by subject, and shows the distribution of standard deviations. The result is shown in Figure <a href="ch-hierarchical.html#fig:postpredsumbysubj">5.16</a>. It is clear now that, for some subjects, the observed standard deviation lies outside the distribution of predictive standard deviations.</p>

<div class="sourceCode" id="cb302"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb302-1" data-line-number="1"><span class="kw">pp_check</span>(fit_N400_sih,</a>
<a class="sourceLine" id="cb302-2" data-line-number="2">  <span class="dt">type =</span> <span class="st">&quot;stat_grouped&quot;</span>,</a>
<a class="sourceLine" id="cb302-3" data-line-number="3">  <span class="dt">ndraws =</span> <span class="dv">1000</span>,</a>
<a class="sourceLine" id="cb302-4" data-line-number="4">  <span class="dt">group =</span> <span class="st">&quot;subj&quot;</span>,</a>
<a class="sourceLine" id="cb302-5" data-line-number="5">  <span class="dt">stat =</span> <span class="st">&quot;sd&quot;</span></a>
<a class="sourceLine" id="cb302-6" data-line-number="6">)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postpredsumbysubj"></span>
<img src="bookdown_files/figure-html/postpredsumbysubj-1.svg" alt="Distribution of posterior predicted standard deviations in gray and observed standard deviation in black lines by subject." width="672" />
<p class="caption">
FIGURE 5.16: Distribution of posterior predicted standard deviations in gray and observed standard deviation in black lines by subject.
</p>
</div>
<p>Why is our “maximal” hierarchical model misfitting the by-subject distribution of data? This is because, the maximal models are, in general and implicitly, models with the maximal group-level effect structure for the location parameter (e.g., the mean, <span class="math inline">\(\mu\)</span>, in a normal model). Other parameters (e.g., scale or shape parameters) are estimated as auxiliary parameters, and are assumed to be constant across observations and clusters. This assumption is so common that researchers may not be aware that it is just an assumption. In the Bayesian framework, it is easy to change such default assumptions if necessary. Changing the assumption that all subjects have the same residual standard deviation leads to the distributional regression model. Such models can be fit in <code>brms</code> ; see also the brms vignette, <a href="https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html" class="uri">https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html</a>.</p>
<!--
https://web.archive.org/web/20191206093021/https://paul-buerkner.github.io/brms/articles/brms_distreg.html
-->
<p>We are going to change our previous likelihood, so that the scale, <span class="math inline">\(\sigma\)</span> has also a group-level effect structure. We exponentiate <span class="math inline">\(\sigma\)</span> to make sure that the negative adjustments do not cause <span class="math inline">\(\sigma\)</span> to become negative.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  signal_n &amp;\sim \mathit{Normal}(\alpha + u_{subj[n],1} + w_{item[n],1} + \\             &amp;  \hspace{2cm} c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma_n)\\
  \sigma_n &amp;= \exp(\sigma_\alpha + \sigma_{u_{subj[n]}})
\end{aligned}
\end{equation}\]</span></p>
<p>We just need to add priors to our new parameters (that replace the old prior for <span class="math inline">\(\sigma\)</span>). We set the prior to the intercept of the standard deviation, <span class="math inline">\(\sigma_\alpha\)</span>, to be similar to our previous <span class="math inline">\(\sigma\)</span>. For the variance component of <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\tau_{\sigma_u}\)</span>, we set rather uninformative hyperpriors. Recall that everything is exponentiated when it goes inside the likelihood; that is why we use <span class="math inline">\(\log(50)\)</span> rather than 50 in <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  \sigma_\alpha &amp;\sim \mathit{Normal}(0,log(50))\\
  \sigma_u &amp;\sim \mathit{Normal}(0, \tau_{\sigma_u}) \\
  \tau_{\sigma_u} &amp;\sim \mathit{Normal}_+(0, 5)
\end{aligned}
\end{equation}\]</span></p>
<p>This model can be fit in <code>brms</code> using the internal function <code>brmsformula()</code> (or its shorter alias <code>bf</code>). This is a powerful function that extends the formulas that we used so far allowing for setting a hierarchical regression to any parameter of a model. This will allow us to set a by-subject hierarchical structure to the parameter <span class="math inline">\(\sigma\)</span>. We also need to set new priors; these priors are identified by <code>dpar = sigma</code>.</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb303-1" data-line-number="1">prior_s &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb303-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb303-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb303-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb303-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor),</a>
<a class="sourceLine" id="cb303-6" data-line-number="6">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="kw">log</span>(<span class="dv">50</span>)), <span class="dt">class =</span> Intercept, <span class="dt">dpar =</span> sigma),</a>
<a class="sourceLine" id="cb303-7" data-line-number="7">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">5</span>),</a>
<a class="sourceLine" id="cb303-8" data-line-number="8">    <span class="dt">class =</span> sd, <span class="dt">group =</span> subj,</a>
<a class="sourceLine" id="cb303-9" data-line-number="9">    <span class="dt">dpar =</span> sigma</a>
<a class="sourceLine" id="cb303-10" data-line-number="10">  )</a>
<a class="sourceLine" id="cb303-11" data-line-number="11">)</a>
<a class="sourceLine" id="cb303-12" data-line-number="12">fit_N400_s &lt;-<span class="st"> </span><span class="kw">brm</span>(<span class="kw">brmsformula</span>(</a>
<a class="sourceLine" id="cb303-13" data-line-number="13">  n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb303-14" data-line-number="14">  sigma <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>subj)),</a>
<a class="sourceLine" id="cb303-15" data-line-number="15">  <span class="dt">prior =</span> prior_s, <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb303-16" data-line-number="16">)</a></code></pre></div>
<p>Inspect the output below; notice that our estimate for the effect of cloze
remains very similar to that of the model <code>fit_N400_sih</code>.</p>
<p>Compare the two models’ estimates:</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb304-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_N400_sih, <span class="dt">variable =</span> <span class="st">&quot;b_c_cloze&quot;</span>)</a></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## b_c_cloze     2.31     0.678 0.969  3.64</code></pre>
<div class="sourceCode" id="cb306"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb306-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_N400_s, <span class="dt">variable =</span> <span class="st">&quot;b_c_cloze&quot;</span>)</a></code></pre></div>
<pre><code>##           Estimate Est.Error Q2.5 Q97.5
## b_c_cloze     2.29     0.673 0.97  3.62</code></pre>
<p>Nonetheless, Figure <a href="ch-hierarchical.html#fig:postpreddensbysubj2">5.17</a> shows that the fit of the model with respect to the by-subject variability is much better than before. Furthermore, Figure <a href="ch-hierarchical.html#fig:postpredsumbysubj2">5.18</a> shows that the observed standard deviations for each subject are well inside the posterior predictive distributions. The code below produces Figure <a href="ch-hierarchical.html#fig:postpreddensbysubj2">5.17</a>.</p>

<div class="sourceCode" id="cb308"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb308-1" data-line-number="1"><span class="kw">ppc_dens_overlay_grouped</span>(df_eeg<span class="op">$</span>n400,</a>
<a class="sourceLine" id="cb308-2" data-line-number="2">  <span class="dt">yrep =</span></a>
<a class="sourceLine" id="cb308-3" data-line-number="3">    <span class="kw">posterior_predict</span>(fit_N400_s,</a>
<a class="sourceLine" id="cb308-4" data-line-number="4">      <span class="dt">ndraws =</span> <span class="dv">100</span></a>
<a class="sourceLine" id="cb308-5" data-line-number="5">    ),</a>
<a class="sourceLine" id="cb308-6" data-line-number="6">  <span class="dt">group =</span> df_eeg<span class="op">$</span>subj</a>
<a class="sourceLine" id="cb308-7" data-line-number="7">) <span class="op">+</span></a>
<a class="sourceLine" id="cb308-8" data-line-number="8"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Signal in the N400 spatiotemporal window&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postpreddensbysubj2"></span>
<img src="bookdown_files/figure-html/postpreddensbysubj2-1.svg" alt="The gray density plots show 100 predicted distributions from a model that includes a hierarchical structure for \(\sigma\). The black density plots show the distribution of the average signal data for the 37 subjects in the experiment." width="672" />
<p class="caption">
FIGURE 5.17: The gray density plots show 100 predicted distributions from a model that includes a hierarchical structure for <span class="math inline">\(\sigma\)</span>. The black density plots show the distribution of the average signal data for the 37 subjects in the experiment.
</p>
</div>

<div class="figure"><span style="display:block;" id="fig:postpredsumbysubj2"></span>
<img src="bookdown_files/figure-html/postpredsumbysubj2-1.svg" alt="The gray lines show the distributions of posterior predicted standard deviations from a model that includes a hierarchical structure for \(\sigma\), and observed mean standard deviations by subject (black vertical lines)." width="672" />
<p class="caption">
FIGURE 5.18: The gray lines show the distributions of posterior predicted standard deviations from a model that includes a hierarchical structure for <span class="math inline">\(\sigma\)</span>, and observed mean standard deviations by subject (black vertical lines).
</p>
</div>
<p>The model <code>fit_N400_s</code> raises the question: how much structure should we add to our statistical model? Should we also assume that <span class="math inline">\(\sigma\)</span> can vary by items, and also by our experimental manipulation? Should we also have a maximal model for <span class="math inline">\(\sigma\)</span>? Unfortunately, there are no clear answers that apply to every situation. The amount of complexity that we can introduce in a statistical model depends on (i) the answers we are looking for (we should include parameters that represent what we want to estimate), (ii) the size of the data at hand (more complex models require more data), (iii) our computing power (as the complexity increases models take increasingly long to converge and require more computer power to finish the computations in a feasible time frame), and (iv) our domain knowledge.</p>
<p>Ultimately, all models are approximations (that’s in the best case; often, they are plainly wrong) and we need to think carefully about which aspects of our data we have to account and which aspects we can abstract away from.</p>
<p>In the context of cognitive modeling, <span class="citation">McClelland (<a href="#ref-mcclellandPlaceModelingCognitive2009">2009</a>)</span> argues that models should not focus on a every single detail of the process they intend to explain. In order to understand a model, it needs to be simple enough. However, <span class="citation">McClelland (<a href="#ref-mcclellandPlaceModelingCognitive2009">2009</a>)</span> warns us that one must bear in mind that oversimplification does have an impact on what we can conclude from our analysis: A simplification can limit the phenomena that a model addresses, or can even lead to incorrect predictions. There is a continuum between purely statistical models (e.g., a linear regression) and computational cognitive models. For example, we can define “hybrid” models such as the linear ballistic accumulator <span class="citation">(Brown and Heathcote <a href="#ref-brownSimplestCompleteModel2008">2008</a>; and see Nicenboim <a href="#ref-Nicenboim2018StanCon">2018</a> for an implementation in Stan)</span>, where a great deal of cognitive detail is sacrificed for tractability. The conclusions of <span class="citation">McClelland (<a href="#ref-mcclellandPlaceModelingCognitive2009">2009</a>)</span> apply to any type of model in cognitive science: “Simplification is essential, but it comes at a cost, and real understanding depends in part on understanding the effects of the simplification”.</p>
</div>
</div>
<div id="sec-stroop" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.3</span> A hierarchical log-normal model: The Stroop effect<a href="ch-hierarchical.html#sec-stroop" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Next, using data from <span class="citation">Ebersole et al. (<a href="#ref-ManyLabs3">2016</a>)</span>, we illustrate some of the issues that arise with a log-normal likelihood in a hierarchical model. The data are from a Stroop task <span class="citation">(Stroop <a href="#ref-stroop1935studies">1935</a>; for a review, see MacLeod <a href="#ref-macleod1991half">1991</a>)</span>. We will analyze a subset of the data of 3337 subjects that participated in one variant of the Stroop task; this was part of a battery of tasks run in <span class="citation">Ebersole et al. (<a href="#ref-ManyLabs3">2016</a>)</span>.</p>
<p>For this variant of the Stroop task, subjects were presented with one word at the center of the screen (“red”, “blue”, or “green”). The word was written in either red, blue, or green color. In one third of the trials, the word matched the color of the text (“congruent” condition); and in the rest of the trials it did not match (“incongruent” condition). Subjects were instructed to only pay attention to the color that the word was written in, and press <code>1</code> if the color was red, <code>2</code> if it was blue, and <code>3</code> if it was green. In the incongruent condition, it is difficult to identify the color when it mismatches the word that is written on the screen. For example, it is hard to respond that the color is blue if the word written on the screen is green but the color it is presented in is blue; naming the color blue here is difficult in comparison to a baseline condition (the congruent condition), in which the word green appears in the color green. This increased difficulty in the incongruent condition is called the Stroop effect; the effect is extremely robust across variations in the task.</p>
<p>This task yields two measures: the accuracy of the decision made, and the time it took to respond. For the Stroop task, accuracy is usually almost at ceiling; to simplify the model, we will ignore accuracy. For a cognitive model that incorporates accuracy and response times into a model to analyze these Stroop data, see <span class="citation">Nicenboim (<a href="#ref-Nicenboim2018StanCon">2018</a>)</span>.</p>
<div id="a-correlated-varying-intercept-varying-slopes-log-normal-model" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.3.1</span> A correlated varying intercept varying slopes log-normal model<a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If our theory only focuses on the difference between the response times for the “congruent” vs. “incongruent” condition, we can ignore the actual color presented and the word that was written. We can simply focus on whether a trial was congruent or incongruent. Define a predictor <code>c_cond</code> to represent these two conditions. For simplicity, we will also assume that all subjects share the same variance (as we saw in section <a href="ch-hierarchical.html#sec-distrmodel">5.2.6</a>, changing this assumption leads to distributional regression models).</p>
<p>The above assumptions mean that we are going to fit the data with the following likelihood. The likelihood function is identical to the one that we fit in section <a href="ch-hierarchical.html#sec-mcvivs">5.2.4</a>, except that here the location and scale are embedded in a log-normal likelihood rather than a normal likelihood. Equation <a href="ch-hierarchical.html#eq:stroopmodel">(5.2)</a> states that we are dealing with a hierarchical model with by-subjects varying intercepts and varying slopes model:</p>
<p><span class="math display" id="eq:stroopmodel">\[\begin{equation}
  rt_n \sim \mathit{LogNormal}(\alpha + u_{subj[n],1}  + c\_cond_n \cdot  (\beta + u_{subj[n],2}), \sigma)
\tag{5.2}
\end{equation}\]</span></p>
<p>In chapter <a href="ch-contr.html#ch-contr">8</a>, we will discuss the sum-contrast coding of the two conditions (<code>c_cond</code>). For now, it suffices to say that we assign a <code>+1</code> to <code>c_cond</code> for the “incongruent” condition, and a <code>-1</code> for the “congruent” condition (i.e., a sum-contrast coding). Under this contrast coding, if the posterior mean of the parameter <span class="math inline">\(\beta\)</span> turns out to be positive, that would mean that the model predicts that the incongruent condition has slower reaction times than the congruent one. This is because on average the location of the log-normal likelihood for each condition will be as follows. In Equation <a href="ch-hierarchical.html#eq:stroopmodelmeans">(5.3)</a>, <span class="math inline">\(\mu_{incongruent}\)</span> refers to the location of the incongruent condition, and <span class="math inline">\(\mu_{congruent}\)</span> to the location of the congruent condition.</p>
<p><span class="math display" id="eq:stroopmodelmeans">\[\begin{equation}
\begin{aligned}
  \mu_{incongruent} &amp;= \alpha + 1 \cdot  \beta \\
  \mu_{congruent} &amp;= \alpha + -1 \cdot  \beta
  \end{aligned}
  \tag{5.3}
\end{equation}\]</span></p>
<p>We could have chosen to do the opposite contrast coding assignments: <span class="math inline">\(-1\)</span> for the incongruent condition, and <span class="math inline">\(+1\)</span> for congruent condition. In that case, if the posterior mean of the parameter <span class="math inline">\(\beta\)</span> turns out to be positive, that would mean that the incongruent condition has a <em>faster</em> reaction time than the congruent condition. Given that the Stroop effect is very robust, we do not expect such an outcome. In order to make the <span class="math inline">\(\beta\)</span> parameter easier to interpret, we have chosen the contrast coding where a positive sign on the mean of <span class="math inline">\(\beta\)</span> implies that the inconguent condition has slower reaction times.</p>
<p>As always, we need priors for all the parameters in our model. For the population-level parameters (or fixed effects), we use the same priors as we did when we were fitting a regression with a log-normal likelihood in section <a href="ch-compbda.html#sec-lognormal">3.6.3</a>.</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
   \alpha &amp; \sim \mathit{Normal}(6, 1.5) \\
   \beta  &amp; \sim \mathit{Normal}(0, 0.01) \\
    \sigma  &amp;\sim \mathit{Normal}_+(0, 1)
 \end{aligned}
 \end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\beta\)</span> represents, on the log scale, the change in the intercept <span class="math inline">\(\alpha\)</span> as a function of the experimental manipulation. In this model, <span class="math inline">\(\beta\)</span> will probably be larger in magnitude than for the model that examined the difference in pressing the spacebar for two consecutive trials in section <a href="ch-compbda.html#sec-lognormal">3.6.3</a>. We might need to examine the prior for <span class="math inline">\(\beta\)</span> with predictive distributions, but we will delay this for now.</p>
<p>In contrast to our previous models, the intercept <span class="math inline">\(\alpha\)</span> is not the grand mean of the location. This is because the conditions were not balanced in the experiment (one-third of the conditions were congruent and two-thirds incongruent). The intercept could be interpreted here as the time (in log-scale) it takes to respond if we ignore the experimental manipulation.<br />
Next, we turn our attention to the prior specification for the group-level parameters (or random effects). If we assume a possible correlation between by-subject intercepts and slopes, our model will have the following structure. In particular, we have to define priors for the parameters in the variance-covariance matrix <span class="math inline">\(\Sigma_u\)</span>.</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
    {\begin{pmatrix}
    u_{i,1} \\
    u_{i,2}
    \end{pmatrix}}
   &amp;\sim {\mathcal {N}}
    \left(
   {\begin{pmatrix} 
    0\\
    0
   \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right) 
 \end{aligned}
 \end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
 \boldsymbol{\Sigma_u} &amp; = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_2} &amp; \tau_{u_2}^2
\end{pmatrix}}
\end{aligned}
\end{equation}\]</span></p>
<p>In practice, this means that we need priors for the by-subject standard deviations and correlations. For the variance components, we will set a similar prior as for <span class="math inline">\(\sigma\)</span>. We don’t expect the by-group adjustments to the intercept and slope to have more variance than the within-subject variance, so this prior will be quite conservative because it allows for a large range of prior uncertainty. We assign the same prior for the correlations as we did in section <a href="ch-hierarchical.html#sec-sih">5.2.5</a>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim \mathit{Normal}_+(0,1)\\
\tau_{u_2} &amp;\sim \mathit{Normal}_+(0,1)\\
\rho_u &amp;\sim \mathit{LKJcorr}(2) 
\end{aligned}
\end{equation}\]</span></p>
<p>We are now ready to fit the model. To speed up computation, we subset 50 subjects of the original data set; both the subsetted data and the original data set can be found in the package <code>bcogsci</code>. If we were analyzing these data for publication in a journal article or the like, we would obviously not subset the data.</p>
<p>We restrict ourselves to the correct trials only, and add a <code>c_cond</code> predictor, sum-coded as described earlier.</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb309-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_stroop&quot;</span>)</a>
<a class="sourceLine" id="cb309-2" data-line-number="2">(df_stroop &lt;-<span class="st"> </span>df_stroop <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb309-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cond =</span> <span class="kw">if_else</span>(condition <span class="op">==</span><span class="st"> &quot;Incongruent&quot;</span>, <span class="dv">1</span>, <span class="dv">-1</span>)))</a></code></pre></div>
<pre><code>## # A tibble: 3,058 × 5
##    subj trial condition      RT c_cond
##   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt;
## 1     1     0 Congruent    1484     -1
## 2     1     1 Incongruent  1316      1
## 3     1     2 Incongruent   628      1
## # … with 3,055 more rows</code></pre>
<p>Fit the model. <!-- with 4000 iterations rather than with the default 2000 iterations by chain. If we were to run the model with the default number of iterations, the following warning would appear:  `Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and media Running the chains for more iterations may help. See http://mc-stan.org/misc/warnings.html#bulk-ess`. --></p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb311-1" data-line-number="1">fit_stroop &lt;-<span class="st"> </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">|</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb311-2" data-line-number="2">  <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb311-3" data-line-number="3">  <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb311-4" data-line-number="4">    <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb311-5" data-line-number="5">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb311-6" data-line-number="6">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">.01</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb311-7" data-line-number="7">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb311-8" data-line-number="8">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb311-9" data-line-number="9">      <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb311-10" data-line-number="10">    ),</a>
<a class="sourceLine" id="cb311-11" data-line-number="11">  <span class="dt">data =</span> df_stroop</a>
<a class="sourceLine" id="cb311-12" data-line-number="12">)</a></code></pre></div>
<p>We will focus on <span class="math inline">\(\beta\)</span> (but you can verify that there is nothing surprising in the other parameters in the model <code>fit_stroop</code> ).</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb312-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_stroop, <span class="dt">variable =</span> <span class="st">&quot;b_c_cond&quot;</span>)</a></code></pre></div>
<pre><code>##          Estimate Est.Error   Q2.5  Q97.5
## b_c_cond   0.0271    0.0054 0.0165 0.0374</code></pre>
<p>As shown in Figure <a href="ch-hierarchical.html#fig:priorposteriordiscrepancy">5.19</a>,
if we overlay the density plots for the prior and posterior distributions of <span class="math inline">\(\beta\)</span>, it becomes evident that the prior might have been too restrictive: the posterior is relatively far from the prior, and the prior strongly down-weights the values that the posterior is centered around. Such a strong discrepancy between the prior and posterior can be investigated with a sensitivity analysis.</p>

<div class="sourceCode" id="cb314"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb314-1" data-line-number="1">sample_b_post &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_stroop)<span class="op">$</span>b_c_cond</a>
<a class="sourceLine" id="cb314-2" data-line-number="2"><span class="co"># We generate samples from the prior as well:</span></a>
<a class="sourceLine" id="cb314-3" data-line-number="3">N &lt;-<span class="st"> </span><span class="kw">length</span>(sample_b_post)</a>
<a class="sourceLine" id="cb314-4" data-line-number="4">sample_b_prior &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dv">0</span>, <span class="fl">.01</span>)</a>
<a class="sourceLine" id="cb314-5" data-line-number="5">samples &lt;-<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb314-6" data-line-number="6">  <span class="dt">sample =</span> <span class="kw">c</span>(sample_b_post, sample_b_prior),</a>
<a class="sourceLine" id="cb314-7" data-line-number="7">  <span class="dt">distribution =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;posterior&quot;</span>, N), <span class="kw">rep</span>(<span class="st">&quot;prior&quot;</span>, N))</a>
<a class="sourceLine" id="cb314-8" data-line-number="8">)</a>
<a class="sourceLine" id="cb314-9" data-line-number="9"><span class="kw">ggplot</span>(samples, <span class="kw">aes</span>(<span class="dt">x =</span> sample, <span class="dt">fill =</span> distribution)) <span class="op">+</span></a>
<a class="sourceLine" id="cb314-10" data-line-number="10"><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> <span class="fl">.5</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:priorposteriordiscrepancy"></span>
<img src="bookdown_files/figure-html/priorposteriordiscrepancy-1.svg" alt="The discrepancy between the prior and the posterior distributions for the slope parameter in the model fit_stroop." width="672" />
<p class="caption">
FIGURE 5.19: The discrepancy between the prior and the posterior distributions for the slope parameter in the model <code>fit_stroop</code>.
</p>
</div>
<div id="sensitivity-analysis" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.3.1.1</span> Sensitivity analysis<a href="ch-hierarchical.html#sensitivity-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Here, the discrepancy evident in Figure <a href="ch-hierarchical.html#fig:priorposteriordiscrepancy">5.19</a> is investigated with a sensitivity analysis. We will examine what happens for the following priors for <span class="math inline">\(\beta\)</span>. In the models we fit below, all the other parameters have the same priors as in the model <code>fit_stroop</code>; we vary only the priors for <span class="math inline">\(\beta\)</span>.
The different priors are:</p>
<ul>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0,0.05)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0,0.1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0,1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0,2)\)</span></li>
</ul>
<p>We can summarize the estimates of <span class="math inline">\(\beta\)</span> given different priors as shown in Table <a href="ch-hierarchical.html#tab:priorsslopesmeansCIs">5.1</a>.</p>
<table>
<caption>
<span id="tab:priorsslopesmeansCIs">TABLE 5.1: </span>The summary (mean and 95% credible interval) for the posterior distribution of the slope in the model fit_stroop, given different priors on the slope parameter.
</caption>
<thead>
<tr>
<th style="text-align:left;">
prior
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Q2.5
</th>
<th style="text-align:right;">
Q97.5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 0.001)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.001\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(-0.001\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.003\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 0.01)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.027\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.016\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.037\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 0.05)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.037\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.025\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.049\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 0.1)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.037\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.025\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.049\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 1)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.037\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.025\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.049\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 2)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.038\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.026\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.050\)</span>
</td>
</tr>
</tbody>
</table>
<p>It might be easier to see how much the posterior difference between conditions changes depending on the prior. In order to answer this question, we need to remember that the median difference between conditions (<span class="math inline">\(\mathit{MedianRT}_{diff}\)</span>) can be calculated as the difference between the exponents of each condition’s medians:</p>
<p><span class="math display" id="eq:medianrt">\[\begin{equation}
\begin{aligned}
\mathit{MedianRT}_{diff} &amp;= \mathit{MedianRT}_{incongruent} - \mathit{MedianRT}_{congruent}\\
\mathit{MedianRT}_{diff} &amp;= \exp(\alpha + \beta) - \exp(\alpha - \beta)
\end{aligned}
\tag{5.4}
\end{equation}\]</span></p>
<p>Equation <a href="ch-hierarchical.html#eq:medianrt">(5.4)</a> gives us the posterior distributions of the median difference between conditions for the different models. We calculate the median difference rather than the mean difference because the mean depends on the parameter <span class="math inline">\(\sigma\)</span>, but the median doesn’t: The mean of a log-normal distribution is <span class="math inline">\(\exp(\mu +\sigma ^{2}/2)\)</span>, and the median is simply <span class="math inline">\(\exp(\mu)\)</span>; see also <a href="ch-compbda.html#sec-lognormal">3.6.3</a>.</p>
<p>Table <a href="ch-hierarchical.html#tab:meanrtdiffsummary">5.2</a> summarizes the posterior distributions under different priors using the means of the difference in medians, along with 95% credible intervals. It’s important to realize that the use of mean to summarize the posterior distribution is orthogonal to our use of the median to summarize the response times by condition: In the first case, we use the median to summarize a group of <em>observations</em>, and in the second case, we use the mean to summarize a group of <em>samples</em> from the posterior–we could have summarized the samples from the posterior with its median instead of the mean.</p>
<table>
<caption>
<span id="tab:meanrtdiffsummary">TABLE 5.2: </span>A summary, under a range of priors, of the posterior distributions of the mean difference between the two conditions, back-transformed to the millisecond scale.
</caption>
<thead>
<tr>
<th style="text-align:left;">
prior
</th>
<th style="text-align:right;">
mean diff (ms)
</th>
<th style="text-align:right;">
Q2.5
</th>
<th style="text-align:right;">
Q97.5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 0.001)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(0.68\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(-1.50\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(2.91\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 0.01)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(30.08\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(17.06\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(42.04\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 0.05)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(41.54\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(27.81\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(55.65\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 0.1)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(42.18\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(28.34\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(55.81\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 1)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(42.16\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(28.33\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(56.21\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathit{Normal}(0, 2)\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(42.18\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(28.70\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(55.78\)</span>
</td>
</tr>
</tbody>
</table>
<p>Table <a href="ch-hierarchical.html#tab:meanrtdiffsummary">5.2</a> shows us that the posterior changes substantially when we use wider priors. It seems that the posterior is relatively unaffected when we use priors with a standard deviation larger than <span class="math inline">\(0.05\)</span>. However, if we assume a priori that the effect of the manipulation must be small, we will end up obtaining a posterior that is consistent with that belief. When we include less information about the possible effect sizes by using a less informative prior, we allow the data to influence the posterior more. A sensitivity analysis is always an important component of a good-quality Bayesian analysis.</p>
<p>Which analysis should one report after carrying out a sensitivity analysis? In the above example, the priors ranging from <span class="math inline">\(\mathit{Normal}(0,0.05)\)</span> to <span class="math inline">\(\mathit{Normal}(0,2)\)</span> show rather similar posterior distributions for the mean difference. The most common approach in Bayesian analysis is to report the results of such relatively uninformative priors (e.g., one could report the posterior associated with the <span class="math inline">\(\mathit{Normal}(0,2)\)</span> here), because this kind of prior allows for a broader range of possible effects and is relatively agnostic. However, if there is a good reason to use a prior from a previous analysis, then of course it makes sense to report the analysis with the informative prior alongside an analysis with an uninformative prior. Reporting only informative priors in a Bayesian analysis is generally not a good idea. The issue is transparency: the reader should know what the posterior looks like for both an informative and an uninformative prior.</p>
<p>Another situation where posterior distributions associated with multiple priors should be reported is when one is carrying out an adversarial sensitivity analysis <span class="citation">(Spiegelhalter, Abrams, and Myles <a href="#ref-spiegelhalter2004bayesian">2004</a>)</span>: one can take a group of agnostic, enthusiastic, and adversarial or skeptical priors that, respectively, reflect a non-committal a priori position, an informed position based on the researcher’s prior beliefs, and an adversarial position based on a scientific opponent’s beliefs. In such a situation, analyses using all three priors can be reported, so that the reader can determine how different prior beliefs influence the posterior. For an example of such an adversarial analysis, see <span class="citation">Vasishth and Engelmann (<a href="#ref-VasishthEngelmann2022">2022</a>)</span>. Finally, when carrying out hypothesis testing using Bayes factors, the choice of the prior on the parameter of interest becomes critically important; in that situation, it is very important to report a sensitivity analysis, showing the Bayes factor as well as a summary of the posterior distributions <span class="citation">(Schad et al. <a href="#ref-SchadEtAlBF">2021</a>)</span>; we return to this point in chapter <a href="ch-bf.html#ch-bf">15</a>, which covers Bayes factors.</p>
</div>
</div>
</div>
<div id="why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.4</span> Why fitting a Bayesian hierarchical model is worth the effort<a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Carrying out Bayesian data analysis clearly requires much more effort than fitting a frequentist model: we have to define priors, verify that our model works, and decide how to interpret the results. By comparison, fitting a linear mixed model using <code>lme4</code> consists of only a single line of code. But there is a hidden cost to the relatively high speed furnished by the functions such as <code>lmer</code>. First, the model fit using <code>lmer</code> or the like makes many assumptions, but they are hidden from the user. This is not a problem for the knowledgeable modeler, but very dangerous for the naive user. A second conceptual problem is that the way frequentist models are typically used is to answer a binary question: is the effect “significant” or not? If a result is significant, the paper is considered worth publishing; if not, it is not. Although frequentist models can quickly answer the question that the null hypothesis test poses, the frequentist test answers the wrong question. For discussion, see <span class="citation">Vasishth and Nicenboim (<a href="#ref-VasishthNicenboimStatMeth">2016</a>)</span>.</p>
<p>Nevertheless, it is natural to ask why one should bother to go through all the trouble of fitting a Bayesian model. An important reason is the flexibility in model specification. The approach we have presented here can be used to extend essentially any parameter of any model. This includes popular uses, such as logistic and Poisson regressions, and also useful models that are relatively rarely used in cognitive science, such as multi-logistic regression <span class="citation">(e.g., accuracy in some task with more than two answers), ordered logistic (e.g., ratings, Bürkner and Vuorre <a href="#ref-burkner2018ordinal">2018</a>)</span>, models with a shifted log-normal distribution <span class="citation">(see exercise <a href="ch-custom.html#exr:shiftedlogn">12.1</a> and chapter <a href="ch-lognormalrace.html#ch-lognormalrace">20</a> which deals with a log-normal race mode, and see Nicenboim, Logačev, et al. <a href="#ref-NicenboimEtAl2016Frontiersb">2016</a>; Rouder <a href="#ref-Rouder2005">2005</a>)</span>, and distributional regression models (as shown in <a href="ch-hierarchical.html#sec-distrmodel">5.2.6</a>). By contrast, a frequentist model, although easy to fit quickly, forces the user to use an inflexible canned model, which may not necessarily make sense for their data.</p>
<p>This flexibility allows us also to go beyond the statistical models discussed before, and to develop complex hierarchical computational process models that are tailored to specific phenomena. An example are computational cognitive models, these can be extended hierarchically in a straightforward way, see <span class="citation">Lee (<a href="#ref-Lee2011">2011</a><a href="#ref-Lee2011">b</a>)</span> and <span class="citation">Lee and Wagenmakers (<a href="#ref-LeeWagenmakers2014">2014</a>)</span>. This is because, as we have seen with distributional regression models in section <a href="ch-hierarchical.html#sec-distrmodel">5.2.6</a>, any parameter can have a group-level effect structure. Some examples of hierarchical computational cognitive models in psycholinguistics are <span class="citation">Logačev and Vasishth (<a href="#ref-LogacevVasishth2015">2016</a>)</span>, <span class="citation">Nicenboim and Vasishth (<a href="#ref-nicenboimModelsRetrievalSentence2018">2018</a>)</span>, <span class="citation">Vasishth et al. (<a href="#ref-VasishthEtAl2017Modelling">2017</a>)</span>, <span class="citation">Vasishth, Jaeger, and Nicenboim (<a href="#ref-VasishthEtAl2017Feature">2017</a>)</span>, <span class="citation">Lissón et al. (<a href="#ref-lisson_2020">2021</a>)</span>, <span class="citation">Logačev and Dokudan (<a href="#ref-logacev-dokudan-2021-multinomial">2021</a>)</span>, <span class="citation">Paape et al. (<a href="#ref-PaapeEtAlMPT2020">2021</a>)</span>, <span class="citation">Yadav, Smith, and Vasishth (<a href="#ref-yadavencret2021">2021</a><a href="#ref-yadavencret2021">a</a>)</span>, and <span class="citation">Yadav, Smith, and Vasishth (<a href="#ref-yadaviccm2021">2021</a><a href="#ref-yadaviccm2021">b</a>)</span>. The hierarchical Bayesian modeling approach can even be extended to process models that cannot be expressed as a likelihood function, although in such cases one may have to write one’s own sampler; for an example from psycholinguistics, see <span class="citation">Yadav et al. (<a href="#ref-yadavindiff2021">2021</a>)</span>.<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>. We discuss and implement in Stan some relatively simple computational cognitive models in chapters <a href="ch-cogmod.html#ch-cogmod">17</a>-<a href="ch-lognormalrace.html#ch-lognormalrace">20</a>.</p>
</div>
<div id="summary-4" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.5</span> Summary<a href="ch-hierarchical.html#summary-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter presents two very commonly used classes of hierarchical model: those with normal and log-normal likelihoods. We saw several common variants of such models: varying intercepts, varying intercepts and varying slopes with or without a correlation parameter, and crossed random effects for subjects and items. We also experienced the flexibility of the Stan modeling framework through the example of a model that assumes a different residual standard deviation for each subject.</p>
</div>
<div id="further-reading-2" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.6</span> Further reading<a href="ch-hierarchical.html#further-reading-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Chapter 5 of <span class="citation">Gelman et al. (<a href="#ref-Gelman14">2014</a>)</span> provides a rather technical but complete treatment of exchangeability in Bayesian hierarchical models. <span class="citation">Bernardo and Smith (<a href="#ref-bernardosmith">2009</a>)</span> is a brief but useful article explaining exchangeability, and <span class="citation">Lunn et al. (<a href="#ref-lunn2012bugs">2012</a>)</span> also has a helpful discussion that we have drawn on in this chapter. <span class="citation">Gelman and Hill (<a href="#ref-GelmanHill2007">2007</a>)</span> is a comprehensive treatment of hierarchical modeling, although it uses WinBUGS. <span class="citation">Yarkoni (<a href="#ref-yarkoni_2020">2020</a>)</span> discusses the importance of modeling variability in variables that researchers clearly intend to generalize over (e.g., stimuli, tasks, or research sites), and how under-specification of population-level (or random) effects imposes strong constraints on the generalizability of results. <span class="citation">Sorensen, Hohenstein, and Vasishth (<a href="#ref-SorensenVasishthTutorial">2016</a>)</span> provides an introduction, using Stan, to the Laird-Ware style matrix formulation <span class="citation">(Laird and Ware <a href="#ref-laird1982random">1982</a>)</span> of hierarchical models; this formulation has the advantage of flexibility and efficiency when specifying models in Stan syntax.</p>
</div>
<div id="sec-HLMexercises" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.7</span> Exercises<a href="ch-hierarchical.html#sec-HLMexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:hierarchical-normal" class="exercise"><strong>Exercise 5.1  </strong></span>A hierarchical model (normal likelihood) of cognitive load on pupil size.</p>
</div>
<p>As in section <a href="ch-reg.html#sec-pupil">4.1</a>, we focus on the effect of cognitive load on pupil size, but this time we look at all the subjects of <span class="citation">Wahn et al. (<a href="#ref-wahnPupilSizesScale2016">2016</a>)</span>:</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb315-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_pupil_complete&quot;</span>)</a>
<a class="sourceLine" id="cb315-2" data-line-number="2">df_pupil_complete</a></code></pre></div>
<pre><code>## # A tibble: 2,228 × 4
##    subj trial  load p_size
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
## 1   701     1     2  1021.
## 2   701     2     1   951.
## 3   701     3     5  1064.
## # … with 2,225 more rows</code></pre>
<p>You should be able to now fit a “maximal” model (correlated varying intercept and slopes for subjects) assuming a normal likelihood. Base your priors in the priors discussed in section <a href="ch-reg.html#sec-pupil">4.1</a>.</p>
<ol style="list-style-type: lower-alpha">
<li>Examine the effect of load on pupil size, and the average pupil size. What do you conclude?</li>
<li>Do a sensitivity analysis for the prior on the intercept (<span class="math inline">\(\alpha\)</span>). What is the estimate of the effect (<span class="math inline">\(\beta\)</span>) under different priors?</li>
<li>Is the effect of load consistent across subjects? Investigate this visually.</li>
</ol>
<div class="exercise">
<p><span id="exr:hierarchical-logn" class="exercise"><strong>Exercise 5.2  </strong></span>Are subject relatives easier to process than object relatives (log-normal likelihood)?</p>
</div>
<p>We begin with a classic question from the psycholinguistics literature: Are subject relatives easier to process than object relatives? The data come from Experiment 1 in a paper by <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span>.</p>
<p><em>Scientific question</em>: Is there a subject relative advantage in reading?</p>
<p><span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> investigate an old claim in psycholinguistics that object relative clause (ORC) sentences are more difficult to process than subject relative clause (SRC) sentences. One explanation for this predicted difference is that the distance between the relative clause verb (<em>sent</em> in the example below) and the head noun phrase of the relative clause (<em>reporter</em> in the example below) is longer in ORC vs. SRC. Examples are shown below. The relative clause is shown in square brackets.</p>
<p>(1a) The <em>reporter</em> [who the photographer <em>sent</em> to the editor] was hoping for a good story. (ORC)</p>
<p>(1b) The <em>reporter</em> [who <em>sent</em> the photographer to the editor] was hoping for a good story. (SRC)</p>
<p>The underlying explanation has to do with memory processes: Shorter linguistic dependencies are easier to process due to either reduced interference or decay, or both. For implemented computational models that spell this point out, see <span class="citation">Lewis and Vasishth (<a href="#ref-lewisvasishth:cogsci05">2005</a>)</span> and <span class="citation">Engelmann, Jäger, and Vasishth (<a href="#ref-EngelmannJaegerVasishth2019">2020</a>)</span>.</p>
<p>In the Grodner and Gibson data, the dependent measure is reading time at the relative clause verb, (e.g., <em>sent</em>) of different sentences with either ORC or SRC. The dependent variable is in milliseconds and was measured in a self-paced reading task. Self-paced reading is a task where subjects read a sentence or a short text word-by-word or phrase-by-phrase, pressing a button to get each word or phrase displayed; the preceding word disappears every time the button is pressed. In <a href="ch-priors.html#sec-simpleexamplepriors">6.1</a>, we provide a more detailed explanation of this experimental method.</p>
<p>For this experiment, we are expecting longer reading times at the relative clause verbs of ORC sentences in comparison to the relative clause verb of SRC sentences.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb317-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_gg05_rc&quot;</span>)</a>
<a class="sourceLine" id="cb317-2" data-line-number="2">df_gg05_rc</a></code></pre></div>
<pre><code>## # A tibble: 672 × 7
##    subj  item condition    RT residRT qcorrect experiment
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;     
## 1     1     1 objgap      320   -21.4        0 tedrg3    
## 2     1     2 subjgap     424    74.7        1 tedrg2    
## 3     1     3 objgap      309   -40.3        0 tedrg3    
## # … with 669 more rows</code></pre>
<p>You should use a sum coding for the predictors. Here, object relative clauses (<code>&quot;objgaps&quot;</code>) are coded <span class="math inline">\(+1\)</span>, subject relative clauses
<span class="math inline">\(-1\)</span>.</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb319-1" data-line-number="1">df_gg05_rc &lt;-<span class="st"> </span>df_gg05_rc <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb319-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cond =</span> <span class="kw">if_else</span>(condition <span class="op">==</span><span class="st"> &quot;objgap&quot;</span>, <span class="dv">1</span>, <span class="dv">-1</span>))</a></code></pre></div>
<p>You should be able to now fit a “maximal” model (correlated varying intercept and slopes for subjects and for items) assuming a log-normal likelihood.</p>
<ol style="list-style-type: lower-alpha">
<li>Examine the effect of relative clause attachment site (the predictor <code>c_cond</code>) on reading times <code>RT</code> (<span class="math inline">\(\beta\)</span>).</li>
<li>Estimate the median difference between relative clause attachment sites in milliseconds, and report the mean and 95% CI.</li>
<li>Do a sensitivity analysis. What is the estimate of the effect (<span class="math inline">\(\beta\)</span>) under different priors? What is the difference in milliseconds between conditions under different priors?</li>
</ol>
<div class="exercise">
<p><span id="exr:HLMExerciseMandarinRC" class="exercise"><strong>Exercise 5.3  </strong></span>Relative clause processing in Mandarin Chinese</p>
</div>
<p>Load the following two data sets:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb320-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_gibsonwu&quot;</span>)</a>
<a class="sourceLine" id="cb320-2" data-line-number="2"><span class="kw">data</span>(<span class="st">&quot;df_gibsonwu2&quot;</span>)</a></code></pre></div>
<p>The data are taken from two experiments that investigate (inter alia) the effect of relative clause type on reading time in Chinese. The data are from <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span> and <span class="citation">Vasishth et al. (<a href="#ref-VasishthetalPLoSOne2013">2013</a>)</span> respectively. The second data set is a direct replication attempt of the <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span> experiment.</p>
<p>Chinese relative clauses are interesting theoretically because they are prenominal: the relative clause appears before the head noun. For example, the English relative clauses shown above would appear in the following order in Mandarin. The square brackets mark the relative clause, and REL refers to the Chinese equivalent of the English relative pronoun <em>who</em>.</p>
<p>(2a) [The photographer <em>sent</em> to the editor] REL the <em>reporter</em> was hoping for a good story. (ORC)</p>
<p>(2b) [<em>sent</em> the photographer to the editor] REL the <em>reporter</em> who was hoping for a good story. (SRC)</p>
<p>As discussed in <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span>, the consequence of Chinese relative clauses being prenominal is that the distance between the verb in relative clause and the head noun is larger in subject relatives than object relatives. <span class="citation">Hsiao and Gibson (<a href="#ref-hsiao03">2003</a>)</span> were the first to suggest that the larger distance in subject relatives leads to longer reading time at the head noun. Under this view, the prediction is that subject relatives are harder to process than object relatives. If this is true, this is interesting and surprising because in most other languages that have been studied, subject relatives are easier to process than object relatives; so Chinese will be a very unusual exception cross-linguistically.</p>
<p>The data provided are for the critical region (the head noun; here, <em>reporter</em>). The experiment method is self-paced reading, so we have reading times in milliseconds. The second data set is a direct replication attempt of the first data set, which is from <span class="citation">Gibson and Wu (<a href="#ref-gibsonwu">2013</a>)</span>.</p>
<p>The research hypothesis is whether the difference in reading times between object and subject relative clauses is negative. For the first data set (<code>df_gibsonwu</code>), investigate this question by fitting two “maximal” hierarchical models (correlated varying intercept and slopes for subjects and items). The dependent variable in both models is the raw reading time in milliseconds. The first model should use the normal likelihood in the model; the second model should use the log-normal likelihood. In both models, use <span class="math inline">\(\pm 0.5\)</span> sum coding to model the effect of relative clause type. You will need to decide on appropriate priors for the various parameters.</p>
<ol style="list-style-type: lower-alpha">
<li>Plot the posterior predictive distributions from the two models. What is the difference in the posterior predictive distributions of the two models; and why is there a difference?</li>
<li>Examine the posterior distributions of the effect estimates (in milliseconds) in the two models. Why are these different?</li>
<li>Given the posterior predictive distributions you plotted above, why is the log-normal likelihood model better for carrying out inference and hypothesis testing?</li>
</ol>
<p>Next, work out a normal approximation of the log-normal model’s posterior distribution for the relative clause effect that you obtained from the above data analysis. Then use that normal approximation as an informative prior for the slope parameter when fitting a hierarchical model to the second data set. This is an example of incrementally building up knowledge by successively using a previous study’s posterior as a prior for the next study; this is essentially equivalent to pooling both data sets (check that pooling the data and using a Normal(0,1) prior for the effect of interest, with a log-normal likelihood, gives you approximately the same posterior as the informative-prior model fit above).</p>
<div class="exercise">
<p><span id="exr:HLMExerciseEnglishAgrmt" class="exercise"><strong>Exercise 5.4  </strong></span>Agreement attraction in comprehension</p>
</div>
<p>Load the following data:</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb321-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_dillonE1&quot;</span>)</a>
<a class="sourceLine" id="cb321-2" data-line-number="2">dillonE1 &lt;-<span class="st"> </span>df_dillonE1</a>
<a class="sourceLine" id="cb321-3" data-line-number="3"><span class="kw">head</span>(dillonE1)</a></code></pre></div>
<pre><code>##         subj       item   rt int     expt
## 49 dillonE11 dillonE119 2918 low dillonE1
## 56 dillonE11 dillonE119 1338 low dillonE1
## 63 dillonE11 dillonE119  424 low dillonE1
## 70 dillonE11 dillonE119  186 low dillonE1
## 77 dillonE11 dillonE119  195 low dillonE1
## 84 dillonE11 dillonE119 1218 low dillonE1</code></pre>
<p>The data are taken from an experiment that investigate (inter alia) the effect of number similarity between a noun and the auxiliary verb in sentences like the following. There are two levels to a factor called Int(erference): low and high.</p>
<p>(3a) low: The key to the cabinet <em>are</em> on the table
(3b) high: The key to the <em>cabinets</em> <em>are</em> on the table</p>
<p>Here, in (3b), the auxiliary verb <em>are</em> is predicted to be read faster than in (3a), because the plural marking on the noun <em>cabinets</em> leads the reader to think that the sentence is grammatical. (Both sentences are ungrammatical.) This phenomenon, where the high condition is read faster than the low condition, is called <strong>agreement attraction</strong>.</p>
<p>The data provided are for the critical region (the auxiliary verb <em>are</em>). The experiment method is eye-tracking; we have total reading times in milliseconds.</p>
<p>The research question is whether the difference in reading times between high and low conditions is negative.</p>
<ul>
<li>First, using a log-normal likelihood, fit a hierarchical model with correlated varying intercept and slopes for subjects and items. You will need to decide on the priors for the model.</li>
<li>By simply looking at the posterior distribution of the slope parameter <span class="math inline">\(\beta\)</span>, what would you conclude about the theoretical claim relating to agreement attraction?</li>
</ul>
<div class="exercise">
<p><span id="exr:ab" class="exercise"><strong>Exercise 5.5  </strong></span>Attentional blink (Bernoulli likelihood)</p>
</div>
<p>The attentional blink <span class="citation">(AB; first described by Raymond, Shapiro, and Arnell <a href="#ref-raymond1992temporary">1992</a>; though it has been noticed before e.g., Broadbent and Broadbent <a href="#ref-Broadbent1987">1987</a>)</span> refers to a temporary reduction in the accuracy of detecting a <em>probe</em> (e.g., a letter “X”) presented closely after a <em>target</em> that has been detected (e.g., a white letter). We will focus on the experimental condition of Experiment 2 of <span class="citation">Raymond, Shapiro, and Arnell (<a href="#ref-raymond1992temporary">1992</a>)</span>. Subjects are presented with letters in rapid serial visual presentation (RSVP) at the center of the screen at a constant rate and are required to identify the only white letter (target) in the stream of black letters, and then to report whether the letter X (probe) occurred in the subsequent letter stream. The AB is defined as having occurred when the target is reported correctly but the report of the probe is inaccurate at a short <em>lag</em> or <em>target-probe</em> interval.</p>
<p>The data set <code>df_ab</code> is a subset of the data of this paradigm from a replication conducted by <span class="citation">Grassi et al. (<a href="#ref-grassi_two_2021">2021</a>)</span>. In this subset, the probe was always present and the target was correctly identified. We want to find out how the lag affects the accuracy of the identification of the probe.</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb323-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_ab&quot;</span>)</a>
<a class="sourceLine" id="cb323-2" data-line-number="2">df_ab</a></code></pre></div>
<pre><code>## # A tibble: 2,101 × 4
##    subj probe_correct trial   lag
##   &lt;int&gt;         &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1     1             0     2     5
## 2     1             1     4     4
## 3     1             1     8     6
## # … with 2,098 more rows</code></pre>
<p>Fit a logistic regression assuming a linear relationship between <code>lag</code> and accuracy (<code>probe_correct</code>). Assume a hierarchical structure with correlated varying inter-
cept and slopes for subjects. You will need to decide on the priors for this model.</p>
<ol style="list-style-type: lower-alpha">
<li>How is the accuracy of the probe identification affected by the lag? Estimate this in log-odds and percentages.</li>
<li>Is the linear relationship justified? Use posterior predictive checks to verify this.</li>
<li>Can you think about a better relationship between lag and accuracy? Fit a new model and use posterior predictive checks to verify if the fit improved.</li>
</ol>
<div class="exercise">
<p><span id="exr:strooplogis-brms" class="exercise"><strong>Exercise 5.6  </strong></span>Is there a Stroop effect in accuracy?</p>
</div>
<p>Instead of the response times of the correct answers, we want to find out whether accuracy also changes by condition in the Stroop task. Fit the Stroop data with a hierarchical logistic regression (i.e., a Bernoulli likelihood with a logit link). Use the complete data set, <code>df_stroop_complete</code> which also includes incorrect answers, and subset it selecting the first 50 subjects.</p>
<div class="exercise">
<p><span id="exr:stroop-dist" class="exercise"><strong>Exercise 5.7  </strong></span>Distributional regression for the Stroop effect.</p>
</div>
<p>We will relax some of the assumptions of the model of Stroop presented in section <a href="ch-hierarchical.html#sec-stroop">5.3</a>. We will no longer assume that all subjects share the same variance component, and, in addition, we’ll investigate whether the experimental manipulation affects the scale of the response times. A reasonable hypothesis could be that the incongruent condition is noisier than the congruent one.</p>
<p>Assume the following likelihood, and fit the model with sensible priors (recall that our initial prior for <span class="math inline">\(\beta\)</span> wasn’t reasonable). (Priors for all the <code>sigma</code> parameters require us to set <code>dpar = sigma</code>).</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  rt_n &amp;\sim \mathit{LogNormal}(\alpha + u_{subj[n],1}  + c\_cond_n \cdot  (\beta + u_{subj[n],2}), \sigma_n)\\
  \sigma_n &amp;= \exp(\sigma_\alpha + \sigma_{u_{subj[n],1}} + c\_cond \cdot (\sigma_\beta + \sigma_{u_{subj[n],2}}) )
\end{aligned}
\end{equation}\]</span></p>
<p>In this likelihood <span class="math inline">\(\sigma_n\)</span> has both population- and group-level parameters: <span class="math inline">\(\sigma_\alpha\)</span> and <span class="math inline">\(\sigma_\beta\)</span> are the intercept and slope of the population level effects repectively, and <span class="math inline">\(\sigma_{u_{subj[n],1}}\)</span> and <span class="math inline">\(\sigma_{u_{subj[n],2}}\)</span> are the intercept and slope of the group-level effects.</p>
<ol style="list-style-type: decimal">
<li>Is our hypothesis reasonable in light of the results?</li>
<li>Why is the intercept for the scale negative?</li>
<li>What’s the posterior estimate of the scale for congruent and incongruent conditions?</li>
</ol>
<div class="exercise">
<p><span id="exr:HLMExerciseGramCE" class="exercise"><strong>Exercise 5.8  </strong></span>The grammaticality illusion</p>
</div>
<p>Load the following two data sets:</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb325-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_english&quot;</span>)</a>
<a class="sourceLine" id="cb325-2" data-line-number="2">english &lt;-<span class="st"> </span>df_english</a>
<a class="sourceLine" id="cb325-3" data-line-number="3"><span class="kw">data</span>(<span class="st">&quot;df_dutch&quot;</span>)</a>
<a class="sourceLine" id="cb325-4" data-line-number="4">dutch &lt;-<span class="st"> </span>df_dutch</a></code></pre></div>
<p>In an offline accuracy rating study on English double center-embedding constructions, <span class="citation">Gibson and Thomas (<a href="#ref-gibsonthomas99">1999</a>)</span> found that grammatical constructions (e.g., example 4a below) were no less acceptable than ungrammatical constructions (e.g., example 4b) where a middle verb phrase (e.g., <em>was cleaning every week</em>) was missing.</p>
<p>(4a) The apartment that the maid who the service had sent over was cleaning every week was well decorated.</p>
<p>(4b) *The apartment that the maid who the service had sent over — was well decorated</p>
<p>Based on these results from English, <span class="citation">Gibson and Thomas (<a href="#ref-gibsonthomas99">1999</a>)</span> proposed that working-memory overload leads the comprehender to forget the prediction of the upcoming verb phrase (VP), which reduces working-memory load. This came to be known as the <em>VP-forgetting hypothesis</em>. The prediction is that in the word immediately following the final verb, the grammatical condition (which is coded as +1 in the data frames) should be harder to read than the ungrammatical condition (which is coded as -1).</p>
<p>The design shown above is set up to test this hypothesis using self-paced reading for English <span class="citation">(Vasishth et al. <a href="#ref-VSLK08">2011</a>)</span>, and for Dutch <span class="citation">(Frank, Trompenaars, and Vasishth <a href="#ref-FrankEtAl2015">2015</a>)</span>. The data provided are for the critical region (the noun phrase, labeled NP1, following the final verb); this is the region for which the theory predicts differences between the two conditions. We have reading times in log milliseconds.</p>
<ol style="list-style-type: lower-alpha">
<li>First, fit a linear model with a full hierarchical structure by subjects and by items for the English data. Because we have log milliseconds data, we can simply use the normal likelihood (not the log-normal). What scale will be the parameters be in, milliseconds or log milliseconds?</li>
<li>Second, using the posterior for the effect of interest from the English data, derive a prior distribution for the effect in the Dutch data. Then fit two linear mixed models: (i) one model with relatively uninformative priors for <span class="math inline">\(\beta\)</span> (for example, <span class="math inline">\(Normal(0,1)\)</span>), and (ii) one model with the prior for <span class="math inline">\(\beta\)</span> you derived from the English data. Do the posterior distributions of the Dutch data’s effect show any important differences given the two priors? If yes, why; if not, why not?</li>
<li>Finally, just by looking at the English and Dutch posteriors, what can we say about the VP-forgetting hypothesis? Are the posteriors of the effect from these two languages consistent with the hypothesis?</li>
</ol>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-barr2013">
<p>Barr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013. “Random Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.” <em>Journal of Memory and Language</em> 68 (3). Elsevier: 255–78.</p>
</div>
<div id="ref-R-lme4">
<p>Bates, Douglas M, Martin Mächler, Ben Bolker, and Steve Walker. 2015b. “Fitting Linear Mixed-Effects Models Using lme4.” <em>Journal of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01" class="uri">https://doi.org/10.18637/jss.v067.i01</a>.</p>
</div>
<div id="ref-bernardosmith">
<p>Bernardo, José M, and Adrian FM Smith. 2009. <em>Bayesian Theory</em>. Vol. 405. John Wiley &amp; Sons.</p>
</div>
<div id="ref-Broadbent1987">
<p>Broadbent, Donald E., and Margaret H. P. Broadbent. 1987. “From Detection to Identification: Response to Multiple Targets in Rapid Serial Visual Presentation.” <em>Perception &amp; Psychophysics</em> 42 (2): 105–13. <a href="https://doi.org/10.3758/BF03210498" class="uri">https://doi.org/10.3758/BF03210498</a>.</p>
</div>
<div id="ref-brownSimplestCompleteModel2008">
<p>Brown, Scott D., and Andrew Heathcote. 2008. “The Simplest Complete Model of Choice Response Time: Linear Ballistic Accumulation.” <em>Cognitive Psychology</em> 57 (3): 153–78. <a href="https://doi.org/10.1016/j.cogpsych.2007.12.002" class="uri">https://doi.org/10.1016/j.cogpsych.2007.12.002</a>.</p>
</div>
<div id="ref-burkner2018ordinal">
<p>Bürkner, Paul-Christian, and Matti Vuorre. 2018. “Ordinal Regression Models in Psychological Research: A Tutorial.” <em>PsyArXiv Preprints</em>.</p>
</div>
<div id="ref-deFinetti">
<p>de Finetti, Bruno. 1931. “Funcione Caratteristica Di Un Fenomeno Aleatorio.” <em>Atti Dela Reale Accademia Nazionale Dei Lincei, Serie 6. Memorie, Classe Di Scienze Fisiche, Mathematice E Naturale</em> 4: 251–99.</p>
</div>
<div id="ref-delongProbabilisticWordPreactivation2005">
<p>DeLong, Katherine A, Thomas P Urbach, and Marta Kutas. 2005. “Probabilistic Word Pre-Activation During Language Comprehension Inferred from Electrical Brain Activity.” <em>Nature Neuroscience</em> 8 (8): 1117–21. <a href="https://doi.org/10.1038/nn1504" class="uri">https://doi.org/10.1038/nn1504</a>.</p>
</div>
<div id="ref-ManyLabs3">
<p>Ebersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” <em>Journal of Experimental Social Psychology</em> 67: 68–82. <a href="https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012" class="uri">https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012</a>.</p>
</div>
<div id="ref-EngelmannJaegerVasishth2019">
<p>Engelmann, Felix, Lena A. Jäger, and Shravan Vasishth. 2020. “The Effect of Prominence and Cue Association in Retrieval Processes: A Computational Account.” <em>Cognitive Science</em> 43 (12): e12800. <a href="https://doi.org/10.1111/cogs.12800" class="uri">https://doi.org/10.1111/cogs.12800</a>.</p>
</div>
<div id="ref-frankERPResponseAmount2015">
<p>Frank, Stefan L., Leun J. Otten, Giulia Galli, and Gabriella Vigliocco. 2015. “The ERP Response to the Amount of Information Conveyed by Words in Sentences.” <em>Brain and Language</em> 140: 1–11. <a href="https://doi.org/10.1016/j.bandl.2014.10.006" class="uri">https://doi.org/10.1016/j.bandl.2014.10.006</a>.</p>
</div>
<div id="ref-FrankEtAl2015">
<p>Frank, Stefan L., Thijs Trompenaars, and Shravan Vasishth. 2015. “Cross-Linguistic Differences in Processing Double-Embedded Relative Clauses: Working-Memory Constraints or Language Statistics?” <em>Cognitive Science</em> 40: 554–78. <a href="https://doi.org/10.1111/cogs.12247" class="uri">https://doi.org/10.1111/cogs.12247</a>.</p>
</div>
<div id="ref-Gelman14">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2014. <em>Bayesian Data Analysis</em>. Third Edition. Boca Raton, FL: Chapman; Hall/CRC Press.</p>
</div>
<div id="ref-GelmanHill2007">
<p>Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge University Press.</p>
</div>
<div id="ref-gibsonthomas99">
<p>Gibson, Edward, and James Thomas. 1999. “Memory Limitations and Structural Forgetting: The Perception of Complex Ungrammatical Sentences as Grammatical.” <em>Language and Cognitive Processes</em> 14(3): 225–48.</p>
</div>
<div id="ref-gibsonwu">
<p>Gibson, Edward, and H-H Iris Wu. 2013. “Processing Chinese Relative Clauses in Context.” <em>Language and Cognitive Processes</em> 28 (1-2). Taylor &amp; Francis: 125–55.</p>
</div>
<div id="ref-grassi_two_2021">
<p>Grassi, Massimo, Camilla Crotti, David Giofrè, Ingrid Boedker, and Enrico Toffalini. 2021. “Two Replications of Raymond, Shapiro, and Arnell (1992), the Attentional Blink.” <em>Behavior Research Methods</em> 53 (2): 656–68. <a href="https://doi.org/10.3758/s13428-020-01457-6" class="uri">https://doi.org/10.3758/s13428-020-01457-6</a>.</p>
</div>
<div id="ref-grodner">
<p>Grodner, Daniel, and Edward Gibson. 2005. “Consequences of the Serial Nature of Linguistic Input.” <em>Cognitive Science</em> 29: 261–90.</p>
</div>
<div id="ref-hsiao03">
<p>Hsiao, Fanny Pai-Fang, and Edward Gibson. 2003. “Processing Relative Clauses in Chinese.” <em>Cognition</em> 90: 3–27.</p>
</div>
<div id="ref-kutasThirtyYearsCounting2011">
<p>Kutas, Marta, and Kara D. Federmeier. 2011. “Thirty Years and Counting: Finding Meaning in the N400 Componentof the Event-Related Brain Potential (ERP).” <em>Annual Review of Psychology</em> 62 (1): 621–47. <a href="https://doi.org/10.1146/annurev.psych.093008.131123" class="uri">https://doi.org/10.1146/annurev.psych.093008.131123</a>.</p>
</div>
<div id="ref-kutasReadingSenselessSentences1980">
<p>Kutas, Marta, and Steven A Hillyard. 1980. “Reading Senseless Sentences: Brain Potentials Reflect Semantic Incongruity.” <em>Science</em> 207 (4427): 203–5. <a href="https://doi.org/10.1126/science.7350657" class="uri">https://doi.org/10.1126/science.7350657</a>.</p>
</div>
<div id="ref-kutasBrainPotentialsReading1984">
<p>Kutas, Marta, and Steven A Hillyard. 1984. “Brain Potentials During Reading Reflect Word Expectancy and Semantic Association.” <em>Nature</em> 307 (5947): 161–63. <a href="https://doi.org/10.1038/307161a0" class="uri">https://doi.org/10.1038/307161a0</a>.</p>
</div>
<div id="ref-laird1982random">
<p>Laird, Nan M, and James H Ware. 1982. “Random-Effects Models for Longitudinal Data.” <em>Biometrics</em>. JSTOR, 963–74.</p>
</div>
<div id="ref-Lee2011">
<p>Lee, Michael D., ed. 2011a. “Special Issue on Hierarchical Bayesian Models.” <em>Journal of Mathematical Psychology</em> 55 (1). <a href="https://www.sciencedirect.com/journal/journal-of-mathematical-psychology/vol/55/issue/1" class="uri">https://www.sciencedirect.com/journal/journal-of-mathematical-psychology/vol/55/issue/1</a>.</p> 2011b. “How Cognitive Modeling Can Benefit from Hierarchical Bayesian Models.” <em>Journal of Mathematical Psychology</em> 55 (1). Elsevier BV: 1–7. <a href="https://doi.org/10.1016/j.jmp.2010.08.013" class="uri">https://doi.org/10.1016/j.jmp.2010.08.013</a>.</p>
</div>
<div id="ref-LeeWagenmakers2014">
<p>Lee, Michael D., and Eric-Jan Wagenmakers. 2014. <em>Bayesian Cognitive Modeling: A Practical Course</em>. Cambridge University Press.</p>
</div>
<div id="ref-lewisvasishth:cogsci05">
<p>Lewis, Richard L., and Shravan Vasishth. 2005. “An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval.” <em>Cognitive Science</em> 29: 1–45.</p>
</div>
<div id="ref-lisson_2020">
<p>Lissón, Paula, Dorothea Pregla, Bruno Nicenboim, Dario Paape, Mick van het Nederend, Frank Burchert, Nicole Stadie, David Caplan, and Shravan Vasishth. 2021. “A Computational Evaluation of Two Models of Retrieval Processes in Sentence Processing in Aphasia.” <em>Cognitive Science</em> 45 (4): e12956. <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12956" class="uri">https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12956</a>.</p>
</div>
<div id="ref-logacev-dokudan-2021-multinomial">
<p>Logačev, Pavel, and Noyan Dokudan. 2021. “A Multinomial Processing Tree Model of RC Attachment.” In <em>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</em>, 39–47. Online: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/2021.cmcl-1.4" class="uri">https://www.aclweb.org/anthology/2021.cmcl-1.4</a>.</p>
</div>
<div id="ref-LogacevVasishth2015">
<p>Logačev, Pavel, and Shravan Vasishth. 2016. “A Multiple-Channel Model of Task-Dependent Ambiguity Resolution in Sentence Comprehension.” <em>Cognitive Science</em> 40 (2): 266–98. <a href="https://doi.org/10.1111/cogs.12228" class="uri">https://doi.org/10.1111/cogs.12228</a>.</p>
</div>
<div id="ref-lunn2012bugs">
<p>Lunn, David, Chris Jackson, David J Spiegelhalter, Nicky Best, and Andrew Thomas. 2012. <em>The BUGS Book: A Practical Introduction to Bayesian Analysis</em>. Vol. 98. CRC Press.</p>
</div>
<div id="ref-macleod1991half">
<p>MacLeod, Colin M. 1991. “Half a Century of Research on the Stroop Effect: An Integrative Review.” <em>Psychological Bulletin</em> 109 (2). American Psychological Association: 163.</p>
</div>
<div id="ref-mcclellandPlaceModelingCognitive2009">
<p>McClelland, James L. 2009. “The Place of Modeling in Cognitive Science.” <em>Topics in Cognitive Science</em> 1 (1): 11–38. <a href="https://doi.org/10.1111/j.1756-8765.2008.01003.x" class="uri">https://doi.org/10.1111/j.1756-8765.2008.01003.x</a>.</p>
</div>
<div id="ref-mcelreath2015statistical">
<p>McElreath, Richard. 2020. <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</em>. Boca Raton, Florida: Chapman; Hall/CRC.</p>
</div>
<div id="ref-Nicenboim2018StanCon">
<p>Nicenboim, Bruno. 2018. “The Implementation of a Model of Choice: The (Truncated) Linear Ballistic Accumulator.” In <em>StanCon</em>. Aalto University, Helsinki, Finland. <a href="https://doi.org/10.5281/zenodo.1465990" class="uri">https://doi.org/10.5281/zenodo.1465990</a>.</p>
</div>
<div id="ref-NicenboimEtAl2016Frontiersb">
<p>Nicenboim, Bruno, Pavel Logačev, Carolina Gattei, and Shravan Vasishth. 2016. “When High-Capacity Readers Slow down and Low-Capacity Readers Speed up: Working Memory and Locality Effects.” <em>Frontiers in Psychology</em> 7 (280). <a href="https://doi.org/10.3389/fpsyg.2016.00280" class="uri">https://doi.org/10.3389/fpsyg.2016.00280</a>.</p>
</div>
<div id="ref-nicenboimModelsRetrievalSentence2018">
<p>Nicenboim, Bruno, and Shravan Vasishth. 2018. “Models of Retrieval in Sentence Comprehension: A Computational Evaluation Using Bayesian Hierarchical Modeling.” <em>Journal of Memory and Language</em> 99: 1–34. <a href="https://doi.org/10.1016/j.jml.2017.08.004" class="uri">https://doi.org/10.1016/j.jml.2017.08.004</a>.</p>
</div>
<div id="ref-nicenboim_vasishth_rosler_2020">
<p>Nicenboim, Bruno, Shravan Vasishth, and Frank Rösler. 2020a. “Are Words Pre-Activated Probabilistically During Sentence Comprehension? Evidence from New Data and a Bayesian Random-Effects Meta-Analysis Using Publicly Available Data.” <em>Neuropsychologia</em> 142. <a href="https://doi.org/10.1016/j.neuropsychologia.2020.107427" class="uri">https://doi.org/10.1016/j.neuropsychologia.2020.107427</a>.</p>
</div>
<div id="ref-nieuwlandLargescaleReplicationStudy2018">
<p>Nieuwland, Mante S, Stephen Politzer-Ahles, Evelien Heyselaar, Katrien Segaert, Emily Darley, Nina Kazanina, Sarah Von Grebmer Zu Wolfsthurn, et al. 2018. “Large-Scale Replication Study Reveals a Limit on Probabilistic Prediction in Language Comprehension.” <em>eLife</em> 7. <a href="https://doi.org/10.7554/eLife.33468" class="uri">https://doi.org/10.7554/eLife.33468</a>.</p>
</div>
<div id="ref-PaapeEtAlMPT2020">
<p>Paape, Dario, Serine Avetisyan, Sol Lago, and Shravan Vasishth. 2021. “Modeling Misretrieval and Feature Substitution in Agreement Attraction: A Computational Evaluation.” <em>Cognitive Science</em> 45 (8). <a href="https://doi.org/https://doi.org/10.1111/cogs.13019" class="uri">https://doi.org/https://doi.org/10.1111/cogs.13019</a>.</p>
</div>
<div id="ref-picton_etal_2000">
<p>Picton, T.W., S. Bentin, P. Berg, E. Donchin, S.A. Hillyard, R. Johnson JR., G.A. Miller, et al. 2000. “Guidelines for Using Human Event-Related Potentials to Study Cognition: Recording Standards and Publication Criteria.” <em>Psychophysiology</em> 37 (2): 127–52. <a href="https://doi.org/10.1111/1469-8986.3720127" class="uri">https://doi.org/10.1111/1469-8986.3720127</a>.</p>
</div>
<div id="ref-pinheirobates">
<p>Pinheiro, José C, and Douglas M Bates. 2000. <em>Mixed-Effects Models in S and S-PLUS</em>. New York: Springer-Verlag.</p>
</div>
<div id="ref-raymond1992temporary">
<p>Raymond, Jane E, Kimron L Shapiro, and Karen M Arnell. 1992. “Temporary Suppression of Visual Processing in an RSVP Task: An Attentional Blink?” <em>Journal of Experimental Psychology: Human Perception and Performance</em> 18 (3). American Psychological Association: 849.</p>
</div>
<div id="ref-Rouder2005">
<p>Rouder, Jeffrey N. 2005. “Are Unshifted Distributional Models Appropriate for Response Time?” <em>Psychometrika</em> 70 (2). Springer Science + Business Media: 377–81. <a href="https://doi.org/10.1007/s11336-005-1297-7" class="uri">https://doi.org/10.1007/s11336-005-1297-7</a>.</p>
</div>
<div id="ref-SchadEtAlBF">
<p>Schad, Daniel J., Bruno Nicenboim, Paul-Christian Bürkner, Michael J. Betancourt, and Shravan Vasishth. 2021. “Workflow Techniques for the Robust Use of Bayes Factors.”</p>
</div>
<div id="ref-SorensenVasishthTutorial">
<p>Sorensen, Tanner, Sven Hohenstein, and Shravan Vasishth. 2016. “Bayesian Linear Mixed Models Using Stan: A Tutorial for Psychologists, Linguists, and Cognitive Scientists.” <em>Quantitative Methods for Psychology</em> 12 (3): 175–200.</p>
</div>
<div id="ref-spiegelhalter2004bayesian">
<p>Spiegelhalter, David J, Keith R Abrams, and Jonathan P Myles. 2004. <em>Bayesian Approaches to Clinical Trials and Health-Care Evaluation</em>. Vol. 13. John Wiley &amp; Sons.</p>
</div>
<div id="ref-stroop1935studies">
<p>Stroop, J Ridley. 1935. “Studies of Interference in Serial Verbal Reactions.” <em>Journal of Experimental Psychology</em> 18 (6). Psychological Review Company: 643.</p>
</div>
<div id="ref-VasishthetalPLoSOne2013">
<p>Vasishth, Shravan, Zhong Chen, Qiang Li, and Gueilan Guo. 2013. “Processing Chinese Relative Clauses: Evidence for the Subject-Relative Advantage.” <em>PLoS ONE</em> 8 (10). Public Library of Science: 1–14.</p>
</div>
<div id="ref-VasishthEtAl2017Modelling">
<p>Vasishth, Shravan, Nicolas Chopin, Robin Ryder, and Bruno Nicenboim. 2017. “Modelling Dependency Completion in Sentence Comprehension as a Bayesian Hierarchical Mixture Process: A Case Study Involving Chinese Relative Clauses.” In <em>Proceedings of Cognitive Science Conference</em>. London, UK. <a href="https://arxiv.org/abs/1702.00564v2" class="uri">https://arxiv.org/abs/1702.00564v2</a>.</p>
</div>
<div id="ref-VasishthEngelmann2022">
<p>Vasishth, Shravan, and Felix Engelmann. 2022. <em>Sentence Comprehension as a Cognitive Process: A Computational Approach</em>. Cambridge, UK: Cambridge University Press. <a href="https://books.google.de/books?id=6KZKzgEACAAJ" class="uri">https://books.google.de/books?id=6KZKzgEACAAJ</a>.</p>
</div>
<div id="ref-VasishthNicenboimStatMeth">
<p>Vasishth, Shravan, and Bruno Nicenboim. 2016. “Statistical Methods for Linguistic Research: Foundational Ideas – Part I.” <em>Language and Linguistics Compass</em> 10 (8): 349–69.</p>
</div>
<div id="ref-VSLK08">
<p>Vasishth, Shravan, Katja Suckow, Richard L. Lewis, and Sabine Kern. 2011. “Short-Term Forgetting in Sentence Comprehension: Crosslinguistic Evidence from Head-Final Structures.” <em>Language and Cognitive Processes</em> 25: 533–67.</p>
</div>
<div id="ref-VasishthEtAl2017Feature">
<p>Vasishth, S., L. A. Jaeger, and B. Nicenboim. 2017. “Feature overwriting as a finite mixture process: Evidence from comprehension data.” In <em>Proceedings of MathPsych/ICCM Conference</em>. Warwick, UK. <a href="https://arxiv.org/abs/1703.04081" class="uri">https://arxiv.org/abs/1703.04081</a>.</p>
</div>
<div id="ref-wahnPupilSizesScale2016">
<p>Wahn, Basil, Daniel P. Ferris, W. David Hairston, and Peter König. 2016. “Pupil Sizes Scale with Attentional Load and Task Experience in a Multiple Object Tracking Task.” <em>PLOS ONE</em> 11 (12): e0168087. <a href="https://doi.org/10.1371/journal.pone.0168087" class="uri">https://doi.org/10.1371/journal.pone.0168087</a>.</p>
</div>
<div id="ref-yadavindiff2021">
<p>Yadav, Himanshu, Dario Paape, Garrett Smith, Brian Dillon, and Shravan Vasishth. 2021. “Individual Differences in Cue-Weighting in Sentence Comprehension: An Evaluation Using Approximate Bayesian Computation.”</p>
</div>
<div id="ref-yadavencret2021">
<p>Yadav, Himanshu, Garrett Smith, and Shravan Vasishth. 2021a. “Feature Encoding Modulates Cue-Based Retrieval: Modeling Interference Effects in Both Grammatical and Ungrammatical Sentences.” <em>Proceedings of the Cognitive Science Conference</em>.</p>
</div>
<div id="ref-yadaviccm2021">
<p>Yadav, Himanshu, Garrett Smith, and Shravan Vasishth. 2021b. “Is Similarity-Based Interference Caused by Lossy Compression or Cue-Based Retrieval? A Computational Evaluation.” <em>Proceedings of the International Conference on Cognitive Modeling</em>.</p>
</div>
<div id="ref-yarkoni_2020">
<p>Yarkoni, Tal. 2020. “The Generalizability Crisis.” <em>Behavioral and Brain Sciences</em>. Cambridge University Press, 1–37. <a href="https://doi.org/10.1017/S0140525X20001685" class="uri">https://doi.org/10.1017/S0140525X20001685</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="14">
<li id="fn14"><p>In this sense, even the frequentist hierarchical modeling software in R, <code>lmer</code> from the package <code>lme4</code>, is essentially Bayesian in formulation (except of course that there is no prior as such on <span class="math inline">\(\mu\)</span>).<a href="ch-hierarchical.html#fnref14" class="footnote-back">↩</a></p></li>
<li id="fn15"><p>For simplicity, we assume that they share the same standard deviation.<a href="ch-hierarchical.html#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p>If we don’t remove the intercept, that is, if we use the formula <code>n400 ~ 1 + factor(subj) + c_cloze:factor(subj)</code>, with <code>factor(subj)</code> we are going estimate the deviation between the first subject and the each of the other subjects.<a href="ch-hierarchical.html#fnref16" class="footnote-back">↩</a></p></li>
<li id="fn17"><p>The analogous frequentist model can be fit using <code>lmer</code> from the package <code>lme4</code>, using <code>(1+c_cloze||subj)</code> or, equivalently, <code>(c_cloze||subj)</code> for the by-subject random effects.<a href="ch-hierarchical.html#fnref17" class="footnote-back">↩</a></p></li>
<li id="fn18"><p>The intercept adjustment is often called <span class="math inline">\(u_0\)</span> in statistics books, where the intercept might be called <span class="math inline">\(\alpha\)</span> or (sometimes also <span class="math inline">\(\beta_0\)</span>), and thus <span class="math inline">\(u_1\)</span> refers to the adjustment to the slope. However, in this book, we start the indexing with 1 to be consistent with the Stan language.<a href="ch-hierarchical.html#fnref18" class="footnote-back">↩</a></p></li>
<li id="fn19"><p>By contrast, in the frequentist <code>lmer</code> model, the adjustments <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are not parameters; they are called conditional modes; see <span class="citation">Bates, Mächler, et al. (<a href="#ref-R-lme4">2015</a><a href="#ref-R-lme4">b</a>)</span>.<a href="ch-hierarchical.html#fnref19" class="footnote-back">↩</a></p></li>
<li id="fn20"><p>Another source of confusion here is that <em>hyperparameters</em> is also used in the machine learning literature with a different meaning.<a href="ch-hierarchical.html#fnref20" class="footnote-back">↩</a></p></li>
<li id="fn21"><p>One could in theory keep going deeper and deeper, defining hyper-hyperpriors etc., but the model would quickly become impossible to fit.<a href="ch-hierarchical.html#fnref21" class="footnote-back">↩</a></p></li>
<li id="fn22"><p>The double pipe is also used in the same way in <code>lmer</code> from the package <code>lme4</code>.<a href="ch-hierarchical.html#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p>This is because an LKJ correlation distribution with a large <span class="math inline">\(\eta\)</span> corresponds to a correlation matrix with values close to zero in the lower and upper triangles<a href="ch-hierarchical.html#fnref23" class="footnote-back">↩</a></p></li>
<li id="fn24"><p>Most of the papers mentioned above provide example code using Stan or <code>brms</code><a href="ch-hierarchical.html#fnref24" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-reg.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-priors.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
