<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction to Bayesian data analysis | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.38 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction to Bayesian data analysis | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction to Bayesian data analysis | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2024-03-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-intro.html"/>
<link rel="next" href="ch-compbda.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b>  Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The  law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b>  Discrete random variables: An example using the  binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b>  Continuous random variables: An example using the  normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1:  Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2:  Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate  simulated bivariate  (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The  marginal likelihood  (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b>  Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the  posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a  likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a  prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The  posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the  posterior through  sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b>  Bayesian Regression Models using Stan:  brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b>  Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors:  sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b>  Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b>  Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b>  Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b>  Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b>  Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The  log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
<li class="chapter" data-level="3.11" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first  linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b>  Likelihood and  priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b>  Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b>  Logistic regression: Does  set size affect  free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b>  Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b>  Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b>  No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b>  Varying intercepts and  varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A  hierarchical log-normal model: The  Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes  log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of  Prior Elicitation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English  relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the  variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b>  Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from  meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’  posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#building-a-model"><i class="fa fa-check"></i><b>7.1</b>  Building a model</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-to-ask-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions to ask on a model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#checking-whether-assumptions-are-consist-with-domain-expertise-prior-predictive-checks"><i class="fa fa-check"></i><b>7.2.1</b>  Checking whether assumptions are consist with  domain expertise: Prior predictive checks</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#testing-for-correct-posterior-approximations-checks-of-computational-faithfulness"><i class="fa fa-check"></i><b>7.2.2</b>  Testing for correct posterior approximations: Checks of computational faithfulness</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#sensitivity-of-the-model"><i class="fa fa-check"></i><b>7.2.3</b>  Sensitivity of the model</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#does-the-model-adequately-capture-the-dataposterior-predictive-checks"><i class="fa fa-check"></i><b>7.2.4</b>  Does the model adequately capture the data?–Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b>  Contrast coding</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding:  Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b>  Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b>  Cell means parameterization and  posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b>  Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The  hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The  <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b>  Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b>  Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or  model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b>  Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts:  monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b>  Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b>  Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the  intercept in  non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-6"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial  <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b>  Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b>  Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one  covariate</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a  group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-7"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan:  Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example:  Cloze probability with Stan with the  binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b>  Regression models in Stan</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first  linear regression in Stan: Does attentional load affect  pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b>  Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-8"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Hierarchical models and reparameterization </a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated  varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b>  Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-9"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b>  Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The  simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> An example where simulation-based calibration reveals a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues with and limitations of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-re-implementing-the-exponential-distribution-manually"><i class="fa fa-check"></i><b>12.3</b> Another  custom distribution: Re-implementing the  exponential distribution manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-10"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b>  Meta-analysis and  measurement error models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b>  Measurement-error models</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-11"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#prior-predictive-vs.-posterior-predictive-model-comparison"><i class="fa fa-check"></i><b>14.1</b> Prior predictive vs. posterior predictive model comparison</a></li>
<li class="chapter" data-level="14.2" data-path="ch-comparison.html"><a href="ch-comparison.html#some-important-points-to-consider-when-comparing-models"><i class="fa fa-check"></i><b>14.2</b> Some important points to consider when comparing models</a></li>
<li class="chapter" data-level="14.3" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b>  Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b>  Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and  accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="15.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>15.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#sample-size-determination-using-bayes-factors"><i class="fa fa-check"></i><b>15.6</b> Sample size determination using Bayes factors</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#summary-12"><i class="fa fa-check"></i><b>15.7</b> Summary</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.8</b> Further reading</a></li>
<li class="chapter" data-level="15.9" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b>  Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#issues-with-cross-validation"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b>  PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-13"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#what-characterizes-a-computational-cognitive-model"><i class="fa fa-check"></i><b>17.1</b> What characterizes a computational cognitive model?</a></li>
<li class="chapter" data-level="17.2" data-path="ch-cogmod.html"><a href="ch-cogmod.html#some-advantages-of-taking-the-latent-variable-modeling-approach"><i class="fa fa-check"></i><b>17.2</b> Some advantages of taking the latent-variable modeling approach</a></li>
<li class="chapter" data-level="17.3" data-path="ch-cogmod.html"><a href="ch-cogmod.html#types-of-computational-cognitive-model"><i class="fa fa-check"></i><b>17.3</b> Types of computational cognitive model</a></li>
<li class="chapter" data-level="17.4" data-path="ch-cogmod.html"><a href="ch-cogmod.html#summary-14"><i class="fa fa-check"></i><b>17.4</b> Summary</a></li>
<li class="chapter" data-level="17.5" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling  multiple categorical responses</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A  hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-15"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
<li class="chapter" data-level="18.5" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A  multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A  hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-16"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with  contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-17"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="ch-closing.html"><a href="ch-closing.html"><i class="fa fa-check"></i><b>21</b> In closing</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-introBDA" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction to Bayesian data analysis<a href="ch-introBDA.html#ch-introBDA" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Before we can start analyzing realistic data sets using Bayes’ rule, it is important to understand the application of Bayes’ rule in one of the simplest of cases, data involving the binomial likelihood. This simple case is important to understand because it encapsulates the essence of the Bayesian approach to data analysis, and because it allows us to analytically work out the posterior distribution of the parameter of interest, using just a pen and paper. This simple case also helps us to appreciate a crucial point: The posterior distribution of a parameter is a compromise between the prior and the likelihood. This important insight will play a central role in the realistic data analysis situations we will cover in the remainder of this book.</p>
<div id="bayes-rule" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span>  Bayes’ rule<a href="ch-introBDA.html#bayes-rule" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall Bayes’ rule: When <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are observable discrete events (such as “it has been raining” or “the streets are wet”), we can state the rule as follows:</p>
<p><span class="math display" id="eq:bayes-P">\[\begin{equation}
P(A\mid B) = \frac{P(B\mid A) P(A)}{P(B)}
\tag{2.1}
\end{equation}\]</span></p>
<p>Given a vector of data <span class="math inline">\(\boldsymbol{y}\)</span>, Bayes’ rule allows us to work out the posterior distributions of the parameters of interest, which we can represent as the vector of parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span>. This computation is achieved by rewriting <a href="ch-introBDA.html#eq:bayes-P">(2.1)</a> as <a href="ch-introBDA.html#eq:bayes">(2.2)</a>. What is different here is that Bayes’ rule is written in terms of probability distributions. Here, <span class="math inline">\(p(\cdot)\)</span> is a probability density function (continuous case) or a probability mass function (discrete case).</p>
<p><span class="math display" id="eq:bayes">\[\begin{equation}
p(\boldsymbol{\Theta}|\boldsymbol{y}) = \cfrac{ p(\boldsymbol{y}|\boldsymbol{\Theta}) \times p(\boldsymbol{\Theta}) }{p(\boldsymbol{y})}
\tag{2.2}
\end{equation}\]</span></p>
<p>The above statement can be rewritten in words as follows:</p>
<p><span class="math display">\[\begin{equation}
\hbox{Posterior} = \frac{\hbox{Likelihood} \times \hbox{Prior}}{\hbox{Marginal Likelihood}}
\end{equation}\]</span></p>
<p>The terms here have the following meaning. We elaborate on each point with an example below.</p>
<ul>
<li><p>The <em>Posterior</em>, <span class="math inline">\(p(\boldsymbol{\Theta}|\boldsymbol{y})\)</span>, is the probability distribution of the parameters conditional on the data.</p></li>
<li><p>The <em>Likelihood</em>, <span class="math inline">\(p(\boldsymbol{y}|\boldsymbol{\Theta}\)</span>) is as described in chapter <a href="ch-intro.html#ch-intro">1</a>: it is the PMF (discrete case) or the PDF (continuous case) expressed as a function of <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p></li>
<li><p>The <em>Prior</em>, <span class="math inline">\(p(\boldsymbol{\Theta})\)</span>, is the initial probability distribution of the parameter(s), before seeing the data.</p></li>
<li><p>The <em>Marginal Likelihood</em>, <span class="math inline">\(p(\boldsymbol{y})\)</span>, was introduced in chapter <a href="ch-intro.html#ch-intro">1</a> and standardizes the posterior distribution to ensure that the area under the curve of the distribution sums to 1, that is, it ensures that the posterior is a valid probability distribution.</p></li>
</ul>
<p>An example will clarify all these terms, as we explain below.</p>
</div>
<div id="sec-analytical" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Deriving the  posterior using Bayes’ rule: An analytical example<a href="ch-introBDA.html#sec-analytical" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall our cloze probability example earlier. Subjects are shown sentences like</p>
<p>“It’s raining. I’m going to take the …”</p>
<p>Suppose that 100 subjects are asked to complete the sentence.
If <span class="math inline">\(80\)</span> out of <span class="math inline">\(100\)</span> subjects complete the sentence with “umbrella,” the estimated cloze probability or predictability (given the preceding context) would be <span class="math inline">\(\frac{80}{100}=0.8\)</span>. This is the maximum likelihood estimate of the probability of producing this word; we will designate the estimate with a “hat” on the parameter name: <span class="math inline">\(\hat \theta=0.8\)</span>. In the frequentist paradigm, <span class="math inline">\(\hat \theta=0.8\)</span> is an estimate of an unknown point value <span class="math inline">\(\theta\)</span> “out there in nature.”</p>
<p>A crucial point to notice here is that the proportion 0.80 that we estimated above from the data can vary from one data set to another, and the variability in the estimate will be influenced by the sample size. For example, assuming that the true value of the <span class="math inline">\(\theta\)</span> parameter is in fact 0.80, if we repeatedly carry out the above experiment with say 10 participants, we will get some variability in the estimated proportion. Let’s check this by carrying out 100 simulated experiments and computing the variability of the estimated means under repeated sampling:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb80-1"><a href="ch-introBDA.html#cb80-1" aria-hidden="true"></a>estimated_means &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.80</span>) <span class="op">/</span><span class="st"> </span><span class="dv">10</span></span>
<span id="cb80-2"><a href="ch-introBDA.html#cb80-2" aria-hidden="true"></a><span class="kw">sd</span>(estimated_means)</span></code></pre></div>
<pre><code>## [1] 0.136</code></pre>
<p>The repeated runs of the (simulated) experiment are the sole underlying cause for the variability (shown by the output of the <code>sd(estimated)</code> command above) in the estimated proportion; the parameter <span class="math inline">\(\theta=0.80\)</span> itself is invariant here (we are repeatedly estimating this point value).</p>
<p>However, consider now an alternative radical idea: what if we treat <span class="math inline">\(\theta\)</span> as a random variable? That is, suppose now that <span class="math inline">\(\theta\)</span> has a PDF associated with it. This PDF would now represent our belief about possible values of <span class="math inline">\(\theta\)</span>, even before we have seen any data. For example, if at the outset of the experiment, we believe that all possible values between 0 and 1 are equally likely, we could represent that belief by stating that <span class="math inline">\(\theta \sim \mathit{Uniform}(0,1)\)</span>. The radical new idea here is that we now have a way to represent our prior belief or knowledge about plausible values of the parameter.</p>
<p>Now, if we were to run our simulated experiments again and again, there would be <em>two</em> sources of variability in the estimate of the parameter: the data as well as the uncertainty associated with <span class="math inline">\(\theta\)</span>.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb82-1"><a href="ch-introBDA.html#cb82-1" aria-hidden="true"></a>theta &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">100</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb82-2"><a href="ch-introBDA.html#cb82-2" aria-hidden="true"></a>estimated_means &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> theta) <span class="op">/</span><span class="st"> </span><span class="dv">10</span></span>
<span id="cb82-3"><a href="ch-introBDA.html#cb82-3" aria-hidden="true"></a><span class="kw">sd</span>(estimated_means)</span></code></pre></div>
<pre><code>## [1] 0.336</code></pre>
<p>The higher standard deviation is now coming from the uncertainty associated with the <span class="math inline">\(\theta\)</span> parameter. To see this, assume a “tighter” PDF for <span class="math inline">\(\theta\)</span>, say <span class="math inline">\(\theta \sim \mathit{Uniform}(0.3,0.8)\)</span>, then the variability in the estimated means would again be smaller, but not as small as when we assumed that <span class="math inline">\(\theta\)</span> was a point value:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb84-1"><a href="ch-introBDA.html#cb84-1" aria-hidden="true"></a>theta&lt;-<span class="kw">runif</span>(<span class="dv">100</span>,<span class="dt">min=</span><span class="fl">0.3</span>,<span class="dt">max=</span><span class="fl">0.8</span>)</span>
<span id="cb84-2"><a href="ch-introBDA.html#cb84-2" aria-hidden="true"></a>estimated_means&lt;-<span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span>theta)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb84-3"><a href="ch-introBDA.html#cb84-3" aria-hidden="true"></a><span class="kw">sd</span>(estimated_means)</span></code></pre></div>
<pre><code>## [1] 0.195</code></pre>
<p>In other words, the greater the uncertainty associated with the parameter <span class="math inline">\(\theta\)</span>, the greater the variability in the data.</p>
<p>The Bayesian approach to parameter estimation makes this radical departure from the standard frequentist assumption that <span class="math inline">\(\theta\)</span> is a point value; in the Bayesian approach, <span class="math inline">\(\theta\)</span> is a random variable with a probability density/mass function associated with it. This PDF is called a prior distribution, and represents our prior belief or prior knowledge about possible values of this parameter. Once we obtain data, these data serve to modify our prior belief about this distribution; this updated probability density function of the parameter is called the posterior distribution. These ideas are unpacked in the sections below.</p>
<div id="choosing-a-likelihood" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Choosing a  likelihood<a href="ch-introBDA.html#choosing-a-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Under the assumptions we have set up above, the responses follow a binomial distribution, and so the PMF can be written as follows.</p>
<p><span class="math display" id="eq:binom">\[\begin{equation}
p(k|n,\theta) = \binom{n}
{k} \theta^k (1-\theta)^{n-k}
\tag{2.3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(k\)</span> indicates the number of times “umbrella” is given as an answer, and <span class="math inline">\(n\)</span> the total number of answers given. Here, <span class="math inline">\(k\)</span> can be any whole number going from <span class="math inline">\(0\)</span> to <span class="math inline">\(100\)</span>.</p>
<p>In a particular experiment that we carry out, if we collect <span class="math inline">\(100\)</span> data points (<span class="math inline">\(n=100\)</span>) and it turns out that <span class="math inline">\(k = 80\)</span>, these data are now a fixed quantity. The only variable now in the PMF above is <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[\begin{equation}
p(k=80 | n= 100,  \theta) = \binom{n}{k} \theta^{80} (1-\theta)^{20}
\end{equation}\]</span></p>
<p>The above function is a now a continuous function of the value <span class="math inline">\(\theta\)</span>, which has possible values ranging from 0 to 1. Compare this to the PMF of the binomial, which treats <span class="math inline">\(\theta\)</span> as a fixed value and defines a discrete distribution over the n+1 possible discrete values <span class="math inline">\(k\)</span> that we can observe (the possible number of successes).</p>
<p>Recall that the  PMF and the  likelihood are the same function seen from different points of view. The only difference between the two is what is considered to be fixed and what is varying. The PMF treats data as varying from experiment to experiment and <span class="math inline">\(\theta\)</span> as fixed, whereas the likelihood function treats the data as fixed and the parameter <span class="math inline">\(\theta\)</span> as varying.</p>
<p>We now turn our attention back to our main goal, which is to find out, using Bayes’ rule, the posterior distribution of <span class="math inline">\(\theta\)</span> given our data: <span class="math inline">\(p(\theta|n,k)\)</span>. In order to use Bayes’ rule to calculate this posterior distribution, we need to define a prior distribution over the parameter <span class="math inline">\(\theta\)</span>. In doing so, we are explicitly expressing our prior uncertainty about plausible values of <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="sec-choosepriortheta" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Choosing a  prior for <span class="math inline">\(\theta\)</span><a href="ch-introBDA.html#sec-choosepriortheta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the choice of prior for <span class="math inline">\(\theta\)</span> in the binomial distribution, we need to assume that the parameter <span class="math inline">\(\theta\)</span> is a random variable that has a PDF whose range lies within [0,1], the range over which <span class="math inline">\(\theta\)</span> can vary (this is because <span class="math inline">\(\theta\)</span> represents a probability). The  beta distribution, which is a PDF for a continuous random variable, is commonly used as prior for parameters representing probabilities. One reason for this choice is that its PDF ranges over the interval <span class="math inline">\([0,1]\)</span>. The other reason for this choice is that it makes the Bayes’ rule calculation remarkably easy.</p>
<p>The beta distribution has the following PDF.</p>
<p><span class="math display" id="eq:betach2">\[\begin{equation}
p(\theta|a,b)=  \frac{1}{B(a,b)} \theta^{a - 1} (1-\theta)^{b-1}   
\tag{2.4}
\end{equation}\]</span></p>
<p>The term <span class="math inline">\(B(a,b)\)</span> expands to <span class="math inline">\(\int_0^1 \theta^{a-1}(1-\theta)^{b-1}\, d\theta\)</span>, and is a normalizing constant that ensures that the area under the curve sums to one. In some textbooks, you may see the PDF of the beta distribution with the normalizing constant <span class="math inline">\(\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\)</span> (the expression <span class="math inline">\(\Gamma(n)\)</span> is defined as (n-1)!): <span class="math display">\[p(\theta|a,b)=  \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \theta^{a - 1} (1-\theta)^{b-1}\]</span> These two statements for the beta distribution are identical because <span class="math inline">\(B(a,b)\)</span> can be shown to be equal to <span class="math inline">\(\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\)</span> <span class="citation">(Ross <a href="#ref-RossProb" role="doc-biblioref">2002</a>)</span>.</p>
<p>The beta distribution’s parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> can be interpreted as expressing our prior beliefs about the probability of success; <span class="math inline">\(a\)</span> represents the number of “successes”, in our case, answers that are “umbrella” and <span class="math inline">\(b\)</span> the number of failures, the answers that are not “umbrella”. Figure <a href="ch-introBDA.html#fig:betas2">2.1</a> shows the different beta distribution shapes given different values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:betas2"></span>
<img src="bookdown_files/figure-html/betas2-1.svg" alt="Examples of beta distributions with different parameters." width="672" />
<p class="caption">
FIGURE 2.1: Examples of beta distributions with different parameters.
</p>
</div>
<p>As in the binomial and normal distributions that we saw in chapter 1, one can analytically derive the formulas for the expectation and variance of the beta distribution. These are:</p>
<p><span class="math display" id="eq:meanvar">\[\begin{equation}
\operatorname{E}[X] = \frac{a}{a+b} \quad \operatorname{Var}(X)=\frac {a \times b }{(a + b )^{2}(a + b +1)}
\tag{2.5}
\end{equation}\]</span></p>
<p>As an example, choosing <span class="math inline">\(a=4\)</span> and <span class="math inline">\(b=4\)</span> would mean that the answer “umbrella” is as likely as a different answer, but we are relatively unsure about this. We could express our uncertainty by computing the region over which we are 95% certain that the value of the parameter lies; this is the  <em>95% credible interval</em>. For this, we would use the <code>qbeta</code> function in R; the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are called <code>shape1</code> and <code>shape2</code> in <code>R</code>.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb86-1"><a href="ch-introBDA.html#cb86-1" aria-hidden="true"></a><span class="kw">qbeta</span>(<span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dt">shape1 =</span> <span class="dv">4</span>, <span class="dt">shape2 =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## [1] 0.184 0.816</code></pre>
<p>The credible interval chosen above is an equal-tailed interval: the area below the lower bound and above the upper bound is the same (0.025 in the above case). One could define alternative intervals; for example, in a distribution with only one mode (one peak; a unimodal distribution), one could choose to use the narrowest interval that contains the mode. This is called the  highest posterior density interval (HDI). In skewed posterior distributions, the equal-tailed credible interval and the HDI will not be identical, because the HDI will have unequal tail probabilities. Some authors, such as <span class="citation">Kruschke (<a href="#ref-kruschke2014doing" role="doc-biblioref">2014</a>)</span>, prefer to report the HDI. We will use the equal-tailed interval in this book, simply because this is the standard output in <code>Stan</code> and <code>brms</code>.</p>
<p>If we were to choose <span class="math inline">\(a=10\)</span> and <span class="math inline">\(b=10\)</span>, we would still be assuming that a priori the answer “umbrella” is just as likely as some other answer, but now our prior uncertainty about this mean is lower, as the 95% credible interval computed below shows.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb88-1"><a href="ch-introBDA.html#cb88-1" aria-hidden="true"></a><span class="kw">qbeta</span>(<span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dt">shape1 =</span> <span class="dv">10</span>, <span class="dt">shape2 =</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## [1] 0.289 0.711</code></pre>
<p>In Figure <a href="ch-introBDA.html#fig:betas2">2.1</a>, we can see also the difference in uncertainty in these two examples graphically.</p>
<p>Which prior should we choose? In a real data analysis problem, the choice of prior would depend on what prior knowledge we want to bring into the analysis (see chapter <a href="ch-priors.html#ch-priors">6</a>). If we don’t have much prior information, we could use <span class="math inline">\(a=b=1\)</span>; this gives us a uniform prior (i.e., <span class="math inline">\(\mathit{Uniform}(0,1)\)</span>). This kind of prior goes by various names, such as  <em>flat, non-informative prior</em>, or  <em>uninformative prior</em>. By contrast, if we have a lot of prior knowledge and/or a strong belief (e.g., based on a particular theory’s predictions, or prior data) that <span class="math inline">\(\theta\)</span> has a particular range of plausible values, we can use a different set of <span class="math inline">\(a, b\)</span> values to reflect our belief about the parameter. Generally speaking, the larger our parameters a and b, the narrower the spread of the distribution; i.e., the lower our uncertainty about the mean value of the parameter.</p>
<p>We will discuss prior specification in detail later in chapter <a href="ch-priors.html#ch-priors">6</a>. For the moment, just for illustration, we choose the values <span class="math inline">\(a=4\)</span> and <span class="math inline">\(b=4\)</span> for the beta prior. Then, our prior for <span class="math inline">\(\theta\)</span> is the following beta PDF:</p>
<p><span class="math display">\[\begin{equation}
p(\theta) = \frac{1}{B(4,4)} \theta^{3} (1-\theta)^{3}
\end{equation}\]</span></p>
<p>Having chosen a likelihood, and having defined a prior on <span class="math inline">\(\theta\)</span>, we are ready to carry out our first Bayesian analysis to derive a posterior distribution for <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="using-bayes-rule-to-compute-the-posterior-pthetank" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having specified the likelihood and the prior, we will now use Bayes’ rule to calculate <span class="math inline">\(p(\theta|n,k)\)</span>. Using Bayes’ rule simply involves replacing the Likelihood and the Prior we defined above into the equation we saw earlier:</p>
<p><span class="math display">\[\begin{equation}
\hbox{Posterior} = \frac{\hbox{Likelihood} \times  \hbox{Prior}}{\hbox{Marginal Likelihood}}
\end{equation}\]</span></p>
<p>Replace the terms for likelihood and prior into this equation:</p>
<p><span class="math display" id="eq:betaunpost">\[\begin{equation}
p(\theta|n=100,k=80) = \frac{\left[\binom{100}{80} \theta^{80} \times  (1-\theta)^{20}\right]  \times \left[\frac{1}{B(4,4)} \times \theta^{3} (1-\theta)^{3}\right]}{p(k=80)}
\tag{2.6}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p(k=80)\)</span> is <span class="math inline">\(\int_{0}^1 p(k=80|n=100,\theta) p(\theta)\, d\theta\)</span>. This term will be a constant once the number of successes <span class="math inline">\(k\)</span> is known; this is the marginal likelihood we encountered in chapter 1. In fact, once <span class="math inline">\(k\)</span> is known, there are several constant values in the above equation; they are constants because none of them depend on the parameter of interest, <span class="math inline">\(\theta\)</span>. We can collect all of these together:</p>
<p><span class="math display" id="eq:betaunpost2">\[\begin{equation}
p(\theta|n=100,k=80) =   \left[ \frac{\binom{100}{80}}{B(4,4)\times p(k=80)} \right]   [\theta^{80} (1-\theta)^{20} \times  \theta^{3} (1-\theta)^{3}]
\tag{2.7}
\end{equation}\]</span></p>
<p>The first term that is in square brackets, <span class="math inline">\(\frac{\binom{100}{80}}{B(4,4)\times p(k=80)}\)</span>, is all the constants collected together, and is the normalizing constant we have seen before; it makes the posterior distribution <span class="math inline">\(p(\theta|n=100,k=80)\)</span> sum to one. Since it is a constant, we can ignore it for now and focus on the two other terms in the equation. Because we are ignoring the constant, we will now say that the posterior is proportional to the right-hand side.</p>
<p><span class="math display" id="eq:betaunpost3">\[\begin{equation}
p(\theta|n=100,k=80) \propto   [\theta^{80} (1-\theta)^{20} \times \theta^{3} (1-\theta)^{3} ]
\tag{2.8}
\end{equation}\]</span></p>
<p>A common way of writing the above equation is:</p>
<p><span class="math display">\[\begin{equation}
\hbox{Posterior} \propto \hbox{Likelihood} \times \hbox{Prior}
\end{equation}\]</span></p>
<p>Resolving the right-hand side now simply involves adding up the exponents! In this example, computing the posterior really does boil down to this simple addition operation on the exponents.</p>
<p><span class="math display" id="eq:betaunpost4">\[\begin{equation}
p(\theta|n=100,k=80) \propto   [\theta^{80+3} (1-\theta)^{20+3}] = \theta^{83} (1-\theta)^{23}
\tag{2.9}
\end{equation}\]</span></p>
<p>The expression on the right-hand side corresponds to a beta distribution with parameters <span class="math inline">\(a=84\)</span>, and <span class="math inline">\(b=24\)</span>. This becomes evident if we rewrite the right-hand side such that it represents the core part of a beta PDF (see equation <a href="ch-introBDA.html#eq:betach2">(2.4)</a>). All that is missing is a normalizing constant which would make the area under the curve sum to one.</p>
<p><span class="math display">\[\begin{equation}
\theta^{83} (1-\theta)^{23} = \theta^{84-1} (1-\theta)^{24-1} 
\end{equation}\]</span></p>
<p>This core part of any PDF or PMF is called the  kernel of that distribution. Without a normalizing constant, the  area under the curve will not sum to one. Let’s check this:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb90-1"><a href="ch-introBDA.html#cb90-1" aria-hidden="true"></a>PostFun &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</span>
<span id="cb90-2"><a href="ch-introBDA.html#cb90-2" aria-hidden="true"></a>  theta<span class="op">^</span><span class="dv">84</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">24</span></span>
<span id="cb90-3"><a href="ch-introBDA.html#cb90-3" aria-hidden="true"></a>}</span>
<span id="cb90-4"><a href="ch-introBDA.html#cb90-4" aria-hidden="true"></a>(AUC &lt;-<span class="st"> </span><span class="kw">integrate</span>(PostFun, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value)</span></code></pre></div>
<pre><code>## [1] 1.42e-26</code></pre>
<p>So the area under the curve (AUC) is not 1—the posterior that we computed above is not a proper probability distribution. What we have just done above is to compute the following integral:</p>
<p><span class="math display">\[\begin{equation}
\int_{0}^{1} \theta^{84} (1-\theta)^{24} 
\end{equation}\]</span></p>
<p>We can use this integral to figure out what the normalizing constant is. Basically, we want to know what the constant k is such that the area under the curve sums to 1:</p>
<p><span class="math display">\[\begin{equation}
k \int_{0}^{1} \theta^{84} (1-\theta)^{24} = 1
\end{equation}\]</span></p>
<p>We know what <span class="math inline">\(\int_{0}^{1} \theta^{84} (1-\theta)^{24}\)</span> is; we just computed that value (called <code>AUC</code> in the <code>R</code> code above). So, the  normalizing constant is:</p>
<p><span class="math display">\[\begin{equation}
k  = \frac{1}{\int_{0}^{1} \theta^{84} (1-\theta)^{24}} = \frac{1}{AUC}
\end{equation}\]</span></p>
<p>So, all that is needed to make the kernel <span class="math inline">\(\theta^{84} (1-\theta)^{24}\)</span> into a proper probability distribution is to include a normalizing constant, which, according to the definition of the beta distribution (equation <a href="ch-introBDA.html#eq:betach2">(2.4)</a>), would be <span class="math inline">\(B(84,24)\)</span>. This term is in fact the integral we computed above.</p>
<p>So, what we have is the distribution of <span class="math inline">\(\theta\)</span> given the data, expressed as a PDF:</p>
<p><span class="math display">\[\begin{equation}
p(\theta|n=100,k=80) = \frac{1}{B(84,24)} \theta^{84-1} (1-\theta)^{24-1} 
\end{equation}\]</span></p>
<p>Now, this function will sum to one:</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb92-1"><a href="ch-introBDA.html#cb92-1" aria-hidden="true"></a>PostFun &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</span>
<span id="cb92-2"><a href="ch-introBDA.html#cb92-2" aria-hidden="true"></a>  theta<span class="op">^</span><span class="dv">84</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">24</span> <span class="op">/</span><span class="st"> </span>AUC</span>
<span id="cb92-3"><a href="ch-introBDA.html#cb92-3" aria-hidden="true"></a>}</span>
<span id="cb92-4"><a href="ch-introBDA.html#cb92-4" aria-hidden="true"></a><span class="kw">integrate</span>(PostFun, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
</div>
<div id="summary-of-the-procedure" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Summary of the procedure<a href="ch-introBDA.html#summary-of-the-procedure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To summarize, we started with data (<span class="math inline">\(n=100, k=80\)</span>) and a binomial likelihood, multiplied it with the prior <span class="math inline">\(\theta \sim \mathit{Beta}(4,4)\)</span>, and obtained the posterior <span class="math inline">\(p(\theta|n,k) \sim \mathit{Beta}(84,24)\)</span>. The constants were ignored when carrying out the multiplication; we say that we computed the posterior <em>up to proportionality</em>. Finally, we showed how, in this simple example, the posterior can be rescaled to become a probability distribution, by including a proportionality constant.</p>
<p>The above example is a case of a  <em>conjugate</em> analysis: the posterior on the parameter has the same form (belongs to the same family of probability distributions) as the prior. The above combination of likelihood and prior is called the  beta-binomial conjugate case. There are several other such combinations of Likelihoods and Priors that yield a posterior that has a PDF that belongs to the same family as the PDF on the prior; some examples will appear in the exercises.</p>
<p>Formally, conjugacy is defined as follows: Given the likelihood <span class="math inline">\(p(y| \theta)\)</span>, if the prior <span class="math inline">\(p(\theta)\)</span> results in a posterior <span class="math inline">\(p(\theta|y)\)</span> that has the same form as <span class="math inline">\(p(\theta)\)</span>, then we call <span class="math inline">\(p(\theta)\)</span> a  conjugate prior.</p>
<p>For the beta-binomial conjugate case, we can derive a very general relationship between the likelihood, prior, and posterior. Given the binomial likelihood up to proportionality (ignoring the constant) <span class="math inline">\(\theta^k (1-\theta)^{n-k}\)</span>, and given the prior, also up to proportionality, <span class="math inline">\(\theta^{a-1} (1-\theta)^{b-1}\)</span>, their product will be:</p>
<p><span class="math display">\[\begin{equation}
\theta^k (1-\theta)^{n-k} \theta^{a-1} (1-\theta)^{b-1} = \theta^{a+k-1} (1-\theta)^{b+n-k-1} 
\end{equation}\]</span></p>
<p>Thus, given a <span class="math inline">\(\mathit{Binomial}(n,k|\theta)\)</span> likelihood, and a <span class="math inline">\(\mathit{Beta}(a,b)\)</span> prior on <span class="math inline">\(\theta\)</span>, the posterior will be <span class="math inline">\(\mathit{Beta}(a+k,b+n-k)\)</span>.</p>
</div>
<div id="visualizing-the-prior-likelihood-and-posterior" class="section level3 hasAnchor" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Visualizing the prior, likelihood, and posterior<a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We established in the example above that the posterior is a beta distribution with parameters <span class="math inline">\(a=84\)</span>, and <span class="math inline">\(b = 24\)</span>. We visualize the likelihood, prior, and the posterior alongside each other in Figure <a href="ch-introBDA.html#fig:postbeta-viz">2.2</a>.</p>

<div class="figure"><span style="display:block;" id="fig:postbeta-viz"></span>
<img src="bookdown_files/figure-html/postbeta-viz-1.svg" alt="The (scaled) likelihood, prior, and posterior in the beta-binomial conjugate example. The likelihood is scaled to integrate to 1 to make it easier to compare to the prior and posterior distributions." width="672" />
<p class="caption">
FIGURE 2.2: The (scaled) likelihood, prior, and posterior in the beta-binomial conjugate example. The likelihood is scaled to integrate to 1 to make it easier to compare to the prior and posterior distributions.
</p>
</div>
<p>We can summarize the posterior distribution either graphically as we did above, or summarize it by computing the mean and the variance. The mean gives us an estimate of the cloze probability of producing “umbrella” in that sentence (given the model, i.e., given the likelihood and prior):</p>
<p><span class="math display" id="eq:meanPb">\[\begin{equation}
\operatorname{E}[\hat\theta] = \frac{84}{84+24}=0.78
\tag{2.10}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:varPb">\[\begin{equation}
\operatorname{var}[\hat\theta]=\frac {84 \times 24 }{(84+24 )^{2}(84+24 +1)}= 0.0016
\tag{2.11}
\end{equation}\]</span></p>
<p>We could also display the 95% credible interval, the range over which we are 95% certain the true value of <span class="math inline">\(\theta\)</span> lies, given the data and model.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb94-1"><a href="ch-introBDA.html#cb94-1" aria-hidden="true"></a><span class="kw">qbeta</span>(<span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dt">shape1 =</span> <span class="dv">84</span>, <span class="dt">shape2 =</span> <span class="dv">24</span>)</span></code></pre></div>
<pre><code>## [1] 0.695 0.851</code></pre>
<p>Typically, we would summarize the results of a Bayesian analysis by displaying the posterior distribution of the parameter (or parameters) graphically, along with the above summary statistics: the mean, the standard deviation or variance, and the 95% credible interval. You will see many examples of such summaries later.</p>
</div>
<div id="the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood" class="section level3 hasAnchor" number="2.2.6">
<h3><span class="header-section-number">2.2.6</span> The  posterior distribution is a compromise between the prior and the likelihood<a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall from the preceding sections that the <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> parameters in the beta distribution determine the shape of the prior distribution on the <span class="math inline">\(\theta\)</span> parameter. Just for the sake of illustration, let’s take four different beta priors, which reflect increasing prior certainty about <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><span class="math inline">\(\mathit{Beta}(a=2,b=2)\)</span></li>
<li><span class="math inline">\(\mathit{Beta}(a=3,b=3)\)</span></li>
<li><span class="math inline">\(\mathit{Beta}(a=6,b=6)\)</span></li>
<li><span class="math inline">\(\mathit{Beta}(a=21,b=21)\)</span></li>
</ul>
<p>Each of these priors reflects a belief that <span class="math inline">\(\theta=0.5\)</span>, but with varying degrees of (un)certainty. Given the general formula we developed above for the beta-binomial case, we just need to plug in the likelihood and the prior to get the posterior:</p>
<p><span class="math display">\[\begin{equation}
p(\theta | n,k) \propto p(k |n,\theta) p(\theta)
\end{equation}\]</span></p>
<p>The four corresponding posterior distributions would be:</p>
<p><span class="math display">\[\begin{equation}
p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{2-1}(1-\theta)^{2-1}] = \theta^{82-1} (1-\theta)^{22-1}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{3-1}(1-\theta)^{3-1}] = \theta^{83-1} (1-\theta)^{23-1}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{6-1}(1-\theta)^{6-1}] = \theta^{86-1} (1-\theta)^{26-1}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{21-1}(1-\theta)^{21-1}] = \theta^{101-1} (1-\theta)^{41-1}
\end{equation}\]</span></p>
<p>We can visualize each of these triplets of priors, likelihoods and posteriors; see Figure <a href="ch-introBDA.html#fig:postbetavizvar">2.3</a>.</p>
<pre><code> ## Warning in get_plot_component(plot, &quot;guide-box&quot;):
 ## Multiple components found; returning the first one. To
 ## return all, use `return_all = TRUE`. </code></pre>
<div class="figure"><span style="display:block;" id="fig:postbetavizvar"></span>
<img src="bookdown_files/figure-html/postbetavizvar-1.svg" alt="The (scaled) likelihood, prior, and posterior in the beta-binomial conjugate example, for different uncertainties in the prior. The likelihood is scaled to integrate to 1 to make its comparison easier.  " width="672" />
<p class="caption">
FIGURE 2.3: The (scaled) likelihood, prior, and posterior in the beta-binomial conjugate example, for different uncertainties in the prior. The likelihood is scaled to integrate to 1 to make its comparison easier.
</p>
</div>
<p>Given some data and given a likelihood function, the tighter the prior, the greater the extent to which the posterior orients itself towards the prior. In general, we can say the following about the likelihood-prior-posterior relationship:</p>
<ul>
<li>The posterior distribution of a parameter is a compromise between the prior and the likelihood.</li>
<li>For a given set of data, the greater the certainty in the prior, the more heavily will the posterior be influenced by the prior mean.</li>
<li>Conversely, for a given set of data, the greater the <em>un</em>certainty in the prior, the more heavily will the posterior be influenced by the likelihood.</li>
</ul>
<p>Another important observation emerges if we increase the sample size from <span class="math inline">\(100\)</span> to, say, <span class="math inline">\(1000000\)</span>. Suppose we still get a sample mean of <span class="math inline">\(0.8\)</span> here, so that <span class="math inline">\(k=800000\)</span>. Now, the posterior mean will be influenced almost entirely by the sample mean. This is because, in the general form for the posterior <span class="math inline">\(\mathit{Beta}(a+k,b+n-k)\)</span> that we computed above, the <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> become very large relative to the a, b values, and dominate in determining the posterior mean.</p>
<p>Whenever we do a Bayesian analysis, it is good practice to check whether the parameter you are interested in estimating is sensitive to the prior specification. Such an investigation is called a  <em>sensitivity analysis</em>. Later in this book, we will see many examples of sensitivity analyses in realistic data-analysis settings.</p>
</div>
<div id="incremental-knowledge-gain-using-prior-knowledge" class="section level3 hasAnchor" number="2.2.7">
<h3><span class="header-section-number">2.2.7</span> Incremental knowledge gain using prior knowledge<a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the above example, we used an artificial example where we asked <span class="math inline">\(100\)</span> subjects to complete the sentence shown at the beginning of the chapter, and then we counted the number of times that they produced “umbrella” vs. some other word as a continuation. Given 80 instances of “umbrella”, and using a <span class="math inline">\(\mathit{Beta}(4,4)\)</span> prior, we derived the posterior to be <span class="math inline">\(\mathit{Beta}(84,24)\)</span>. We could now use this posterior as our prior for the next study. Suppose that we were to carry out a second experiment, again with 100 subjects, and this time <span class="math inline">\(60\)</span> produced “umbrella”. We could now use our new prior (<span class="math inline">\(\mathit{Beta}(84,24)\)</span>) to obtain an updated posterior. We have <span class="math inline">\(a=84, b=24, n=100, k=60\)</span>. This gives us as posterior:
<span class="math inline">\(\mathit{Beta}(a+k,b+n-k) = \mathit{Beta}(84+60,24+100-60)=\mathit{Beta}(144,64)\)</span>.</p>
<p>Now, if we were to pool all our data that we have from the two experiments, then we would have as data <span class="math inline">\(n=200, k=140\)</span>. Suppose that we keep our initial prior of <span class="math inline">\(a=4,b=4\)</span>. Then, our posterior would be <span class="math inline">\(\mathit{Beta}(4+140,4+200-140)=\mathit{Beta}(144,64)\)</span>. This is exactly the same posterior that we got when first analyzed the first <span class="math inline">\(100\)</span> subjects’ data, derived the posterior, and then used that posterior as a prior for the next <span class="math inline">\(100\)</span> subjects’ data.</p>
<p>This toy example illustrates an important point that has great practical importance for cognitive science. One can incrementally gain information about a research question by using information from previous studies and deriving a posterior, and then use that posterior as a prior. For practical examples from psycholinguistics showing how information can be pooled from previous studies, see <span class="citation">Jäger, Engelmann, and Vasishth (<a href="#ref-JaegerEngelmannVasishth2017" role="doc-biblioref">2017</a>)</span> and <span class="citation">Nicenboim, Roettger, and Vasishth (<a href="#ref-NicenboimRoettgeretal" role="doc-biblioref">2018</a>)</span>. <span class="citation">Vasishth and Engelmann (<a href="#ref-VasishthEngelmann2022" role="doc-biblioref">2022</a>)</span> illustrates an example of how the posterior from a previous study or collection of studies can be used to compute the posterior derived from new data.</p>
</div>
</div>
<div id="summary-1" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Summary<a href="ch-introBDA.html#summary-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we learned how to use Bayes’ rule in the specific case of a binomial likelihood, and a beta prior on the <span class="math inline">\(\theta\)</span> parameter in the likelihood function. Our goal in any Bayesian analysis will follow the path we took in this simple example: decide on an appropriate likelihood function, decide on priors for all the parameters involved in the likelihood function, and use this model (i.e., the likelihood and the priors) to derive the posterior distribution of each parameter. Then we draw inferences about our research question based on the posterior distribution of the parameter.</p>
<p>In the example discussed in this chapter, Bayesian analysis was easy. This was because we considered the simple conjugate case of the beta-binomial. In realistic data-analysis settings, our likelihood function will be very complex, and many parameters will be involved. Multiplying the likelihood function and the priors will become mathematically difficult or impossible. For such situations, we use computational methods to obtain samples from the posterior distributions of the parameters.</p>
</div>
<div id="further-reading-1" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Further reading<a href="ch-introBDA.html#further-reading-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Accessible introductions to conjugate Bayesian analysis are
<span class="citation">Lynch (<a href="#ref-lynch2007introduction" role="doc-biblioref">2007</a>)</span>, and <span class="citation">Lunn et al. (<a href="#ref-lunn2012bugs" role="doc-biblioref">2012</a>)</span>. Somewhat more demanding discussions of conjugate analysis are in <span class="citation">Lee (<a href="#ref-lee2012bayesian" role="doc-biblioref">2012</a>)</span>, <span class="citation">Carlin and Louis (<a href="#ref-carlin2008bayesian" role="doc-biblioref">2008</a>)</span>, <span class="citation">Christensen et al. (<a href="#ref-christensen2011" role="doc-biblioref">2011</a>)</span>, <span class="citation">O’Hagan and Forster (<a href="#ref-kendall2004" role="doc-biblioref">2004</a>)</span> and <span class="citation">Bernardo and Smith (<a href="#ref-bernardosmith" role="doc-biblioref">2009</a>)</span>.</p>
</div>
<div id="sec-BDAexercises" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Exercises<a href="ch-introBDA.html#sec-BDAexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:BDAexercisesDerivingBayes" class="exercise"><strong>Exercise 2.1  </strong></span>Deriving Bayes’ rule</p>
</div>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two observable events. <span class="math inline">\(P(A)\)</span> is the probability that <span class="math inline">\(A\)</span> occurs, and <span class="math inline">\(P(B)\)</span> is the probability that <span class="math inline">\(B\)</span> occurs. <span class="math inline">\(P(A|B)\)</span> is the conditional probability that <span class="math inline">\(A\)</span> occurs given that <span class="math inline">\(B\)</span> has happened. <span class="math inline">\(P(A,B)\)</span> is the joint probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> both occurring.</p>
<p>You are given the definition of conditional probability:</p>
<p><span class="math display">\[\begin{equation}
P(A|B)= \frac{P(A,B)}{P(B)} \hbox{ where } P(B)&gt;0
\end{equation}\]</span></p>
<p>Using the above definition, and using the fact that <span class="math inline">\(P(A,B)=P(B,A)\)</span> (i.e., the probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> both occurring is the same as the probability of <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> both occurring),
derive an expression for <span class="math inline">\(P(B|A)\)</span>. Show the steps clearly in the derivation.</p>
<div class="exercise">
<p><span id="exr:BDAexercisesConj1" class="exercise"><strong>Exercise 2.2  </strong></span>Conjugate forms 1</p>
</div>
<ul>
<li>Computing the general form of a PDF for a posterior</li>
</ul>
<p>Suppose you are given data <span class="math inline">\(k\)</span> consisting of the number of successes, coming from a <span class="math inline">\(\mathit{Binomial}(n,\theta)\)</span> distribution.
Given <span class="math inline">\(k\)</span> successes in n trials coming from a binomial distribution, we define a <span class="math inline">\(\mathit{Beta}(a,b)\)</span> prior on the parameter <span class="math inline">\(\theta\)</span>.</p>
<p>Write down the Beta distribution that represents the posterior, in terms of <span class="math inline">\(a,b, n,\)</span> and <span class="math inline">\(k\)</span>.</p>
<ul>
<li>Practical application</li>
</ul>
<p>We ask 10 yes/no questions from a subject, and the subject returns 0 correct answers. We assume a binomial likelihood function for these data. Also assume a <span class="math inline">\(\mathit{Beta}(1,1)\)</span> prior on the parameter <span class="math inline">\(\theta\)</span>, which represents the probability of success. Use the result you derived above to write down the posterior distribution of the <span class="math inline">\(\theta\)</span> parameter.</p>
<div class="exercise">
<p><span id="exr:BDAexercisesConj2" class="exercise"><strong>Exercise 2.3  </strong></span>Conjugate forms 2</p>
</div>
<p>Suppose that we perform <span class="math inline">\(n\)</span> independent trials until we get a success (e.g., a heads in a coin toss). For coin tosses, the possible outcomes could be, H, TH, . The probability of success in each trial is <span class="math inline">\(\theta\)</span>. Then, the Geometric random variable, call it <span class="math inline">\(X\)</span>, gives us the probability of getting a success in <span class="math inline">\(n\)</span> trials as follows:</p>
<p><span class="math display">\[\begin{equation}
Prob(X=n)=\theta(1-\theta)^{ n-1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(n=1,2,\dots\)</span>.</p>
<p>Let the prior on <span class="math inline">\(\theta\)</span> be <span class="math inline">\(\mathit{Beta}(a,b)\)</span>, a beta distribution with parameters a,b.
The posterior distribution is a beta distribution with parameters a* and b*. Determine these parameters in terms of <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(n\)</span>.</p>
<div class="exercise">
<p><span id="exr:BDAexercisesConj3" class="exercise"><strong>Exercise 2.4  </strong></span>Conjugate forms 3</p>
</div>
<p>The  Gamma distribution is defined in terms of the parameters a, b: Ga(a,b). If there is a random variable <span class="math inline">\(Y\)</span> (where <span class="math inline">\(y\geq 0\)</span>) that has a Gamma distribution as a PDF (<span class="math inline">\(Y\sim Gamma(a,b)\)</span>), then:</p>
<p><span class="math display">\[\begin{equation}
Ga(y | a,b)=\frac{b^a y^{a-1} \exp\{-by\}}{\Gamma(a)} 
\end{equation}\]</span></p>
<p>Suppose that we have <span class="math inline">\(n\)</span> data points, <span class="math inline">\(x_1,\dots, x_n\)</span>, that are drawn from an exponentially distributed. The  exponential likelihood function is:</p>
<p><span class="math display">\[\begin{equation}
p(x_1,\dots,x_n | \lambda)=\lambda^n \exp \{-\lambda \sum_{i=1}^n x_i \}
\end{equation}\]</span></p>
<p>It turns out that if we assume a Ga(a,b) prior distribution for <span class="math inline">\(\lambda\)</span> and the above Exponential likelihood, the posterior distribution of <span class="math inline">\(\lambda\)</span> is a Gamma distribution. In other words, the Gamma(a,b) prior on the <span class="math inline">\(\lambda\)</span> parameter in the Exponential distribution will be written:</p>
<p><span class="math display">\[\begin{equation}
Ga(\lambda | a,b)=\frac{b^a \lambda^{a-1} \exp\{-b\lambda\}}{\Gamma(a)}
\end{equation}\]</span></p>
<p>Find the parameters <span class="math inline">\(a&#39;\)</span> and <span class="math inline">\(b&#39;\)</span> of the posterior distribution.</p>
<div class="exercise">
<p><span id="exr:BDAexercisesConj4" class="exercise"><strong>Exercise 2.5  </strong></span>Conjugate forms 4</p>
</div>
<ul>
<li>Computing the posterior</li>
</ul>
<p>This is a contrived example. Suppose we are modeling the number of times that a speaker says the word “I” per day. This could be of interest if we are studying, for example, how self-oriented a speaker is. The number of times <span class="math inline">\(x\)</span> that the word is uttered in over a particular time period (here, one day) can be modeled by a  Poisson distribution (<span class="math inline">\(x=0,1,2,\dots\)</span>):</p>
<p><span class="math display">\[\begin{equation}
f(x\mid \theta) = \frac{\exp(-\theta) \theta^x}{x!}
\end{equation}\]</span></p>
<p>where the rate <span class="math inline">\(\theta\)</span> is unknown, and the numbers of utterances of the target word on each day are independent given <span class="math inline">\(\theta\)</span>.</p>
<p>We are told that the prior mean of <span class="math inline">\(\theta\)</span> is 100 and prior variance for <span class="math inline">\(\theta\)</span> is 225. This information is based on the results of previous studies on the topic. We will use the Gamma(a,b) density (see previous question) as a prior for <span class="math inline">\(\theta\)</span> because this is a conjugate prior to the Poisson distribution.</p>
<ol style="list-style-type: lower-alpha">
<li>First, visualize the prior, a Gamma density prior for <span class="math inline">\(\theta\)</span> based on the above information.</li>
</ol>
<p>[Hint: we know that for a Gamma density with parameters a, b, the mean is <span class="math inline">\(\frac{a}{b}\)</span> and the variance is <span class="math inline">\(\frac{a}{b^2}\)</span>. Since we are given values for the mean and variance, we can solve for a,b, which gives us the Gamma density.]</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Next, derive the posterior distribution of the parameter <span class="math inline">\(\theta\)</span> up to proportionality, and write down the posterior distribution in terms of the parameters of a Gamma distribution.</li>
</ol>
<ul>
<li>Practical application</li>
</ul>
<p>Suppose we know that the number of “I” utterances from a particular individual is <span class="math inline">\(115, 97, 79, 131\)</span>. Use the result you derived above to obtain the posterior distribution. In other words, write down the parameters of the Gamma distribution (call them <span class="math inline">\(a*,b*\)</span>) representing the posterior distribution of <span class="math inline">\(\theta\)</span>.</p>
<p>Plot the prior and the posterior distributions alongside each other.</p>
<p>Now suppose you get one new data point: 200. Using the posterior <span class="math inline">\(Gamma(a*,b*)\)</span> as your prior, write down the updated posterior (in terms of the updated parameters of the Gamma distribution) given this new data point. Add the updated posterior to the plot you made above.</p>
<div class="exercise">
<p><span id="exr:BDAexercisesWeightedMean" class="exercise"><strong>Exercise 2.6  </strong></span>The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)</p>
</div>
<p>The number of times an event happens per unit time can be modeled using a Poisson distribution, whose PMF is:</p>
<p><span class="math display">\[\begin{equation}
f(x\mid \theta) = \frac{\exp(-\theta) \theta^x}{x!}
\end{equation}\]</span></p>
<p>Suppose that we define a Gamma(a,b) prior for the rate parameter <span class="math inline">\(\theta\)</span>. It is a fact (see exercises above) that the posterior of the <span class="math inline">\(\theta\)</span> parameter is a <span class="math inline">\(Gamma(a*,b*)\)</span> distribution, where <span class="math inline">\(a*\)</span> and <span class="math inline">\(b*\)</span> are the updated parameters given the data: <span class="math inline">\(\theta \sim Gamma(a*,b*)\)</span>.</p>
<ul>
<li>Prove that the posterior mean is a weighted mean of the prior mean and the maximum likelihood estimate (mean) of the Poisson-distributed data, <span class="math inline">\(\bar{x} = \sum_{i=1}^n x/n\)</span>. Hint: the mean of a Gamma distribution is <span class="math inline">\(\frac{a}{b}\)</span>.</li>
</ul>
<p>Specifically, what you have to prove is that:</p>
<p><span class="math display" id="eq:weightingpoisga">\[\begin{equation} 
\frac{a*}{b*} = \frac{a}{b} \times \frac{w_1}{w_1 + w_2} + \bar{x} \times \frac{w_2}{w_1 + w_2} 
\tag{2.12}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(w_1 = 1\)</span> and <span class="math inline">\(w_2=\frac{n}{b}\)</span>.</p>
<ul>
<li><p>Given equation <a href="ch-introBDA.html#eq:weightingpoisga">(2.12)</a>, show that as <span class="math inline">\(n\)</span> increases (as sample size goes up), the maximum likelihood estimate <span class="math inline">\(\bar{x}\)</span> dominates in determining the posterior mean, and when <span class="math inline">\(n\)</span> gets smaller and smaller, the prior mean dominates in determining the posterior mean.</p></li>
<li><p>Finally, given that the variance of a Gamma distribution is <span class="math inline">\(\frac{a}{b^2}\)</span>, show that as <span class="math inline">\(n\)</span> increases, the posterior variance will get smaller and smaller (the uncertainty on the posterior will go down).</p></li>
</ul>

</div>
</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references hanging-indent">
<div id="ref-bernardosmith">
<p>Bernardo, José M., and Adrian F. M. Smith. 2009. <em>Bayesian Theory</em>. Vol. 405. John Wiley &amp; Sons.</p>
</div>
<div id="ref-carlin2008bayesian">
<p>Carlin, Bradley P., and Thomas A Louis. 2008. <em>Bayesian Methods for Data Analysis</em>. CRC Press.</p>
</div>
<div id="ref-christensen2011">
<p>Christensen, Ronald, Wesley Johnson, Adam Branscum, and Timothy Hanson. 2011. “Bayesian Ideas and Data Analysis.” CRC Press.</p>
</div>
<div id="ref-JaegerEngelmannVasishth2017">
<p>Jäger, Lena A., Felix Engelmann, and Shravan Vasishth. 2017. “Similarity-Based Interference in Sentence Comprehension: Literature review and Bayesian meta-analysis.” <em>Journal of Memory and Language</em> 94: 316–39. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004">https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004</a>.</p>
</div>
<div id="ref-kruschke2014doing">
<p>Kruschke, John K. 2014. <em>Doing Bayesian Data Analysis: A tutorial with R, JAGS, and Stan</em>. Academic Press.</p>
</div>
<div id="ref-lee2012bayesian">
<p>Lee, Peter M. 2012. <em>Bayesian Statistics: An Introduction</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-lunn2012bugs">
<p>Lunn, David J., Chris Jackson, David J. Spiegelhalter, Nichola G. Best, and Andrew Thomas. 2012. <em>The BUGS Book: A Practical Introduction to Bayesian Analysis</em>. Vol. 98. CRC Press.</p>
</div>
<div id="ref-lynch2007introduction">
<p>Lynch, Scott Michael. 2007. <em>Introduction to Applied Bayesian Statistics and Estimation for Social Scientists</em>. New York, NY: Springer.</p>
</div>
<div id="ref-NicenboimRoettgeretal">
<p>Nicenboim, Bruno, Timo B. Roettger, and Shravan Vasishth. 2018. “Using Meta-Analysis for Evidence Synthesis: The case of incomplete neutralization in German.” <em>Journal of Phonetics</em> 70: 39–55. <a href="https://doi.org/https://doi.org/10.1016/j.wocn.2018.06.001">https://doi.org/https://doi.org/10.1016/j.wocn.2018.06.001</a>.</p>
</div>
<div id="ref-kendall2004">
<p>O’Hagan, Anthony, and Jonathan J. Forster. 2004. “Kendall’s Advanced Theory of Statistics, Vol. 2B: Bayesian Inference.” Wiley.</p>
</div>
<div id="ref-RossProb">
<p>Ross, Sheldon. 2002. <em>A First Course in Probability</em>. Pearson Education.</p>
</div>
<div id="ref-VasishthEngelmann2022">
<p>Vasishth, Shravan, and Felix Engelmann. 2022. <em>Sentence Comprehension as a Cognitive Process: A Computational Approach</em>. Cambridge, UK: Cambridge University Press. <a href="https://books.google.de/books?id=6KZKzgEACAAJ">https://books.google.de/books?id=6KZKzgEACAAJ</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-compbda.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
