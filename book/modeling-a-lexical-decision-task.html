<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>20.1 Modeling a lexical decision task | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="20.1 Modeling a lexical decision task | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="20.1 Modeling a lexical decision task | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2022-02-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-lognormalrace.html"/>
<link rel="next" href="posterior-predictive-check-with-the-quantile-probability-plots.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="why-read-this-book-anyway.html"><a href="why-read-this-book-anyway.html"><i class="fa fa-check"></i>Why read this book anyway?</a></li>
<li class="chapter" data-level="" data-path="some-conventions-used-in-this-book.html"><a href="some-conventions-used-in-this-book.html"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="the-law-of-total-probability.html"><a href="the-law-of-total-probability.html"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#sec:contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#sec:generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="sec-marginal.html"><a href="sec-marginal.html"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>2.1</b> Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="sec-analytical.html"><a href="sec-analytical.html#sec:choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="sec-BDAexercises.html"><a href="sec-BDAexercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="sec-sampling.html"><a href="sec-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec-sampling.html"><a href="sec-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using Stan: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single subject pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="list-of-the-most-important-commands.html"><a href="list-of-the-most-important-commands.html"><i class="fa fa-check"></i><b>3.6</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.7" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>3.7</b> Summary</a></li>
<li class="chapter" data-level="3.8" data-path="sec-ch3furtherreading.html"><a href="sec-ch3furtherreading.html"><i class="fa fa-check"></i><b>3.8</b> Further reading</a></li>
<li class="chapter" data-level="3.9" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect response times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="sec-trial.html"><a href="sec-trial.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#sec:priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#sec:comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="sec-ch4furtherreading.html"><a href="sec-ch4furtherreading.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="sec-LMexercises.html"><a href="sec-LMexercises.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="exchangeability-and-hierarchical-models.html"><a href="exchangeability-and-hierarchical-models.html"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.2.1</b> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b> No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.3</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.3.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort.html"><a href="why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort.html"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="sec-HLMexercises.html"><a href="sec-HLMexercises.html"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a><ul>
<li class="chapter" data-level="6.1" data-path="sec-simpleexamplepriors.html"><a href="sec-simpleexamplepriors.html"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a><ul>
<li class="chapter" data-level="6.1.1" data-path="sec-simpleexamplepriors.html"><a href="sec-simpleexamplepriors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="sec-simpleexamplepriors.html"><a href="sec-simpleexamplepriors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="sec-simpleexamplepriors.html"><a href="sec-simpleexamplepriors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="sec-simpleexamplepriors.html"><a href="sec-simpleexamplepriors.html#sec:varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="eliciting-priors-from-experts.html"><a href="eliciting-priors-from-experts.html"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="deriving-priors-from-meta-analyses.html"><a href="deriving-priors-from-meta-analyses.html"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="using-previous-experiments-posteriors-as-priors-for-a-new-study.html"><a href="using-previous-experiments-posteriors-as-priors-for-a-new-study.html"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="summary-5.html"><a href="summary-5.html"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a><ul>
<li class="chapter" data-level="7.1" data-path="model-building.html"><a href="model-building.html"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a><ul>
<li class="chapter" data-level="7.3.1" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="summary-6.html"><a href="summary-6.html"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a><ul>
<li class="chapter" data-level="8.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="8.1.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#sec:cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="8.2.1" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a><ul>
<li class="chapter" data-level="8.3.1" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="8.4.1" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="computing-condition-means-from-estimated-contrasts.html"><a href="computing-condition-means-from-estimated-contrasts.html"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="summary-7.html"><a href="summary-7.html"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="sec-Contrastsexercises.html"><a href="sec-Contrastsexercises.html"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="9.1" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a><ul>
<li class="chapter" data-level="9.1.1" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sec-contrast-covariate.html"><a href="sec-contrast-covariate.html"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-contrast-covariate.html"><a href="sec-contrast-covariate.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="sec-contrast-covariate.html"><a href="sec-contrast-covariate.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="sec-interactions-NLM.html"><a href="sec-interactions-NLM.html"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="summary-8.html"><a href="summary-8.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="sec-Contrasts2x2exercises.html"><a href="sec-Contrasts2x2exercises.html"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="10.1" data-path="stan-syntax.html"><a href="stan-syntax.html"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="sec-firststan.html"><a href="sec-firststan.html"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="sec-clozestan.html"><a href="sec-clozestan.html"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the Binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="10.4.1" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="summary-9.html"><a href="summary-9.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="further-reading-7.html"><a href="further-reading-7.html"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="11.1" data-path="sec-hierstan.html"><a href="sec-hierstan.html"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="11.1.1" data-path="sec-hierstan.html"><a href="sec-hierstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec-hierstan.html"><a href="sec-hierstan.html#sec:uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="sec-hierstan.html"><a href="sec-hierstan.html#sec:corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="sec-hierstan.html"><a href="sec-hierstan.html#sec:crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="summary-10.html"><a href="summary-10.html"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="further-reading-8.html"><a href="further-reading-8.html"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a><ul>
<li class="chapter" data-level="12.1" data-path="sec-change.html"><a href="sec-change.html"><i class="fa fa-check"></i><b>12.1</b> A change of variables with reciprocal normal distribution</a><ul>
<li class="chapter" data-level="12.1.1" data-path="sec-change.html"><a href="sec-change.html#simulation-based-calibration"><i class="fa fa-check"></i><b>12.1.1</b> Simulation based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="a-custom-distribution-re-implementing-the-exponential-distribution-manually.html"><a href="a-custom-distribution-re-implementing-the-exponential-distribution-manually.html"><i class="fa fa-check"></i><b>12.2</b> A custom distribution: Re-implementing the exponential distribution manually</a></li>
<li class="chapter" data-level="12.3" data-path="further-reading-9.html"><a href="further-reading-9.html"><i class="fa fa-check"></i><b>12.3</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>IV Other useful models</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="13.1" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="13.1.1" data-path="meta-analysis.html"><a href="meta-analysis.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="measurement-error-models.html"><a href="measurement-error-models.html#accounting-for-measurement-error-in-a-voice-onset-time-model"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in a voice onset time model</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="summary-11.html"><a href="summary-11.html"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="further-reading-10.html"><a href="further-reading-10.html"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="sec-REMAMEexercises.html"><a href="sec-REMAMEexercises.html"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a><ul>
<li class="chapter" data-level="14.1" data-path="further-reading-11.html"><a href="further-reading-11.html"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a><ul>
<li class="chapter" data-level="15.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="15.1.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="sec-N400BF.html"><a href="sec-N400BF.html"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a><ul>
<li class="chapter" data-level="15.2.1" data-path="sec-N400BF.html"><a href="sec-N400BF.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="sec-N400BF.html"><a href="sec-N400BF.html#sec:BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest.html"><a href="the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest.html"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="bayes-factor-in-stan.html"><a href="bayes-factor-in-stan.html"><i class="fa fa-check"></i><b>15.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="bayes-factors-in-theory-and-in-practice.html"><a href="bayes-factors-in-theory-and-in-practice.html"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a><ul>
<li class="chapter" data-level="15.5.1" data-path="bayes-factors-in-theory-and-in-practice.html"><a href="bayes-factors-in-theory-and-in-practice.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="bayes-factors-in-theory-and-in-practice.html"><a href="bayes-factors-in-theory-and-in-practice.html#bayes-factors-in-practice-variability-with-the-data"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="summary-12.html"><a href="summary-12.html"><i class="fa fa-check"></i><b>15.6</b> Summary</a></li>
<li class="chapter" data-level="15.7" data-path="further-reading-12.html"><a href="further-reading-12.html"><i class="fa fa-check"></i><b>15.7</b> Further reading</a></li>
<li class="chapter" data-level="15.8" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>15.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a><ul>
<li class="chapter" data-level="16.1" data-path="expected-log-predictive-density-of-a-model.html"><a href="expected-log-predictive-density-of-a-model.html"><i class="fa fa-check"></i><b>16.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="k-fold-and-leave-one-out-cross-validation.html"><a href="k-fold-and-leave-one-out-cross-validation.html"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="sec-logcv.html"><a href="sec-logcv.html"><i class="fa fa-check"></i><b>16.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="issues-with-cross-validation.html"><a href="issues-with-cross-validation.html"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="cross-validation-in-stan.html"><a href="cross-validation-in-stan.html"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a><ul>
<li class="chapter" data-level="16.6.1" data-path="cross-validation-in-stan.html"><a href="cross-validation-in-stan.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="summary-13.html"><a href="summary-13.html"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="further-reading-13.html"><a href="further-reading-13.html"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="17.1" data-path="further-reading-14.html"><a href="further-reading-14.html"><i class="fa fa-check"></i><b>17.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="18.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>18.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="18.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html"><a href="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a><ul>
<li class="chapter" data-level="18.2.1" data-path="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html"><a href="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html"><a href="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html#sec:mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html"><a href="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html#sec:MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html"><a href="modeling-picture-naming-abilities-in-aphasia-with-mpt-models.html#sec:MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="further-reading-15.html"><a href="further-reading-15.html"><i class="fa fa-check"></i><b>18.3</b> Further reading</a></li>
<li class="chapter" data-level="18.4" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a><ul>
<li class="chapter" data-level="19.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a><ul>
<li class="chapter" data-level="19.1.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#sec:simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#sec:multmix"><i class="fa fa-check"></i><b>19.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#sec:fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="summary-14.html"><a href="summary-14.html"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="further-reading-16.html"><a href="further-reading-16.html"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a><ul>
<li class="chapter" data-level="20.1" data-path="modeling-a-lexical-decision-task.html"><a href="modeling-a-lexical-decision-task.html"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a><ul>
<li class="chapter" data-level="20.1.1" data-path="modeling-a-lexical-decision-task.html"><a href="modeling-a-lexical-decision-task.html#modeling-the-lexical-decision-task-with-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="modeling-a-lexical-decision-task.html"><a href="modeling-a-lexical-decision-task.html#sec:genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="modeling-a-lexical-decision-task.html"><a href="modeling-a-lexical-decision-task.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="modeling-a-lexical-decision-task.html"><a href="modeling-a-lexical-decision-task.html#sec:lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="modeling-a-lexical-decision-task.html"><a href="modeling-a-lexical-decision-task.html#sec:contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="posterior-predictive-check-with-the-quantile-probability-plots.html"><a href="posterior-predictive-check-with-the-quantile-probability-plots.html"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="summary-15.html"><a href="summary-15.html"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="further-reading-17.html"><a href="further-reading-17.html"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="exercises-6.html"><a href="exercises-6.html"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling-a-lexical-decision-task" class="section level2">
<h2><span class="header-section-number">20.1</span> Modeling a lexical decision task</h2>
<p>In a lexical decision task<!-- [which is closely related to visual word recognition, e.g., @balota1994visual] -->, a subject is presented with a string of letters on the screen and they need to decide whether the string is a word or a non-word; see Figure <a href="modeling-a-lexical-decision-task.html#fig:LDT-tikz">20.1</a>. In the example developed below, a subset of 600 words and 600 non-words from 20 subjects (<span class="math inline">\(600\times2 \times 20\)</span> data points) are used from the data of the British Lexicon project <span class="citation">(Keuleers et al. <a href="#ref-keuleers2012british">2012</a>)</span>. The data are stored as the object <code>df_blp</code> in the package <code>bcogsci</code>. In this data set, the lexicality of the string (word or non-word) is indicated in the column <code>lex</code>. The goal is to investigate how word frequency, shown in the column <code>freq</code> (frequency is counted per million words using the British National Corpus), affects the lexical decision task as quantified by accuracy and response time. For more details about the data set, type <code>?df_blp</code> on the <code>R</code> command line after loading the library <code>bcogsci</code>.</p>

<div class="figure"><span id="fig:LDT-tikz"></span>
<img src="bookdown_files/figure-html/LDT-tikz-1.svg" alt="Two trials in a lexical decision task. For the first trial, rurble, the correct answer would be to press the key on a keyboard or a response console that is mapped to the “non-word” response, for the second trial, monkey, the correct answer would be to press the key that is mapped to the “word” response." width="672" />
<p class="caption">
FIGURE 20.1: Two trials in a lexical decision task. For the first trial, <code>rurble</code>, the correct answer would be to press the key on a keyboard or a response console that is mapped to the “non-word” response, for the second trial, <code>monkey</code>, the correct answer would be to press the key that is mapped to the “word” response.
</p>
</div>
<div class="sourceCode" id="cb1173"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1173-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_blp&quot;</span>)</a>
<a class="sourceLine" id="cb1173-2" data-line-number="2">df_blp</a></code></pre></div>
<pre><code>## # A tibble: 24,000 × 8
##    subj block lex      trial string    acc    rt  freq
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1    57 non-word 28263 paybods     1   591     0
## 2     1    53 non-word 26414 lunned      1   621     0
## 3     1    49 non-word 24333 pertrax     1   575     0
## # … with 23,997 more rows</code></pre>
<p>The following code chunk adds <span class="math inline">\(0.01\)</span> (which corresponds to a word that appears only once in the corpus) to avoid word frequencies of zero, and then log-transforms the frequencies to compress their range of values <span class="citation">(see Brysbaert, Mandera, and Keuleers <a href="#ref-BrysbaertEtAl2018">2018</a> for a more in-depth treatment of word frequencies)</span> and centers them. It also creates a new variable that sum-codes the lexicality of the each given string (either a word, <span class="math inline">\(0.5\)</span>, or a non-word, <span class="math inline">\(-0.5\)</span>).</p>
<div class="sourceCode" id="cb1175"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1175-1" data-line-number="1">df_blp &lt;-<span class="st"> </span>df_blp <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1175-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">lfreq =</span> <span class="kw">log</span>(freq <span class="op">+</span><span class="st"> </span><span class="fl">0.01</span>),</a>
<a class="sourceLine" id="cb1175-3" data-line-number="3">         <span class="dt">c_lfreq =</span> lfreq <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(lfreq),</a>
<a class="sourceLine" id="cb1175-4" data-line-number="4">         <span class="dt">c_lex =</span> <span class="kw">ifelse</span>(lex <span class="op">==</span><span class="st"> &quot;word&quot;</span>, <span class="fl">0.5</span>, <span class="fl">-0.5</span>))</a></code></pre></div>
<!-- https://github.com/stan-dev/stancon_talks/blob/master/2018-helsinki/Contributed-Talks/nicenboim/LBA_stancon2018.Rmd -->
<p>If one wants to study the effect of frequency on words, the “traditional” way to analyze these data would be to fit response times and choice data in two separate models on words, ignoring non-words. One model would be fit on the response times of correct responses, and a second model on the accuracy. These two models are fit below.</p>
<p>To fit the response times model, subset the correct responses given to strings that are words.</p>
<div class="sourceCode" id="cb1176"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1176-1" data-line-number="1">df_blp_word_c &lt;-<span class="st"> </span>df_blp <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1176-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(acc <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, lex <span class="op">==</span><span class="st"> &quot;word&quot;</span>)</a></code></pre></div>
<p>Fit a hierarchical model with a log-normal likelihood and log-transformed frequency as a predictor (using <code>brms</code> here) and relateively weak priors.</p>
<div class="sourceCode" id="cb1177"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1177-1" data-line-number="1">fit_rt_word_c &lt;-<span class="st"> </span><span class="kw">brm</span>(rt <span class="op">~</span><span class="st"> </span>c_lfreq <span class="op">+</span><span class="st"> </span>(c_lfreq <span class="op">|</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb1177-2" data-line-number="2">              <span class="dt">data =</span> df_blp_word_c,</a>
<a class="sourceLine" id="cb1177-3" data-line-number="3">              <span class="dt">family =</span> lognormal,</a>
<a class="sourceLine" id="cb1177-4" data-line-number="4">              <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb1177-5" data-line-number="5">                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb1177-6" data-line-number="6">                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb1177-7" data-line-number="7">                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb1177-8" data-line-number="8">                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb1177-9" data-line-number="9">                <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb1177-10" data-line-number="10">              ), <span class="dt">iter =</span> <span class="dv">3000</span>)</a></code></pre></div>
<p>Show the estimate of the effect of log-frequency on the log-ms scale.</p>
<div class="sourceCode" id="cb1178"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1178-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_rt_word_c, <span class="dt">variable =</span> <span class="st">&quot;b_c_lfreq&quot;</span>)</a></code></pre></div>
<pre><code>##           Estimate Est.Error    Q2.5   Q97.5
## b_c_lfreq  -0.0379   0.00267 -0.0431 -0.0326</code></pre>
<p>To fit the accuracy model, subset the responses given to strings that are words.</p>
<div class="sourceCode" id="cb1180"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1180-1" data-line-number="1">df_blp_word &lt;-<span class="st"> </span>df_blp <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1180-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(lex <span class="op">==</span><span class="st"> &quot;word&quot;</span>)</a></code></pre></div>
<p>Fit a hierarchical model with a Bernoulli likelihood (and logit link) using log-transformed frequency as a predictor (using <code>brms</code>) and relatively weak priors.</p>
<div class="sourceCode" id="cb1181"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1181-1" data-line-number="1">fit_acc_word &lt;-<span class="st"> </span><span class="kw">brm</span>(acc <span class="op">~</span><span class="st"> </span>c_lfreq <span class="op">+</span><span class="st"> </span>(c_lfreq <span class="op">|</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb1181-2" data-line-number="2">              <span class="dt">data =</span> df_blp_word,</a>
<a class="sourceLine" id="cb1181-3" data-line-number="3">              <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> logit),</a>
<a class="sourceLine" id="cb1181-4" data-line-number="4">              <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb1181-5" data-line-number="5">                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb1181-6" data-line-number="6">                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb1181-7" data-line-number="7">                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb1181-8" data-line-number="8">                <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb1181-9" data-line-number="9">              ), <span class="dt">iter =</span> <span class="dv">3000</span>)</a></code></pre></div>
<p>Show the estimate of the effect of log-frequency on the log-odds scale.</p>
<div class="sourceCode" id="cb1182"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1182-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_acc_word, <span class="dt">variable =</span> <span class="st">&quot;b_c_lfreq&quot;</span>)</a></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## b_c_lfreq    0.573    0.0258 0.523 0.627</code></pre>
<p>For this specific data set, it does not matter whether response times or accuracy are chosen as the dependent variable, since both of them yield results with a similar interpretation: More frequent words are identified more easily, that is, with shorter reading times (this is evident from the negative sign on the estimate of the mean effect) and with better accuracy (positive sign on the estimate). However, it might be the case that some data set shows divergent directions in response times and accuracy. For example, more frequent words might take longer to identify, leading to a slowdown in response time as frequency increases but might still be identified more accurately.</p>
<p>Furthermore, two models are fit above, treating response times and accuracy as independent. In reality, there is plenty of evidence that they are related (e.g., speed-accuracy trade-off). Even in these data, as frequency increases, correct answers are given faster, and most errors are for low-frequency words (see Figure <a href="modeling-a-lexical-decision-task.html#fig:rtlexical">20.2</a>).</p>

<div class="sourceCode" id="cb1184"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb1184-1" data-line-number="1"></a>
<a class="sourceLine" id="cb1184-2" data-line-number="2">acc_lbl &lt;-<span class="st"> </span><span class="kw">as_labeller</span>(<span class="kw">c</span>(<span class="st">`</span><span class="dt">0</span><span class="st">`</span> =<span class="st"> &quot;Incorrect&quot;</span>, <span class="st">`</span><span class="dt">1</span><span class="st">`</span> =<span class="st"> &quot;Correct&quot;</span>))</a>
<a class="sourceLine" id="cb1184-3" data-line-number="3"><span class="kw">ggplot</span>(df_blp, <span class="kw">aes</span>(<span class="dt">y =</span> rt, <span class="dt">x =</span> freq <span class="op">+</span><span class="st"> </span><span class="fl">.01</span>, <span class="dt">shape =</span> lex, <span class="dt">color =</span> lex)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1184-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1184-5" data-line-number="5"><span class="st">  </span><span class="kw">facet_grid</span>(. <span class="op">~</span><span class="st"> </span>acc,  <span class="dt">labeller =</span>  <span class="kw">labeller</span>(<span class="dt">acc =</span> acc_lbl)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1184-6" data-line-number="6"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Frequency per million (log-scaled axis)&quot;</span>,</a>
<a class="sourceLine" id="cb1184-7" data-line-number="7">                     <span class="dt">limits =</span> <span class="kw">c</span>(.<span class="dv">0001</span>, <span class="dv">2000</span>),</a>
<a class="sourceLine" id="cb1184-8" data-line-number="8">                     <span class="dt">breaks =</span> <span class="kw">c</span>(.<span class="dv">01</span>, <span class="dv">1</span>, <span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">2000</span>, <span class="dv">5</span>)),</a>
<a class="sourceLine" id="cb1184-9" data-line-number="9">                     <span class="dt">labels =</span> <span class="op">~</span><span class="st"> </span><span class="kw">ifelse</span>(.x <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(.<span class="dv">01</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">100</span>, <span class="dv">2000</span>), .x, <span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1184-10" data-line-number="10">                     ) <span class="op">+</span></a>
<a class="sourceLine" id="cb1184-11" data-line-number="11"><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Response times in ms (log-scaled axis)&quot;</span>,</a>
<a class="sourceLine" id="cb1184-12" data-line-number="12">                     <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">150</span>, <span class="dv">8000</span>),</a>
<a class="sourceLine" id="cb1184-13" data-line-number="13">                     <span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">500</span>,<span class="dv">7500</span>,<span class="dv">500</span>),</a>
<a class="sourceLine" id="cb1184-14" data-line-number="14">                     <span class="dt">labels =</span> <span class="op">~</span><span class="st"> </span><span class="kw">ifelse</span>(.x <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">500</span>,<span class="dv">1000</span>,<span class="dv">2000</span>, <span class="dv">7500</span>), .x, <span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1184-15" data-line-number="15">                     ) <span class="op">+</span></a>
<a class="sourceLine" id="cb1184-16" data-line-number="16"><span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="st">&quot;lexicality&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1184-17" data-line-number="17"><span class="st">  </span><span class="kw">scale_shape_discrete</span>(<span class="st">&quot;lexicality&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1184-18" data-line-number="18"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1184-19" data-line-number="19"><span class="st">  </span><span class="kw">coord_trans</span>(<span class="dt">x =</span> <span class="st">&quot;log&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;log&quot;</span>) </a></code></pre></div>
<div class="figure"><span id="fig:rtlexical"></span>
<img src="bookdown_files/figure-html/rtlexical-1.svg" alt="Distribution of response times for words and non-words, and correct and incorrect answers." width="672" />
<p class="caption">
FIGURE 20.2: Distribution of response times for words and non-words, and correct and incorrect answers.
</p>
</div>
<p>A powerful way to convey the relationship between response times and accuracy is using <em>quantile probability plots</em> <span class="citation">(Ratcliff and Tuerlinckx <a href="#ref-ratcliff2002estimating">2002</a>; these are closely related to the latency probability plots of Audley and Pike <a href="#ref-audley1965some">1965</a>)</span>.</p>
<p>A quantile probability plot shows quantiles of the response times distribution (typically .1, .3, .5, .7, and .9) for correct and incorrect responses on the y-axis against probabilities of correct and incorrect responses for experimental conditions on the x-axis. The plot is built by first aggregating the data.</p>
<p>To display a quantile probability plot, create a custom function <code>qpf()</code> that takes as arguments a data set grouped by an experimental condition (e.g., words vs non-words, here by <code>lex</code>), and the quantiles that need to be displayed (by default, <span class="math inline">\(0.1\)</span>, <span class="math inline">\(0.3\)</span>, <span class="math inline">\(0.5\)</span>, <span class="math inline">\(0.7\)</span>, <span class="math inline">\(0.9\)</span>). The function works as follows:
First, calculate the desired quantiles of the response times for incorrect and correct responses by condition (these are stored in <code>rt_q</code>). Second, calculate the proportion of incorrect and correct responses by condition (these are stored in <code>p</code>); because this information is needed for each quantile, repeat it for the number of quantiles chosen (here, five times). Last, record the quantile that each response time and response probability corresponds to (this is recorded in <code>q</code>), and whether it corresponds to an incorrect or a correct response (this information is stored in <code>response</code>).</p>
<div class="sourceCode" id="cb1185"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1185-1" data-line-number="1">qpf &lt;-<span class="st"> </span><span class="cf">function</span>(df_grouped, <span class="dt">quantiles =</span> <span class="kw">c</span>(.<span class="dv">1</span>, <span class="fl">.3</span>, <span class="fl">.5</span>, <span class="fl">.7</span>, <span class="fl">.9</span>)) {</a>
<a class="sourceLine" id="cb1185-2" data-line-number="2">  df_grouped <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb1185-3" data-line-number="3">    <span class="dt">rt_q =</span> <span class="kw">c</span>(<span class="kw">quantile</span>(rt[acc <span class="op">==</span><span class="st"> </span><span class="dv">0</span>], quantiles),</a>
<a class="sourceLine" id="cb1185-4" data-line-number="4">             <span class="kw">quantile</span>(rt[acc <span class="op">==</span><span class="st"> </span><span class="dv">1</span>], quantiles)),</a>
<a class="sourceLine" id="cb1185-5" data-line-number="5">    <span class="dt">p =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="kw">mean</span>(acc <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">length</span>(quantiles)),</a>
<a class="sourceLine" id="cb1185-6" data-line-number="6">          <span class="kw">rep</span>(<span class="kw">mean</span>(acc <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">length</span>(quantiles))),</a>
<a class="sourceLine" id="cb1185-7" data-line-number="7">    <span class="dt">q =</span> <span class="kw">rep</span>(quantiles, <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb1185-8" data-line-number="8">    <span class="dt">response =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;incorrect&quot;</span>, <span class="kw">length</span>(quantiles)),</a>
<a class="sourceLine" id="cb1185-9" data-line-number="9">                 <span class="kw">rep</span>(<span class="st">&quot;correct&quot;</span>, <span class="kw">length</span>(quantiles))))</a>
<a class="sourceLine" id="cb1185-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1185-11" data-line-number="11">df_blp_lex_q &lt;-<span class="st"> </span>df_blp <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1185-12" data-line-number="12"><span class="st">  </span><span class="kw">group_by</span>(lex) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1185-13" data-line-number="13"><span class="st">  </span><span class="kw">qpf</span>()</a></code></pre></div>
<p>The aggregated data look like this:</p>
<div class="sourceCode" id="cb1186"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1186-1" data-line-number="1">df_blp_lex_q <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">print</span>(<span class="dt">n =</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>## # A tibble: 20 × 5
## # Groups:   lex [2]
##    lex       rt_q      p     q response 
##    &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    
##  1 non-word  433. 0.0521   0.1 incorrect
##  2 non-word  521. 0.0521   0.3 incorrect
##  3 non-word  613  0.0521   0.5 incorrect
##  4 non-word  779. 0.0521   0.7 incorrect
##  5 non-word 1110  0.0521   0.9 incorrect
##  6 non-word  448  0.948    0.1 correct  
##  7 non-word  513  0.948    0.3 correct  
##  8 non-word  575  0.948    0.5 correct  
##  9 non-word  666  0.948    0.7 correct  
## 10 non-word  905  0.948    0.9 correct  
## # … with 10 more rows</code></pre>
<p>Plot the data by joining the points that belong to the same quantiles with lines. Given that incorrect responses in most tasks occur in less than 50% of the trials and correct responses occur in a complementary distribution (i.e., in more than 50% of the trials), incorrect responses usually appear in the left half of the plot, and correct ones in the right half. The code that appears below produces Figure <a href="modeling-a-lexical-decision-task.html#fig:qpplex">20.3</a>.</p>

<div class="sourceCode" id="cb1188"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1188-1" data-line-number="1"><span class="kw">ggplot</span>(df_blp_lex_q, <span class="kw">aes</span>(<span class="dt">x =</span> p, <span class="dt">y =</span> rt_q)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1188-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">.5</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1188-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">shape =</span> lex)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1188-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group =</span> <span class="kw">interaction</span>(q, response))) <span class="op">+</span></a>
<a class="sourceLine" id="cb1188-5" data-line-number="5"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;RT quantiles (ms)&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1188-6" data-line-number="6"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Response proportion&quot;</span>, <span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">.2</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1188-7" data-line-number="7"><span class="st">  </span><span class="kw">scale_shape_discrete</span>(<span class="st">&quot;Lexicality&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1188-8" data-line-number="8"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> <span class="fl">.40</span>, <span class="dt">y =</span> <span class="dv">500</span>, <span class="dt">label =</span> <span class="st">&quot;incorrect&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1188-9" data-line-number="9"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> <span class="fl">.60</span>, <span class="dt">y =</span> <span class="dv">500</span>, <span class="dt">label =</span> <span class="st">&quot;correct&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:qpplex"></span>
<img src="bookdown_files/figure-html/qpplex-1.svg" alt="Quantile probability plot showing 0.1, 0.3, 0.5, 0.7, and 0.9-th response time quantiles plotted against proportion of incorrect responses (left) and proportion of correct responses (right) for strings that are words and non-words." width="672" />
<p class="caption">
FIGURE 20.3: Quantile probability plot showing 0.1, 0.3, 0.5, 0.7, and 0.9-th response time quantiles plotted against proportion of incorrect responses (left) and proportion of correct responses (right) for strings that are words and non-words.
</p>
</div>
<p>The vertical spread among the lines shows the shape of the response time distribution. The lower quantile lines correspond to the left part of the response time distribution, and the higher quantiles to the right part of the distribution. Since the response time distribution is long tailed and right skewed, the higher quantiles are more spread apart than the lower quantiles.</p>
<p>A quantile probability plot can also be used to corroborate the observation that high-frequency words are easier to recognize. To do that, subset the data to only words, and group the strings according to their “frequency group” (that is, according to the quantile of frequency that the strings belong to).
Whereas we previously aggregated over all the observations, ignoring subjects, we can also aggregate by subjects first, and then average the results. This will prevent some idiosyncratic subjects from dominating in the plot. (We can also plot individual quantile probability plots by subject). Apart from the fact that the aggregation is by subjects, the code below follows the same steps as before, and the result is shown in Figure <a href="modeling-a-lexical-decision-task.html#fig:qppfreq">20.4</a>. The plot shows that for more frequent words, accuracy improves and responses are faster.</p>

<div class="sourceCode" id="cb1189"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1189-1" data-line-number="1">df_blp_freq_q &lt;-<span class="st"> </span>df_blp <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1189-2" data-line-number="2"><span class="st">  </span><span class="co"># Subset only words</span></a>
<a class="sourceLine" id="cb1189-3" data-line-number="3"><span class="st">  </span><span class="kw">filter</span>(lex <span class="op">==</span><span class="st"> &quot;word&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1189-4" data-line-number="4"><span class="st">  </span><span class="co"># Create 5 word frequencies group </span></a>
<a class="sourceLine" id="cb1189-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">freq_group =</span></a>
<a class="sourceLine" id="cb1189-6" data-line-number="6">           <span class="kw">cut</span>(lfreq,</a>
<a class="sourceLine" id="cb1189-7" data-line-number="7">               <span class="kw">quantile</span>(lfreq, <span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">2</span>,.<span class="dv">4</span>, <span class="fl">.6</span>, <span class="fl">.8</span>, <span class="dv">1</span>)),</a>
<a class="sourceLine" id="cb1189-8" data-line-number="8">               <span class="dt">include.lowest =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb1189-9" data-line-number="9">               <span class="dt">labels =</span></a>
<a class="sourceLine" id="cb1189-10" data-line-number="10">                 <span class="kw">c</span>(<span class="st">&quot;0-.2&quot;</span>, <span class="st">&quot;.2-.4&quot;</span>, <span class="st">&quot;.4-.6&quot;</span>, <span class="st">&quot;.6-.8&quot;</span>, <span class="st">&quot;.8-1&quot;</span>))</a>
<a class="sourceLine" id="cb1189-11" data-line-number="11">         ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1189-12" data-line-number="12"><span class="st">  </span><span class="co"># Group by condition  and subject</span></a>
<a class="sourceLine" id="cb1189-13" data-line-number="13"><span class="st">  </span><span class="kw">group_by</span>(freq_group, subj) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1189-14" data-line-number="14"><span class="st">  </span><span class="co"># Apply the quantile probability function</span></a>
<a class="sourceLine" id="cb1189-15" data-line-number="15"><span class="st">  </span><span class="kw">qpf</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1189-16" data-line-number="16"><span class="st">  </span><span class="co"># Group again removing subject</span></a>
<a class="sourceLine" id="cb1189-17" data-line-number="17"><span class="st">  </span><span class="kw">group_by</span>(freq_group, q, response) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1189-18" data-line-number="18"><span class="st">  </span><span class="co"># Get averages of all the quantities</span></a>
<a class="sourceLine" id="cb1189-19" data-line-number="19"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">rt_q =</span> <span class="kw">mean</span>(rt_q),</a>
<a class="sourceLine" id="cb1189-20" data-line-number="20">           <span class="dt">p =</span> <span class="kw">mean</span>(p))</a>
<a class="sourceLine" id="cb1189-21" data-line-number="21"><span class="co"># Plot</span></a>
<a class="sourceLine" id="cb1189-22" data-line-number="22"><span class="kw">ggplot</span>(df_blp_freq_q, <span class="kw">aes</span>(<span class="dt">x =</span> p, <span class="dt">y =</span> rt_q)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1189-23" data-line-number="23"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">4</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1189-24" data-line-number="24"><span class="st">  </span><span class="kw">geom_text</span>(</a>
<a class="sourceLine" id="cb1189-25" data-line-number="25">    <span class="dt">data =</span> df_blp_freq_q <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1189-26" data-line-number="26"><span class="st">      </span><span class="kw">filter</span>(q <span class="op">==</span><span class="st"> </span><span class="fl">.1</span>),</a>
<a class="sourceLine" id="cb1189-27" data-line-number="27">    <span class="kw">aes</span>(<span class="dt">label =</span> freq_group), <span class="dt">nudge_y =</span> <span class="dv">12</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1189-28" data-line-number="28"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group =</span> <span class="kw">interaction</span>(q, response))) <span class="op">+</span></a>
<a class="sourceLine" id="cb1189-29" data-line-number="29"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;RT quantiles (ms)&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1189-30" data-line-number="30"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Response proportion&quot;</span>, <span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">.2</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1189-31" data-line-number="31"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> <span class="fl">.40</span>, <span class="dt">y =</span> <span class="dv">900</span>, <span class="dt">label =</span> <span class="st">&quot;incorrect&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1189-32" data-line-number="32"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> <span class="fl">.60</span>, <span class="dt">y =</span> <span class="dv">900</span>, <span class="dt">label =</span> <span class="st">&quot;correct&quot;</span>) </a></code></pre></div>
<div class="figure"><span id="fig:qppfreq"></span>
<img src="bookdown_files/figure-html/qppfreq-1.svg" alt="Quantile probability plot showing 0.1, 0.3, 0.5, 0.7, and 0.9 response times quantiles plotted against proportion of incorrect responses (left) and proportion of correct responses (right) for words of different frequency. Word frequency is grouped according to quantiles: The first group is words with frequencies smaller than the 0.2-th quantile, the second group is words with frequencies smaller than the 0.4-th quantile and larger than the 0.2-th quantile, and so forth." width="672" />
<p class="caption">
FIGURE 20.4: Quantile probability plot showing 0.1, 0.3, 0.5, 0.7, and 0.9 response times quantiles plotted against proportion of incorrect responses (left) and proportion of correct responses (right) for words of different frequency. Word frequency is grouped according to quantiles: The first group is words with frequencies smaller than the 0.2-th quantile, the second group is words with frequencies smaller than the 0.4-th quantile and larger than the 0.2-th quantile, and so forth.
</p>
</div>
<p>So far, several ways were shown to describe the data by representing them graphically. Next, we turn to modeling the data.</p>
<div id="modeling-the-lexical-decision-task-with-the-log-normal-race-model" class="section level3">
<h3><span class="header-section-number">20.1.1</span> Modeling the lexical decision task with the log-normal race model</h3>
<p>The log-normal race model is used here to examine the effect of word frequency in both response times and choice (word vs. non-word) in the lexical decision task presented earlier. In this example, the log-normal race model is limited to fitting two choices; as mentioned earlier, this model can in principle fit more than two choices. When modeling a task with two choices, there are two ways to account for the data: either fit the response times and the accuracy (i.e., accuracy coding: correct vs. incorrect), or fit the response times and actual responses (i.e., stimulus coding: in this case word vs. non-word). In this example, we will use the stimulus-coding approach. <!-- ; see Box \@ref(thm:acc-choice) for more details about this choice. --></p>
<p>The following code chunk adds a new column that incorporates the actual choice made (as <code>word</code> vs <code>non-word</code> in <code>choice</code> and as <code>1</code> vs <code>2</code> in <code>nchoice</code>):</p>
<div class="sourceCode" id="cb1190"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1190-1" data-line-number="1">df_blp &lt;-<span class="st"> </span>df_blp <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1190-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">choice =</span> <span class="kw">ifelse</span>((lex <span class="op">==</span><span class="st"> &quot;word&quot;</span> <span class="op">&amp;</span></a>
<a class="sourceLine" id="cb1190-3" data-line-number="3"><span class="st">                            </span>acc <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="op">|</span></a>
<a class="sourceLine" id="cb1190-4" data-line-number="4"><span class="st">                           </span>(lex <span class="op">==</span><span class="st"> &quot;non-word&quot;</span> <span class="op">&amp;</span></a>
<a class="sourceLine" id="cb1190-5" data-line-number="5"><span class="st">                              </span>acc <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="st">&quot;word&quot;</span>, <span class="st">&quot;non-word&quot;</span>),</a>
<a class="sourceLine" id="cb1190-6" data-line-number="6">         <span class="dt">nchoice =</span> <span class="kw">ifelse</span>(choice <span class="op">==</span><span class="st"> &quot;word&quot;</span>, <span class="dv">1</span>, <span class="dv">2</span>))</a></code></pre></div>
<p>To start modeling the data, think about the behavior of one synthetic subject. This subject simultaneously accumulates evidence for the response, “word” in one accumulator, and for “non-word” in another independent accumulator. Unlike other sequential sampling models, an increase in evidence for one choice doesn’t necessarily reduce the evidence for the other choices.
<span class="citation">Rouder et al. (<a href="#ref-RouderEtAl2015">2015</a>)</span> points out that it might seem odd to assume that we accumulate evidence for a non-word in the same manner as we accumulate evidence for a word, since non-words may be conceptualized as the absence of a word. However, they stress that this approach is closely related to novelty detection, where the salience of never-before experienced stimuli seems to indicate that novelty is psychologically represented as more than the absence of familiarity. Nevertheless, notions of words and non-word evidence accumulation are indeed controversial <span class="citation">(see Dufau, Grainger, and Ziegler <a href="#ref-dufau2012say">2012</a>)</span>. The alternative approach of fitting accuracy rather than stimuli discussed before doesn’t really circumvent the problem. This is because when the correct answer is <code>word</code>, we assume that the “correct” accumulator accumulates evidence for <code>word</code>, and the incorrect one for <code>non-word</code>, and the other way around when the correct answer is <code>non-word</code>. <!-- See Box \@ref(thm:acc-choice) for more details. --></p>
<!-- \Begin{extra} -->
<!-- <div class="extra"> -->
<!-- ```{theorem, acc-choice} -->
<!-- **Fitting accuracy or response choice** -->
<!-- ``` -->
<!-- In sequential sampling models where there is only one rate of evidence accumulation (e.g., in the drift diffusion model shown in Figure \@ref(fig:ddm)), a subject is assumed to accumulate evidence either towards an upper or lower threshold (rather than in independent accumulators). In such models, accuracy coding would be easier to interpret. This is because the sign and magnitude of drift rate are a direct measure of performance (with higher drift indicating better performance). However, a limitation of accuracy coding is that incorporating a response bias (the tendency to answer word rather than non-word, or vice-versa, all things being equal)  is not straight-forward. Although stimulus coding would make the interpretation of the drift rate harder, the interpretation of possible bias is straightforward since it is represented in the intercept: a more positive intercept corresponds to a stronger bias to the response mapped to the higher threshold. -->
<!-- (ref:ddm) Drift diffusion model, where $\mu$ represents the drift rate, $z$ and $-z$ the threshold of evidence for the two decisions, $x_0$ the starting point of accumulation of evidence, and $T_0$ the non-decision time. Adapted from @Goldfarb2014; licensed under CC BY 4.0. -->
<!-- ```{r ddm, fig.cap = "(ref:ddm)", fig.height =5, echo = FALSE, fig.pos = "H",out.extra ='' } -->
<!-- knitr::include_graphics("cc_figure/ddm.jpg") -->
<!-- ``` -->
<!-- A limitation of accuracy coding is that the coding of incorrect responses might not be straightforward when there are more than two choices. As an example, consider a global motion detection task similar to the one presented in chapter \@ref(ch:mixture) but with dots moving left, right, up, or bottom; it might not be a good idea to consider any wrong response as just  incorrect (there might be less evidence for the opposite direction to the true direction than to the sides), but it is not clear how one would code each one of the incorrect choices.  -->
<!-- Even when we have two choices, accuracy coding is not as clearly showing performance in race models (as in models such as the drift diffusion model). This is because in the case of the log-normal race model (and other race models), we still need a rate of accumulation for each accumulator regardless of the coding.  If we decide to use accuracy coding, we'll end up with a rate of evidence for the correct accumulator and one for the incorrect accumulator, which together inform about the performance in the task. This is true at least when the rates of accumulation have some independence, that is when rate$_2$ is different from $1 -$ rate$_2$. However, while this latter parametrization might allows us for a more transparent accuracy coding, it seems to be overly constraining [@evans_2020]. -->
<!-- \End{extra} -->
<!-- <div/> -->
</div>
<div id="sec:genaccum" class="section level3">
<h3><span class="header-section-number">20.1.2</span> A generative model for a race between accumulators</h3>
<p>To build a generative model of the task based on the log-normal race model, start by spelling out the assumptions. In a race of accumulators model, the assumption is that the time <span class="math inline">\(T\)</span> taken for each accumulator of evidence to reach <span class="math inline">\(D\)</span>, the distance to the threshold, is simply defined by</p>
<p><span class="math display">\[\begin{equation}
T = D/V
\end{equation}\]</span></p>
<p>where the denominator <span class="math inline">\(V\)</span> is the rate (velocity, sometimes also called drift rate) of evidence accumulation.</p>
<p>The log-normal race model assumes that the rate in each trial is sampled from a log-normal distribution:</p>
<p><span class="math display">\[\begin{equation}
V \sim \mathit{LogNormal}(\mu_v, \sigma_v)
\end{equation}\]</span></p>
<div class="figure"><span id="fig:lognormalrace"></span>
<img src="bookdown_files/figure-html/lognormalrace-1.svg" alt="A schematic illustration of the log-normal race model for the lexical-decision task with a word stimulus. A larger rate of accumulation (V) leads to a larger angle." width="672" />
<p class="caption">
FIGURE 20.5: A schematic illustration of the log-normal race model for the lexical-decision task with a word stimulus. A larger rate of accumulation (V) leads to a larger angle.
</p>
</div>
<p>A log-normal distribution is partly justified by the work from <span class="citation">Ulrich and Miller (<a href="#ref-ulrichInformationProcessingModels1993">1993</a>)</span> (also see Box <a href="sec-trial.html#thm:lognormal">4.3</a>), and as discussed later, it is very convenient mathematically.</p>
<p>For simplicity, assume that the threshold <span class="math inline">\(D\)</span> is kept constant. This might not be a good assumption if the experiment is designed so that subjects change their threshold depending on speed or accuracy incentives (that was not the case in this experiment), or if the subject gets fatigued as the experiment progresses, or if there is reason to believe that there might be random fluctuations in this threshold. Later in this chapter, we will discuss what happens if this assumption is relaxed.</p>
<p>Assume that, for trial <span class="math inline">\(n\)</span>, the location <span class="math inline">\(\mu_{w}\)</span> of the distribution of rates of accumulation of evidence for a string <span class="math inline">\(w\)</span> is a function of the lexicality of the string presented (only a word will increase this rate of accumulation and not a non-word), frequency (i.e., high-frequency words might be easier to identify, leading to a faster rate of accumulation than low-frequency words), and bias (i.e., a subject might have a tendency to answer that a string is a word rather than non-word or vice-versa, regardless of the stimuli). This assumption can be modeled with a linear regression over <span class="math inline">\(\mu_{w}\)</span>, with parameters that represent the bias, <span class="math inline">\(\alpha_{w}\)</span>, the effect of lexicality, <span class="math inline">\(\beta_{lex_{w}}\)</span>, and the effect of log-frequency <span class="math inline">\(\beta_{\mathit{lfreq}_{w}}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\mu_{w,n} = \alpha_w + \mathit{lex}_n \cdot \beta_{lex_{w}} + \mathit{\mathit{lfreq}}_n \cdot \beta_{\mathit{lfreq}_{w}}
\end{equation}\]</span></p>
<p>The location for the rate of accumulation of evidence for the non-word accumulator is defined similarly:</p>
<p><span class="math display">\[\begin{equation}
\mu_{nw,n} = \alpha_{nw} + \mathit{lex}_n \cdot \beta_{lex_{nw}} + \mathit{lfreq}_n \cdot \beta_{\mathit{lfreq}_{nw}}
\end{equation}\]</span></p>
<p>Thus the rates are generated as follows:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
V_{w,n} &amp;\sim \mathit{LogNormal}(\mu_{w,n}, \sigma)\\
V_{nw,n} &amp;\sim \mathit{LogNormal}(\mu_{nw,n}, \sigma)
\end{aligned}
\end{equation}\]</span></p>
<p>The accumulators reach the threshold in times:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
T_{w,n} &amp;= D/V_{w,n}\\
T_{nw,n} &amp;= D/ V_{nw,n}
\end{aligned}
\end{equation}\]</span></p>
<p>The choice for trial <span class="math inline">\(n\)</span> corresponds to the accumulator with the shortest time for that trial,</p>
<p><span class="math display">\[\begin{equation}
\mathit{choice}_n =
\begin{cases}
\mathit{word}, &amp; \text{ if } T_{w,n} &lt; T_{nw,n} \\
\mathit{non}\text{-}{word}, &amp; \text{ otherwise }
\end{cases}
\end{equation}\]</span></p>
<p>and the decision for trial <span class="math inline">\(n\)</span> is made in time</p>
<p><span class="math display">\[\begin{equation}
T_n = min(T_{w,n},T_{nw,n})
\end{equation}\]</span></p>
<p>We also need to take into account that not all the time spent in the task involves making the
decision: Time is spent fixating the gaze on the screen, pressing a button, etc. We’ll add a shift to the distribution, representing the minimum amount of time that a subject needs for all the peripheral processes that happened before and after the decision <span class="citation">(also see Rouder <a href="#ref-Rouder2005">2005</a>)</span>. We represent this with <span class="math inline">\(T_{nd}\)</span>; “nd” stands for non-decision. Although some variation in the non-decision time is highly likely, we use a constant as an approximation that will be reasonable if its variation is small relative to the variation associated with the decision time <span class="citation">(Heathcote and Love <a href="#ref-HeathcoteLove2012">2012</a>)</span>.</p>
<p><span class="math display">\[\begin{equation}
rt_n = T_{nd} + T_n
\end{equation}\]</span></p>
<p>The following chunk of code generates synthetic data for one subject, by setting true values to the parameters and translating the previous equations to R. The true values are relatively arbitrary and were decided by trial and error until a relatively realistic distribution of response times was obtained. Considering that this is only one subject (unlike what was shown in previous figures), Figure <a href="modeling-a-lexical-decision-task.html#fig:scatterracesim">20.6</a> looks relatively fine. (One can also inspect the quantile probability plots of individual subjects in the real data set and compare it to the synthetic data).</p>
<p>First, set a seed to always generate the same “random” values, take a subset of the data set to keep the same structure of the data frame for our synthetic subject, and set true values.</p>
<div class="sourceCode" id="cb1191"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1191-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb1191-2" data-line-number="2">df_blp_1subj &lt;-<span class="st"> </span>df_blp <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1191-3" data-line-number="3"><span class="st">  </span><span class="kw">filter</span>(subj <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1191-4" data-line-number="4">D &lt;-<span class="st"> </span><span class="dv">1800</span></a>
<a class="sourceLine" id="cb1191-5" data-line-number="5">alpha_w &lt;-<span class="st"> </span><span class="fl">.8</span></a>
<a class="sourceLine" id="cb1191-6" data-line-number="6">beta_wlex &lt;-<span class="st"> </span><span class="fl">.5</span></a>
<a class="sourceLine" id="cb1191-7" data-line-number="7">beta_wlfreq &lt;-<span class="st"> </span><span class="fl">.2</span></a>
<a class="sourceLine" id="cb1191-8" data-line-number="8">alpha_nw &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1191-9" data-line-number="9">beta_nwlex &lt;-<span class="st"> </span><span class="fl">-.5</span></a>
<a class="sourceLine" id="cb1191-10" data-line-number="10">beta_nwlfreq &lt;-<span class="st"> </span><span class="fl">-.05</span></a>
<a class="sourceLine" id="cb1191-11" data-line-number="11">sigma &lt;-<span class="st"> </span><span class="fl">.8</span></a>
<a class="sourceLine" id="cb1191-12" data-line-number="12">T_nd &lt;-<span class="st"> </span><span class="dv">150</span></a></code></pre></div>
<p>Second, generate the locations of both accumulators, <code>mu_w</code> and <code>mu_nw</code>, for every trial. This means that both variables are vectors of length, <code>N</code>, that is, the number of trials.</p>
<div class="sourceCode" id="cb1192"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1192-1" data-line-number="1">mu_w &lt;-<span class="st"> </span>alpha_w <span class="op">+</span><span class="st"> </span>df_blp_1subj<span class="op">$</span>c_lfreq <span class="op">*</span><span class="st"> </span>beta_wlfreq <span class="op">+</span></a>
<a class="sourceLine" id="cb1192-2" data-line-number="2"><span class="st">  </span>df_blp_1subj<span class="op">$</span>c_lex <span class="op">*</span><span class="st"> </span>beta_wlex</a>
<a class="sourceLine" id="cb1192-3" data-line-number="3">mu_nw &lt;-<span class="st"> </span>alpha_nw <span class="op">+</span><span class="st"> </span>df_blp_1subj<span class="op">$</span>c_lfreq <span class="op">*</span><span class="st"> </span>beta_nwlfreq <span class="op">+</span></a>
<a class="sourceLine" id="cb1192-4" data-line-number="4"><span class="st">  </span>df_blp_1subj<span class="op">$</span>c_lex <span class="op">*</span><span class="st"> </span>beta_nwlex</a>
<a class="sourceLine" id="cb1192-5" data-line-number="5">N &lt;-<span class="st"> </span><span class="kw">nrow</span>(df_blp_1subj)</a></code></pre></div>
<p>Third, generate values for the rates of accumulation, <code>V_w</code> and <code>V_nw</code>, for every trial. Use those rates to calculate <code>T_w</code> and <code>T_nw</code>, how long it will take for each accumulator to reach its threshold for every trial.</p>
<div class="sourceCode" id="cb1193"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1193-1" data-line-number="1">V_w &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(N, mu_w, sigma)</a>
<a class="sourceLine" id="cb1193-2" data-line-number="2">V_nw &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(N, mu_nw, sigma)</a>
<a class="sourceLine" id="cb1193-3" data-line-number="3">T_w &lt;-<span class="st">  </span>D <span class="op">/</span><span class="st"> </span>V_w</a>
<a class="sourceLine" id="cb1193-4" data-line-number="4">T_nw &lt;-<span class="st"> </span>D <span class="op">/</span><span class="st"> </span>V_nw</a></code></pre></div>
<p>Fourth, calculate the time it takes to reach to a decision in every trial, <code>T_winner</code> as the by-trial minimum between <code>T_w</code> and <code>T_nw</code>. Similarly, store the winner accumulator for each trial in <code>accumulator_winner</code>.</p>
<div class="sourceCode" id="cb1194"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1194-1" data-line-number="1">T_winner &lt;-<span class="st"> </span><span class="kw">pmin</span>(T_w, T_nw)</a>
<a class="sourceLine" id="cb1194-2" data-line-number="2">accumulator_winner &lt;-<span class="st"> </span><span class="kw">ifelse</span>(T_w <span class="op">==</span><span class="st"> </span><span class="kw">pmin</span>(T_w, T_nw),</a>
<a class="sourceLine" id="cb1194-3" data-line-number="3">                             <span class="st">&quot;word&quot;</span>,</a>
<a class="sourceLine" id="cb1194-4" data-line-number="4">                             <span class="st">&quot;non-word&quot;</span>)</a></code></pre></div>
<p>Finally, add this information to the data frame that now indicates choice, time, and accuracy for each trial.</p>
<div class="sourceCode" id="cb1195"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1195-1" data-line-number="1">df_blp1_sim &lt;-<span class="st"> </span>df_blp_1subj <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1195-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">rt =</span> T_nd <span class="op">+</span><span class="st"> </span>T_winner,</a>
<a class="sourceLine" id="cb1195-3" data-line-number="3">         <span class="dt">choice =</span> accumulator_winner,</a>
<a class="sourceLine" id="cb1195-4" data-line-number="4">         <span class="dt">nchoice =</span> <span class="kw">ifelse</span>(choice <span class="op">==</span><span class="st"> &quot;word&quot;</span>, <span class="dv">1</span>, <span class="dv">2</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1195-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">acc =</span> <span class="kw">ifelse</span>(lex <span class="op">==</span><span class="st"> </span>choice, <span class="dv">1</span>, <span class="dv">0</span>))</a></code></pre></div>

<div class="sourceCode" id="cb1196"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb1196-1" data-line-number="1"></a>
<a class="sourceLine" id="cb1196-2" data-line-number="2">acc_lbl &lt;-<span class="st"> </span><span class="kw">as_labeller</span>(<span class="kw">c</span>(<span class="st">`</span><span class="dt">0</span><span class="st">`</span> =<span class="st"> &quot;Incorrect&quot;</span>, <span class="st">`</span><span class="dt">1</span><span class="st">`</span> =<span class="st"> &quot;Correct&quot;</span>))</a>
<a class="sourceLine" id="cb1196-3" data-line-number="3"><span class="kw">ggplot</span>(df_blp1_sim, <span class="kw">aes</span>(<span class="dt">y =</span> rt, <span class="dt">x =</span> freq <span class="op">+</span><span class="st"> </span><span class="fl">.01</span>, <span class="dt">shape =</span> lex, <span class="dt">color =</span> lex)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1196-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1196-5" data-line-number="5"><span class="st">  </span><span class="kw">facet_grid</span>(. <span class="op">~</span><span class="st"> </span>acc,  <span class="dt">labeller =</span>  <span class="kw">labeller</span>(<span class="dt">acc =</span> acc_lbl)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1196-6" data-line-number="6"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Frequency per million (log-scaled axis)&quot;</span>,</a>
<a class="sourceLine" id="cb1196-7" data-line-number="7">                     <span class="dt">limits =</span> <span class="kw">c</span>(.<span class="dv">0001</span>, <span class="dv">2000</span>),</a>
<a class="sourceLine" id="cb1196-8" data-line-number="8">                     <span class="dt">breaks =</span> <span class="kw">c</span>(.<span class="dv">01</span>, <span class="dv">1</span>, <span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">2000</span>, <span class="dv">5</span>)),</a>
<a class="sourceLine" id="cb1196-9" data-line-number="9">                     <span class="dt">labels =</span> <span class="op">~</span><span class="st"> </span><span class="kw">ifelse</span>(.x <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(.<span class="dv">01</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">100</span>, <span class="dv">2000</span>), .x, <span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1196-10" data-line-number="10">                     )<span class="op">+</span></a>
<a class="sourceLine" id="cb1196-11" data-line-number="11"><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Response times in ms (log-scaled axis)&quot;</span>,</a>
<a class="sourceLine" id="cb1196-12" data-line-number="12">                     <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">150</span>, <span class="dv">8000</span>),</a>
<a class="sourceLine" id="cb1196-13" data-line-number="13">                     <span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">500</span>,<span class="dv">7500</span>,<span class="dv">500</span>),</a>
<a class="sourceLine" id="cb1196-14" data-line-number="14">                     <span class="dt">labels =</span> <span class="op">~</span><span class="st"> </span><span class="kw">ifelse</span>(.x <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">500</span>,<span class="dv">1000</span>,<span class="dv">2000</span>, <span class="dv">7500</span>), .x, <span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1196-15" data-line-number="15">                     ) <span class="op">+</span></a>
<a class="sourceLine" id="cb1196-16" data-line-number="16"><span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="st">&quot;lexicality&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1196-17" data-line-number="17"><span class="st">  </span><span class="kw">scale_shape_discrete</span>(<span class="st">&quot;lexicality&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1196-18" data-line-number="18"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1196-19" data-line-number="19"><span class="st">  </span><span class="kw">coord_trans</span>(<span class="dt">x =</span> <span class="st">&quot;log&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;log&quot;</span>) </a></code></pre></div>
<div class="figure"><span id="fig:scatterracesim"></span>
<img src="bookdown_files/figure-html/scatterracesim-1.svg" alt="Distribution of response times for words and non-words, and correct and incorrect answers for the synthetic data of one subject." width="672" />
<p class="caption">
FIGURE 20.6: Distribution of response times for words and non-words, and correct and incorrect answers for the synthetic data of one subject.
</p>
</div>
</div>
<div id="fitting-the-log-normal-race-model" class="section level3">
<h3><span class="header-section-number">20.1.3</span> Fitting the log-normal race model</h3>
<p>A first issue that we face when we attempt to fit the log-normal race model, is that we need to fit its likelihood to a ratio of the random variables <span class="math inline">\(D\)</span> and <span class="math inline">\(V\)</span>; that is, we need a ratio or quotient distribution function. Although for arbitrary distributions this requires solving (sometimes extremely complex) integrals <span class="citation">(see, for example, Nelson <a href="#ref-Nelson1981">1981</a>)</span>, there are two situations that are compatible with our assumptions and are mathematically simple:</p>
<ol style="list-style-type: decimal">
<li>If we assume that <span class="math inline">\(D\)</span> is a constant <span class="math inline">\(k\)</span>, then <span class="math inline">\(T = k/V\)</span>, and</li>
</ol>
<p><span class="math display">\[\begin{equation}
\log(T) = \log(k/V) = \log(k) - \log(V)
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(V\)</span> is log-normally distributed, <span class="math inline">\(\log(V) \sim \mathit{Normal}(\mu_v, \sigma_v)\)</span>, and <span class="math inline">\(\log(k)\)</span> is a constant:</p>
<p><span class="math display" id="eq:lognormalkV">\[\begin{equation}
\begin{aligned}
\log(T) &amp;\sim \mathit{Normal}(\log(k) - \mu_v, \sigma_v)\\
T &amp;\sim \mathit{LogNormal}(\log(k) - \mu_v, \sigma_v)
\tag{20.1}
\end{aligned}
\end{equation}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>A log-normally distributed time is not uniquely predicted by assuming that distance is a constant. It also follows if distance is also a log-normally distributed variable: If we assume that <span class="math inline">\(D \sim \mathit{LogNormal}(\mu_d, \sigma_d)\)</span> then <span class="math inline">\(T\)</span> is the ratio of two random variables <span class="math inline">\(D/V\)</span>, and</li>
</ol>
<p><span class="math display">\[\begin{equation}
\log(T) = \log(D/V) = \log(D) - \log(V)
\end{equation}\]</span></p>
<p>We have a difference of independent, normally distributed random variables. It follows from random variable theory that:</p>
<p><span class="math display" id="eq:lognormalDV">\[\begin{equation}
\begin{aligned}
\log(T) &amp;\sim \mathit{Normal}(\mu_d - \mu_v, \sqrt{\sigma_d^2+\sigma_v^2})\\
T &amp;\sim \mathit{LogNormal}(\mu_d - \mu_v, \sqrt{\sigma_d^2+\sigma_v^2})
\tag{20.2}
\end{aligned}
\end{equation}\]</span></p>
<p>From Equations <a href="modeling-a-lexical-decision-task.html#eq:lognormalkV">(20.1)</a> and <a href="modeling-a-lexical-decision-task.html#eq:lognormalDV">(20.2)</a>, it should be clear that the threshold and accumulation rate cannot be disentangled: a manipulation that affects the rate or the decision threshold will affect the location of the distribution in the same way. That is, the log-normal race model lacks identifiable decision thresholds <span class="citation">(also see Rouder et al. <a href="#ref-RouderEtAl2015">2015</a>)</span>. Another important observation is that <span class="math inline">\(T\)</span> won’t have a log-normal distribution when <span class="math inline">\(D\)</span> has any other distributional form.</p>
<p>Following <span class="citation">Rouder et al. (<a href="#ref-RouderEtAl2015">2015</a>)</span>, we assume that the noise parameter is the same for each the accumulator, since this means that contrasts between finishing time distributions are captured completely by contrasts of the locations of the log-normal distributions. We discuss at the end of the chapter why one would need to relax this assumption (also see Exercise <a href="exercises-6.html#exr:lnracescale">20.2</a>).</p>
<p>In each trial <span class="math inline">\(n\)</span>, with an accumulator for words, indicated with the subscript <span class="math inline">\(w\)</span>, and one for
non-words, indicated with <span class="math inline">\(nw\)</span>, we can model the time it takes for each accumulator to get to
the threshold <span class="math inline">\(D\)</span> in the following way. For the word accumulator,</p>
<p><span class="math display" id="eq:alphaprime">\[\begin{equation}
\begin{aligned}
\mu&#39;_{w,n} &amp;= \mu_{d_w} - \mu_{w,n}\\
\mu&#39;_{w,n} &amp;= \mu_{d_{w}} - (\alpha_w + \mathit{lex}_n \cdot \beta_{lex_{w}} + \mathit{lfreq}_n \cdot \beta_{\mathit{lfreq}_{w}})\\
\mu&#39;_{w,n} &amp;= (\mu_{d_w} - \alpha_w) - \mathit{lex}_n \cdot \beta_{lex_{w}} - \mathit{lfreq}_n \cdot \beta_{\mathit{lfreq}_{w}}\\
\mu&#39;_{w,n} &amp;=  \alpha&#39;_w - \mathit{lex}_n \cdot \beta_{lex_{w}} - \mathit{lfreq}_n \cdot \beta_{\mathit{lfreq}_{w}}\\
T_{w,n} &amp;\sim \mathit{LogNormal}(\mu&#39;_{w,n}, \sigma) \\
\end{aligned}
\tag{20.3}
\end{equation}\]</span></p>
<p>The parameter <span class="math inline">\(\alpha&#39;_w\)</span> absorbs the location of the threshold distribution minus the intercept of the rate distribution, and represents a bias. As <span class="math inline">\(\alpha&#39;_w\)</span> gets smaller, the accumulator will be more likely to reach the threshold first all things being equal, biasing the responses to <code>word</code>.</p>
<p>Similarly, for the non-word accumulator,</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mu&#39;_{nw,n} &amp;= \alpha&#39;_{nw} - \mathit{lex}_n \cdot \beta_{lex_{nw}} - \mathit{lfreq}_n \cdot \beta_{\mathit{lfreq}_{nw}}\\
T_{nw,n} &amp;\sim \mathit{LogNormal}( \mu&#39;_{nw,n}, \sigma)
\end{aligned}
\end{equation}\]</span></p>
<p>The only observed time is the one associated with the winner accumulator, the response selected <span class="math inline">\(s\)</span>, which corresponds to the fastest accumulator:</p>
<p><span class="math display">\[\begin{equation}
T_{\mathit{accum}=s,n} \sim \mathit{LogNormal}(\mu_{\mathit{accum}=s,n}, \sigma)
\end{equation}\]</span></p>
<p>If we only fit the observed finishing times of the accumulators, we’re always ignoring that in a given trial the accumulator that lost was slower than the accumulator for which we have the latency; this means that we underestimate the time it takes to reach the threshold and we overestimate the rate of accumulation of both accumulators. We can treat this as a problem of <em>censored data</em>, where for each trial we don’t know the slower observations.</p>
<p>Since the potential decision time for the accumulator that wasn’t selected is for sure longer than the one of the winner accumulator, we obtain the likelihood for each unobserved time by integrating out all the possible decision times that the accumulator could have, that is, from the time it took for the winner accumulator to reach the threshold to infinitely large decision times:</p>
<p><span class="math display">\[\begin{equation}
P(T_{\mathit{accum} \neq s,n}) = \int_{T_{\mathit{accum}=s,n}}^{\infty} \mathit{LogNormal}(T|\mu_{a=s,n}, \sigma) dT
\end{equation}\]</span></p>
<p>This integral is the complement of the CDF of the log-normal distribution evaluated at
<span class="math inline">\(T_{\mathit{accum} = s,n}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
P(T_{\mathit{accum} \neq s,n}) = 1 -  \mathit{LogNormal}\_CDF(T_{\mathit{accum}=s,n}| \mu_{\mathit{accum} =s,n}, \sigma)
\end{equation}\]</span></p>
<p>So far we have been fitting the decision time <span class="math inline">\(T\)</span>, but our dependent variable is response times, <span class="math inline">\(rt\)</span>, the sum of the decision time <span class="math inline">\(T\)</span> and the non-decision time <span class="math inline">\(T_{nd}\)</span>. This requires a change of variables in our model, <span class="math inline">\(T_{n} = rt_{n} - T_{nd}\)</span>, since <span class="math inline">\(rt\)</span> but not <span class="math inline">\(T\)</span> is available as data. In this case, however, no Jacobian adjustment is necessary, since adjusting the likelihood is equal to multiplying the likelihood by one (or to sum zero to the log likelihood). This is because <span class="math inline">\(|d\frac{\mathit{rt}_n - T_{0}}{d\mathit{rt}_n}| = 1\)</span>; for more details see section <a href="sec-change.html#sec:change">12.1</a> in chapter <a href="ch-custom.html#ch:custom">12</a>.</p>
<p>To sum up, our model can be stated as follows:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
T_n &amp;= rt_n - T_{nd}\\
\mu&#39;_{w,n} &amp;= \alpha&#39;_w - \mathit{lex}_n \cdot \beta_{lex_{w}} - \mathit{lfreq}_n \cdot \beta_{\mathit{lfreq}_{w}}\\
\mu&#39;_{nw,n} &amp;= \alpha&#39;_{nw} - \mathit{lex}_n \cdot \beta_{lex_{nw}} - \mathit{lfreq}_n \cdot \beta_{\mathit{lfreq}_{nw}}\\
T_n &amp;\sim
\begin{cases}
\mathit{LogNormal}(\mu&#39;_{w,n}, \sigma) \text{ if } \mathit{choice}= \text{word}\\
\mathit{LogNormal}(\mu&#39;_{nw,n}, \sigma) \text{ otherwise }
\end{cases}\\
T_{censored,n} &amp;= rt_{censored,n} - T_{nd}\\
\end{aligned}
\end{equation}\]</span></p>
<p>Rather than trying to estimate all the censored observations, we integrate them out:<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a></p>
<p><span class="math display">\[\begin{equation}
P(T_{censored}) =
\begin{cases}
1 -  \mathit{LogNormal}\_CDF(T_{n}| \mu&#39;_{w,n}, \sigma) \text{, if } \mathit{choice}= \text{word}\\
1 -  \mathit{LogNormal}\_CDF(T_{n}| \mu&#39;_{nw,n}, \sigma) \text{, otherwise }\\
\end{cases}
\end{equation}\]</span></p>
<p>We need priors for all the parameters. An added complication here is the prior for the non-decision time, <span class="math inline">\(T_{nd}\)</span>: we need to make sure that it’s strictly positive and also that it’s smaller than the shortest observed response time. This is because the decision time for each observation, <span class="math inline">\(T_n\)</span> should also be strictly positive:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
T_n = rt_n - T_{nd} &amp;&gt;0 \\
rt_n   &amp;&gt; T_{nd}\\ 
min(\mathbf{rt})  &amp;&gt; T_{nd}
\end{aligned}
\end{equation}\]</span></p>
<p>We thus truncate the prior of <span class="math inline">\(T_{nd}\)</span> so that the values lie between zero and <span class="math inline">\(min(\mathbf{rt})\)</span>, the minimum value of the vector of response times. Given the time it takes to fixate the gaze on the screen and a minimal motor response time, centering the prior in 150 ms seems reasonable. The rest of the priors are in log-scale. One should use prior predictive checks to verify that the order of magnitude of all the priors is appropriate. We skip this step here and present the priors below.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
T_{nd} &amp;\sim \mathit{Normal}(150, 100) \text{ with } 0 &lt; T_{nd} &lt; min(rt_n)\\
\boldsymbol{\alpha} &amp;\sim \mathit{Normal}(6, 1) \\
\boldsymbol{\beta} &amp;\sim \mathit{Normal}(0, .5) \\
\sigma &amp;\sim \mathit{Normal}_+(.5, .2)\\
\end{aligned}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\alpha}\)</span> is a vector of <span class="math inline">\(\{\alpha&#39;_{n}, \alpha&#39;_{nw} \}\)</span>, and <span class="math inline">\(\boldsymbol{\beta}\)</span> is a vector of all the <span class="math inline">\(\beta\)</span> used in the likelihoods.</p>
<p>To translate the model into Stan, we need a normal distribution truncated so that the values lie between zero and <span class="math inline">\(min(\mathbf{rt})\)</span> for the prior of <code>T_nd</code>. This means dividing the original distribution by the difference of the CDFs evaluated at these two points; see Box <a href="sec-pupil.html#thm:truncation">4.1</a>. In log-space, this is a difference between the log-transformed original distribution and the logarithm of the difference of the CDFs. The function <code>log_diff_exp</code> is a more stable version of this last operation. What <code>log_diff_exp</code> does is to take the log of the difference of the exponent of two functions. In this case the functions are two log-CDFs.</p>
<pre class="stan fold-show"><code>target += normal_lpdf(T_nd | 150, 100)
          - log_diff_exp(normal_lcdf(min(rt) | 150, 100),
                         normal_lcdf(0 | 150, 100));</code></pre>
<p>We implement the likelihood of each joint observation of response time and choice with an if-else clause that calls the likelihood of the accumulator that corresponds to the choice selected in the trial <code>n</code>, and the complement CDF for the accumulator that was not selected:</p>
<pre class="stan fold-show"><code>    if(nchoice[n] == 1)
      target += lognormal_lpdf(T[n] | alpha[1] -
                               c_lex[n] * beta[1] -
                               c_lfreq[n] * beta[2] , sigma)  +
        lognormal_lccdf(T[n] | alpha[2] -
                        c_lex[n] * beta[3] -
                        c_lfreq[n] * beta[4], sigma);
    else
       target += lognormal_lpdf(T[n] | alpha[2] -
                                c_lex[n] * beta[3] -
                                c_lfreq[n] * beta[4], sigma) +
        lognormal_lccdf(T[n] | alpha[1] -
                        c_lex[n] * beta[1] -
                        c_lfreq[n] * beta[2], sigma);
  }
</code></pre>
<p>The complete Stan code for this model is shown below as <code>lnrace.stan</code>.</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] c_lfreq;
  vector[N] c_lex;
  vector[N] rt;
  int nchoice[N];
}
parameters {
  real alpha[2];
  real beta[4];
  real&lt;lower = 0&gt; sigma;
  real&lt;lower = 0, upper = min(rt)&gt; T_nd;
}
model {
  vector[N] T = rt - T_nd;
  target += normal_lpdf(alpha | 6, 1);
  target += normal_lpdf(beta | 0, .5);
  target += normal_lpdf(sigma | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += normal_lpdf(T_nd | 150, 100)
    - log_diff_exp(normal_lcdf(min(rt) | 150, 100),
                   normal_lcdf(0 | 150, 100));
  for(n in 1:N){
    if(nchoice[n] == 1)
      target += lognormal_lpdf(T[n] | alpha[1] -
                               c_lex[n] * beta[1] -
                               c_lfreq[n] * beta[2], sigma)  +
        lognormal_lccdf(T[n] | alpha[2] -
                        c_lex[n] * beta[3] -
                        c_lfreq[n] * beta[4], sigma);
    else
       target += lognormal_lpdf(T[n] | alpha[2] -
                                c_lex[n] * beta[3] -
                                c_lfreq[n] * beta[4], sigma) +
        lognormal_lccdf(T[n] | alpha[1] -
                        c_lex[n] * beta[1] -
                        c_lfreq[n] * beta[2], sigma);
  }
}</code></pre>
<p>Store the data in a list and fit the model. Some warnings might appear during the warm-up, but we can ignore them, since they no longer appear afterwards, and all the convergence checks look fine (omitted here).</p>
<div class="sourceCode" id="cb1200"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1200-1" data-line-number="1">lnrace &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb1200-2" data-line-number="2">                      <span class="st">&quot;lnrace.stan&quot;</span>,</a>
<a class="sourceLine" id="cb1200-3" data-line-number="3">                      <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</a>
<a class="sourceLine" id="cb1200-4" data-line-number="4">ls_blp1_sim &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> <span class="kw">nrow</span>(df_blp1_sim),</a>
<a class="sourceLine" id="cb1200-5" data-line-number="5">                    <span class="dt">rt =</span> df_blp1_sim<span class="op">$</span>rt,</a>
<a class="sourceLine" id="cb1200-6" data-line-number="6">                    <span class="dt">nchoice =</span> df_blp1_sim<span class="op">$</span>nchoice,</a>
<a class="sourceLine" id="cb1200-7" data-line-number="7">                    <span class="dt">c_lex =</span> df_blp1_sim<span class="op">$</span>c_lex,</a>
<a class="sourceLine" id="cb1200-8" data-line-number="8">                    <span class="dt">c_lfreq =</span> df_blp1_sim<span class="op">$</span>c_lfreq)</a>
<a class="sourceLine" id="cb1200-9" data-line-number="9">fit_blp1_sim &lt;-<span class="st"> </span><span class="kw">stan</span>(lnrace,</a>
<a class="sourceLine" id="cb1200-10" data-line-number="10">                     <span class="dt">data =</span> ls_blp1_sim)</a></code></pre></div>
<p>Print the parameters values.</p>
<div class="sourceCode" id="cb1201"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1201-1" data-line-number="1"><span class="kw">print</span>(fit_blp1_sim, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;T_nd&quot;</span>, <span class="st">&quot;sigma&quot;</span>)) </a></code></pre></div>
<pre><code>##            mean   2.5%  97.5% n_eff Rhat
## alpha[1]   6.67   6.60   6.73  4580    1
## alpha[2]   6.53   6.46   6.59  4211    1
## beta[1]    0.36   0.16   0.55  3372    1
## beta[2]    0.21   0.18   0.25  3210    1
## beta[3]   -0.46  -0.69  -0.22  3207    1
## beta[4]   -0.07  -0.13  -0.02  3125    1
## T_nd     143.41 131.36 152.30  3372    1
## sigma      0.78   0.74   0.82  3673    1</code></pre>
<p>As we did in previous chapters, we use <code>mcmc_recover_hist</code> to compare the posterior distributions of the relevant parameters of the model with their true values in Figure <a href="modeling-a-lexical-decision-task.html#fig:recoverlnrace">20.7</a>. First, however, we need to re-parametrize the true values, since <span class="math inline">\(D\)</span> cannot be known, and we don’t fit <span class="math inline">\(V\)</span>, but rather <span class="math inline">\(D/V\)</span>, with <span class="math inline">\(V\)</span> log-normally distributed. Then we obtain an estimate of <span class="math inline">\(\alpha&#39;\)</span>, rather than <span class="math inline">\(\alpha\)</span>, such that <span class="math inline">\(\alpha&#39; = log(mu_{d}) - \alpha\)</span>.</p>

<div class="sourceCode" id="cb1203"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1203-1" data-line-number="1">true_values &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">log</span>(D) <span class="op">-</span><span class="st"> </span>alpha_w,</a>
<a class="sourceLine" id="cb1203-2" data-line-number="2">                 <span class="kw">log</span>(D) <span class="op">-</span><span class="st"> </span>alpha_nw,</a>
<a class="sourceLine" id="cb1203-3" data-line-number="3">                 beta_wlex,</a>
<a class="sourceLine" id="cb1203-4" data-line-number="4">                 beta_wlfreq,</a>
<a class="sourceLine" id="cb1203-5" data-line-number="5">                 beta_nwlex,</a>
<a class="sourceLine" id="cb1203-6" data-line-number="6">                 beta_nwlfreq,</a>
<a class="sourceLine" id="cb1203-7" data-line-number="7">                 sigma,</a>
<a class="sourceLine" id="cb1203-8" data-line-number="8">                 T_nd)</a>
<a class="sourceLine" id="cb1203-9" data-line-number="9">estimates &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_blp1_sim) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1203-10" data-line-number="10"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="st"> </span>lp__)</a>
<a class="sourceLine" id="cb1203-11" data-line-number="11"><span class="kw">mcmc_recover_hist</span>(estimates, true_values)</a></code></pre></div>
<div class="figure"><span id="fig:recoverlnrace"></span>
<img src="bookdown_files/figure-html/recoverlnrace-1.svg" alt="Posterior distributions of the main parameters of the log-normal race model fit_blp1_sim together with their true values." width="672" />
<p class="caption">
FIGURE 20.7: Posterior distributions of the main parameters of the log-normal race model <code>fit_blp1_sim</code> together with their true values.
</p>
</div>
<p>Before moving on to a more complex version of this model, it’s worth spending some time making the code more modular. We encapsulate the likelihood of the log-normal race model by writing it as a function. The function has four arguments, the decision time <code>T</code>, the choice <code>nchoice</code> (this will only work with two choices, <code>1</code> and <code>2</code>), an array of locations <code>mu</code> (which again we implicitly assume that has two elements), and a common scale <code>sigma</code>.</p>
<pre class="stan fold-show"><code>functions {
  real lognormal_race2_lpdf(real T, int nchoice, real[] mu, real sigma){
    real lpdf;
    if(nchoice == 1)
        lpdf = lognormal_lpdf(T | mu[1] , sigma)  +
          lognormal_lccdf(T | mu[2], sigma);
      else
        lpdf = lognormal_lpdf(T | mu[2], sigma) +
          lognormal_lccdf(T | mu[1], sigma);
    return lpdf;
  }
}</code></pre>
<p>Now for each iteration <code>n</code> of the original for loop, we generate an auxiliary variable <code>T</code> which contains the decision time for the current trial, and <code>mu</code> as an array of size two that contains all the parameters that affect the location at each trial. This will allow us to use our new function as follows in the <code>model</code> block:<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a></p>
<pre class="stan fold-show"><code>  real log_lik[N];
  for(n in 1:N){
    real T = rt[n] - T_nd;
    real mu[2] = {alpha[1]  -
                    c_lex[n] * beta[1]  -
                    c_lfreq[n] * beta[2],
                    alpha[2]  -
                    c_lex[n] * beta[3] -
                    c_lfreq[n] * beta[4]};
    log_lik[n] = lognormal_race2_lpdf(T | nchoice[n], mu, sigma);
  }</code></pre>
<p>The variable <code>log_lik</code> contains the log-likelihood for each trial. We must not forget to add the total log-likelihood to the <code>target</code> variable. This is done simply by <code>target += sum(log_lik)</code>. <!-- There are two advantage of iterating first and then adding the total log likelihood to `target`: (i) we can use the variable `log_lik` for model comparison with cross validation (see chapter \@ref(ch:cv)) without the need to repeating code in the generated quantities, and (ii) using `sum` and adding to `target` once is slighter faster than adding to `target` at each iteration. --></p>
<p>The complete Stan code for this model can be found in the <code>bcogsci</code> package as <code>lnrace_mod.stan</code>, it is left for the reader to verify that the results are the same as from the non-modular model <code>lnrace.stan</code> fit earlier.</p>
<!-- ```{stan output.var = "lnrace_stan", code = readLines(lnrace_mod),  tidy = TRUE, comment="", eval = FALSE, cache = FALSE, cache.lazy = FALSE} -->
<!-- ``` -->
</div>
<div id="sec:lognormalh" class="section level3">
<h3><span class="header-section-number">20.1.4</span> A hierarchical implementation of the log-normal race model</h3>
<p>A simple hierarchical version of the previous model assumes that that all the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> have by-subject adjustments.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mu&#39;_{w,n} &amp;= \alpha&#39;_w + u_{subj[n], 1} - \mathit{lex}_n \cdot (\beta_{lex_{w}} + u_{subj[n], 2})  - \mathit{lfreq}_n \cdot (\beta_{\mathit{lfreq}_{w}} + u_{subj[n], 3}) \\
\mu&#39;_{nw,n} &amp;= \alpha&#39;_{nw} + u_{subj[n], 4} - \mathit{lex}_n \cdot (\beta_{lex_{nw}} + u_{subj[n], 5}) - \mathit{lfreq}_n \cdot (\beta_{\mathit{lfreq}_{nw}}+ u_{subj[n], 6}) \\
\end{aligned}
\end{equation}\]</span></p>
<p>Similarly to what we did in the hierarchical implementation of the fast-guess model in section <a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#sec:fastguessh">19.1.5</a> of the previous chapter, we assume that <span class="math inline">\(\boldsymbol{u}\)</span> is a matrix with as many rows as subjects and six columns, which follows a multivariate normal distribution centered at zero. For lack of more information, we assume the same (weakly informative) prior distribution for the six variance components <span class="math inline">\(\tau_{u_{1,...,6}}\)</span> with a somewhat smaller effect than we assumed for the prior of <span class="math inline">\(\sigma\)</span>. As we did with previous hierarchical models, we assign a regularizing LKJ prior for the correlations between the adjustments.<a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\boldsymbol{u} &amp;\sim\mathcal{N}(0, \Sigma_u)\\
\tau_{u_{1..6}} &amp; \sim \mathit{ \mathit{Normal}}_+(.1, .1)\\
\rho_u &amp;\sim \mathit{LKJcorr}(2) 
\end{aligned}
\end{equation}\]</span></p>
<p>Before we fit the model to the real data, we’ll verify that it works with simulated data. To create synthetic data of several subjects, we repeat the same generative process we used before and we add the by-subject adjustments <code>u</code> in the same way as in section <a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#sec:fastguessh">19.1.5</a> from chapter <a href="ch-mixture.html#ch:mixture">19</a>. This version of the log-normal race model assumes that all the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> have by-subject adjustments; that is, 6 adjustments. To simplify the model, we ignore the possibility of an adjustment for the non-decision time <span class="math inline">\(T_{nd}\)</span>, but see <span class="citation">Nicenboim and Vasishth (<a href="#ref-nicenboimModelsRetrievalSentence2018">2018</a>)</span> for an implementation of the log-normal race model with a hierarchical non-decision time.
For simplicity, all the adjustments, <code>u</code> are normally distributed with the same standard deviation of <span class="math inline">\(0.2\)</span>, and they have a <span class="math inline">\(0.3\)</span> correlation between pairs of <code>u</code>’s; see <code>tau_u</code> and <code>rho</code> below.</p>
<p>First, set a seed, take a subset of the data set to keep the same structure, set true values, and auxiliary variables that indicate the number of observations, subjects, etc.</p>
<div class="sourceCode" id="cb1206"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1206-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>) </a>
<a class="sourceLine" id="cb1206-2" data-line-number="2">df_blp_sim &lt;-<span class="st"> </span>df_blp <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1206-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(subj)  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1206-4" data-line-number="4"><span class="st">  </span><span class="kw">slice_sample</span>(<span class="dt">n =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1206-5" data-line-number="5"><span class="st">  </span><span class="kw">ungroup</span>()</a>
<a class="sourceLine" id="cb1206-6" data-line-number="6">D &lt;-<span class="st"> </span><span class="dv">1800</span></a>
<a class="sourceLine" id="cb1206-7" data-line-number="7">alpha_w &lt;-<span class="st"> </span><span class="fl">.8</span></a>
<a class="sourceLine" id="cb1206-8" data-line-number="8">beta_wlex &lt;-<span class="st"> </span><span class="fl">.5</span></a>
<a class="sourceLine" id="cb1206-9" data-line-number="9">beta_wlfreq &lt;-<span class="st"> </span><span class="fl">.2</span></a>
<a class="sourceLine" id="cb1206-10" data-line-number="10">alpha_nw &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1206-11" data-line-number="11">beta_nwlex &lt;-<span class="st"> </span><span class="fl">-.5</span></a>
<a class="sourceLine" id="cb1206-12" data-line-number="12">beta_nwlfreq &lt;-<span class="st"> </span><span class="fl">-.05</span></a>
<a class="sourceLine" id="cb1206-13" data-line-number="13">sigma &lt;-<span class="st"> </span><span class="fl">.8</span></a>
<a class="sourceLine" id="cb1206-14" data-line-number="14">T_nd &lt;-<span class="st"> </span><span class="dv">150</span></a>
<a class="sourceLine" id="cb1206-15" data-line-number="15">N &lt;-<span class="st"> </span><span class="kw">nrow</span>(df_blp_sim)</a>
<a class="sourceLine" id="cb1206-16" data-line-number="16">N_subj &lt;-<span class="st"> </span><span class="kw">max</span>(df_blp_sim<span class="op">$</span>subj)</a>
<a class="sourceLine" id="cb1206-17" data-line-number="17">N_adj &lt;-<span class="st"> </span><span class="dv">6</span> </a>
<a class="sourceLine" id="cb1206-18" data-line-number="18">tau_u &lt;-<span class="st"> </span><span class="kw">rep</span>(.<span class="dv">2</span>, N_adj)</a>
<a class="sourceLine" id="cb1206-19" data-line-number="19">rho &lt;-<span class="st"> </span><span class="fl">.3</span></a>
<a class="sourceLine" id="cb1206-20" data-line-number="20">Cor_u &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(rho, N_adj <span class="op">*</span><span class="st"> </span>N_adj), <span class="dt">nrow =</span> N_adj)</a>
<a class="sourceLine" id="cb1206-21" data-line-number="21"><span class="kw">diag</span>(Cor_u) &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1206-22" data-line-number="22">Sigma_u &lt;-<span class="st"> </span><span class="kw">diag</span>(tau_u, N_adj, N_adj) <span class="op">%*%</span></a>
<a class="sourceLine" id="cb1206-23" data-line-number="23"><span class="st">  </span>Cor_u <span class="op">%*%</span></a>
<a class="sourceLine" id="cb1206-24" data-line-number="24"><span class="st">  </span><span class="kw">diag</span>(tau_u, N_adj, N_adj)</a>
<a class="sourceLine" id="cb1206-25" data-line-number="25">u &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> N_subj, <span class="kw">rep</span>(<span class="dv">0</span>, N_adj), Sigma_u)</a>
<a class="sourceLine" id="cb1206-26" data-line-number="26">subj &lt;-<span class="st"> </span>df_blp_sim<span class="op">$</span>subj</a></code></pre></div>
<p>Second, generate the locations of both accumulators, <code>mu_w</code> and <code>mu_nw</code>, for every trial.</p>
<div class="sourceCode" id="cb1207"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1207-1" data-line-number="1">mu_w &lt;-<span class="st"> </span>alpha_w <span class="op">+</span><span class="st"> </span>u[subj, <span class="dv">1</span>] <span class="op">+</span></a>
<a class="sourceLine" id="cb1207-2" data-line-number="2"><span class="st">  </span>df_blp_sim<span class="op">$</span>c_lfreq <span class="op">*</span><span class="st"> </span>(beta_wlfreq <span class="op">+</span><span class="st"> </span>u[subj, <span class="dv">2</span>] ) <span class="op">+</span></a>
<a class="sourceLine" id="cb1207-3" data-line-number="3"><span class="st">  </span>df_blp_sim<span class="op">$</span>c_lex <span class="op">*</span><span class="st"> </span>(beta_wlex <span class="op">+</span><span class="st"> </span>u[subj, <span class="dv">3</span>]) </a>
<a class="sourceLine" id="cb1207-4" data-line-number="4">mu_nw &lt;-<span class="st"> </span>alpha_nw <span class="op">+</span><span class="st"> </span>u[subj, <span class="dv">4</span>] <span class="op">+</span></a>
<a class="sourceLine" id="cb1207-5" data-line-number="5"><span class="st">  </span>df_blp_sim<span class="op">$</span>c_lfreq <span class="op">*</span><span class="st"> </span>(beta_nwlfreq <span class="op">+</span><span class="st"> </span>u[subj, <span class="dv">5</span>] ) <span class="op">+</span></a>
<a class="sourceLine" id="cb1207-6" data-line-number="6"><span class="st">  </span>df_blp_sim<span class="op">$</span>c_lex <span class="op">*</span><span class="st"> </span>(beta_nwlex <span class="op">+</span><span class="st"> </span>u[subj, <span class="dv">6</span>] )</a></code></pre></div>
<p>Third, generate values for the rates of accumulation and use those rates to calculate <code>T_w</code> and <code>T_nw</code>.</p>
<div class="sourceCode" id="cb1208"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1208-1" data-line-number="1">V_w &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(N, mu_w, sigma)</a>
<a class="sourceLine" id="cb1208-2" data-line-number="2">V_nw &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(N, mu_nw, sigma)</a>
<a class="sourceLine" id="cb1208-3" data-line-number="3">T_w &lt;-<span class="st">  </span>D <span class="op">/</span><span class="st"> </span>V_w</a>
<a class="sourceLine" id="cb1208-4" data-line-number="4">T_nw &lt;-<span class="st"> </span>D <span class="op">/</span><span class="st"> </span>V_nw</a></code></pre></div>
<p>Fourth, calculate the time it takes to reach to a decision and the winner accumulator for each trial.</p>
<div class="sourceCode" id="cb1209"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1209-1" data-line-number="1">T_winner &lt;-<span class="st"> </span><span class="kw">pmin</span>(T_w, T_nw)</a>
<a class="sourceLine" id="cb1209-2" data-line-number="2">accumulator_winner &lt;-<span class="st"> </span><span class="kw">ifelse</span>(T_w <span class="op">==</span><span class="st"> </span><span class="kw">pmin</span>(T_w, T_nw),</a>
<a class="sourceLine" id="cb1209-3" data-line-number="3">                             <span class="st">&quot;word&quot;</span>,</a>
<a class="sourceLine" id="cb1209-4" data-line-number="4">                             <span class="st">&quot;non-word&quot;</span>)</a></code></pre></div>
<p>Finally, add this information to the data frame.</p>
<div class="sourceCode" id="cb1210"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1210-1" data-line-number="1">df_blp_sim &lt;-<span class="st"> </span>df_blp_sim <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1210-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">rt =</span> T_nd <span class="op">+</span><span class="st"> </span>T_winner,</a>
<a class="sourceLine" id="cb1210-3" data-line-number="3">         <span class="dt">choice =</span> accumulator_winner,</a>
<a class="sourceLine" id="cb1210-4" data-line-number="4">         <span class="dt">nchoice =</span> <span class="kw">ifelse</span>(choice <span class="op">==</span><span class="st"> &quot;word&quot;</span>, <span class="dv">1</span>, <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb1210-5" data-line-number="5">         <span class="dt">acc =</span> <span class="kw">ifelse</span>(lex <span class="op">==</span><span class="st"> </span>choice, <span class="dv">1</span>, <span class="dv">0</span>))</a></code></pre></div>
<p>The Stan code for this model implements the non-centered parametrization for correlated adjustments (see section <a href="sec-hierstan.html#sec:corrstan">11.1.3</a> for more details). The model is shown below as <code>lnrace_h.stan</code>.</p>
<pre class="stan fold-show"><code>functions {
  real lognormal_race2_lpdf(real T, int nchoice, real[] mu, real sigma){
    real lpdf;
    if(nchoice == 1)
        lpdf = lognormal_lpdf(T | mu[1] , sigma)  +
          lognormal_lccdf(T | mu[2], sigma);
      else
        lpdf = lognormal_lpdf(T | mu[2], sigma) +
          lognormal_lccdf(T | mu[1], sigma);
    return lpdf;
  }
}
data {
  int&lt;lower = 1&gt; N;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_lfreq;
  vector[N] c_lex;
  vector[N] rt;
  int nchoice[N];
  int subj[N];
}
transformed data{
  real min_rt = min(rt);
  real max_rt = max(rt);
  int N_re = 6;
}
parameters {
  real alpha[2];
  real beta[4];
  real&lt;lower = 0&gt; sigma;
  real&lt;lower = 0, upper = min(rt)&gt; T_nd;
  vector&lt;lower = 0&gt;[N_re] tau_u;
  matrix[N_re, N_subj] z_u;
  cholesky_factor_corr[N_re] L_u;
}
transformed parameters {
  matrix[N_subj, N_re] u;
  u = (diag_pre_multiply(tau_u, L_u) * z_u)&#39;;
  }
model {
  real log_lik[N];
  target += normal_lpdf(alpha | 6, 1);
  target += normal_lpdf(beta | 0, .5);
  target += normal_lpdf(sigma | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += normal_lpdf(T_nd | 150, 100)
    - log_diff_exp(normal_lcdf(min(rt) | 150, 100),
                   normal_lcdf(0 | 150, 100));
  target += normal_lpdf(tau_u | .1, .1)
    - N_re * normal_lccdf(0 | .1, .1);
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += std_normal_lpdf(to_vector(z_u));
  for(n in 1:N){
    real T = rt[n] - T_nd;
    real mu[2] = {alpha[1] + u[subj[n], 1] -
                    c_lex[n] * (beta[1] + u[subj[n], 2]) -
                    c_lfreq[n] * (beta[2] + u[subj[n], 3]),
                    alpha[2] + u[subj[n], 4] -
                    c_lex[n] * (beta[3] + u[subj[n], 5]) -
                    c_lfreq[n] * (beta[4] + u[subj[n], 6])};
     log_lik[n] = lognormal_race2_lpdf(T | nchoice[n], mu, sigma);
  }
  target += sum(log_lik);
}
generated quantities {
  corr_matrix[N_re] rho_u = L_u * L_u&#39;;
}</code></pre>
<p>Store the simulated data in a list and fit it.</p>
<div class="sourceCode" id="cb1212"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1212-1" data-line-number="1">lnrace_h &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb1212-2" data-line-number="2">                      <span class="st">&quot;lnrace_h.stan&quot;</span>,</a>
<a class="sourceLine" id="cb1212-3" data-line-number="3">                      <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</a>
<a class="sourceLine" id="cb1212-4" data-line-number="4">ls_blp_h_sim &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> <span class="kw">nrow</span>(df_blp_sim),</a>
<a class="sourceLine" id="cb1212-5" data-line-number="5">                     <span class="dt">N_subj =</span> <span class="kw">max</span>(df_blp_sim<span class="op">$</span>subj),</a>
<a class="sourceLine" id="cb1212-6" data-line-number="6">                     <span class="dt">subj =</span> df_blp_sim<span class="op">$</span>subj,</a>
<a class="sourceLine" id="cb1212-7" data-line-number="7">                     <span class="dt">rt =</span> df_blp_sim<span class="op">$</span>rt,</a>
<a class="sourceLine" id="cb1212-8" data-line-number="8">                     <span class="dt">nchoice =</span> df_blp_sim<span class="op">$</span>nchoice,</a>
<a class="sourceLine" id="cb1212-9" data-line-number="9">                     <span class="dt">c_lex =</span> df_blp_sim<span class="op">$</span>c_lex,</a>
<a class="sourceLine" id="cb1212-10" data-line-number="10">                     <span class="dt">c_lfreq =</span> df_blp_sim<span class="op">$</span>c_lfreq)</a>
<a class="sourceLine" id="cb1212-11" data-line-number="11">fit_blp_h_sim &lt;-<span class="st"> </span><span class="kw">stan</span>(lnrace_h, <span class="dt">data =</span> ls_blp_h_sim)</a></code></pre></div>
<p>The code below compares the posterior distributions of the relevant parameters of the model with their true values, and plots them in Figure <a href="modeling-a-lexical-decision-task.html#fig:recoverlnraceh">20.8</a>. The true value for all the correlations was <span class="math inline">\(0.3\)</span>, but we need to correct the sign depending on whether there was a minus sign in front of the adjustment when we built <code>mu_w</code> and <code>mu_wn</code> or not: For example, there is no minus before <code>u[subj, 1]</code>, but there is one before <code>u[subj, 2]</code>, thus the true correlation between <code>u[subj, 1]</code> and <code>u[subj, 2]</code> we generated should be negative (plus times minus is minus); and there is a minus before <code>u[subj, 3]</code> and thus the correlation between <code>u[subj, 2]</code> and <code>u[subj, 3]</code> should positive (minus times minus is positive).</p>

<div class="sourceCode" id="cb1213"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1213-1" data-line-number="1">rho_us &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">paste0</span>(<span class="st">&quot;rho_u[1,&quot;</span>, <span class="dv">2</span><span class="op">:</span><span class="dv">6</span> , <span class="st">&quot;]&quot;</span>),</a>
<a class="sourceLine" id="cb1213-2" data-line-number="2">            <span class="kw">paste0</span>(<span class="st">&quot;rho_u[2,&quot;</span>, <span class="dv">3</span><span class="op">:</span><span class="dv">6</span> , <span class="st">&quot;]&quot;</span>),</a>
<a class="sourceLine" id="cb1213-3" data-line-number="3">            <span class="kw">paste0</span>(<span class="st">&quot;rho_u[3,&quot;</span>, <span class="dv">4</span><span class="op">:</span><span class="dv">6</span> , <span class="st">&quot;]&quot;</span>),</a>
<a class="sourceLine" id="cb1213-4" data-line-number="4">            <span class="kw">paste0</span>(<span class="st">&quot;rho_u[4,&quot;</span>, <span class="dv">5</span><span class="op">:</span><span class="dv">6</span> , <span class="st">&quot;]&quot;</span>),</a>
<a class="sourceLine" id="cb1213-5" data-line-number="5">            <span class="st">&quot;rho_u[5,6]&quot;</span>)</a>
<a class="sourceLine" id="cb1213-6" data-line-number="6">corrs &lt;-<span class="st"> </span>rho <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">-1</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1213-7" data-line-number="7">true_values &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">log</span>(D) <span class="op">-</span><span class="st"> </span>alpha_w,</a>
<a class="sourceLine" id="cb1213-8" data-line-number="8">                 <span class="kw">log</span>(D) <span class="op">-</span><span class="st"> </span>alpha_nw,</a>
<a class="sourceLine" id="cb1213-9" data-line-number="9">                 beta_wlex,</a>
<a class="sourceLine" id="cb1213-10" data-line-number="10">                 beta_wlfreq,</a>
<a class="sourceLine" id="cb1213-11" data-line-number="11">                 beta_nwlex,</a>
<a class="sourceLine" id="cb1213-12" data-line-number="12">                 beta_nwlfreq,</a>
<a class="sourceLine" id="cb1213-13" data-line-number="13">                 T_nd,</a>
<a class="sourceLine" id="cb1213-14" data-line-number="14">                 sigma,</a>
<a class="sourceLine" id="cb1213-15" data-line-number="15">                 tau_u,</a>
<a class="sourceLine" id="cb1213-16" data-line-number="16">                 corrs)</a>
<a class="sourceLine" id="cb1213-17" data-line-number="17">par_names =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>,</a>
<a class="sourceLine" id="cb1213-18" data-line-number="18">              <span class="st">&quot;beta&quot;</span>,</a>
<a class="sourceLine" id="cb1213-19" data-line-number="19">              <span class="st">&quot;T_nd&quot;</span>,</a>
<a class="sourceLine" id="cb1213-20" data-line-number="20">              <span class="st">&quot;sigma&quot;</span>,</a>
<a class="sourceLine" id="cb1213-21" data-line-number="21">              <span class="st">&quot;tau_u&quot;</span>,</a>
<a class="sourceLine" id="cb1213-22" data-line-number="22">              rho_us)</a>
<a class="sourceLine" id="cb1213-23" data-line-number="23">estimates &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_blp_h_sim) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1213-24" data-line-number="24"><span class="st">  </span><span class="kw">select</span>(<span class="kw">contains</span>(par_names))</a>
<a class="sourceLine" id="cb1213-25" data-line-number="25"><span class="kw">mcmc_recover_hist</span>(estimates, true_values)</a></code></pre></div>
<div class="figure"><span id="fig:recoverlnraceh"></span>
<img src="bookdown_files/figure-html/recoverlnraceh-1.svg" alt="Posterior distributions of the main parameters of the log-normal race model fit_blp_h_sim together with their true values." width="672" />
<p class="caption">
FIGURE 20.8: Posterior distributions of the main parameters of the log-normal race model <code>fit_blp_h_sim</code> together with their true values.
</p>
</div>
<p>We see that we can recover the true values quite well, even if there is a great deal of uncertainty over the posteriors of the correlations.</p>
<p>We are now ready to fit the model to the observed data.</p>
<p>Create a list with the real data and fit the same Stan model.</p>
<div class="sourceCode" id="cb1214"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1214-1" data-line-number="1">lnrace_h &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb1214-2" data-line-number="2">                      <span class="st">&quot;lnrace_h.stan&quot;</span>,</a>
<a class="sourceLine" id="cb1214-3" data-line-number="3">                      <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</a>
<a class="sourceLine" id="cb1214-4" data-line-number="4">ls_blp_h &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> <span class="kw">nrow</span>(df_blp),</a>
<a class="sourceLine" id="cb1214-5" data-line-number="5">                     <span class="dt">N_subj =</span> <span class="kw">max</span>(df_blp<span class="op">$</span>subj),</a>
<a class="sourceLine" id="cb1214-6" data-line-number="6">                     <span class="dt">subj =</span> df_blp<span class="op">$</span>subj,</a>
<a class="sourceLine" id="cb1214-7" data-line-number="7">                     <span class="dt">rt =</span> df_blp<span class="op">$</span>rt,</a>
<a class="sourceLine" id="cb1214-8" data-line-number="8">                     <span class="dt">nchoice =</span> df_blp<span class="op">$</span>nchoice,</a>
<a class="sourceLine" id="cb1214-9" data-line-number="9">                     <span class="dt">c_lex =</span> df_blp<span class="op">$</span>c_lex,</a>
<a class="sourceLine" id="cb1214-10" data-line-number="10">                     <span class="dt">c_lfreq =</span> df_blp<span class="op">$</span>c_lfreq)</a>
<a class="sourceLine" id="cb1214-11" data-line-number="11">fit_blp_h_sim &lt;-<span class="st"> </span><span class="kw">stan</span>(lnrace_h, <span class="dt">data =</span> ls_blp_h)</a></code></pre></div>
<p>Print the summary (omit the correlations for now).</p>
<div class="sourceCode" id="cb1215"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1215-1" data-line-number="1"><span class="kw">print</span>(fit_blp_h, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>,</a>
<a class="sourceLine" id="cb1215-2" data-line-number="2">                          <span class="st">&quot;beta&quot;</span>,</a>
<a class="sourceLine" id="cb1215-3" data-line-number="3">                          <span class="st">&quot;T_nd&quot;</span>,</a>
<a class="sourceLine" id="cb1215-4" data-line-number="4">                          <span class="st">&quot;sigma&quot;</span>,</a>
<a class="sourceLine" id="cb1215-5" data-line-number="5">                          <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<pre><code>##           mean  2.5% 97.5% n_eff Rhat
## alpha[1]  6.83  6.75  6.91   610    1
## alpha[2]  6.64  6.58  6.69  1109    1
## beta[1]   0.37  0.31  0.42  2110    1
## beta[2]   0.06  0.06  0.07  1334    1
## beta[3]  -0.14 -0.18 -0.09  2732    1
## beta[4]  -0.07 -0.07 -0.06  4032    1
## T_nd     11.31 10.00 12.45  9007    1
## sigma     0.34  0.34  0.35  7575    1
## tau_u[1]  0.17  0.13  0.23  1505    1
## tau_u[2]  0.10  0.07  0.15  3076    1
## tau_u[3]  0.02  0.01  0.02  2108    1
## tau_u[4]  0.14  0.10  0.18  1840    1
## tau_u[5]  0.08  0.05  0.12  3344    1
## tau_u[6]  0.01  0.00  0.02  1335    1</code></pre>
<p>Even though the model converged, the posterior summary shows a clear problem with the model: The estimate for the non-decision time, <span class="math inline">\(T_{nd}\)</span> is less than 12 milliseconds! This is just not possible; physiological research shows that the eye-to-brain lag, the time it takes for the visual features on the screen until they are propagated from the retina to the brain is at least 50 milliseconds <span class="citation">(Clark, Fan, and Hillyard <a href="#ref-clark1994identification">1994</a>)</span>. Besides identifying the stimuli, the subjects also need to initiate a motor response which takes at least 100 milliseconds. Then how is it possible that we obtained this extremely fast non-decision time? The reason is that the parameter <code>T_nd</code> is constrained to be between zero and the shortest reaction time.</p>
<p>Verify what is the shortest reaction time in the data set and how many observations are below 150 milliseconds.</p>
<div class="sourceCode" id="cb1217"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1217-1" data-line-number="1"><span class="kw">min</span>(df_blp<span class="op">$</span>rt)</a></code></pre></div>
<pre><code>## [1] 16</code></pre>
<div class="sourceCode" id="cb1219"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1219-1" data-line-number="1"><span class="kw">sum</span>(df_blp<span class="op">$</span>rt <span class="op">&lt;</span><span class="st"> </span><span class="dv">150</span>)</a></code></pre></div>
<pre><code>## [1] 2</code></pre>
<p>This shows that some responses must have been initiated even before the stimuli was presented! We turn to deal with the <em>contaminant</em> observations next.</p>
</div>
<div id="sec:contaminant" class="section level3">
<h3><span class="header-section-number">20.1.5</span> Dealing with contaminant responses</h3>
<p>So far we have assumed that all the observations were coming from responses done after a decision was made. But what happens if there are anticipations or lapses of attention where the subject responds either before the stimuli is presented or after the stimulus was presented, but without attending to the stimulus? We are in a situation analogous to what we described before in chapter <a href="ch-mixture.html#ch:mixture">19</a> with the fast-guess model <span class="citation">(Ollman <a href="#ref-Ollman1966">1966</a>)</span>. There, we assumed that the behavior of a subject would be the mixture of two distributions, one corresponding to a guessing mode of responses and another one to a task-engaged mode. There are two major differences with the fast guess model, however. First, we assume here that guesses can be fast (e.g., anticipations) as well as slow. Second, here guesses occur in a minority of the cases and choice and response times are mostly explained by the log-normal race model.
The distribution that corresponds to these guesses is sometimes called a <em>contaminant distribution</em>. When the contaminant response times are outside the usual range
of response times (either shorter or longer), they can cause major problems in data analysis, distorting estimates. As we saw before, extremely short response times caused by anticipating the response can make it virtually impossible to estimate the non-decision time.</p>
<p>A recommended approach for dealing with this problem that we follow here is to assume that the responses come from a mixture between the sequential sampling model (in this case the log-normal race model) and a uniform distribution bounded at the minimum and maximum observed response time <span class="citation">(e.g., Ratcliff and Tuerlinckx <a href="#ref-ratcliff2002estimating">2002</a>)</span>.</p>
<p>The new likelihood function will look as follows:</p>
<p><span class="math display" id="eq:lnracecont">\[\begin{equation}
\begin{aligned}
p(rt_n, choice_n) =&amp; \theta_c \cdot \mathit{Uniform}(rt_n | min(rt), max(rt)) \cdot \mathit{Bernoulli}(choice_n | \theta_{bias}) +\\
&amp; (1-\theta_c) \cdot p_{lnrace}(rt_n, choice_n | \mu&#39;, \sigma) 
\end{aligned}
\tag{20.4}
\end{equation}\]</span></p>
<p>The first term of the sum represents the contaminant component that occurs with probability <span class="math inline">\(\theta_{c}\)</span> and has a likelihood that depends on the response time, represented with the uniform PDF, and on the response given represented with a Bernoulli PMF. When a subject is guessing, the likelihood of each choice depends on <span class="math inline">\(\theta_{bias}\)</span>.</p>
<p>The second term of the likelihood represents the log-normal race model that occurs with probability <span class="math inline">\(1-\theta_c\)</span>. We use <span class="math inline">\(p_{lnrace}(rt_n, choice_n)\)</span> as a shorthand for the following function (which we have already used in the models before):</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p_{lnrace}(rt_n, choice_n)&amp; =\\
&amp;\begin{cases}
\mathit{LogNormal}(\mu&#39;_{w,n}, \sigma) \cdot \\
(1 -  \mathit{LogNormal}\_CDF(T_{n}| \mu&#39;_{w,n}, \sigma)) \text{, if } \mathit{choice}= \text{word}\\
\\
\mathit{LogNormal}(\mu&#39;_{nw,n}, \sigma) \cdot \\
(1 -  \mathit{LogNormal}\_CDF(T_{n}| \mu&#39;_{nw,n}, \sigma))\text{, otherwise }\\
\end{cases}
\end{aligned}
\end{equation}\]</span></p>
<p>To simplify the model, we assume that contaminant responses are completely random (i.e., there is no bias to word or non-word) by setting <span class="math inline">\(\theta_{bias} = 0.5\)</span>.<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a> This makes <span class="math inline">\(\mathit{Bernoulli}(choice_n | \theta_{bias}) = .5\)</span>.</p>
<p>For this model to be identifiable, we need to assume that <span class="math inline">\(\theta_{c} &lt;&lt; 1\)</span>. This is a sensible assumption for this particular model, since the contaminant distribution is assumed to only happen in a minority of the cases. We set the following prior to <span class="math inline">\(\theta_{c}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\theta_c \sim  \mathit{Beta}(0.9, 70)
\end{equation}\]</span></p>
<p>By setting the first parameter of the beta distribution to a number smaller than 1 we get a distribution of possible probabilities with a “horn” on the left, see Figure <a href="modeling-a-lexical-decision-task.html#fig:thetac">20.9</a>. Our prior belief for <span class="math inline">\(\theta_{c}\)</span> is on average <span class="math inline">\(0.007\)</span>, and its 95% CrI is
<span class="math inline">\([0, 0.048]\)</span>.<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a></p>

<div class="sourceCode" id="cb1221"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb1221-1" data-line-number="1"></a>
<a class="sourceLine" id="cb1221-2" data-line-number="2"> <span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">tibble</span>(<span class="dt">theta_c =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)), <span class="kw">aes</span>(theta_c)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1221-3" data-line-number="3"><span class="st">  </span><span class="kw">stat_function</span>(</a>
<a class="sourceLine" id="cb1221-4" data-line-number="4">    <span class="dt">fun =</span> dbeta,</a>
<a class="sourceLine" id="cb1221-5" data-line-number="5">    <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">shape1 =</span> <span class="fl">.9</span>, <span class="dt">shape2 =</span> <span class="dv">70</span>),</a>
<a class="sourceLine" id="cb1221-6" data-line-number="6">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb1221-7" data-line-number="7"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;density&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:thetac"></span>
<img src="bookdown_files/figure-html/thetac-1.svg" alt="Prior distribution for \(\theta_{c}\). Most of the probability mass is close to 0." width="672" />
<p class="caption">
FIGURE 20.9: Prior distribution for <span class="math inline">\(\theta_{c}\)</span>. Most of the probability mass is close to 0.
</p>
</div>
<p>We also want to “push” the non-decision time further from zero to get more realistic values. For this reason we increase the informativity of the prior of <span class="math inline">\(T_{nd}\)</span>. A log-normal prior discourages values too close to zero, even with a similar location (on log-scale) than the truncated normal prior. We settle on the following prior:</p>
<p><span class="math display">\[\begin{equation}
T_{nd} \sim  \mathit{LogNormal}(log(150), .6)
\end{equation}\]</span></p>

<div class="sourceCode" id="cb1222"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb1222-1" data-line-number="1"></a>
<a class="sourceLine" id="cb1222-2" data-line-number="2">sdlog &lt;-<span class="st"> </span><span class="fl">.6</span></a>
<a class="sourceLine" id="cb1222-3" data-line-number="3">lq =<span class="st"> </span><span class="kw">qlnorm</span>(.<span class="dv">025</span>,<span class="kw">log</span>(<span class="dv">150</span>), sdlog)</a>
<a class="sourceLine" id="cb1222-4" data-line-number="4">hq =<span class="st"> </span><span class="kw">qlnorm</span>(.<span class="dv">975</span>,<span class="kw">log</span>(<span class="dv">150</span>), sdlog)</a>
<a class="sourceLine" id="cb1222-5" data-line-number="5"><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">tibble</span>(<span class="dt">T_nd =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1000</span>)), <span class="kw">aes</span>(T_nd)) <span class="op">+</span></a>
<a class="sourceLine" id="cb1222-6" data-line-number="6"><span class="st">  </span><span class="kw">stat_function</span>(</a>
<a class="sourceLine" id="cb1222-7" data-line-number="7">    <span class="dt">fun =</span> dlnorm,</a>
<a class="sourceLine" id="cb1222-8" data-line-number="8">    <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">meanlog =</span> <span class="kw">log</span>(<span class="dv">150</span>), <span class="dt">sdlog =</span> sdlog),</a>
<a class="sourceLine" id="cb1222-9" data-line-number="9">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb1222-10" data-line-number="10"><span class="st">   </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> lq, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1222-11" data-line-number="11"><span class="st">   </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> hq, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1222-12" data-line-number="12"><span class="st">   </span><span class="kw">geom_text</span>(<span class="dt">label =</span> <span class="kw">round</span>(lq), <span class="dt">x =</span> lq <span class="op">-</span><span class="st"> </span><span class="dv">50</span>, <span class="dt">y =</span> <span class="fl">0.0025</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1222-13" data-line-number="13"><span class="st">   </span><span class="kw">geom_text</span>(<span class="dt">label =</span> <span class="kw">round</span>(hq), <span class="dt">x =</span> hq <span class="op">+</span><span class="st"> </span><span class="dv">50</span>, <span class="dt">y =</span> <span class="fl">0.0025</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1222-14" data-line-number="14"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;density&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:tnd"></span>
<img src="bookdown_files/figure-html/tnd-1.svg" alt="Prior distribution for \(T_{nd}\). The dashed lines show the 95% credible interval." width="672" />
<p class="caption">
FIGURE 20.10: Prior distribution for <span class="math inline">\(T_{nd}\)</span>. The dashed lines show the 95% credible interval.
</p>
</div>
<p>Finally, we want to be able to account for response times that are actually faster than the non-decision time. If the observed response time, <code>rt</code> is smaller than the non-decision time, <code>T_nd</code>, we can be sure that the observation belongs to the contaminant distribution, because otherwise the decision time, <code>T</code> should be negative! This means that in this case, the log-normal race likelihood is <span class="math inline">\(0\)</span> (and its logarithm is negative infinity). When <span class="math inline">\(T&lt;0\)</span>, the log-likelihood of our model is <span class="math inline">\(log(\theta \cdot Uniform(rt_{n} | min, max) \cdot 0.5 + (1-\theta) \cdot p_{lnrace})\)</span> with <span class="math inline">\(p_{lnrace} =0\)</span>. This means that we only use the following code:</p>
<pre class="stan fold-show"><code>   target += log(theta_c) + uniform_lpdf(rt[n] | min_rt, max_rt) +
                 log(0.5);
</code></pre>
<p>This also means that we need to relax the constraints on <code>T_nd</code>, it doesn’t need to be smaller than the smallest observed response time, since some of the observations are responses from the contaminant distribution.</p>
<pre class="stan fold-show"><code>  real&lt;lower = 0&gt; T_nd;</code></pre>
<p>When <span class="math inline">\(T&gt;0\)</span>, the likelihood is a mixture of the contaminant distribution and the log-normal race model as defined in <a href="modeling-a-lexical-decision-task.html#eq:lnracecont">(20.4)</a>. We use <code>log_sum_exp</code> exactly as we did in <a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#sec:simplefastguess">19.1.2</a> and <a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#sec:multmix">19.1.3</a> for the fast-guess model.
We fit a mixture distribution between a contaminant distribution and the log-normal race model.</p>
<pre class="stan fold-show"><code> target += log_sum_exp(
                  log(theta_c) + uniform_lpdf(rt[n] | min_rt, max_rt)
                           + log(0.5),
                  log1m(theta_c) + lognormal_race2_lpdf(T[n] | nchoice[n],
                                                        mu, sigma));</code></pre>
<p>Finally, the complete Stan code for this model is shown below as <code>lnrace_cont.stan</code>.</p>
<pre class="stan fold-show"><code>functions {
  real lognormal_race2_lpdf(real T, int nchoice, real[] mu, real sigma){
    real lpdf;
    if(nchoice == 1)
        lpdf = lognormal_lpdf(T | mu[1] , sigma)  +
          lognormal_lccdf(T | mu[2], sigma);
      else
        lpdf = lognormal_lpdf(T | mu[2], sigma) +
          lognormal_lccdf(T | mu[1], sigma);
    return lpdf;
  }
}
data {
  int&lt;lower = 1&gt; N;
  int&lt;lower = 1&gt; N_subj;
  vector[N] c_lfreq;
  vector[N] c_lex;
  vector[N] rt;
  int nchoice[N];
  int subj[N];
}
transformed data{
  real min_rt = min(rt);
  real max_rt = max(rt);
  int N_re = 6;
}
parameters {
  real alpha[2];
  real beta[4];
  real&lt;lower = 0&gt; sigma;
  real&lt;lower = 0&gt; T_nd;
  real&lt;lower = 0, upper = 1&gt; theta_c;
  vector&lt;lower = 0&gt;[N_re] tau_u;
  matrix[N_re, N_subj] z_u;
  cholesky_factor_corr[N_re] L_u;
}
transformed parameters {
  matrix[N_subj, N_re] u;
  u = (diag_pre_multiply(tau_u, L_u) * z_u)&#39;;
}
model {
  real log_lik[N];
  target += normal_lpdf(alpha | 6, 1);
  target += normal_lpdf(beta | 0, .5);
  target += normal_lpdf(sigma | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += lognormal_lpdf(T_nd | log(150), .6);
  target += beta_lpdf(theta_c | .9, 70);
  target += normal_lpdf(tau_u | .1, .1)
    - N_re * normal_lccdf(0 | .1, .1);
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += std_normal_lpdf(to_vector(z_u));
    for(n in 1:N){
    real T = rt[n] - T_nd;
    if(T &gt; 0){
    real mu[2] = {alpha[1] + u[subj[n], 1] -
                    c_lex[n] * (beta[1] + u[subj[n], 2]) -
                    c_lfreq[n] * (beta[2] + u[subj[n], 3]),
                    alpha[2] + u[subj[n], 4] -
                    c_lex[n] * (beta[3] + u[subj[n], 5]) -
                    c_lfreq[n] * (beta[4] + u[subj[n], 6])};
    log_lik[n] = log_sum_exp(
                  log(theta_c) + uniform_lpdf(rt[n] | min_rt, max_rt)
                         + log(.5),
                  log1m(theta_c) + lognormal_race2_lpdf(T | nchoice[n], mu, sigma));
    } else {
      // T &lt; 0, observed time is smaller than the non-decision time
      log_lik[n] = log(theta_c) + uniform_lpdf(rt[n] | min_rt, max_rt)
                   + log(.5);
    }
  }
  target += sum(log_lik);
}</code></pre>
<p>In practice, we should verify that this model can recover the true values of its parameters by first simulating data and fitting the model to simulated data. We skip this step here.</p>
<p>Store the real data in a list and fit the model.</p>
<div class="sourceCode" id="cb1227"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1227-1" data-line-number="1">lnrace_h_cont &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb1227-2" data-line-number="2">                             <span class="st">&quot;lnrace_h_cont.stan&quot;</span>,</a>
<a class="sourceLine" id="cb1227-3" data-line-number="3">                             <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</a>
<a class="sourceLine" id="cb1227-4" data-line-number="4">ls_blp_h &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> <span class="kw">nrow</span>(df_blp),</a>
<a class="sourceLine" id="cb1227-5" data-line-number="5">                 <span class="dt">N_subj =</span> <span class="kw">max</span>(df_blp<span class="op">$</span>subj),</a>
<a class="sourceLine" id="cb1227-6" data-line-number="6">                 <span class="dt">subj =</span> df_blp<span class="op">$</span>subj,</a>
<a class="sourceLine" id="cb1227-7" data-line-number="7">                 <span class="dt">rt =</span> df_blp<span class="op">$</span>rt,</a>
<a class="sourceLine" id="cb1227-8" data-line-number="8">                 <span class="dt">nchoice =</span> df_blp<span class="op">$</span>nchoice,</a>
<a class="sourceLine" id="cb1227-9" data-line-number="9">                 <span class="dt">c_lex =</span> df_blp<span class="op">$</span>c_lex,</a>
<a class="sourceLine" id="cb1227-10" data-line-number="10">                 <span class="dt">c_lfreq =</span> df_blp<span class="op">$</span>c_lfreq)</a>
<a class="sourceLine" id="cb1227-11" data-line-number="11">fit_blp_h_cont &lt;-<span class="st"> </span><span class="kw">stan</span>(lnrace_h_cont, <span class="dt">data =</span> ls_blp_h)</a></code></pre></div>
<p>This model takes more than a day to finish in a relatively powerful computer and, dissapointingly, it doesn’t converge, as is clear from the traceplots in Figure <a href="modeling-a-lexical-decision-task.html#fig:tracebadcont">20.11</a>. We’ll see later that even if the converging model finishes faster, it still takes a considerable amount of time. If one has a powerful computer (with for example multiple cores) available, it is possible to parallelize the sampling further than what we did so far. This is possible with special functions that allow for <em>multithreading</em>, which is discussed in the Stan’s user guide <span class="citation">(Stan Development Team <a href="#ref-Stan2021">2021</a>, ch. 25)</span>.</p>

<div class="sourceCode" id="cb1228"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><a class="sourceLine" id="cb1228-1" data-line-number="1"></a>
<a class="sourceLine" id="cb1228-2" data-line-number="2"><span class="kw">traceplot</span>(fit_blp_h_cont, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>,</a>
<a class="sourceLine" id="cb1228-3" data-line-number="3">                          <span class="st">&quot;beta&quot;</span>,</a>
<a class="sourceLine" id="cb1228-4" data-line-number="4">                          <span class="st">&quot;T_nd&quot;</span>,</a>
<a class="sourceLine" id="cb1228-5" data-line-number="5">                          <span class="st">&quot;theta_c&quot;</span>,</a>
<a class="sourceLine" id="cb1228-6" data-line-number="6">                          <span class="st">&quot;sigma&quot;</span>,</a>
<a class="sourceLine" id="cb1228-7" data-line-number="7">                          <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<div class="figure"><span id="fig:tracebadcont"></span>
<img src="bookdown_files/figure-html/tracebadcont-1.svg" alt="The traceplots of the fit_blp_h shows that the chains are clearly not mixing well for the real data set." width="672" />
<p class="caption">
FIGURE 20.11: The traceplots of the <code>fit_blp_h</code> shows that the chains are clearly not mixing well for the real data set.
</p>
</div>
<p>The traceplots in Figure <a href="modeling-a-lexical-decision-task.html#fig:tracebadcont">20.11</a> shows that the chains get stuck and don’t mix well. It seems that there is not enough information to constraint the model. If we look at the parameter <code>theta_c</code>, we see that its chains are getting stuck at very unlikely values that are over <span class="math inline">\(0.25\)</span>. Although in general is not recommended to cut off values from a prior just because they’re unlikely, in this case restricting the parameter <code>theta_c</code> to be smaller than <span class="math inline">\(0.1\)</span> helps solving the convergence problems. To truncate the prior for <span class="math inline">\(\theta_c\)</span> in Stan we declare the parameter to have an upper bound of <span class="math inline">\(0.1\)</span>.</p>
<pre><code>real&lt;lower = 0, upper = 0.1&gt; theta_c;</code></pre>
<p>Change its prior distribution in the <code>model</code> block to the following.</p>
<pre><code>target += beta_lpdf(theta_c | 0.9, 70) -
  beta_lcdf(0.1 | 0.9, 70);</code></pre>
<p>If we would fit this new model, we would see that some chains of <code>T_nd</code> mix in values around 300 ms and some other chains (sometimes) get stuck in values very close to zero. This indicates that the model needs more information regarding the non-decision time. Another issue that slows down convergence is that this parameter <code>T_nd</code> is in a different scale (with a value above 100) than the rest of the parameters (with values below 10). Rather than sampling from <code>T_nd</code>, we sample from <code>lT_nd</code> in a new model, so that <code>T_nd = exp(lTnd)</code>. We assign the following prior to <code>lT_nd</code>.</p>
<p><span class="math display">\[\begin{equation}
lT_{nd} \sim \mathit{Normal}(log(200), 0.3)
\end{equation}\]</span></p>
<p>This is mathematically equivalent to assigning the following prior to <span class="math inline">\(T_{nd}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
T_{nd} \sim \mathit{LogNormal}(log(200), 0.3)
\end{equation}\]</span></p>
<p>The new version of the model is omitted here, but can be found as <code>lnrace_h_contb.stan</code> in the <code>bcogsci</code> package. Fit the new modified model to the same data.</p>
<div class="sourceCode" id="cb1231"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1231-1" data-line-number="1">lnrace_h_contb &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb1231-2" data-line-number="2">                      <span class="st">&quot;lnrace_h_contb.stan&quot;</span>,</a>
<a class="sourceLine" id="cb1231-3" data-line-number="3">                      <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</a>
<a class="sourceLine" id="cb1231-4" data-line-number="4">fit_blp_h_contb &lt;-<span class="st"> </span><span class="kw">stan</span>(lnrace_h_cont, <span class="dt">data =</span> ls_blp_h)</a></code></pre></div>
<p>This time the model took considerably less time, but it still took nine hours. However, the model did converge and the posterior distribution does make sense now.</p>
<p>Print the summary of the main parameters.</p>
<div class="sourceCode" id="cb1232"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1232-1" data-line-number="1"><span class="kw">print</span>(fit_blp_h_contb, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>,</a>
<a class="sourceLine" id="cb1232-2" data-line-number="2">                          <span class="st">&quot;beta&quot;</span>,</a>
<a class="sourceLine" id="cb1232-3" data-line-number="3">                          <span class="st">&quot;T_nd&quot;</span>,</a>
<a class="sourceLine" id="cb1232-4" data-line-number="4">                          <span class="st">&quot;sigma&quot;</span>,</a>
<a class="sourceLine" id="cb1232-5" data-line-number="5">                          <span class="st">&quot;theta_c&quot;</span>,</a>
<a class="sourceLine" id="cb1232-6" data-line-number="6">                          <span class="st">&quot;tau_u&quot;</span>))</a></code></pre></div>
<pre><code>##            mean   2.5%  97.5% n_eff Rhat
## alpha[1]   6.36   6.23   6.48   565 1.01
## alpha[2]   6.02   5.91   6.14  1133 1.00
## beta[1]    0.60   0.52   0.68  2339 1.00
## beta[2]    0.13   0.11   0.14  1969 1.00
## beta[3]   -0.23  -0.30  -0.16  3036 1.00
## beta[4]   -0.12  -0.14  -0.11  3454 1.00
## T_nd     315.66 312.48 318.63  5504 1.00
## sigma      0.62   0.61   0.62  5065 1.00
## theta_c    0.01   0.00   0.01  6811 1.00
## tau_u[1]   0.29   0.23   0.37  1718 1.00
## tau_u[2]   0.15   0.10   0.21  2321 1.00
## tau_u[3]   0.02   0.02   0.04  1879 1.00
## tau_u[4]   0.26   0.20   0.33  2173 1.00
## tau_u[5]   0.12   0.07   0.19  3071 1.00
## tau_u[6]   0.02   0.00   0.04  1255 1.00</code></pre>
<p>Print the summary of the correlations</p>
<div class="sourceCode" id="cb1234"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb1234-1" data-line-number="1"><span class="kw">print</span>(fit_blp_h_contb, <span class="dt">pars =</span> rho_us)</a></code></pre></div>
<pre><code>##             mean  2.5% 97.5% n_eff Rhat
## rho_u[1,2] -0.06 -0.42  0.29  2798    1
## rho_u[1,3]  0.21 -0.15  0.53  2295    1
## rho_u[1,4]  0.61  0.33  0.80  2288    1
## rho_u[1,5]  0.00 -0.38  0.37  3742    1
## rho_u[1,6] -0.07 -0.54  0.42  5335    1
## rho_u[2,3] -0.39 -0.73  0.09  1935    1
## rho_u[2,4]  0.06 -0.30  0.43  2046    1
## rho_u[2,5] -0.51 -0.84 -0.06  2565    1
## rho_u[2,6] -0.34 -0.79  0.28  3303    1
## rho_u[3,4]  0.04 -0.33  0.40  2213    1
## rho_u[3,5]  0.35 -0.12  0.73  3550    1
## rho_u[3,6]  0.00 -0.53  0.53  3869    1
## rho_u[4,5] -0.33 -0.67  0.06  3976    1
## rho_u[4,6] -0.06 -0.53  0.44  4631    1
## rho_u[5,6]  0.04 -0.52  0.62  3615    1</code></pre>
<p>What can we say about the fit of the model now?</p>
<p><em>Under the assumptions that we have made</em>, we can look at the parameters and conclude the following:</p>
<ul>
<li>All other things being equal, there was an overall bias to respond <code>non-word</code> rather than <code>word</code>. We can deduce this because the parameters <span class="math inline">\(\alpha\)</span> represent the boundary separation of each accumulator minus their rate of accumulation (see Equation <a href="modeling-a-lexical-decision-task.html#eq:alphaprime">(20.3)</a>). A smaller <code>alpha</code> indicates a closer boundary of evidence and/or a faster rate for a given accumulator. In this case <code>alpha[2]</code> is smaller than <code>alpha[1]</code>. However, the fact that <code>tau_u[1]</code> is relatively large suggests large individual differences.</li>
<li>The task was well-understood given that, when a <code>word</code> appears, the rate of accumulation of the word accumulator increased (<code>beta[1]</code> <span class="math inline">\(&gt; 0\)</span>), and the rate of the non-word accumulator decreased (<code>beta[3]</code> <span class="math inline">\(&lt;0\)</span>).</li>
<li>As expected, and replicating previous findings in the literature, words with higher frequency were easier to identify correctly as words compared to with lower frequency words (<code>beta[2]</code> <span class="math inline">\(&gt;0\)</span> and <code>beta[4]</code> <span class="math inline">\(&lt;0\)</span>).</li>
<li>The non-decision time (<code>T_nd</code>) was relatively large, considering that the normal reading of words in a sentence takes around 200-400 ms.</li>
<li>The proportion of contaminant responses was quite small (<span class="math inline">\(1\%\)</span>), but without taking them into account, the non-decision time could not be estimated.</li>
<li>Subjects that were faster to answer in the <code>word</code> trials tended to be also faster to answer in the <code>non-word</code> trials. We can deduce this given the high correlation between by-subject adjustments to the parameters <code>alpha</code> (<code>rho_u[1,4]</code> <span class="math inline">\(&gt;&gt; 0\)</span>).</li>
</ul>
<p>Our assumptions include both the likelihood we have chosen and the priors. It’s clear from the difficulties fitting the data that the model is very sensitive to the prior choices. Since there seem to be not enough information in the data, we need to provide information through the prior distributions.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-audley1965some">
<p>Audley, RJ, and AR Pike. 1965. “Some Alternative Stochastic Models of Choice 1.” <em>British Journal of Mathematical and Statistical Psychology</em> 18 (2). Wiley Online Library: 207–25.</p>
</div>
<div id="ref-BrysbaertEtAl2018">
<p>Brysbaert, Marc, Paweł Mandera, and Emmanuel Keuleers. 2018. “The Word Frequency Effect in Word Processing: An Updated Review.” <em>Current Directions in Psychological Science</em> 27 (1): 45–50. <a href="https://doi.org/10.1177/0963721417727521" class="uri">https://doi.org/10.1177/0963721417727521</a>.</p>
</div>
<div id="ref-clark1994identification">
<p>Clark, Vincent P, Silu Fan, and Steven A Hillyard. 1994. “Identification of Early Visual Evoked Potential Generators by Retinotopic and Topographic Analyses.” <em>Human Brain Mapping</em> 2 (3). Wiley Online Library: 170–87.</p>
</div>
<div id="ref-dufau2012say">
<p>Dufau, Stéphane, Jonathan Grainger, and Johannes C Ziegler. 2012. “How to Say ‘No’ to a Nonword: A Leaky Competing Accumulator Model of Lexical Decision.” <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em> 38 (4). American Psychological Association: 1117.</p>
</div>
<div id="ref-HeathcoteLove2012">
<p>Heathcote, Andrew, and Jonathon Love. 2012. “Linear Deterministic Accumulator Models of Simple Choice.” <em>Frontiers in Psychology</em> 3: 292. <a href="https://doi.org/10.3389/fpsyg.2012.00292" class="uri">https://doi.org/10.3389/fpsyg.2012.00292</a>.</p>
</div>
<div id="ref-keuleers2012british">
<p>Keuleers, Emmanuel, Paula Lacey, Kathleen Rastle, and Marc Brysbaert. 2012. “The British Lexicon Project: Lexical Decision Data for 28,730 Monosyllabic and Disyllabic English Words.” <em>Behavior Research Methods</em> 44 (1). Springer: 287–304.</p>
</div>
<div id="ref-Nelson1981">
<p>Nelson, Peter R. 1981. “The Algebra of Random Variables.” <em>Technometrics</em> 23 (2). Taylor &amp; Francis: 197–98. <a href="https://doi.org/10.1080/00401706.1981.10486266" class="uri">https://doi.org/10.1080/00401706.1981.10486266</a>.</p>
</div>
<div id="ref-nicenboimModelsRetrievalSentence2018">
<p>Nicenboim, Bruno, and Shravan Vasishth. 2018. “Models of Retrieval in Sentence Comprehension: A Computational Evaluation Using Bayesian Hierarchical Modeling.” <em>Journal of Memory and Language</em> 99: 1–34. <a href="https://doi.org/10.1016/j.jml.2017.08.004" class="uri">https://doi.org/10.1016/j.jml.2017.08.004</a>.</p>
</div>
<div id="ref-Ollman1966">
<p>Ollman, Robert. 1966. “Fast Guesses in Choice Reaction Time.” <em>Psychonomic Science</em> 6 (4). Springer: 155–56.</p>
</div>
<div id="ref-ratcliff2002estimating">
<p>Ratcliff, Roger, and Francis Tuerlinckx. 2002. “Estimating Parameters of the Diffusion Model: Approaches to Dealing with Contaminant Reaction Times and Parameter Variability.” <em>Psychonomic Bulletin &amp; Review</em> 9 (3). Springer: 438–81.</p>
</div>
<div id="ref-Rouder2005">
<p>Rouder, Jeffrey N. 2005. “Are Unshifted Distributional Models Appropriate for Response Time?” <em>Psychometrika</em> 70 (2). Springer Science + Business Media: 377–81. <a href="https://doi.org/10.1007/s11336-005-1297-7" class="uri">https://doi.org/10.1007/s11336-005-1297-7</a>.</p>
</div>
<div id="ref-RouderEtAl2015">
<p>Rouder, Jeffrey N., Jordan M. Province, Richard D. Morey, Pablo Gomez, and Andrew Heathcote. 2015. “The Lognormal Race: A Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties.” <em>Psychometrika</em> 80 (2): 491–513. <a href="https://doi.org/10.1007/s11336-013-9396-3" class="uri">https://doi.org/10.1007/s11336-013-9396-3</a>.</p>
</div>
<div id="ref-Stan2021">
<p>Stan Development Team. 2021. “Stan Modeling Language Users Guide and Reference Manual, Version 2.27.” <a href="https://mc-stan.org" class="uri">https://mc-stan.org</a>.</p>
</div>
<div id="ref-ulrichInformationProcessingModels1993">
<p>Ulrich, Rolf, and Jeff Miller. 1993. “Information Processing Models Generating Lognormally Distributed Reaction Times.” <em>Journal of Mathematical Psychology</em> 37 (4): 513–25. <a href="https://doi.org/10.1006/jmps.1993.1032" class="uri">https://doi.org/10.1006/jmps.1993.1032</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="52">
<li id="fn52"><p>One could estimate the censored times by fitting log-normal distributions that are truncated at <span class="math inline">\(T_{n}\)</span>, since this is the minimum possible time for each censored observation:
<span class="math display">\[\begin{equation}
T_{censored,n} \sim
\begin{cases}
\mathit{LogNormal}(\mu&#39;_{nw,n}, \sigma) \text{ with } T_{censored,n} &gt; T_n,  \text{, if } \mathit{choice}= \text{word}\\
\mathit{LogNormal}(\mu&#39;_{w,n}, \sigma) \text{ with } T_{censored,n} &gt; T_n, \text{, otherwise }
\end{cases}
\end{equation}\]</span><a href="modeling-a-lexical-decision-task.html#fnref52" class="footnote-back">↩</a></p></li>
<li id="fn53"><p>This for-loop can also be implemented in the <code>transformed parameter</code> block; the advantage is that the log-likelihood of each observation can be used for cross validation for example; the disadvantage is that the R object might be very large, because it will store the log-likelihood during the warm-up period as well.<a href="modeling-a-lexical-decision-task.html#fnref53" class="footnote-back">↩</a></p></li>
<li id="fn54"><p>There are 15 correlations since there are 15 ways to choose 2 variables out of 6, where order doesn’t matter. This is calculated with <span class="math inline">\({6 \choose 2}\)</span> which is <code>choose(6, 2)</code> in R.<a href="modeling-a-lexical-decision-task.html#fnref54" class="footnote-back">↩</a></p></li>
<li id="fn55"><p>This is, of course, just an assumption that could be verified. But we’ll see that the model is already quite complex and achieving convergence is not trivial.<a href="modeling-a-lexical-decision-task.html#fnref55" class="footnote-back">↩</a></p></li>
<li id="fn56"><p>We calculate the average as follows: <span class="math inline">\(.9/(.9+70)\)</span>. The 95% quantile can be calculated in R with <code>qbeta(c(.025, .975), .9, 70)</code>.<a href="modeling-a-lexical-decision-task.html#fnref56" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-lognormalrace.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="posterior-predictive-check-with-the-quantile-probability-plots.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
