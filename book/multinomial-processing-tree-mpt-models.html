<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>14.2 Multinomial processing tree (MPT) models | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.20.2 and GitBook 2.6.7" />

  <meta property="og:title" content="14.2 Multinomial processing tree (MPT) models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="14.2 Multinomial processing tree (MPT) models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2020-09-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modeling-multiple-categorical-responses.html"/>
<link rel="next" href="further-reading-11.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script type="text/javascript">
 $(document).ready(function() {
     $folds = $(".solution");
     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
     $folds.prepend("<button class=\"solution-btn\">Show solution</button>");  // add a button
     $(".solution-blck").toggle();  // fold all blocks
     $(".solution-btn").on("click", function() {  // add onClick event
         $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution" 
         $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
     })
 });
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i><b>0.2</b> Developing the right mindset for this book</a></li>
<li class="chapter" data-level="0.3" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.3</b> How to read this book</a></li>
<li class="chapter" data-level="0.4" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.4</b> Online materials</a></li>
<li class="chapter" data-level="0.5" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.5</b> Software needed</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="the-law-of-total-probability.html"><a href="the-law-of-total-probability.html"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-2-continuous-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#generate-simulated-bivariate-multivariate-data"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="sec-marginal.html"><a href="sec-marginal.html"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.9</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.10" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.11</b> Exercises</a><ul>
<li class="chapter" data-level="1.11.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.11.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.11.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.11.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.11.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.11.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.11.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.11.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.11.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.11.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#sec:choosepriortheta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#exercise-deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Exercise: Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Exercise: Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Exercise: Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Exercise: Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Exercise: Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Regression models</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="sec-sampling.html"><a href="sec-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec-sampling.html"><a href="sec-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using ‘Stan’: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3.9</b> Appendix</a><ul>
<li class="chapter" data-level="3.9.1" data-path="appendix.html"><a href="appendix.html#app:pp"><i class="fa fa-check"></i><b>3.9.1</b> Generating prior predictive distributions with <code>brms</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect reaction times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-the-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#sec:comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="appendix-1.html"><a href="appendix-1.html"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="appendix-1.html"><a href="appendix-1.html#sec:preprocessingpupil"><i class="fa fa-check"></i><b>4.7.1</b> Preparation of the pupil size data (section @ref(sec:pupil))</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal models–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.2.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>5.3</b> Summary</a><ul>
<li class="chapter" data-level="5.3.1" data-path="summary-2.html"><a href="summary-2.html#why-should-we-take-the-trouble-of-fitting-a-bayesian-hierarchical-model"><i class="fa fa-check"></i><b>5.3.1</b> Why should we take the trouble of fitting a Bayesian hierarchical model?</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.4</b> Further reading</a></li>
<li class="chapter" data-level="5.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a><ul>
<li class="chapter" data-level="6.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="6.1.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining hypotheses</a></li>
<li class="chapter" data-level="6.1.3" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#cell-means-parameterization-and-posterior-comparisons"><i class="fa fa-check"></i><b>6.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="6.2.1" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><i class="fa fa-check"></i><b>6.3</b> Further examples of contrasts illustrated with a factor with four levels</a><ul>
<li class="chapter" data-level="6.3.1" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.2</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="6.3.3" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.3</b> Polynomial contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="6.4.1" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html"><i class="fa fa-check"></i><b>6.5</b> Examples of contrast coding in a factorial design with two factors</a><ul>
<li class="chapter" data-level="6.5.1" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html#the-difference-between-an-anova-and-a-multiple-regression"><i class="fa fa-check"></i><b>6.5.1</b> The difference between an ANOVA and a multiple regression</a></li>
<li class="chapter" data-level="6.5.2" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html#nestedEffects"><i class="fa fa-check"></i><b>6.5.2</b> Nested effects</a></li>
<li class="chapter" data-level="6.5.3" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>6.5.3</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>7</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="7.1" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>7.1</b> Meta-analysis</a></li>
<li class="chapter" data-level="7.2" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>7.2</b> Measurement-error models</a></li>
<li class="chapter" data-level="7.3" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>7.3</b> Further reading</a></li>
<li class="chapter" data-level="7.4" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>7.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Model comparison</b></span></li>
<li class="chapter" data-level="8" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>8</b> Introduction to model comparison</a></li>
<li class="chapter" data-level="9" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>9</b> Bayes factors</a><ul>
<li class="chapter" data-level="9.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html"><i class="fa fa-check"></i><b>9.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="9.1.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#marginal-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#bayes-factor"><i class="fa fa-check"></i><b>9.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sec-N400BF.html"><a href="sec-N400BF.html"><i class="fa fa-check"></i><b>9.2</b> Testing the N400 effect using null hypothesis testing</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-N400BF.html"><a href="sec-N400BF.html#sec:BFnonnested"><i class="fa fa-check"></i><b>9.2.1</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="understanding-the-in-stability-of-bayes-factors.html"><a href="understanding-the-in-stability-of-bayes-factors.html"><i class="fa fa-check"></i><b>9.3</b> Understanding the (in-)stability of Bayes factors</a><ul>
<li class="chapter" data-level="9.3.1" data-path="understanding-the-in-stability-of-bayes-factors.html"><a href="understanding-the-in-stability-of-bayes-factors.html#instability-due-to-the-number-of-iterations-of-the-posterior-sampler"><i class="fa fa-check"></i><b>9.3.1</b> Instability due to the number of iterations of the posterior sampler</a></li>
<li class="chapter" data-level="9.3.2" data-path="understanding-the-in-stability-of-bayes-factors.html"><a href="understanding-the-in-stability-of-bayes-factors.html#instability-due-to-posterior-uncertainty-and-noise-associated-with-subjects-items-and-residual-variability"><i class="fa fa-check"></i><b>9.3.2</b> Instability due to posterior uncertainty and noise associated with subjects, items, and residual variability</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>10</b> Cross validation</a><ul>
<li class="chapter" data-level="10.1" data-path="expected-log-predictive-density-of-a-model.html"><a href="expected-log-predictive-density-of-a-model.html"><i class="fa fa-check"></i><b>10.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="10.2" data-path="k-fold-and-leave-one-out-cross-validation.html"><a href="k-fold-and-leave-one-out-cross-validation.html"><i class="fa fa-check"></i><b>10.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="10.3" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html"><i class="fa fa-check"></i><b>10.3</b> Testing the N400 effect using cross-validation</a></li>
<li class="chapter" data-level="10.4" data-path="summary-5.html"><a href="summary-5.html"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="further-reading-7.html"><a href="further-reading-7.html"><i class="fa fa-check"></i><b>10.5</b> Further reading</a></li>
<li class="chapter" data-level="10.6" data-path="exercises-6.html"><a href="exercises-6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Advanced models with Stan</b></span></li>
<li class="chapter" data-level="11" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>11</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="11.1" data-path="stan-syntax.html"><a href="stan-syntax.html"><i class="fa fa-check"></i><b>11.1</b> Stan syntax</a></li>
<li class="chapter" data-level="11.2" data-path="sec-firststan.html"><a href="sec-firststan.html"><i class="fa fa-check"></i><b>11.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="11.3" data-path="sec-clozestan.html"><a href="sec-clozestan.html"><i class="fa fa-check"></i><b>11.3</b> Another simple example: Cloze probability with Stan: Binomial likelihood</a></li>
<li class="chapter" data-level="11.4" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html"><i class="fa fa-check"></i><b>11.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="11.4.1" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:pupilstan"><i class="fa fa-check"></i><b>11.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="11.4.2" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:interstan"><i class="fa fa-check"></i><b>11.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="11.4.3" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:logisticstan"><i class="fa fa-check"></i><b>11.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html"><i class="fa fa-check"></i><b>11.5</b> Model comparison in Stan</a><ul>
<li class="chapter" data-level="11.5.1" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html#bayes-factor-in-stan"><i class="fa fa-check"></i><b>11.5.1</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="11.5.2" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>11.5.2</b> Cross validation in Stan</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="summary-6.html"><a href="summary-6.html"><i class="fa fa-check"></i><b>11.6</b> Summary</a></li>
<li class="chapter" data-level="11.7" data-path="further-reading-8.html"><a href="further-reading-8.html"><i class="fa fa-check"></i><b>11.7</b> Further reading</a></li>
<li class="chapter" data-level="11.8" data-path="exercises-7.html"><a href="exercises-7.html"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>12</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="12.1" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html"><i class="fa fa-check"></i><b>12.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="12.1.1" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>12.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="12.1.2" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:uncorrstan"><i class="fa fa-check"></i><b>12.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="12.1.3" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:corrstan"><i class="fa fa-check"></i><b>12.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="12.1.4" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:crosscorrstan"><i class="fa fa-check"></i><b>12.1.4</b> By-participant and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="summary-7.html"><a href="summary-7.html"><i class="fa fa-check"></i><b>12.2</b> Summary</a></li>
<li class="chapter" data-level="12.3" data-path="further-reading-9.html"><a href="further-reading-9.html"><i class="fa fa-check"></i><b>12.3</b> Further reading</a></li>
<li class="chapter" data-level="12.4" data-path="exercises-8.html"><a href="exercises-8.html"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Computational cognitive modeling</b></span></li>
<li class="chapter" data-level="13" data-path="introduction-to-computational-cognitive-modeling.html"><a href="introduction-to-computational-cognitive-modeling.html"><i class="fa fa-check"></i><b>13</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="13.1" data-path="further-reading-10.html"><a href="further-reading-10.html"><i class="fa fa-check"></i><b>13.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>14</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="14.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>14.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="14.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>14.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="14.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>14.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html"><i class="fa fa-check"></i><b>14.2</b> Multinomial processing tree (MPT) models</a><ul>
<li class="chapter" data-level="14.2.1" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html#mpts-for-modeling-picture-naming-abilities-in-aphasia"><i class="fa fa-check"></i><b>14.2.1</b> MPTs for modeling picture naming abilities in aphasia</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="further-reading-11.html"><a href="further-reading-11.html"><i class="fa fa-check"></i><b>14.3</b> Further reading</a></li>
<li class="chapter" data-level="14.4" data-path="exercises-9.html"><a href="exercises-9.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>15</b> Mixture models</a><ul>
<li class="chapter" data-level="15.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off.html"><i class="fa fa-check"></i><b>15.1</b> A mixture model of the speed-accuracy trade-off</a><ul>
<li class="chapter" data-level="15.1.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#a-fast-guess-model-account-of-the-global-motion-detection-task"><i class="fa fa-check"></i><b>15.1.1</b> A fast guess model account of the global motion detection task</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="summary-8.html"><a href="summary-8.html"><i class="fa fa-check"></i><b>15.2</b> Summary</a></li>
<li class="chapter" data-level="15.3" data-path="further-reading-12.html"><a href="further-reading-12.html"><i class="fa fa-check"></i><b>15.3</b> Further reading</a></li>
<li class="chapter" data-level="15.4" data-path="exercises-10.html"><a href="exercises-10.html"><i class="fa fa-check"></i><b>15.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Appendix</b></span></li>
<li class="chapter" data-level="16" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>16</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multinomial-processing-tree-mpt-models" class="section level2">
<h2><span class="header-section-number">14.2</span> Multinomial processing tree (MPT) models</h2>
<p>Multinomial processing tree (MPT) modeling is a method that estimates latent variables that have a psychological interpretation given categorical data <span class="citation">(a review is provided in Batchelder and Riefer <a href="#ref-BatchelderRiefer1999">1999</a>)</span>. In other words, an MPT model is just one way to model categorical responses following a multinomial or categorical distribution. MPT models assume that the observed response categories result from a sequences of underlying cognitive events which are represented as a tree.</p>
<div id="mpts-for-modeling-picture-naming-abilities-in-aphasia" class="section level3">
<h3><span class="header-section-number">14.2.1</span> MPTs for modeling picture naming abilities in aphasia</h3>
<p><span class="citation">Walker, Hickok, and Fridriksson (<a href="#ref-WalkerEtAl2018">2018</a>)</span> created an MPT model that specifies a set of possible internal errors that lead to the various possible response types during a picture naming trial for aphasic patients. Here we’ll explore a simplification of the original model.</p>

<div class="figure"><span id="fig:MPT-tikz"></span>
<img src="bookdown_files/figure-html/MPT-tikz-1.svg" alt="Representation of a simplification of the MPT used in Walker, Hickok, and Fridriksson (2018)." width="672" />
<p class="caption">
FIGURE 14.3: Representation of a simplification of the MPT used in <span class="citation">Walker, Hickok, and Fridriksson (<a href="#ref-WalkerEtAl2018">2018</a>)</span>.
</p>
</div>
<table>
<caption><span id="tab:MPT-params">TABLE 14.2: </span> Psychological interpretation of the parameters of the MPT model.</caption>
<thead>
<tr class="header">
<th align="left">Param.</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">a</td>
<td align="left">Probability of initiating an attempt</td>
</tr>
<tr class="even">
<td align="left">t</td>
<td align="left">Probability of selecting a target word over competitors</td>
</tr>
<tr class="odd">
<td align="left">f</td>
<td align="left">Probability of retrieving correct phonemes</td>
</tr>
<tr class="even">
<td align="left">c</td>
<td align="left">Probability of a phoneme change in the target word creating a real word</td>
</tr>
</tbody>
</table>
<div id="calculation-of-the-probabilities" class="section level4">
<h4><span class="header-section-number">14.2.1.1</span> Calculation of the probabilities</h4>
<p>By navigating through the branches of the MPT (figure <a href="multinomial-processing-tree-mpt-models.html#fig:MPT-tikz">14.3</a>), we can calculate the probabilities of the four responses (the categorical outcomes), based on the underlying parameters assumed in the MPT:</p>
<ul>
<li><span class="math inline">\(P(NR| a,t,f,c)= 1-a\)</span></li>
<li><span class="math inline">\(P(Neologism| a,t,f,c)= a \cdot (1-t) \cdot (1-f) \cdot (1-c) + a \cdot t \cdot (1-f) \cdot (1-c)\)</span></li>
<li><span class="math inline">\(P(Formal| a,t,f,c)= a \cdot (1-t) \cdot (1-f) \cdot c + a \cdot t \cdot (1-f) \cdot c\)</span></li>
<li><span class="math inline">\(P(Mixed| a,t,f,c)= a \cdot (1-t) \cdot f\)</span></li>
<li><span class="math inline">\(P(Correct| a,t,f,c)= a \cdot t \cdot f\)</span></li>
</ul>
<p>Given that <span class="math inline">\(P(NR| a,t,f,c) + P(Neologism| a,t,f,c) + P(Formal| a,t,f,c) + P(Mixed| a,t,f,c) + P(Correct| a,t,f,c) = 1\)</span>, there is no need to characterize every outcome: we can always calculate the any one of the remaining responses as <span class="math inline">\(1 - other\text{ }responses\)</span>.</p>
</div>
<div id="generate-simulated-data" class="section level4">
<h4><span class="header-section-number">14.2.1.2</span> Generate simulated data</h4>
<p>First, simulate 200 trials assuming no variability between items and participants. It is necessary to define functions to compute each outcome’s probability, based on the MPT.</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb696"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb696-1" data-line-number="1"><span class="co"># Probabilities of different answers</span></a>
<a class="sourceLine" id="cb696-2" data-line-number="2">Pr_NR &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb696-3" data-line-number="3">    <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>a</a>
<a class="sourceLine" id="cb696-4" data-line-number="4">Pr_Neologism &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb696-5" data-line-number="5">    a <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>t) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>f) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>c) <span class="op">+</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>t <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>f) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>c)</a>
<a class="sourceLine" id="cb696-6" data-line-number="6">Pr_Formal &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb696-7" data-line-number="7">    a <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>t) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>f) <span class="op">*</span><span class="st"> </span>c <span class="op">+</span><span class="st">  </span>a <span class="op">*</span><span class="st"> </span>t <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>f) <span class="op">*</span><span class="st"> </span>c</a>
<a class="sourceLine" id="cb696-8" data-line-number="8">Pr_Mixed &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb696-9" data-line-number="9">    a <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>t) <span class="op">*</span><span class="st"> </span>f</a>
<a class="sourceLine" id="cb696-10" data-line-number="10">Pr_Correct &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb696-11" data-line-number="11">    a <span class="op">*</span><span class="st"> </span>t <span class="op">*</span><span class="st"> </span>f</a>
<a class="sourceLine" id="cb696-12" data-line-number="12"><span class="co"># true underlying values for simulated data</span></a>
<a class="sourceLine" id="cb696-13" data-line-number="13">a_true &lt;-<span class="st"> </span><span class="fl">.75</span></a>
<a class="sourceLine" id="cb696-14" data-line-number="14">t_true &lt;-<span class="st"> </span><span class="fl">.9</span></a>
<a class="sourceLine" id="cb696-15" data-line-number="15">f_true &lt;-<span class="st"> </span><span class="fl">.8</span></a>
<a class="sourceLine" id="cb696-16" data-line-number="16">c_true &lt;-<span class="st"> </span><span class="fl">.1</span></a>
<a class="sourceLine" id="cb696-17" data-line-number="17"><span class="co"># Probability of the different answers:</span></a>
<a class="sourceLine" id="cb696-18" data-line-number="18">Theta &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">NR =</span> <span class="kw">Pr_NR</span>(a_true, t_true, f_true, c_true),</a>
<a class="sourceLine" id="cb696-19" data-line-number="19">                <span class="dt">Neologism =</span> <span class="kw">Pr_Neologism</span>(a_true, t_true, f_true, c_true),</a>
<a class="sourceLine" id="cb696-20" data-line-number="20">                <span class="dt">Formal =</span> <span class="kw">Pr_Formal</span>(a_true, t_true, f_true, c_true),</a>
<a class="sourceLine" id="cb696-21" data-line-number="21">                <span class="dt">Mixed =</span> <span class="kw">Pr_Mixed</span>(a_true, t_true, f_true, c_true),</a>
<a class="sourceLine" id="cb696-22" data-line-number="22">                <span class="dt">Correct =</span> <span class="kw">Pr_Correct</span>(a_true, t_true, f_true, c_true))</a>
<a class="sourceLine" id="cb696-23" data-line-number="23">N_trials &lt;-<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb696-24" data-line-number="24">(ans &lt;-<span class="st"> </span><span class="kw">rmultinom</span>(<span class="dv">1</span>, N_trials, <span class="kw">c</span>(Theta)))</a></code></pre></div>
<pre><code>##           [,1]
## NR          50
## Neologism   23
## Formal       6
## Mixed       10
## Correct    111</code></pre>
</div>
<div id="sec:MPT-s" class="section level4">
<h4><span class="header-section-number">14.2.1.3</span> A simple MPT model in Stan</h4>
<p>The above data can be modeled in Stan as discussed below (see <code>stan_models/mpt_3.stan</code>). The probabilities of the different categories go into the <code>transformed parameters</code> section. The data are modeled as coming from a multinomial likelihood. If priors are not specified, then a beta distribution with <span class="math inline">\(a=1\)</span> and <span class="math inline">\(b=1\)</span> is assumed for the parameters <span class="math inline">\(a\)</span>, <span class="math inline">\(t\)</span>, <span class="math inline">\(f\)</span>, and <span class="math inline">\(c\)</span>. Notice that unlike <span class="math inline">\(\boldsymbol{\theta}\)</span>, the values of these parameters are independent of each other and they do not sum to one. For this reason, we should not use a Dirichlet prior here.</p>
<p>We define the following model:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\theta_{nr} &amp;= 1-a \\
\theta_{neol.} &amp;= a \cdot (1-t) \cdot (1-f) \cdot (1-c) +  a \cdot t \cdot (1-f) \cdot (1-c)\\
\theta_{formal} &amp;= a \cdot (1-t) \cdot (1-f) \cdot c +  a \cdot t \cdot (1-f) \cdot c\\
\theta_{mix} &amp;= a \cdot (1-t) \cdot f\\
\theta_{corr} &amp;= a \cdot t \cdot f\\
\boldsymbol{\theta} &amp;= \{\theta_{nr}, \theta_{neol.}, \theta_{formal}, \theta_{mix}, \theta_{corr}\}\\
ans &amp;\sim Multinomial(\theta)\\
a,t,f,c &amp;\sim Beta(2, 2)
\end{aligned}
\end{equation}\]</span></p>
<p>This translates to the following code:</p>
<pre class="stan"><code>data { 
  int&lt;lower = 1&gt; N_trials;
  int&lt;lower = 0, upper = N_trials&gt; ans[5];
}
parameters {
  real&lt;lower = 0, upper = 1&gt; a;
  real&lt;lower = 0, upper = 1&gt; t;
  real&lt;lower = 0, upper = 1&gt; f;
  real&lt;lower = 0, upper = 1&gt; c;
} 
transformed parameters {
  simplex[5] theta;
  theta[1] = 1 - a; //Pr_NR
  theta[2] = a * (1 - t) * (1 - f) * (1 - c) + a * t * (1 - f) * (1 - c); //Pr_Neologism
  theta[3] = a * (1 - t) * (1 - f) * c +  a * t * (1 - f) * c;  //Pr_Formal
  theta[4] = a * (1 - t) * f; //Pr_Mixed
  theta[5] = a * t * f; //Pr_Correct
}
model {
  target += beta_lpdf(a | 2, 2);
  target += beta_lpdf(t | 2, 2);
  target += beta_lpdf(f | 2, 2);
  target += beta_lpdf(c | 2, 2);
  target += multinomial_lpmf(ans | theta);
}
generated quantities{
    int pred_ans[5];
  pred_ans = multinomial_rng(theta, 5);
}</code></pre>
<p>Fit the model:</p>
<div class="sourceCode" id="cb699"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb699-1" data-line-number="1">data_sMPT &lt;-<span class="st">  </span><span class="kw">list</span>(<span class="dt">N_trials =</span> N_trials,</a>
<a class="sourceLine" id="cb699-2" data-line-number="2">                   <span class="dt">ans =</span> <span class="kw">c</span>(ans)) </a>
<a class="sourceLine" id="cb699-3" data-line-number="3">fit_sMPT &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mpt_3.stan&#39;</span>, <span class="dt">data =</span> data_sMPT)  </a></code></pre></div>
<p>Print out a summary of the posterior of the parameter of interest:</p>
<div class="sourceCode" id="cb700"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb700-1" data-line-number="1"><span class="kw">print</span>(fit_sMPT, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;t&quot;</span>, <span class="st">&quot;f&quot;</span>, <span class="st">&quot;c&quot;</span>))</a></code></pre></div>
<pre><code>##   mean 2.5% 97.5% n_eff Rhat
## a 0.75 0.68  0.80  4701    1
## t 0.90 0.85  0.95  4596    1
## f 0.80 0.73  0.86  4516    1
## c 0.24 0.12  0.40  4367    1</code></pre>
<p>Evaluate whether the model recovers the true parameters that generated the data; see Figure <a href="multinomial-processing-tree-mpt-models.html#fig:sMPT-posterior">14.4</a>.</p>
<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb702-1" data-line-number="1"><span class="kw">as.data.frame</span>(fit_sMPT) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb702-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(<span class="kw">c</span>(<span class="st">&quot;a&quot;</span>,<span class="st">&quot;t&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;c&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb702-3" data-line-number="3"><span class="st">  </span><span class="kw">mcmc_recover_hist</span>(<span class="dt">true =</span> <span class="kw">c</span>(a_true, t_true, f_true, c_true)) <span class="op">+</span></a>
<a class="sourceLine" id="cb702-4" data-line-number="4"><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</a></code></pre></div>
<div class="figure"><span id="fig:sMPT-posterior"></span>
<img src="bookdown_files/figure-html/sMPT-posterior-1.svg" alt="Posterior distributions and true values of the parameters of the simple MPT model (mpt_3.stan)." width="672" />
<p class="caption">
FIGURE 14.4: Posterior distributions and true values of the parameters of the simple MPT model (mpt_3.stan).
</p>
</div>
<p>The posteriors of the <span class="math inline">\(\theta\)</span> parameters can also be summarized:</p>
<div class="sourceCode" id="cb703"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb703-1" data-line-number="1"><span class="kw">print</span>(fit_sMPT, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;theta&quot;</span>))</a></code></pre></div>
<pre><code>##          mean 2.5% 97.5% n_eff Rhat
## theta[1] 0.25 0.20  0.32  4701    1
## theta[2] 0.11 0.07  0.16  4613    1
## theta[3] 0.04 0.02  0.06  4171    1
## theta[4] 0.06 0.03  0.09  4426    1
## theta[5] 0.54 0.47  0.61  4685    1</code></pre>
</div>
<div id="sec:MPT-reg" class="section level4">
<h4><span class="header-section-number">14.2.1.4</span> An MPT assuming by-item variability</h4>
<p>The use of aggregated data implies the assumption that the estimated parameters do not vary too much between subjects and items. If this assumption is incorrect, the analysis of aggregated data may lead to erroneous conclusions: reliance on aggregated data in the presence of parameter heterogeneity may lead to biased parameter estimates and the underestimation of credible intervals.</p>
<p>For example, if it is known that <span class="math inline">\(f\)</span> is affected by the phonological complexity of the individual word (e.g., <em>cat</em> is easier to produce than <em>umbrella</em>), the previous model does not have a way to include that information.</p>
<p>Simulated data can be generated taking into account the <code>complexity</code> of the items. Assume here for simplicity that the complexity of items is scaled and centered; i.e., mean complexity is represented by 0, and the standard deviation is assumed to be 1. Thus, the predictor complexity can be represented as a Normal(0,1). We will assume a regression model that determines the probability to be a function of complexity, determined by some property of the item.</p>
<p>One important detail is that <code>f</code> is a probability and needs to be bounded between 0 and 1. To make sure that this property is met, the computation of <code>f</code> for each item will be converted to probability space using the logistic function. This is achieved as follows.</p>
<p>Suppose that <span class="math inline">\(f\)</span> is a linear function of complexity. For example, two parameters <span class="math inline">\(\alpha_f\)</span> and <span class="math inline">\(\beta_f\)</span> could determine how <span class="math inline">\(f\)</span> is affected by complexity:</p>
<p><span class="math inline">\(f&#39;_j=\alpha_f + complexity_j\cdot \beta_f\)</span>.</p>
<p>The parameters <span class="math inline">\(\alpha_f\)</span> and <span class="math inline">\(\beta_f\)</span> are defined in an unconstrained log-odds space (they can be any real number). The model that is fit then yields an <span class="math inline">\(f&#39;_j\)</span> value for each item <span class="math inline">\(j\)</span> in log-odds space. The log-odds value <span class="math inline">\(f&#39;_j\)</span> can be converted to a probability value <span class="math inline">\(f_{true}\)</span> by applying the logistic function (or the inverse logit, <span class="math inline">\(logit^{-1}\)</span>) to <span class="math inline">\(f&#39;\)</span>. Recall from the generalized linear model discussed earlier that if we have a model in log-odds space:</p>
<p><span class="math display">\[\begin{equation}
\log \left(\frac{p_j}{(1-p_j)}\right) = \alpha + \beta\cdot x_j = \mu_j
\end{equation}\]</span></p>
<p>Then we can recover the probability <span class="math inline">\(p_j\)</span> by solving for <span class="math inline">\(p_j\)</span>:</p>
<p><span class="math display">\[\begin{equation}
p_j = \frac{\exp(\mu_j)}{1+\exp(\mu_j)}
\end{equation}\]</span></p>
<p>The above is the logistic or inverse logit function: it takes as input <span class="math inline">\(\mu_j\)</span> and returns the corresponding probability <span class="math inline">\(p_j\)</span>. The plogis function in R carries out the calculation shown above.</p>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb705-1" data-line-number="1">N_obs &lt;-<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb705-2" data-line-number="2">complexity &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_obs,<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb705-3" data-line-number="3"><span class="co">## choose some hypothetical values:</span></a>
<a class="sourceLine" id="cb705-4" data-line-number="4">f_alpha &lt;-<span class="st"> </span><span class="fl">.3</span></a>
<a class="sourceLine" id="cb705-5" data-line-number="5">f_beta &lt;-<span class="st"> </span><span class="fl">.05</span></a>
<a class="sourceLine" id="cb705-6" data-line-number="6"><span class="co">## get probabilities f for each item:</span></a>
<a class="sourceLine" id="cb705-7" data-line-number="7">f_true &lt;-<span class="st"> </span><span class="kw">plogis</span>(f_alpha <span class="op">+</span><span class="st"> </span>complexity <span class="op">*</span><span class="st"> </span>f_beta)</a></code></pre></div>
<p>This change in our assumptions entails that the probability of each response changes with the item. The parameters <code>theta</code> now have to be a matrix (this is in R, in Stan, we’ll code it as an array of simplexes).<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a></p>
<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb706-1" data-line-number="1">theta_NR_v &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">Pr_NR</span>(a_true, t_true, f_true, c_true), N_obs)</a>
<a class="sourceLine" id="cb706-2" data-line-number="2">theta_Neologism_v &lt;-<span class="st"> </span><span class="kw">Pr_Neologism</span>(a_true, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb706-3" data-line-number="3">theta_Formal_v &lt;-<span class="st"> </span><span class="kw">Pr_Formal</span>(a_true, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb706-4" data-line-number="4">theta_Mixed_v &lt;-<span class="st"> </span><span class="kw">Pr_Mixed</span>(a_true, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb706-5" data-line-number="5">theta_Correct_v &lt;-<span class="st"> </span><span class="kw">Pr_Correct</span>(a_true, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb706-6" data-line-number="6">theta_item &lt;-<span class="st"> </span><span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb706-7" data-line-number="7">  <span class="kw">c</span>(theta_NR_v,</a>
<a class="sourceLine" id="cb706-8" data-line-number="8">    theta_Neologism_v,</a>
<a class="sourceLine" id="cb706-9" data-line-number="9">    theta_Formal_v,</a>
<a class="sourceLine" id="cb706-10" data-line-number="10">    theta_Mixed_v,</a>
<a class="sourceLine" id="cb706-11" data-line-number="11">    theta_Correct_v),</a>
<a class="sourceLine" id="cb706-12" data-line-number="12">  <span class="dt">ncol =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb706-13" data-line-number="13"><span class="kw">dim</span>(theta_item)</a></code></pre></div>
<pre><code>## [1] 50  5</code></pre>
<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb708-1" data-line-number="1"><span class="kw">head</span>(theta_item)</a></code></pre></div>
<pre><code>##      [,1]  [,2]   [,3]   [,4]  [,5]
## [1,] 0.25 0.278 0.0309 0.0441 0.397
## [2,] 0.25 0.291 0.0323 0.0427 0.384
## [3,] 0.25 0.271 0.0301 0.0449 0.404
## [4,] 0.25 0.286 0.0317 0.0433 0.389
## [5,] 0.25 0.260 0.0289 0.0461 0.415
## [6,] 0.25 0.314 0.0349 0.0401 0.361</code></pre>
<p>Generate data:</p>
<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb710-1" data-line-number="1">sim_data_cx &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">item =</span> <span class="dv">1</span><span class="op">:</span>N_obs,</a>
<a class="sourceLine" id="cb710-2" data-line-number="2">                      <span class="dt">complexity =</span> complexity,</a>
<a class="sourceLine" id="cb710-3" data-line-number="3">                      <span class="dt">w_ans =</span> <span class="kw">c</span>(extraDistr<span class="op">::</span><span class="kw">rcat</span>(N_obs,theta_item)))</a>
<a class="sourceLine" id="cb710-4" data-line-number="4"><span class="kw">head</span>(sim_data_cx)</a></code></pre></div>
<pre><code>## # A tibble: 6 x 3
##    item complexity w_ans
##   &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;
## 1     1      1.14      2
## 2     2     -0.411     2
## 3     3      1.95      5
## 4     4      0.187     5
## 5     5      3.35      3
## # … with 1 more row</code></pre>
<p>The following model (saved in <code>stan_models/mpt_4.stan</code>) is essentially doing the same as the previous model but instead of fitting a multinomial to the summary of all the trials, it is fitting a categorical distribution to each individual observation. (This is analogous to the difference between the Bernoulli and Binomial distributions). <em>It is still not the appropriate model for the generative process that we are assuming in this section</em>, but it is a good start.</p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; N_obs;
  int&lt;lower=1,upper=5&gt; w_ans[N_obs];
}
parameters {
  real&lt;lower=0,upper=1&gt; a;
  real&lt;lower=0,upper=1&gt; t;
  real&lt;lower=0,upper=1&gt; f;
  real&lt;lower=0,upper=1&gt; c;
}
transformed parameters {
  simplex[5] theta[N_obs];

  for(n in 1:N_obs){
    //Pr_NR:
    theta[n, 1] = 1 - a;
    //Pr_Neologism:
    theta[n, 2] = a * (1 - t) * (1 - f) * (1 - c) + a * t * (1 - f) * (1 - c);
    //Pr_Formal:
    theta[n, 3] = a * (1 - t) * (1 - f) * c +  a * t * (1 - f) * c;
    //Pr_Mixed:
    theta[n, 4] = a * (1 - t) * f;
    //Pr_Correct:
    theta[n, 5] = a * t * f;
  }
}
model {
  target += beta_lpdf(a | 2, 2);
  target += beta_lpdf(t | 2, 2);
  target += beta_lpdf(f | 2, 2);
  target += beta_lpdf(c | 2, 2);
  for(n in 1:N_obs)
    target += categorical_lpmf(w_ans[n] | theta[n]);
}
generated quantities{
    int pred_w_ans[N_obs];
  for(n in 1:N_obs)
    pred_w_ans[n] = categorical_rng(theta[n]);
}</code></pre>
<p>Before moving to the next section, you might want to try to edit the previous chunk of code to incorporate the fact that <code>f</code> is now a transformed parameter that depends on the trial information and two new parameters.</p>
</div>
<div id="sec:MPT-h" class="section level4">
<h4><span class="header-section-number">14.2.1.5</span> A hierarchical MPT in Stan</h4>
<p>The previous model doesn’t take into account that subjects might vary and that items might vary beyond what is accounted by complexity. Let’s focus on taking into account the differences between subjects.</p>
<p>Different subjects might be differently motivated to the task. This can be accounted by adding a hierarchical structure to the parameter <code>a</code>. Begin by simulating some data that incorporates by-subject variability.</p>
<p>First, define the number of items and subjects, and the number of observations:</p>
<div class="sourceCode" id="cb713"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb713-1" data-line-number="1"><span class="co"># Data:</span></a>
<a class="sourceLine" id="cb713-2" data-line-number="2">N_item &lt;-<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb713-3" data-line-number="3">N_subj &lt;-<span class="st"> </span><span class="dv">30</span></a>
<a class="sourceLine" id="cb713-4" data-line-number="4">N_obs &lt;-<span class="st"> </span>N_item <span class="op">*</span><span class="st"> </span>N_subj </a></code></pre></div>
<p>Then, generate a vector for subjects and for items. Assume here that each subject sees each item.</p>
<div class="sourceCode" id="cb714"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb714-1" data-line-number="1">subj &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>N_subj, <span class="dt">each =</span> N_item)</a>
<a class="sourceLine" id="cb714-2" data-line-number="2">item &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>N_item, <span class="dt">time =</span> N_subj)</a></code></pre></div>
<p>A vector representing complexity is created for the number of items we have, and this vector is repeated as many times as there are subjects:</p>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb715-1" data-line-number="1">complexity &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">rnorm</span>(N_item, <span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">times =</span> N_subj)</a></code></pre></div>
<p>Next, create a data-frame with all the above information:</p>
<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb716-1" data-line-number="1">(exp_sim &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">subj =</span> subj,</a>
<a class="sourceLine" id="cb716-2" data-line-number="2">                  <span class="dt">item =</span> item,</a>
<a class="sourceLine" id="cb716-3" data-line-number="3">                  <span class="dt">complexity =</span> complexity))</a></code></pre></div>
<pre><code>## # A tibble: 600 x 3
##    subj  item complexity
##   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;
## 1     1     1      0.369
## 2     1     2      0.372
## 3     1     3     -3.23 
## 4     1     4     -3.14 
## 5     1     5     -2.48 
## # … with 595 more rows</code></pre>
<p>To create subject-level variability in the data, a between-subject standard deviation needs to be defined. This standard deviation represents the deviations of subjects about the grand mean alpha value. It is important to note here that we are defining this adjustment in log-odds space.</p>
<div class="sourceCode" id="cb718"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb718-1" data-line-number="1"><span class="co"># New parameters, in log-odds space:</span></a>
<a class="sourceLine" id="cb718-2" data-line-number="2">tau_u &lt;-<span class="st"> </span><span class="fl">1.1</span></a>
<a class="sourceLine" id="cb718-3" data-line-number="3"><span class="co">## generate subject adjustments in log-odds space:</span></a>
<a class="sourceLine" id="cb718-4" data-line-number="4">(u &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_subj, <span class="dv">0</span>, tau_u))</a></code></pre></div>
<pre><code>##  [1]  0.7363 -0.4064  0.5335  0.7821 -0.5050 -0.4701 -1.1870  0.3149
##  [9]  0.6687  0.2621  0.0644 -0.3332  0.7395 -0.1493 -2.5440 -1.1823
## [17]  0.4025  0.9672  1.2595  0.0299  1.9688  0.5983  1.3624  1.4506
## [25]  1.1605  0.5885 -1.2440  0.4196 -1.7096  0.1992</code></pre>
<p>Given the fixed <code>a_true</code> probability value of 0.75, the subject-level values for individual <code>a_true</code> can be derived by (a) first converting the overall <code>a_true</code> value to log-odds space, (b) adding the by-subject adjustment to this converted overall value, and (c) then convert back to probability space using the logistic or inverse logit (plogis) function.</p>
<div class="sourceCode" id="cb720"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb720-1" data-line-number="1"><span class="co">## convert to log-odds space:</span></a>
<a class="sourceLine" id="cb720-2" data-line-number="2">alpha_a &lt;-<span class="st"> </span><span class="kw">qlogis</span>(a_true)</a>
<a class="sourceLine" id="cb720-3" data-line-number="3">a_true_h &lt;-<span class="st"> </span><span class="kw">plogis</span>(alpha_a <span class="op">+</span><span class="st"> </span>u[subj])</a></code></pre></div>
<p>What this achieves mathematically is adding varying intercepts by subjects to <code>alpha_a</code>, and then the values adjusted by subject are saved in probability space.</p>
<p>As before, f_true is computed as a function of complexity:</p>
<div class="sourceCode" id="cb721"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb721-1" data-line-number="1">f_true &lt;-<span class="st"> </span><span class="kw">plogis</span>(f_alpha <span class="op">+</span><span class="st"> </span>complexity <span class="op">*</span><span class="st"> </span>f_beta)</a></code></pre></div>
<p>Now, we can define the probabilities of different outcomes:</p>
<div class="sourceCode" id="cb722"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb722-1" data-line-number="1"><span class="co"># Aux. parameters that define the probabilities:</span></a>
<a class="sourceLine" id="cb722-2" data-line-number="2">theta_NR_v_h &lt;-<span class="st"> </span><span class="kw">Pr_NR</span>(a_true_h, t_true, f_true, c_true) </a>
<a class="sourceLine" id="cb722-3" data-line-number="3">theta_Neologism_v_h &lt;-<span class="st"> </span><span class="kw">Pr_Neologism</span>(a_true_h, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb722-4" data-line-number="4">theta_Formal_v_h &lt;-<span class="st"> </span><span class="kw">Pr_Formal</span>(a_true_h, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb722-5" data-line-number="5">theta_Mixed_v_h &lt;-<span class="st"> </span><span class="kw">Pr_Mixed</span>(a_true_h, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb722-6" data-line-number="6">theta_Correct_v_h &lt;-<span class="st"> </span><span class="kw">Pr_Correct</span>(a_true_h, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb722-7" data-line-number="7">theta_h &lt;-<span class="st"> </span><span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb722-8" data-line-number="8">  <span class="kw">c</span>(theta_NR_v_h,</a>
<a class="sourceLine" id="cb722-9" data-line-number="9">    theta_Neologism_v_h,</a>
<a class="sourceLine" id="cb722-10" data-line-number="10">    theta_Formal_v_h,</a>
<a class="sourceLine" id="cb722-11" data-line-number="11">    theta_Mixed_v_h,</a>
<a class="sourceLine" id="cb722-12" data-line-number="12">    theta_Correct_v_h),</a>
<a class="sourceLine" id="cb722-13" data-line-number="13">  <span class="dt">ncol =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb722-14" data-line-number="14"><span class="kw">dim</span>(theta_h)</a></code></pre></div>
<pre><code>## [1] 600   5</code></pre>
<p>The probability specifications shown above can now generate the simulated data:</p>
<div class="sourceCode" id="cb724"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb724-1" data-line-number="1"><span class="co"># simulated data:</span></a>
<a class="sourceLine" id="cb724-2" data-line-number="2">(sim_data_h &lt;-<span class="st"> </span><span class="kw">mutate</span>(exp_sim,</a>
<a class="sourceLine" id="cb724-3" data-line-number="3">                      <span class="dt">w_ans =</span> extraDistr<span class="op">::</span><span class="kw">rcat</span>(N_obs,theta_h)))</a></code></pre></div>
<pre><code>## # A tibble: 600 x 4
##    subj  item complexity w_ans
##   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;
## 1     1     1      0.369     4
## 2     1     2      0.372     1
## 3     1     3     -3.23      5
## 4     1     4     -3.14      5
## 5     1     5     -2.48      2
## # … with 595 more rows</code></pre>
<p>We define now the following model. We start by defining relatively weak priors for all the parameters in the following model.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
tau_u &amp;\sim Normal(0, 1)\\
u &amp;\sim Normal(0, \tau_u)\\
a_n &amp;= logit^{-1}(a_\alpha + u[subj[n]])\\
f_n &amp;= logit^{-1}(f_\alpha + complexity_n \cdot f_\beta)\\
a_\alpha, f_\alpha, f_\beta &amp;\sim Normal(0, 2)\\
\theta_{n,nr} &amp;= 1 - a_n \\
\theta_{n,neol.} &amp;= a_n \cdot (1-t) \cdot (1-f_n) \cdot (1-c) +  a_n \cdot t \cdot (1-f_n) \cdot (1-c)\\
\theta_{n,formal} &amp;= a_n \cdot (1-t) \cdot (1-f_n) \cdot c +  a_n \cdot t \cdot (1-f_n) \cdot c\\
\theta_{n,mix} &amp;= a_n \cdot (1-t) \cdot f_n\\
\theta_{n,corr} &amp;= a_n \cdot t \cdot f_n\\
\theta_n &amp;= \{\theta_{n, nr}, \theta_{n, neol.}, \theta_{n, formal}, \theta_{n, mix}, \theta_{n, corr}\}\\
ans_n &amp;\sim Categorical(\theta_n)\\
t,c &amp;\sim Beta(2, 2)
\end{aligned}
\end{equation}\]</span></p>
<p>The appropriate model <code>mpt_5.stan</code> will look like this:</p>
<pre class="stan"><code>data {
  int&lt;lower = 1&gt; N_obs;
  int&lt;lower = 1,upper = 5&gt; w_ans[N_obs];
  real complexity[N_obs];
  int&lt;lower = 1&gt; N_subj;
  int&lt;lower = 1, upper = N_subj&gt; subj[N_obs];
}
parameters {
  real&lt;lower = 0, upper = 1&gt; t;
  real&lt;lower = 0, upper = 1&gt; c;
  real a_alpha;
  real&lt;lower = 0&gt; tau_u;
  vector[N_subj] u;
  real f_alpha;
  real f_beta;
}
transformed parameters {
  simplex[5] theta[N_obs];
  for (n in 1:N_obs){
    real a = inv_logit(a_alpha + u[subj[n]]);
    real f = inv_logit(f_alpha + complexity[n] * f_beta);
    theta[n, 1] = 1 - a; //Pr_NR
    theta[n, 2] = a * (1 - t) * (1 - f) * (1 - c) + a * t * (1 - f) * (1 - c); //Pr_Neologism
    theta[n, 3] = a * (1 - t) * (1 - f) * c +  a * t * (1 - f) * c;  //Pr_Formal
    theta[n, 4] = a * (1 - t) * f; //Pr_Mixed
    theta[n, 5] = a * t * f; //Pr_Correct
  }
}
model {
  target += beta_lpdf(t | 2, 2);
  target += beta_lpdf(c | 2, 2);
  target += normal_lpdf(a_alpha | 0, 2);
  target += normal_lpdf(f_alpha | 0, 2);
  target += normal_lpdf(f_beta | 0, 2);
  target += normal_lpdf(u | 0, tau_u);
  target += normal_lpdf(tau_u | 0, 1);
  for(n in 1:N_obs)
    target +=  categorical_lpmf(w_ans[n] | theta[n]);
}
generated quantities{
  int&lt;lower = 1, upper = 5&gt; pred_w_ans[N_obs];
  for(n in 1:N_obs)
    pred_w_ans[n] = categorical_rng(theta[n]);
}</code></pre>
<p>It would be a good idea to plot prior predictive distributions for this model; see exercise @ref().
Next, fit the model to the simulated data. The data are defined as a list:</p>
<div class="sourceCode" id="cb727"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb727-1" data-line-number="1">sim_list_h &lt;-<span class="st">  </span><span class="kw">list</span>(<span class="dt">N_obs =</span> <span class="kw">nrow</span>(sim_data_h),</a>
<a class="sourceLine" id="cb727-2" data-line-number="2">                    <span class="dt">w_ans =</span> sim_data_h<span class="op">$</span>w_ans,</a>
<a class="sourceLine" id="cb727-3" data-line-number="3">                    <span class="dt">N_subj =</span> <span class="kw">max</span>(sim_data_h<span class="op">$</span>subj),</a>
<a class="sourceLine" id="cb727-4" data-line-number="4">                    <span class="dt">subj =</span> sim_data_h<span class="op">$</span>subj,</a>
<a class="sourceLine" id="cb727-5" data-line-number="5">                    <span class="dt">complexity =</span> sim_data_h<span class="op">$</span>complexity)</a></code></pre></div>
<div class="sourceCode" id="cb728"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb728-1" data-line-number="1">fit_mpt_h &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mpt_5.stan&#39;</span>, </a>
<a class="sourceLine" id="cb728-2" data-line-number="2">              <span class="dt">data =</span> sim_list_h, </a>
<a class="sourceLine" id="cb728-3" data-line-number="3">              <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">.9</span>))  </a></code></pre></div>
<p>Print out a summary of the posterior; also see figure <a href="multinomial-processing-tree-mpt-models.html#fig:mpt-h">14.5</a></p>
<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb729-1" data-line-number="1"><span class="kw">print</span>(fit_mpt_h, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;t&quot;</span>, <span class="st">&quot;c&quot;</span>, <span class="st">&quot;tau_u&quot;</span>, <span class="st">&quot;a_alpha&quot;</span>, <span class="st">&quot;f_alpha&quot;</span>, <span class="st">&quot;f_beta&quot;</span>))</a></code></pre></div>
<pre><code>##         mean 2.5% 97.5% n_eff Rhat
## t       0.90 0.85  0.93  8532    1
## c       0.11 0.07  0.16  8373    1
## tau_u   1.08 0.74  1.52  2820    1
## a_alpha 0.95 0.53  1.40  1789    1
## f_alpha 0.20 0.01  0.40  8712    1
## f_beta  0.14 0.06  0.23  7245    1</code></pre>
<div class="sourceCode" id="cb731"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb731-1" data-line-number="1"><span class="kw">as.data.frame</span>(fit_mpt_h) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb731-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(<span class="kw">c</span>(<span class="st">&quot;tau_u&quot;</span>,<span class="st">&quot;a_alpha&quot;</span>, <span class="st">&quot;t&quot;</span>,<span class="st">&quot;f_alpha&quot;</span>,<span class="st">&quot;f_beta&quot;</span>,<span class="st">&quot;c&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb731-3" data-line-number="3"><span class="st">  </span><span class="kw">mcmc_recover_hist</span>(<span class="dt">true =</span> <span class="kw">c</span>(tau_u, <span class="kw">qlogis</span>(a_true),  t_true, f_alpha, f_beta, c_true)) </a></code></pre></div>
<div class="figure"><span id="fig:mpt-h"></span>
<img src="bookdown_files/figure-html/mpt-h-1.svg" alt="Posterior of the hierarchical MPT with true values as vertical lines (model `mpt_5.stan`)." width="672" />
<p class="caption">
FIGURE 14.5: Posterior of the hierarchical MPT with true values as vertical lines (model <code>mpt_5.stan</code>).
</p>
</div>
<p>If everything is correctly defined in the model, we should be able to generate posterior predictive data based on our estimates that looks quite similar to the simulated data; see figure <a href="multinomial-processing-tree-mpt-models.html#fig:aggMPT-h">14.6</a>.</p>
<div class="sourceCode" id="cb732"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb732-1" data-line-number="1">gen_data &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">extract</span>(fit_mpt_h)<span class="op">$</span>pred_w_ans</a>
<a class="sourceLine" id="cb732-2" data-line-number="2"><span class="kw">ppc_bars</span>(sim_list_h<span class="op">$</span>w_ans, gen_data) <span class="op">+</span></a>
<a class="sourceLine" id="cb732-3" data-line-number="3"><span class="st">  </span><span class="kw">ggtitle</span> (<span class="st">&quot;Hierarchical model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:aggMPT-h"></span>
<img src="bookdown_files/figure-html/aggMPT-h-1.svg" alt="Posterior predictive check for aggregated data in the hierarchical MPT model" width="672" />
<p class="caption">
FIGURE 14.6: Posterior predictive check for aggregated data in the hierarchical MPT model
</p>
</div>
<p>It is also useful to look at the individual subjects’ posteriors in figure <a href="multinomial-processing-tree-mpt-models.html#fig:pMPT-h">14.7</a>.</p>
<div class="sourceCode" id="cb733"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb733-1" data-line-number="1"><span class="kw">ppc_bars_grouped</span>(sim_list_h<span class="op">$</span>w_ans, </a>
<a class="sourceLine" id="cb733-2" data-line-number="2">                 gen_data, <span class="dt">group =</span> subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb733-3" data-line-number="3"><span class="st">  </span><span class="kw">ggtitle</span> (<span class="st">&quot;By subject plot for the hierarchical model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:pMPT-h"></span>
<img src="bookdown_files/figure-html/pMPT-h-1.svg" alt="Individual subjects in the hierarchical MPT model." width="672" />
<p class="caption">
FIGURE 14.7: Individual subjects in the hierarchical MPT model.
</p>
</div>
<p>But what about the first <em>non-hierarchical</em> MPT model (<code>mpt_4.stan</code>)?:</p>
<div class="sourceCode" id="cb734"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb734-1" data-line-number="1">fit_sh &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&quot;stan_models/mpt_4.stan&quot;</span>, <span class="dt">data =</span> sim_list_h)  </a></code></pre></div>
<p>The aggregated data looks great (Figure <a href="multinomial-processing-tree-mpt-models.html#fig:aggMPT-nh">14.8</a>).</p>
<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb735-1" data-line-number="1">gen_data_sMPT &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">extract</span>(fit_sh)<span class="op">$</span>pred_w_ans</a>
<a class="sourceLine" id="cb735-2" data-line-number="2"><span class="kw">ppc_bars</span>(sim_list_h<span class="op">$</span>w_ans, gen_data_sMPT) <span class="op">+</span></a>
<a class="sourceLine" id="cb735-3" data-line-number="3"><span class="st">  </span><span class="kw">ggtitle</span> (<span class="st">&quot;Non-hierarchical model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:aggMPT-nh"></span>
<img src="bookdown_files/figure-html/aggMPT-nh-1.svg" alt="Posterior predictive check for aggregated data in a non-hierarchical MPT model (mpt_4.stan)." width="672" />
<p class="caption">
FIGURE 14.8: Posterior predictive check for aggregated data in a non-hierarchical MPT model (mpt_4.stan).
</p>
</div>
<p>However, the fit to individual subjects looks less good (Figure <a href="multinomial-processing-tree-mpt-models.html#fig:pMPT">14.9</a>).</p>
<div class="sourceCode" id="cb736"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb736-1" data-line-number="1"><span class="kw">ppc_bars_grouped</span>(sim_list_h<span class="op">$</span>w_ans, gen_data_sMPT, <span class="dt">group =</span> subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb736-2" data-line-number="2"><span class="st">  </span><span class="kw">ggtitle</span> (<span class="st">&quot;By subject plot for the non-hierarchical model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:pMPT"></span>
<img src="bookdown_files/figure-html/pMPT-1.svg" alt="Individual subjects in the non-hierarchical MPT model (mpt_4.stan)." width="672" />
<p class="caption">
FIGURE 14.9: Individual subjects in the non-hierarchical MPT model (mpt_4.stan).
</p>
</div>
<p>The hierachical does a better job of modeling individual-level variability.</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-BatchelderRiefer1999">
<p>Batchelder, William H, and David M Riefer. 1999. “Theoretical and Empirical Review of Multinomial Process Tree Modeling.” <em>Psychonomic Bulletin &amp; Review</em> 6 (1). Springer: 57–86.</p>
</div>
<div id="ref-WalkerEtAl2018">
<p>Walker, Grant M, Gregory Hickok, and Julius Fridriksson. 2018. “A Cognitive Psychometric Model for Assessment of Picture Naming Abilities in Aphasia.” <em>Psychological Assessment</em>. American Psychological Association.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="30">
<li id="fn30"><p>Notice below that because the equations depend on <code>f</code> and <code>f</code> is a vector now, the outcome is automatically a vector. But this is not the case for <code>theta_NR_v</code>, and thus we need to repeat the value.<a href="multinomial-processing-tree-mpt-models.html#fnref30" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modeling-multiple-categorical-responses.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="further-reading-11.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/25-MPT.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
