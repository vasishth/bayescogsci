<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.3 Logistic regression: Does set size affect free recall? | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="4.3 Logistic regression: Does set size affect free recall? | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.3 Logistic regression: Does set size affect free recall? | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2020-07-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-trial.html"/>
<link rel="next" href="summary-1.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script type="text/javascript">

 /* Uncomment this and comment the next one to show the solutions */

 /* $(document).ready(function() {
  *     $folds = $(".solution");
  *     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
  *     $folds.prepend("<button class=\"solution-btn\">Show solution</button>");  // add a button
  *     $(".solution-blck").toggle();  // fold all blocks
  *     $(".solution-btn").on("click", function() {  // add onClick event
  *         $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution" 
  *         $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  *     })
  * }); */

 $(document).ready(function() {
     $folds = $(".solution");
     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
     $folds.prepend("<button class=\"solution-btn\"></button>");  // add a button
     $(".solution-blck").toggle();  // fold all blocks
     $(".solution-btn").on("click", function() {  // add onClick event
         /* $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution"  */
         /* $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself. */
     })
 });

</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i><b>0.2</b> Developing the right mindset for this book</a></li>
<li class="chapter" data-level="0.3" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.3</b> How to read this book</a></li>
<li class="chapter" data-level="0.4" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.4</b> Online materials</a></li>
<li class="chapter" data-level="0.5" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.5</b> Software needed</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.3</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.3.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.4</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.4.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><i class="fa fa-check"></i><b>1.5</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.6" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.7</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a><ul>
<li class="chapter" data-level="1.9.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.9.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.9.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.9.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.9.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.9.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.9.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.9.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.9.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#exercise-deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Exercise: Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Exercise: Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Exercise: Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Exercise: Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Exercise: Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Regression models</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using ‘Stan’: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="3.8.1" data-path="ex-compbda.html"><a href="ex-compbda.html#a-simple-linear-model-exercises-section-refsecsimplenormal"><i class="fa fa-check"></i><b>3.8.1</b> A simple linear model exercises (Section @ref(sec:simplenormal))</a></li>
<li class="chapter" data-level="3.8.2" data-path="ex-compbda.html"><a href="ex-compbda.html#revisiting-the-button-pressing-example-with-different-priors-exercises-section-refsecrevisit"><i class="fa fa-check"></i><b>3.8.2</b> Revisiting the button-pressing example with different priors exercises (Section @ref(sec:revisit))</a></li>
<li class="chapter" data-level="3.8.3" data-path="ex-compbda.html"><a href="ex-compbda.html#posterior-predictive-distribution-and-log-normal-model-exercises-section-refsecppd"><i class="fa fa-check"></i><b>3.8.3</b> Posterior predictive distribution and log-normal model exercises (Section @ref(sec:ppd))</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3.9</b> Appendix</a><ul>
<li class="chapter" data-level="3.9.1" data-path="appendix.html"><a href="appendix.html#app:pp"><i class="fa fa-check"></i><b>3.9.1</b> Generating prior predictive distributions with <code>brms</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect reaction times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-the-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#how-to-communicate-the-results-2"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a><ul>
<li class="chapter" data-level="4.6.1" data-path="exercises-2.html"><a href="exercises-2.html#a-first-linear-regression-exercises-section-refsecpupil"><i class="fa fa-check"></i><b>4.6.1</b> A first linear regression exercises (Section @ref(sec:pupil))</a></li>
<li class="chapter" data-level="4.6.2" data-path="exercises-2.html"><a href="exercises-2.html#log-normal-model-exercises-section-refsectrial"><i class="fa fa-check"></i><b>4.6.2</b> Log-normal model exercises (Section @ref(sec:trial))</a></li>
<li class="chapter" data-level="4.6.3" data-path="exercises-2.html"><a href="exercises-2.html#logistic-regression-exercises-section-refseclogistic"><i class="fa fa-check"></i><b>4.6.3</b> Logistic regression exercises (section @ref(sec:logistic))</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="appendix-1.html"><a href="appendix-1.html"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="appendix-1.html"><a href="appendix-1.html#sec:preprocessingpupil"><i class="fa fa-check"></i><b>4.7.1</b> Preparation of the pupil size data (section @ref(sec:pupil))</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal models–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.2.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>5.3</b> Summary</a><ul>
<li class="chapter" data-level="5.3.1" data-path="summary-2.html"><a href="summary-2.html#why-should-we-take-the-trouble-of-fitting-a-bayesian-hierarchical-model"><i class="fa fa-check"></i><b>5.3.1</b> Why should we take the trouble of fitting a Bayesian hierarchical model?</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.4</b> Further reading</a></li>
<li class="chapter" data-level="5.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a><ul>
<li class="chapter" data-level="5.5.1" data-path="exercises-3.html"><a href="exercises-3.html#ex:hierarchical-logn"><i class="fa fa-check"></i><b>5.5.1</b> Hierarchical model with a lognormal likelihood.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding, interactions, etc</a></li>
<li class="chapter" data-level="7" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>7</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>7.2</b> Meta-analysis</a><ul>
<li class="chapter" data-level="7.2.1" data-path="meta-analysis.html"><a href="meta-analysis.html#using-brms"><i class="fa fa-check"></i><b>7.2.1</b> Using brms</a></li>
<li class="chapter" data-level="7.2.2" data-path="meta-analysis.html"><a href="meta-analysis.html#using-stan"><i class="fa fa-check"></i><b>7.2.2</b> Using Stan</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>7.3</b> Measurement-error models</a><ul>
<li class="chapter" data-level="7.3.1" data-path="measurement-error-models.html"><a href="measurement-error-models.html#using-brms-1"><i class="fa fa-check"></i><b>7.3.1</b> Using brms</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>7.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>III Model comparison</b></span></li>
<li class="chapter" data-level="8" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>8</b> Model comparison</a></li>
<li class="chapter" data-level="9" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>9</b> Bayes factors</a><ul>
<li class="chapter" data-level="9.1" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>9.1</b> Summary</a></li>
<li class="chapter" data-level="9.2" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>9.2</b> Further reading</a></li>
<li class="chapter" data-level="9.3" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>9.3</b> Exercises</a><ul>
<li class="chapter" data-level="9.3.1" data-path="exercises-4.html"><a href="exercises-4.html#ex:bf-logn"><i class="fa fa-check"></i><b>9.3.1</b> Bayes factor for a hierarchical model with a lognormal likelihood.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>10</b> Cross validation</a><ul>
<li class="chapter" data-level="10.1" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>10.1</b> Summary</a></li>
<li class="chapter" data-level="10.2" data-path="further-reading-7.html"><a href="further-reading-7.html"><i class="fa fa-check"></i><b>10.2</b> Further reading</a></li>
<li class="chapter" data-level="10.3" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>10.3</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Advanced models with Stan</b></span></li>
<li class="chapter" data-level="11" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>11</b> Introduction to Stan probabilistic language</a></li>
<li class="chapter" data-level="12" data-path="cognitive-modeling-using-multinomial-processing-trees.html"><a href="cognitive-modeling-using-multinomial-processing-trees.html"><i class="fa fa-check"></i><b>12</b> Cognitive Modeling using multinomial processing trees</a><ul>
<li class="chapter" data-level="12.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>12.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="12.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>12.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="12.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>12.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html"><i class="fa fa-check"></i><b>12.2</b> Multinomial processing tree (MPT) models</a><ul>
<li class="chapter" data-level="12.2.1" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html#mpts-for-modeling-picture-naming-abilities-in-aphasia"><i class="fa fa-check"></i><b>12.2.1</b> MPTs for modeling picture naming abilities in aphasia</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="further-readings.html"><a href="further-readings.html"><i class="fa fa-check"></i><b>12.3</b> Further readings:</a></li>
</ul></li>
<li class="part"><span><b>V Appendix</b></span></li>
<li class="chapter" data-level="13" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>13</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:logistic" class="section level2">
<h2><span class="header-section-number">4.3</span> Logistic regression: Does set size affect free recall?</h2>
<p>We’ll look at the capacity limit of working memory to illustrate how the principles we have learned so far can naturally extend to <em>generalized</em> linear models (GLMs). In this section, we focus on one special case of GLMs, logistic regression.</p>
<p>For this example, we’ll use a subset of the data of <span class="citation">Oberauer (<a href="#ref-oberauerWorkingMemoryCapacity2019">2019</a>)</span> from <a href="https://osf.io/qy5sd/" class="uri">https://osf.io/qy5sd/</a>. We’ll focus on one subject who was presented word lists of varying lengths (2, 4, 6, and 8 elements), and then was asked to recall a word given its position on the list; see Figure <a href="sec-logistic.html#fig:oberauer">4.9</a>.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>

<div class="figure"><span id="fig:oberauer"></span>
<img src="cc_figure/fig1_oberauer_2019_modified.png" alt="Flow of events in a trial with memory set size 4 and free recall. Adapted from Oberauer (2019); licensed under CC BY 4.0." width="320" />
<p class="caption">
FIGURE 4.9: Flow of events in a trial with memory set size 4 and free recall. Adapted from <span class="citation">Oberauer (<a href="#ref-oberauerWorkingMemoryCapacity2019">2019</a>)</span>; licensed under CC BY 4.0.
</p>
</div>
<p>It is well established that as the number of items to be held in working memory increases, performance, that is accuracy, decreases <span class="citation">(among others Oberauer and Kliegl <a href="#ref-oberauerkliegel2001">2001</a>)</span>. We will investigate whether we can establish this finding with data from only one subject.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb163-1" data-line-number="1">df_recall_data &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;./data/PairsRSS1_all.csv&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb163-2" data-line-number="2"><span class="st">    </span><span class="co"># We ignore the type of incorrect responses (the focus of the paper)</span></a>
<a class="sourceLine" id="cb163-3" data-line-number="3"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">correct =</span> <span class="kw">if_else</span>(response_category <span class="op">==</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb163-4" data-line-number="4"><span class="st">    </span><span class="co"># and we only use the data from the free recall task:</span></a>
<a class="sourceLine" id="cb163-5" data-line-number="5"><span class="st">    </span><span class="co"># (when there was no list of possible responses)</span></a>
<a class="sourceLine" id="cb163-6" data-line-number="6"><span class="st">    </span><span class="kw">filter</span>(response_size_list <span class="op">+</span><span class="st"> </span>response_size_new_words <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb163-7" data-line-number="7"><span class="st">    </span><span class="co"># We select one subject</span></a>
<a class="sourceLine" id="cb163-8" data-line-number="8"><span class="st">    </span><span class="kw">filter</span>(subject <span class="op">==</span><span class="st"> </span><span class="dv">10</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb163-9" data-line-number="9"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">c_set_size =</span> set_size <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(set_size)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb163-10" data-line-number="10"><span class="st">    </span><span class="kw">select</span>(subject, set_size, c_set_size, correct, trial, session, block, tested)</a>
<a class="sourceLine" id="cb163-11" data-line-number="11"></a>
<a class="sourceLine" id="cb163-12" data-line-number="12"><span class="co"># we can ignore the warning from read_table</span></a>
<a class="sourceLine" id="cb163-13" data-line-number="13"></a>
<a class="sourceLine" id="cb163-14" data-line-number="14"><span class="co"># Set sizes in the dataset:</span></a>
<a class="sourceLine" id="cb163-15" data-line-number="15">df_recall_data<span class="op">$</span>set_size <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb163-16" data-line-number="16"><span class="st">    </span>unique</a></code></pre></div>
<pre><code>## [1] 4 8 2 6</code></pre>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb165-1" data-line-number="1"><span class="co"># Trials by set size</span></a>
<a class="sourceLine" id="cb165-2" data-line-number="2">df_recall_data <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb165-3" data-line-number="3"><span class="st">    </span><span class="kw">group_by</span>(set_size) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb165-4" data-line-number="4"><span class="st">    </span><span class="kw">count</span>()</a></code></pre></div>
<pre><code>## # A tibble: 4 x 2
## # Groups:   set_size [4]
##   set_size     n
##      &lt;dbl&gt; &lt;int&gt;
## 1        2    23
## 2        4    23
## 3        6    23
## 4        8    23</code></pre>
<p>The data look like this: the column <code>correct</code> records the 0 (incorrect) or 1 (correct) responses, and the column <code>c_set_size</code> records the centered memory set size; these latter scores have continuous values -3, -1, 1, and 3. These continuous values are centered versions of 2, 4, 6, and 8.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" data-line-number="1">df_recall_data</a></code></pre></div>
<pre><code>## # A tibble: 92 x 8
##   subject set_size c_set_size correct trial session
##     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1      10        4         -1       1     1       1
## 2      10        8          3       0     4       1
## 3      10        2         -3       1     9       1
## 4      10        6          1       1    23       1
## 5      10        4         -1       1     5       1
##   block tested
##   &lt;dbl&gt;  &lt;dbl&gt;
## 1     1      2
## 2     1      8
## 3     1      2
## 4     1      2
## 5     2      3
## # … with 87 more rows</code></pre>
<p>We want to model the trial by trial accuracy and examine whether the probability of recalling a word is related to the number of words in the set that the subject needs to remember.</p>
<div id="the-likelihood-for-the-logistic-regression-model" class="section level3">
<h3><span class="header-section-number">4.3.1</span> The likelihood for the logistic regression model</h3>
<p>Recall that the Bernoulli likelihood generates a 0 or 1 response with a particular probability <span class="math inline">\(\theta\)</span>. For example, one can generate simulated data for 10 trials, with 50% chances of getting a one as follows:</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" data-line-number="1"><span class="co"># We use as.numeric to get zeros and ones rather than FALSE and TRUE</span></a>
<a class="sourceLine" id="cb169-2" data-line-number="2"><span class="kw">rbernoulli</span>(<span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">p =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()</a></code></pre></div>
<pre><code>##  [1] 1 1 0 1 0 0 0 0 1 1</code></pre>
<p>We can therefore define each dependent value <code>correct_n</code> in the data as being generated from a Bernoulli random variable with probability of success <span class="math inline">\(\theta_n\)</span>.
Here, <span class="math inline">\(n =1, \ldots, N\)</span> indexes the trial, correct_n is the dependent variable (0 indicates an incorrect recall and 1 a correct recall), and <span class="math inline">\(\theta_n\)</span> is the probability of correctly recalling a probe in a given trial <span class="math inline">\(n\)</span>.</p>
<p><span class="math display" id="eq:bernoullilik">\[\begin{equation}
correct_n \sim Bernoulli(\theta_n)
\tag{4.4}
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(\theta_n\)</span> is bounded to be between 0 and 1 (it is a probability), we cannot just fit a regression model using the normal or lognormal likelihood as we did in the preceding examples. Such a model would be inappropriate because it would assume that the data range from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>, rather than from 0 to 1.</p>
<p>The generalized linear modeling framework solves this problem by defining a so-called <strong>link function</strong> <span class="math inline">\(g(\cdot)\)</span> that connects the linear model to the quantity to be estimated (here, the probabilities <span class="math inline">\(\theta_n\)</span>). The link function used for 0,1 responses is called the <strong>logit link</strong>, and is defined as follows.</p>
<p><span class="math display">\[\begin{equation}
\eta_n = g(\theta_n) = \log\left(\frac{\theta_n}{1-\theta_n}\right)
\end{equation}\]</span></p>
<p>The term <span class="math inline">\(\frac{\theta_n}{1-\theta_n}\)</span> is called the <strong>odds</strong>.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> The logit link function is therefore a log-odds; it maps probability values ranging from <span class="math inline">\([0,1]\)</span> to real numbers ranging from <span class="math inline">\((-\infty,+\infty)\)</span>. Figure <a href="sec-logistic.html#fig:logisticfun">4.10</a> shows the logit link function, <span class="math inline">\(\eta = g(\theta)\)</span>, and the inverse logit, <span class="math inline">\(\theta = g^{-1}(\eta)\)</span>, which is called the <strong>logistic function</strong>; the relevance of this logistic function will become clear in a moment.</p>
<div class="figure"><span id="fig:logisticfun"></span>
<img src="bookdown_files/figure-html/logisticfun-1.svg" alt="The logit and inverse logit (logistic) function." width="672" />
<p class="caption">
FIGURE 4.10: The logit and inverse logit (logistic) function.
</p>
</div>
<p>The linear model is now fit not to the 0,1 responses as the dependent variable, but to <span class="math inline">\(\eta_n\)</span>, i.e., log-odds, as the dependent variable:</p>
<p><span class="math display">\[\begin{equation}
\eta_n = \log\left(\frac{\theta_n}{1-\theta_n}\right) = \alpha + \beta \cdot c\_set\_size
\end{equation}\]</span></p>
<p>Once <span class="math inline">\(\eta_n\)</span> is estimated, one can easily compute the parameters of interest, the estimated probabilities, by solving the above equation for <span class="math inline">\(\theta_n\)</span> (in other words, by computing the inverse of the logit function), which is the above-mentioned logistic regression function:</p>
<p><span class="math display">\[\begin{equation}
\theta_n = g^{-1}(\eta_n) =  \log\left(\frac{\exp(\eta_n)}{1+\exp(\eta_n)}\right)
\end{equation}\]</span></p>
<p>In summary, the generalized linear model with the logit link fits the following Bernoulli likelihood:</p>
<p><span class="math display" id="eq:bernoullilogislik">\[\begin{equation}
correct_n \sim Bernoulli(\theta_n)
\tag{4.5}
\end{equation}\]</span></p>
<p>The model is fit on the log-odds scale, <span class="math inline">\(\eta_n = \alpha + c\_set\_size_n \cdot \beta\)</span>.
Once <span class="math inline">\(\eta_n\)</span> has been estimated, the inverse logit or the logistic function is used to compute the probability estimates
<span class="math inline">\(\theta_n = \log(\frac{\exp(\eta_n)}{1+\exp(\eta_n)})\)</span>`. An example of the calculations will be shown in the next section.</p>
</div>
<div id="priors-for-the-logistic-regression" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Priors for the logistic regression</h3>
<p>In order to decide on priors for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> we need to take into account that these parameter do not represent probabilities or proportions, but <em>log-odds</em>, the x-axis in Figure <a href="sec-logistic.html#fig:logisticfun">4.10</a> (right-hand side figure). As shown in the figure, the relationship between log-odds and probabilities is not linear.</p>
<p>There are two functions in R that implement the logit and inverse logit functions: <code>qlogis(p)</code> for the logit function and <code>plogis(x)</code> for the inverse logit or logistic function.</p>
<p>Now we need to set priors for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.
Given that we centered our predictor, the intercept, <span class="math inline">\(\alpha\)</span>, represents the log-odds of correctly recalling one word in a random position for the average set size of five (since <span class="math inline">\(5 = \frac{2+4+6+8}{4}\)</span>), which, incidentally, was not presented in the experiment. This is one case where the intercept doesn’t have a clear interpretation if we leave the prediction uncentered: With non-centered set size, the intercept will be the log-odds of recalling one word in a set of <em>zero</em> words.</p>
<p>The prior for <span class="math inline">\(\alpha\)</span> will depend on how difficult the recall task is. If we are not sure, we could assume that the probability of recalling a word for an average set size, <span class="math inline">\(\alpha\)</span>, is centered in .5 (a 50/50 chance) with a great deal of uncertainty. The <code>R</code> command <code>qlogis(.5)</code> tells us that .5 corresponds to zero in log-odds. How do we include a great deal of uncertainty? We could look at Figure <a href="sec-logistic.html#fig:logisticfun">4.10</a>, and decide on a standard deviation of 4 in a normal distribution centered in zero:</p>
<p><span class="math display">\[\begin{equation}
\alpha \sim Normal(0, 4) 
\end{equation}\]</span></p>
<p>Let’s plot this prior in log-odds and in probability scale by drawing random samples.</p>

<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" data-line-number="1">samples_logodds &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">alpha =</span> <span class="kw">rnorm</span>(<span class="dv">100000</span>, <span class="dv">0</span>, <span class="dv">4</span>))</a>
<a class="sourceLine" id="cb171-2" data-line-number="2">samples_prob &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">p =</span> <span class="kw">plogis</span>(<span class="kw">rnorm</span>(<span class="dv">100000</span>, <span class="dv">0</span>, <span class="dv">4</span>)))</a>
<a class="sourceLine" id="cb171-3" data-line-number="3"><span class="kw">ggplot</span>(samples_logodds, <span class="kw">aes</span>(alpha)) <span class="op">+</span></a>
<a class="sourceLine" id="cb171-4" data-line-number="4"><span class="st">    </span><span class="kw">geom_density</span>()</a>
<a class="sourceLine" id="cb171-5" data-line-number="5"><span class="kw">ggplot</span>(samples_prob, <span class="kw">aes</span>(p)) <span class="op">+</span></a>
<a class="sourceLine" id="cb171-6" data-line-number="6"><span class="st">    </span><span class="kw">geom_density</span>()</a></code></pre></div>
<div class="figure"><span id="fig:logoddspriorsf"></span>
<img src="bookdown_files/figure-html/logoddspriorsf-1.svg" alt="Prior for \(\alpha \sim Normal(0, 4)\) in log-odds and in probability space." width="45%" /><img src="bookdown_files/figure-html/logoddspriorsf-2.svg" alt="Prior for \(\alpha \sim Normal(0, 4)\) in log-odds and in probability space." width="45%" />
<p class="caption">
FIGURE 4.11: Prior for <span class="math inline">\(\alpha \sim Normal(0, 4)\)</span> in log-odds and in probability space.
</p>
</div>
<p>Figure <a href="sec-logistic.html#fig:logoddspriorsf">4.11</a> shows that our prior assigns more probability mass to extreme probabilities of recall than to intermediate values. Clearly, this is not what we intended.</p>
<p>We could try several values for standard deviation of the prior, until we find a prior that make sense for us. Reducing the standard deviation to 1.5 seems to make sense as shown in Figure <a href="sec-logistic.html#fig:logoddspriorsf2">4.12</a>.</p>
<p><span class="math display">\[\begin{equation}
\alpha \sim Normal(0, 1.5) 
\end{equation}\]</span></p>

<div class="figure"><span id="fig:logoddspriorsf2"></span>
<img src="bookdown_files/figure-html/logoddspriorsf2-1.svg" alt="Prior for \(\alpha \sim Normal(0, 1.5)\) in log-odds and in probability space." width="45%" /><img src="bookdown_files/figure-html/logoddspriorsf2-2.svg" alt="Prior for \(\alpha \sim Normal(0, 1.5)\) in log-odds and in probability space." width="45%" />
<p class="caption">
FIGURE 4.12: Prior for <span class="math inline">\(\alpha \sim Normal(0, 1.5)\)</span> in log-odds and in probability space.
</p>
</div>
<p>We need to decide now on the prior for the effect in log-odds of increasing the set size, <span class="math inline">\(\beta\)</span>. We are going to choose a normal distribution centered on zero, reflecting our lack of any commitment regarding the direction of the effect. Let’s get some intuitions regarding different possible standard deviations for this prior, by testing the following distributions as priors:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\beta \sim Normal(0, 1)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0, .5)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0, .1)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0, .01)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0, .001)\)</span></li>
</ol>
<p>The following function is an edited version of the earlier <code>normal_predictive_distribution_fast</code> from section <a href="sec-ppd.html#sec:ppd">3.5</a>; it has been edited to make it compatible with logistic regression and dependent on set size:</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" data-line-number="1">logistic_model_pred &lt;-<span class="st"> </span><span class="cf">function</span>(alpha_samples,</a>
<a class="sourceLine" id="cb172-2" data-line-number="2">                                beta_samples,</a>
<a class="sourceLine" id="cb172-3" data-line-number="3">                                set_size,</a>
<a class="sourceLine" id="cb172-4" data-line-number="4">                                 N_obs) {</a>
<a class="sourceLine" id="cb172-5" data-line-number="5">    <span class="kw">map2_dfr</span>(alpha_samples, beta_samples,</a>
<a class="sourceLine" id="cb172-6" data-line-number="6">             <span class="cf">function</span>(alpha, beta) {</a>
<a class="sourceLine" id="cb172-7" data-line-number="7">                 <span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb172-8" data-line-number="8">                     <span class="dt">set_size =</span> set_size,</a>
<a class="sourceLine" id="cb172-9" data-line-number="9">                     <span class="co"># we center size:</span></a>
<a class="sourceLine" id="cb172-10" data-line-number="10">                     <span class="dt">c_set_size =</span> set_size <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(set_size),</a>
<a class="sourceLine" id="cb172-11" data-line-number="11">                     <span class="co"># change the likelihood: </span></a>
<a class="sourceLine" id="cb172-12" data-line-number="12">                     <span class="co"># Notice the use of a link function for alpha and beta</span></a>
<a class="sourceLine" id="cb172-13" data-line-number="13">                     <span class="dt">theta =</span> <span class="kw">plogis</span>(alpha <span class="op">+</span><span class="st"> </span>c_set_size <span class="op">*</span><span class="st"> </span>beta),</a>
<a class="sourceLine" id="cb172-14" data-line-number="14">                     <span class="dt">correct_pred =</span> <span class="kw">rbernoulli</span>(N_obs,  <span class="dt">p =</span> theta)</a>
<a class="sourceLine" id="cb172-15" data-line-number="15">                 )</a>
<a class="sourceLine" id="cb172-16" data-line-number="16">             }, <span class="dt">.id =</span> <span class="st">&quot;iter&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb172-17" data-line-number="17"><span class="st">    </span><span class="co"># .id is always a string and needs to be converted to a number</span></a>
<a class="sourceLine" id="cb172-18" data-line-number="18"><span class="st">        </span><span class="kw">mutate</span>(<span class="dt">iter =</span> <span class="kw">as.numeric</span>(iter))</a>
<a class="sourceLine" id="cb172-19" data-line-number="19">}</a></code></pre></div>
<p>Let’s assume 800 observations with 200 observation of each set size:</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" data-line-number="1">N_obs &lt;-<span class="st"> </span><span class="dv">800</span></a>
<a class="sourceLine" id="cb173-2" data-line-number="2">set_size &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">8</span>),<span class="dv">200</span>)</a></code></pre></div>
<p>We iterate over the four possible standard deviations of <span class="math inline">\(\beta\)</span>:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" data-line-number="1">alpha_samples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb174-2" data-line-number="2">sds_beta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.1</span>,<span class="fl">0.01</span>, <span class="fl">0.001</span>) </a>
<a class="sourceLine" id="cb174-3" data-line-number="3">prior_pred &lt;-<span class="st"> </span><span class="kw">map_dfr</span>(sds_beta, <span class="cf">function</span>(sd) {</a>
<a class="sourceLine" id="cb174-4" data-line-number="4">    beta_samples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, sd)</a>
<a class="sourceLine" id="cb174-5" data-line-number="5">    <span class="kw">logistic_model_pred</span>(<span class="dt">alpha_samples =</span> alpha_samples,</a>
<a class="sourceLine" id="cb174-6" data-line-number="6">                        <span class="dt">beta_samples =</span> beta_samples,</a>
<a class="sourceLine" id="cb174-7" data-line-number="7">                        <span class="dt">set_size =</span> set_size,</a>
<a class="sourceLine" id="cb174-8" data-line-number="8">                        <span class="dt">N_obs =</span> N_obs</a>
<a class="sourceLine" id="cb174-9" data-line-number="9">                        ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb174-10" data-line-number="10"><span class="st">        </span><span class="kw">mutate</span>(<span class="dt">prior_beta_sd =</span> sd)</a>
<a class="sourceLine" id="cb174-11" data-line-number="11">})</a></code></pre></div>
<p>And we calculate the accuracy for each one of the priors we want to examine, for each iteration, and for each set size.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" data-line-number="1">mean_accuracy &lt;-</a>
<a class="sourceLine" id="cb175-2" data-line-number="2"><span class="st">     </span>prior_pred <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb175-3" data-line-number="3"><span class="st">     </span><span class="kw">group_by</span>(prior_beta_sd, iter, set_size) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb175-4" data-line-number="4"><span class="st">     </span><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(correct_pred)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb175-5" data-line-number="5"><span class="st">     </span><span class="kw">mutate</span>(<span class="dt">prior =</span> <span class="kw">paste0</span>(<span class="st">&quot;Normal(0, &quot;</span>,prior_beta_sd,<span class="st">&quot;)&quot;</span>))</a></code></pre></div>
<p>We plot it in Figure <a href="sec-logistic.html#fig:priors4beta">4.13</a>, and as expected the priors are centered at zero. We see that the distribution of possible accuracies for the prior that has a standard deviation of one is problematic: There is too much probability mass concentrated near zero and one for set sizes of 2 and 8.</p>

<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" data-line-number="1">mean_accuracy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb176-2" data-line-number="2"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(accuracy)) <span class="op">+</span></a>
<a class="sourceLine" id="cb176-3" data-line-number="3"><span class="st">    </span><span class="kw">geom_histogram</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb176-4" data-line-number="4"><span class="st">    </span><span class="kw">facet_grid</span>(set_size<span class="op">~</span>prior)</a></code></pre></div>
<div class="figure"><span id="fig:priors4beta"></span>
<img src="bookdown_files/figure-html/priors4beta-1.svg" alt="Prior predictive distribution of mean accuracy of the model defined in 4.3, for different set sizes and different priors for \(\beta\)." width="672" />
<p class="caption">
FIGURE 4.13: Prior predictive distribution of mean accuracy of the model defined in <a href="sec-logistic.html#sec:logistic">4.3</a>, for different set sizes and different priors for <span class="math inline">\(\beta\)</span>.
</p>
</div>
<p>It’s hard to tell the differences between the other priors, and it might be more useful to look at the predicted differences in accuracy between set sizes. We calculate them as follows:</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" data-line-number="1">diff_accuracy &lt;-<span class="st"> </span>mean_accuracy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-2" data-line-number="2"><span class="st">    </span><span class="kw">arrange</span>(set_size) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-3" data-line-number="3"><span class="st">    </span><span class="kw">group_by</span>(iter, prior_beta_sd) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-4" data-line-number="4"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">diffaccuracy =</span> accuracy <span class="op">-</span><span class="st"> </span><span class="kw">lag</span>(accuracy) ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-5" data-line-number="5"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">diffsize =</span> <span class="kw">paste</span>(set_size,<span class="st">&quot;-&quot;</span>,  <span class="kw">lag</span>(set_size))) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-6" data-line-number="6"><span class="st">    </span><span class="kw">filter</span>(set_size <span class="op">&gt;</span><span class="dv">2</span>)</a></code></pre></div>

<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1">diff_accuracy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb178-2" data-line-number="2"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(diffaccuracy)) <span class="op">+</span></a>
<a class="sourceLine" id="cb178-3" data-line-number="3"><span class="st">    </span><span class="kw">geom_histogram</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb178-4" data-line-number="4"><span class="st">    </span><span class="kw">facet_grid</span>(diffsize<span class="op">~</span>prior)</a></code></pre></div>
<div class="figure"><span id="fig:priors4beta2"></span>
<img src="bookdown_files/figure-html/priors4beta2-1.svg" alt="Prior predictive distribution of differences in mean accuracy between set sizes of the model defined in 4.3 for different priors for \(\beta\)." width="672" />
<p class="caption">
FIGURE 4.14: Prior predictive distribution of differences in mean accuracy between set sizes of the model defined in <a href="sec-logistic.html#sec:logistic">4.3</a> for different priors for <span class="math inline">\(\beta\)</span>.
</p>
</div>
<!-- log-odds transformation using the logit function (also known as the inverse logistic function): -->
<!-- The logit funcion maps values  -->
<!-- As before, we will assume that the relationship between set size and decrease (or increase) in acc -->
<p>We plot them in Figure <a href="sec-logistic.html#fig:priors4beta2">4.14</a>. If we are not sure whether the increase of set size could produce something between a null effect and a relatively large effect, we can choose the prior with a standard deviation of <span class="math inline">\(0.1\)</span>. Thus we settle on the following priors:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim Normal(0, 1.5) \\
\beta &amp;\sim Normal(0, 0.1) 
\end{aligned}
\end{equation}\]</span></p>
</div>
<div id="the-brms-model-2" class="section level3">
<h3><span class="header-section-number">4.3.3</span> The <code>brms</code> model</h3>
<p>Having decided on the likelihood, the link function, and the priors, the model can now be fit using <code>brms</code>. Notice that we need to specify that the family is <code>bernoulli()</code>, and the link is <code>logit</code>.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" data-line-number="1">fit_recall &lt;-<span class="st"> </span><span class="kw">brm</span>(correct <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_set_size,</a>
<a class="sourceLine" id="cb179-2" data-line-number="2">  <span class="dt">data =</span> df_recall_data,</a>
<a class="sourceLine" id="cb179-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> logit),</a>
<a class="sourceLine" id="cb179-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb179-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb179-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">.1</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_set_size)</a>
<a class="sourceLine" id="cb179-7" data-line-number="7">  )</a>
<a class="sourceLine" id="cb179-8" data-line-number="8">)</a></code></pre></div>
<p>Next, look at the summary of the posteriors of each of the parameters. Keep in mind that the parameters are in log-odds space:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb180-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_recall, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;b_Intercept&quot;</span>, <span class="st">&quot;b_c_set_size&quot;</span>))</a></code></pre></div>
<pre><code>##              Estimate Est.Error  Q2.5  Q97.5
## b_Intercept      1.92     0.309  1.34  2.550
## b_c_set_size    -0.18     0.082 -0.34 -0.023</code></pre>
<p>Plot the posteriors as well:</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb182-1" data-line-number="1"><span class="kw">plot</span>(fit_recall)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-116-1.svg" width="672" /></p>
<p>Next, we turn to the question of what we can report as our results, and what we can conclude from the data.</p>
</div>
<div id="how-to-communicate-the-results-2" class="section level3">
<h3><span class="header-section-number">4.3.4</span> How to communicate the results?</h3>
<p>We are here in a situation analogous as before with the log-normal model. If we want to talk about the effect estimated by the model in log-odds space, we summarize the posterior of <span class="math inline">\(\beta\)</span> in the following way: <span class="math inline">\(\hat\beta = -0.18\)</span>, 95% CrI = <span class="math inline">\([ -0.34 , -0.02 ]\)</span>.</p>
<p>However, the effect might be easier to understand in proportions rather than in log-odds. Let’s look at the average accuracy for the task first:</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" data-line-number="1">alpha_samples&lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit_recall)<span class="op">$</span>b_Intercept</a>
<a class="sourceLine" id="cb183-2" data-line-number="2">av_accuracy &lt;-<span class="st"> </span><span class="kw">plogis</span>(alpha_samples)</a>
<a class="sourceLine" id="cb183-3" data-line-number="3"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(av_accuracy), <span class="kw">quantile</span>(av_accuracy, <span class="kw">c</span>(.<span class="dv">025</span>,.<span class="dv">975</span>)))</a></code></pre></div>
<pre><code>## mean 2.5%  98% 
## 0.87 0.79 0.93</code></pre>
<p>As before, to transform the effect of our manipulation to an easier to interpret scale (i.e., proportion), we need to take into account that the scale is not linear, and that the effect of increasing the set size depends on the average accuracy, and the set size that we start from.</p>
<p>We can do the following calculation, similar to what we did for the trial effects experiment, to find out the decrease in accuracy in proportions or probability scale:</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" data-line-number="1">beta_samples&lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit_recall)<span class="op">$</span>b_c_set_size</a>
<a class="sourceLine" id="cb185-2" data-line-number="2">effect_middle &lt;-<span class="st"> </span><span class="kw">plogis</span>(alpha_samples) <span class="op">-</span><span class="st"> </span><span class="kw">plogis</span>(alpha_samples <span class="op">-</span><span class="st"> </span>beta_samples)</a>
<a class="sourceLine" id="cb185-3" data-line-number="3"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_middle), <span class="kw">quantile</span>(effect_middle, <span class="kw">c</span>(.<span class="dv">025</span>,.<span class="dv">975</span>)))</a></code></pre></div>
<pre><code>##    mean    2.5%     98% 
## -0.0187 -0.0366 -0.0024</code></pre>
<p>Notice the interpretation here, if we increase the set size from the average set size minus one to the average set size, we get a reduction in the accuracy of recall of <span class="math inline">\(-0.02\)</span>, 95% CrI = <span class="math inline">\([ -0.04 , 0 ]\)</span>. Recall that the average set size, 5, was not presented to the subject! We could also look at the decrease in accuracy from a set size of 2 to 4:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" data-line-number="1">effect_4m2 &lt;-<span class="st"> </span><span class="kw">plogis</span>(alpha_samples<span class="op">+</span><span class="st">  </span>(<span class="dv">4</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(df_recall_data<span class="op">$</span>set_size)) <span class="op">*</span><span class="st"> </span>beta_samples) <span class="op">-</span></a>
<a class="sourceLine" id="cb187-2" data-line-number="2"><span class="st">    </span><span class="kw">plogis</span>(alpha_samples<span class="op">+</span><span class="st">  </span>(<span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(df_recall_data<span class="op">$</span>set_size)) <span class="op">*</span><span class="st"> </span>beta_samples)</a>
<a class="sourceLine" id="cb187-3" data-line-number="3"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_4m2), <span class="kw">quantile</span>(effect_4m2, <span class="kw">c</span>(.<span class="dv">025</span>,.<span class="dv">975</span>)))</a></code></pre></div>
<pre><code>##    mean    2.5%     98% 
## -0.0293 -0.0541 -0.0047</code></pre>
<p>We see that increasing the set size does have a detrimental effect in recall, as we suspected.</p>
</div>
<div id="descriptive-adequacy" class="section level3">
<h3><span class="header-section-number">4.3.5</span> Descriptive adequacy</h3>
<p>One potentially useful aspect of posterior distributions is that we could also make predictions for other conditions not presented in the actual experiment, such as set sizes that weren’t tested. We could then verify if our model was right with another experiment. To make predictions for other set sizes, we extend our dataset adding rows with set sizes of 3, 5, and 7. To be consistent with the data of the other set sizes in the experiment, we add 23 trials of each new set size (this is the number of trial by set sizes in the dataset). Something important to notice is that <strong>we need to center our predictor based on the original mean set size</strong>. This is because we want to maintain our interpretation of the intercept. We extend the data as follows, and we summarize the data and plot it in Figure <a href="sec-logistic.html#fig:postpredsum2">4.15</a>.</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" data-line-number="1">df_recall_data_ext &lt;-<span class="st"> </span>df_recall_data <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb189-2" data-line-number="2"><span class="st">    </span><span class="kw">bind_rows</span>(<span class="kw">tibble</span>(<span class="dt">set_size =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>),<span class="dv">23</span>),</a>
<a class="sourceLine" id="cb189-3" data-line-number="3">                     <span class="dt">c_set_size =</span> set_size <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(df_recall_data<span class="op">$</span>set_size)))</a>
<a class="sourceLine" id="cb189-4" data-line-number="4">df_recall_pred_ext &lt;-<span class="st"> </span><span class="kw">posterior_predict</span>(fit_recall,</a>
<a class="sourceLine" id="cb189-5" data-line-number="5">                                 <span class="dt">newdata =</span> df_recall_data_ext,</a>
<a class="sourceLine" id="cb189-6" data-line-number="6">                                 <span class="dt">nsamples =</span> <span class="dv">1000</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb189-7" data-line-number="7"><span class="st">    </span><span class="kw">array_branch</span>(<span class="dt">margin =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb189-8" data-line-number="8"><span class="st">    </span><span class="kw">map_dfr</span>( <span class="cf">function</span>(yrep_iter) {</a>
<a class="sourceLine" id="cb189-9" data-line-number="9">        df_recall_data_ext <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb189-10" data-line-number="10"><span class="st">            </span><span class="kw">mutate</span>(<span class="dt">correct =</span> yrep_iter)</a>
<a class="sourceLine" id="cb189-11" data-line-number="11">    }, <span class="dt">.id =</span> <span class="st">&quot;iter&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb189-12" data-line-number="12"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">iter =</span> <span class="kw">as.numeric</span>(iter))</a></code></pre></div>

<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" data-line-number="1">df_recall_pred_ext_summary &lt;-<span class="st"> </span>df_recall_pred_ext <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb190-2" data-line-number="2"><span class="st">    </span><span class="kw">group_by</span>(iter, set_size) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb190-3" data-line-number="3"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(correct))</a>
<a class="sourceLine" id="cb190-4" data-line-number="4"><span class="co"># observed means:</span></a>
<a class="sourceLine" id="cb190-5" data-line-number="5">df_recall_summary&lt;-<span class="st"> </span>df_recall_data <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb190-6" data-line-number="6"><span class="st">    </span><span class="kw">group_by</span>(set_size) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb190-7" data-line-number="7"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(correct))</a>
<a class="sourceLine" id="cb190-8" data-line-number="8"><span class="kw">ggplot</span>(df_recall_pred_ext_summary, <span class="kw">aes</span>(accuracy)) <span class="op">+</span></a>
<a class="sourceLine" id="cb190-9" data-line-number="9"><span class="st">    </span><span class="kw">geom_histogram</span>(<span class="dt">alpha=</span>.<span class="dv">5</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb190-10" data-line-number="10"><span class="st">    </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept=</span> accuracy),<span class="dt">data=</span> df_recall_summary)<span class="op">+</span></a>
<a class="sourceLine" id="cb190-11" data-line-number="11"><span class="st">    </span><span class="kw">facet_grid</span>(set_size <span class="op">~</span><span class="st"> </span>.)</a></code></pre></div>
<div class="figure"><span id="fig:postpredsum2"></span>
<img src="bookdown_files/figure-html/postpredsum2-1.svg" alt="Distribution of posterior predicted mean accuracies in gray for tested set sizes (2, 4, 6, and 8) and untested ones (3, 5, and 7), and observed mean accuracy in black lines by tested set sizes." width="672" />
<p class="caption">
FIGURE 4.15: Distribution of posterior predicted mean accuracies in gray for tested set sizes (2, 4, 6, and 8) and untested ones (3, 5, and 7), and observed mean accuracy in black lines by tested set sizes.
</p>
</div>
<!-- with https://osf.io/uwdcm/#! -->
<!-- accuracy in the translation from the word in Spanish to English as a function of word experience (the total number of times that a user saw a given word in Duolingo.) -->
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-oberauerWorkingMemoryCapacity2019">
<p>Oberauer, Klaus. 2019. “Working Memory Capacity Limits Memory for Bindings.” <em>Journal of Cognition</em> 2 (1): 40. <a href="https://doi.org/10.5334/joc.86">https://doi.org/10.5334/joc.86</a>.</p>
</div>
<div id="ref-oberauerkliegel2001">
<p>Oberauer, Klaus, and Reinhold Kliegl. 2001. “Beyond Resources: Formal Models of Complexity Effects and Age Differences in Working Memory.” <em>European Journal of Cognitive Psychology</em> 13 (1-2). Routledge: 187–215. <a href="https://doi.org/10.1080/09541440042000278">https://doi.org/10.1080/09541440042000278</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>We will only use data from the recall test in which the participant had to type the probed word (and we will ignore the trials with multiple forced choice for ease of explanation).<a href="sec-logistic.html#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>Odds are defined to be the ratio of the probability of success to the probability of failure. For example, the odds of obtaining a one in a fair six-sided die are <span class="math inline">\(\frac{1/6}{1-1/6}=1/5\)</span>. The odds of obtaining a heads in a fair coin are 1/1.<a href="sec-logistic.html#fnref11" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-trial.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/04-regressions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
