<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Bayes factors | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Bayes factors | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Bayes factors | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2024-08-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-comparison.html"/>
<link rel="next" href="ch-cv.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b>  Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The  law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b>  Discrete random variables: An example using the  binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b>  Continuous random variables: An example using the  normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1:  Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2:  Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate  simulated bivariate  (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The  marginal likelihood  (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b>  Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the  posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a  likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a  prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The  posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the  posterior through  sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b>  Bayesian Regression Models using Stan:  brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b>  Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors:  sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b>  Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b>  Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b>  Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b>  Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b>  Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The  log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
<li class="chapter" data-level="3.11" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first  linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b>  Likelihood and  priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b>  Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b>  Logistic regression: Does  set size affect  free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b>  Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b>  Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b>  No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b>  Varying intercepts and  varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A  hierarchical log-normal model: The  Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes  log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of  Prior Elicitation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English  relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the  variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b>  Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from  meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’  posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
<li class="chapter" data-level="6.7" data-path="ch-priors.html"><a href="ch-priors.html#sec-priorsexercises"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#building-a-model"><i class="fa fa-check"></i><b>7.1</b>  Building a model</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-to-ask-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions to ask on a model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#checking-whether-assumptions-are-consistent-with-domain-expertise-prior-predictive-checks"><i class="fa fa-check"></i><b>7.2.1</b>  Checking whether assumptions are consistent with  domain expertise: Prior predictive checks</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#testing-for-correct-posterior-approximations-checks-of-computational-faithfulness"><i class="fa fa-check"></i><b>7.2.2</b>  Testing for correct posterior approximations: Checks of computational faithfulness</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#sensitivity-of-the-model"><i class="fa fa-check"></i><b>7.2.3</b>  Sensitivity of the model</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#does-the-model-adequately-capture-the-dataposterior-predictive-checks"><i class="fa fa-check"></i><b>7.2.4</b>  Does the model adequately capture the data?–Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b>  Contrast coding</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding:  Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b>  Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b>  Cell means parameterization and  posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b>  Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The  hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The  <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor of four levels</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b>  Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b>  Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or  model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b>  Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts:  Monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b>  Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b>  Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the  intercept in  non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-6"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial  <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b>  Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b>  Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one  covariate</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a  group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-7"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan:  Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example:  Cloze probability with Stan with the  binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b>  Regression models in Stan</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first  linear regression in Stan: Does attentional load affect  pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b>  Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-8"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Hierarchical models and reparameterization </a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated  varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b>  Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-9"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b>  Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The  simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> An example where simulation-based calibration reveals a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues with and limitations of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-the-exponential-distribution-implemented-manually"><i class="fa fa-check"></i><b>12.3</b> Another  custom distribution: The exponential distribution  implemented manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-10"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b>  Meta-analysis and  measurement error models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b>  Measurement-error models</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-11"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#prior-predictive-vs.-posterior-predictive-model-comparison"><i class="fa fa-check"></i><b>14.1</b> Prior predictive vs. posterior predictive model comparison</a></li>
<li class="chapter" data-level="14.2" data-path="ch-comparison.html"><a href="ch-comparison.html#some-important-points-to-consider-when-comparing-models"><i class="fa fa-check"></i><b>14.2</b> Some important points to consider when comparing models</a></li>
<li class="chapter" data-level="14.3" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b>  Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b>  Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and  accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="15.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>15.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#sample-size-determination-using-bayes-factors"><i class="fa fa-check"></i><b>15.6</b> Sample size determination using Bayes factors</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#summary-12"><i class="fa fa-check"></i><b>15.7</b> Summary</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.8</b> Further reading</a></li>
<li class="chapter" data-level="15.9" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b>  Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#sec-issuesCV"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b>  PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-13"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to cognitive modeling</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#what-characterizes-a-computational-cognitive-model"><i class="fa fa-check"></i><b>17.1</b> What characterizes a computational cognitive model?</a></li>
<li class="chapter" data-level="17.2" data-path="ch-cogmod.html"><a href="ch-cogmod.html#some-advantages-of-taking-the-latent-variable-modeling-approach"><i class="fa fa-check"></i><b>17.2</b> Some advantages of taking the latent-variable modeling approach</a></li>
<li class="chapter" data-level="17.3" data-path="ch-cogmod.html"><a href="ch-cogmod.html#types-of-computational-cognitive-model"><i class="fa fa-check"></i><b>17.3</b> Types of computational cognitive model</a></li>
<li class="chapter" data-level="17.4" data-path="ch-cogmod.html"><a href="ch-cogmod.html#summary-14"><i class="fa fa-check"></i><b>17.4</b> Summary</a></li>
<li class="chapter" data-level="17.5" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling  multiple categorical responses</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A  hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-15"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
<li class="chapter" data-level="18.5" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A  multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A  hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-16"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with  contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-17"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="ch-closing.html"><a href="ch-closing.html"><i class="fa fa-check"></i><b>21</b> In closing</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-bf" class="section level1 hasAnchor" number="15">
<h1><span class="header-section-number">Chapter 15</span> Bayes factors<a href="ch-bf.html#ch-bf" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter is based on a longer manuscript available on arXiv: <span class="citation">Schad et al. (<a href="#ref-SchadEtAlBF" role="doc-biblioref">2021</a>)</span>; our terminology used here is based on the conventions used in that paper. A published version of the arXiv article appears in Schad, D. J., Nicenboim, B., Bürkner, P. C., Betancourt, M., and Vasishth, S. (2022). Workflow techniques for the robust use of Bayes factors. Psychological Methods. In this chapter, whenever we refer to the published version of the arXiv paper, we mean this article.</p>
<p>Bayesian approaches provide tools for different aspects of data analysis. A key contribution of Bayesian data analysis to cognitive science is that it furnishes probabilistic ways to quantify the evidence that data provide in support of one model or another. Models provide ways to implement  scientific hypotheses; as a consequence,  model comparison and  hypothesis testing are closely related. Bayesian hypothesis testing comparing any kind of hypotheses is implemented using  Bayes factors <span class="citation">(Rouder, Haaf, and Vandekerckhove <a href="#ref-rouder2018bayesian" role="doc-biblioref">2018</a>; Schönbrodt and Wagenmakers <a href="#ref-schonbrodt2018bayes" role="doc-biblioref">2018</a>; Wagenmakers et al. <a href="#ref-wagenmakers2010BayesianHypothesisTesting" role="doc-biblioref">2010</a>; Kass and Raftery <a href="#ref-kass1995bayes" role="doc-biblioref">1995</a>; Gronau et al. <a href="#ref-gronauTutorialBridgeSampling2017" role="doc-biblioref">2017</a>; Jeffreys <a href="#ref-jeffreys1939theory" role="doc-biblioref">1939</a>)</span>, which quantify evidence in favor of one statistical (or computational) model over another. This chapter will focus on Bayes factors as the way to compare models and to obtain evidence for (general) hypotheses.</p>
<p>There are subtleties associated with Bayes factors that are not widely appreciated. For example, the results of Bayes factor analyses are highly sensitive to and crucially depend on prior assumptions about model parameters (we will illustrate this below), which can vary between experiments/research problems and even differ subjectively between different researchers. Many authors use or recommend so-called default prior distributions, where the prior parameters are fixed, and are independent of the scientific problem in question <span class="citation">(Hammerly, Staub, and Dillon <a href="#ref-hammerly2019grammaticality" role="doc-biblioref">2019</a>; Navarro <a href="#ref-navarro2015learning" role="doc-biblioref">2015</a>)</span>. However,  default priors can result in an overly simplistic perspective on Bayesian hypothesis testing, and can be misleading. For this reason, even though leading experts in the use of Bayes factor, such as <span class="citation">Rouder et al. (<a href="#ref-rouder2009bayesian" role="doc-biblioref">2009</a>)</span>, often provide default priors for computing Bayes factors, they also make it clear that: “simply put, principled inference is a thoughtful process that cannot be performed by rigid adherence to defaults” <span class="citation">(Rouder et al. <a href="#ref-rouder2009bayesian" role="doc-biblioref">2009</a>, 235)</span>. However, this observation does not seem to have had much impact on how Bayes factors are used in fields like psychology and psycholinguistics; the use of default priors when computing Bayes factors seems to be widespread.</p>
<p>Given the key  influence of priors on Bayes factors,  defining priors becomes a central issue when using Bayes factors. The priors determine which models will be compared.</p>
<p>In this chapter, we demonstrate how Bayes factors should be used in practical settings in cognitive science. In doing so, we demonstrate the strength of this approach and some important pitfalls that researchers should be aware of.</p>
<div id="hypothesis-testing-using-the-bayes-factor" class="section level2 hasAnchor" number="15.1">
<h2><span class="header-section-number">15.1</span> Hypothesis testing using the Bayes factor<a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="marginal-likelihood" class="section level3 hasAnchor" number="15.1.1">
<h3><span class="header-section-number">15.1.1</span> Marginal likelihood<a href="ch-bf.html#marginal-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p> Bayes’ rule can be written with reference to a specific statistical model <span class="math inline">\(\mathcal{M}_1\)</span>.</p>
<p><span class="math display">\[\begin{equation}
p(\boldsymbol{\Theta} \mid \boldsymbol{y}, \mathcal{M}_1) = \frac{p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)}{p(\boldsymbol{y} \mid \mathcal{M}_1)}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol{y}\)</span> refers to the data and <span class="math inline">\(\boldsymbol{\Theta}\)</span> is a vector of parameters; for example, this vector could include the intercept, slope, and variance component in a linear regression model.</p>
<p>The denominator <span class="math inline">\(p(\boldsymbol{y} \mid \mathcal{M}_1)\)</span> is the  marginal likelihood, and is a single number that gives us the likelihood of the observed data <span class="math inline">\(\boldsymbol{y}\)</span> given the model <span class="math inline">\(\mathcal{M}_1\)</span> (and only in the discrete case, it gives us the probability of the observed data <span class="math inline">\(\boldsymbol{y}\)</span> given the model; see section <a href="ch-intro.html#sec-marginal">1.7</a>). Because in general it’s not a probability, it should be interpreted relative to another marginal likelihood (evaluated with the same <span class="math inline">\(\boldsymbol{y}\)</span>).</p>
<p>In  frequentist statistics, it’s also common to quantify evidence for the model by determining the maximum likelihood, that is, the likelihood of the data given the  best-fitting model parameter. Thus, the data is used twice: once for fitting the parameter, and then for evaluating the likelihood. Importantly, this inference completely hinges upon this best-fitting parameter to be a meaningful value that represents well what we know about the parameter, and doesn’t take the uncertainty of the estimates into account. Bayesian inference quantifies the uncertainty that is associated with a parameter; that is, one accepts that the knowledge about the parameter value is uncertain. Computing the marginal likelihood entails computing the likelihood given all plausible values for the model parameter.</p>
<p>One difficulty in the above equation showing Bayes’ rule is that the marginal likelihood <span class="math inline">\(p(\boldsymbol{y} \mid \mathcal{M}_1)\)</span> in the denominator cannot be easily computed in Bayes’ rule:</p>
<p><span class="math display">\[\begin{equation}
p(\boldsymbol{\Theta} \mid \boldsymbol{y}, \mathcal{M}_1)  = \frac{p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)}{p(\boldsymbol{y} \mid \mathcal{M}_1)}
\end{equation}\]</span></p>
<p>The marginal likelihood does not depend on the model parameters <span class="math inline">\(\Theta\)</span>; the parameters are “marginalized” or integrated out:</p>
<p><span class="math display" id="eq:marginall">\[\begin{equation}
p(\boldsymbol{y} \mid \mathcal{M}_1) = \int p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1) d \boldsymbol{\Theta}
\tag{15.1}
\end{equation}\]</span></p>
<p>The likelihood is evaluated for every possible parameter value, weighted by the  prior plausibility of the parameter values. The product <span class="math inline">\(p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)\)</span> is then summed up (that is what the integral does).</p>
<p>For this reason, the prior is as important as the likelihood. Equation <a href="ch-bf.html#eq:marginall">(15.1)</a> also looks almost identical to the  prior predictive distribution from section <a href="ch-compbda.html#sec-priorpred">3.3</a> (that is, the predictions that the model makes before seeing any data). The prior predictive distribution is repeated below for convenience:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p(\boldsymbol{y_{pred}}) &amp;= p(y_{pred_1},\dots,y_{pred_n})\\
&amp;= \int_{\boldsymbol{\Theta}} p(y_{pred_1}|\boldsymbol{\Theta})\cdot p(y_{pred_2}|\boldsymbol{\Theta})\cdots p(y_{pred_N}|\boldsymbol{\Theta}) p(\boldsymbol{\Theta}) \, d\boldsymbol{\Theta}
\end{aligned}
\end{equation}\]</span></p>
<p>However, while the prior predictive distribution describes possible observations, the marginal likelihood is evaluated on the actually observed data.</p>
<p>Let’s compute the Bayes factor for a very simple example case. We assume a study where we assess the number of “successes” observed in a fixed number of trials. For example, suppose that we have 80 “successes” out of 100 trials. A simple model of this data can be built by assuming, as we did in section <a href="ch-intro.html#sec-binomialcloze">1.4</a>, that the data are distributed according to a  binomial distribution.
In a binomial distribution, <span class="math inline">\(n\)</span> independent trials are performed, where the result of each experiment is either a “success” or “no success” with probability <span class="math inline">\(\theta\)</span>. The binomial distribution is the probability distribution of the  number of successes <span class="math inline">\(k\)</span> (number of “success” responses) in this situation for a given sample of experiments <span class="math inline">\(X\)</span>.</p>
<p>Suppose now that we have prior information about the probability parameter <span class="math inline">\(\theta\)</span>. As we explained in section <a href="ch-introBDA.html#sec-analytical">2.2</a>, a typical prior distribution for <span class="math inline">\(\theta\)</span> is a  beta distribution. The beta distribution defines a probability distribution on the interval <span class="math inline">\([0, 1]\)</span>, which is the interval on which the probability <span class="math inline">\(\theta\)</span> is defined. It has two parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, which determine the shape of the distribution. The prior parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> can be interpreted as the a priori number of “successes” versus “failures.” These could be based on previous evidence, or on the researcher’s beliefs, drawing on their domain knowledge <span class="citation">(O’Hagan et al. <a href="#ref-ohagan2006uncertain" role="doc-biblioref">2006</a>)</span>.</p>
<p>Here, to illustrate the calculation of the Bayes factor, we assume that the parameters of the beta distribution are <span class="math inline">\(a=4\)</span> and <span class="math inline">\(b=2\)</span>. As mentioned above, these parameters can be interpreted as representing “success” (<span class="math inline">\(4\)</span> prior observations representing success), and “no success” (<span class="math inline">\(2\)</span> prior observations representing “no success”). The resulting prior distribution is visualized in Figure <a href="ch-bf.html#fig:beta24">15.1</a>. A <span class="math inline">\(\mathit{Beta}(a=4,b=2)\)</span> prior on <span class="math inline">\(\theta\)</span> amounts to a  regularizing prior with some, but no clear prior evidence for more than 50% of success.</p>
<div class="figure"><span style="display:block;" id="fig:beta24"></span>
<img src="bookdown_files/figure-html/beta24-1.svg" alt="The beta distribution with parameters a = 4 and b = 2." width="672" />
<p class="caption">
FIGURE 15.1: The beta distribution with parameters a = 4 and b = 2.
</p>
</div>
<p>To compute the marginal likelihood, Equation <a href="ch-bf.html#eq:marginall">(15.1)</a> shows that we need to multiply the likelihood with the prior. The marginal likelihood is then the area under the curve, that is, the likelihood averaged across all possible values for the model parameter (the probability of success).</p>
<p>Based on this data, likelihood, and prior we can calculate the marginal likelihood, that is, this  area under the curve, in the following way using <code>R</code>:</p>
<div class="sourceCode" id="cb856"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb856-1"><a href="ch-bf.html#cb856-1" aria-hidden="true"></a><span class="co"># First we multiply the likelihood with the prior</span></span>
<span id="cb856-2"><a href="ch-bf.html#cb856-2" aria-hidden="true"></a>plik1 &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</span>
<span id="cb856-3"><a href="ch-bf.html#cb856-3" aria-hidden="true"></a>  <span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">80</span>, <span class="dt">size =</span> <span class="dv">100</span>, <span class="dt">prob =</span> theta) <span class="op">*</span></span>
<span id="cb856-4"><a href="ch-bf.html#cb856-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">dbeta</span>(<span class="dt">x =</span> theta, <span class="dt">shape1 =</span> <span class="dv">4</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>)</span>
<span id="cb856-5"><a href="ch-bf.html#cb856-5" aria-hidden="true"></a>}</span>
<span id="cb856-6"><a href="ch-bf.html#cb856-6" aria-hidden="true"></a><span class="co"># Then we integrate (compute the area under the curve):</span></span>
<span id="cb856-7"><a href="ch-bf.html#cb856-7" aria-hidden="true"></a>(MargLik1 &lt;-<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> plik1, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value)</span></code></pre></div>
<pre><code>## [1] 0.02</code></pre>
<p>One would prefer a model that gives a higher marginal likelihood, i.e., a higher likelihood of observing the data after integrating out the influence of the model parameter(s) (here: <span class="math inline">\(\theta\)</span>). A model will yield a high marginal likelihood if it makes a high proportion of good predictions <span class="citation">(i.e., model 2 in Figure <a href="ch-bf.html#fig:OccamFactor">15.2</a>; the figure is adapted from Bishop <a href="#ref-bishop2006pattern" role="doc-biblioref">2006</a>)</span>.
Model predictions are  normalized, that is, the total probability that models assign to different expected data patterns is the same for all models.
Models that are too flexible (model 3 in Figure <a href="ch-bf.html#fig:OccamFactor">15.2</a>) will divide their prior predictive probability density across all of their predictions. Such models can predict many different outcomes. Thus, they likely can also predict the actually observed outcome. However, due to the normalization, they cannot predict it with high probability, because they also predict all kinds of other outcomes. This is true for both models with priors that are too wide or for models with too many parameters. Bayesian model comparison inherently penalizes more complex models, a principle known as <em>Occam’s razor</em> <span class="citation">(MacKay <a href="#ref-mackay" role="doc-biblioref">2003</a>)</span>. This principle favors simpler explanations when multiple viable explanations are possible.</p>
<div class="figure"><span style="display:block;" id="fig:OccamFactor"></span>
<img src="images/OccamFactorBW.png" alt="Shown are the schematic marginal likelihoods that each of three models assigns to different possible data sets. The total probability each model assigns to the data is equal to one, i.e., the areas under the curves of all three models are the same. Model 1 (black), the low complexity model, assigns all the probability to a narrow range of possible data, and can predict these possible data sets with high likelihood. Model 3 (light grey) assigns its probability to a large range of different possible outcomes, but predicts each individual observed data set with low likelihood (high complexity model). Model 2 (dark grey) takes an intermediate position (intermediate complexity). The vertical dashed line (dark grey) illustrates where the actual empirically observed data fall. The data most support model 2, since this model predicts the data with highest likelihood. The figure is closely based on Figure 3.13 in Bishop (2006)." width="100%" />
<p class="caption">
FIGURE 15.2: Shown are the schematic marginal likelihoods that each of three models assigns to different possible data sets. The total probability each model assigns to the data is equal to one, i.e., the areas under the curves of all three models are the same. Model 1 (black), the low complexity model, assigns all the probability to a narrow range of possible data, and can predict these possible data sets with high likelihood. Model 3 (light grey) assigns its probability to a large range of different possible outcomes, but predicts each individual observed data set with low likelihood (high complexity model). Model 2 (dark grey) takes an intermediate position (intermediate complexity). The vertical dashed line (dark grey) illustrates where the actual empirically observed data fall. The data most support model 2, since this model predicts the data with highest likelihood. The figure is closely based on Figure 3.13 in Bishop (2006).
</p>
</div>
<p>By contrast, good models (Figure <a href="ch-bf.html#fig:OccamFactor">15.2</a>, model 2) will make very specific predictions, where the  specific predictions are consistent with the observed data. Here, all the  predictive probability density is located at the “location” where the observed data fall, and little probability density is located at other places, providing good support for the model. Of course, specific predictions can also be wrong, when expectations differ from what the observed data actually look like (Figure <a href="ch-bf.html#fig:OccamFactor">15.2</a>, model 1).</p>
<p>Having a natural Occam’s razor is good for posterior inference, i.e., for assessing how much (continuous) evidence there is for one model or another. However, it doesn’t necessarily imply good  decision making or  hypothesis testing, i.e., to make discrete decisions about which model explains the data best, or on which model to base further actions.</p>
<p>Here, we provide two examples of more flexible models. First, the following model assumes the same likelihood and the same distribution function for the prior. However, we assume a flat,  uninformative prior, with prior parameters <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = 1\)</span> (i.e., only one prior “success” and one prior “failure”), which provides more prior spread than the first model. Again, we can formulate our model as multiplying the likelihood with the prior, and integrate out the influence of the parameter <span class="math inline">\(\theta\)</span>:</p>
<div class="sourceCode" id="cb858"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb858-1"><a href="ch-bf.html#cb858-1" aria-hidden="true"></a>plik2 &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</span>
<span id="cb858-2"><a href="ch-bf.html#cb858-2" aria-hidden="true"></a>  <span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">80</span>, <span class="dt">size =</span> <span class="dv">100</span>, <span class="dt">prob =</span> theta) <span class="op">*</span></span>
<span id="cb858-3"><a href="ch-bf.html#cb858-3" aria-hidden="true"></a><span class="st">    </span><span class="kw">dbeta</span>(<span class="dt">x =</span> theta, <span class="dt">shape1 =</span> <span class="dv">1</span>, <span class="dt">shape2 =</span> <span class="dv">1</span>)</span>
<span id="cb858-4"><a href="ch-bf.html#cb858-4" aria-hidden="true"></a>}</span>
<span id="cb858-5"><a href="ch-bf.html#cb858-5" aria-hidden="true"></a>(MargLik2 &lt;-<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> plik2, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value)</span></code></pre></div>
<pre><code>## [1] 0.0099</code></pre>
<p>We can see that this second model is more flexible: due to the more spread-out prior, it is compatible with a larger range of possible observed data patterns. However, when we integrate out the <span class="math inline">\(\theta\)</span> parameter to obtain the marginal likelihood, we can see that this flexibility also comes with a cost: the model has a smaller marginal likelihood (<span class="math inline">\(0.0099\)</span>) than the first model (<span class="math inline">\(0.02\)</span>). Thus, on average (averaged across all possible values of <span class="math inline">\(\theta\)</span>) the second model performs worse in explaining the specific data that we observed compared to the first model, and has less support from the data.</p>
<p>A model might be more “complex” because it has a more spread-out prior, or alternatively because it has a more  complex likelihood function, which uses a larger number of parameters to explain the same data. Here we implement a third model, which assumes a more complex likelihood by using a  beta-binomial distribution. The beta-binomial distribution is similar to the binomial distribution, with one important difference: In the binomial distribution the probability of success <span class="math inline">\(\theta\)</span> is fixed across trials. In the beta-binomial distribution, the probability of success is fixed for each trial, but is drawn from a beta distribution across trials. Thus, <span class="math inline">\(\theta\)</span> can differ between trials. In the beta-binomial distribution, we thus assume that the likelihood function is a combination of a binomial distribution and a beta distribution of the probability <span class="math inline">\(\theta\)</span>, which yields:</p>
<p><span class="math display">\[\begin{equation}
p(X = k \mid a, b) = \frac{B(k+a, n-k+b)}{B(a,b)}
\end{equation}\]</span></p>
<p>What is important here is that this more complex distribution has two parameters (<span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>; rather than one, <span class="math inline">\(\theta\)</span>) to explain the same data. We assume  log-normally distributed priors for the <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> parameters, with location zero and scale <span class="math inline">\(100\)</span>. The likelihood of this combined beta-binomial distribution is given by the R-function  <code>dbbinom()</code> in the package <code>extraDistr</code>. We can now write down the likelihood times the priors (given as log-normal densities, <code>dlnorm()</code>), and integrate out the influence of the two free model parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> using numerical integration (applying  <code>integrate</code> twice):</p>
<div class="sourceCode" id="cb860"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb860-1"><a href="ch-bf.html#cb860-1" aria-hidden="true"></a>plik3 &lt;-<span class="st"> </span><span class="cf">function</span>(a, b) {</span>
<span id="cb860-2"><a href="ch-bf.html#cb860-2" aria-hidden="true"></a>  <span class="kw">dbbinom</span>(<span class="dt">x =</span> <span class="dv">80</span>, <span class="dt">size =</span> <span class="dv">100</span>, <span class="dt">alpha =</span> a, <span class="dt">beta =</span> b) <span class="op">*</span></span>
<span id="cb860-3"><a href="ch-bf.html#cb860-3" aria-hidden="true"></a><span class="st">    </span><span class="kw">dlnorm</span>(<span class="dt">x =</span> a, <span class="dt">meanlog =</span> <span class="dv">0</span>, <span class="dt">sdlog =</span> <span class="dv">100</span>) <span class="op">*</span></span>
<span id="cb860-4"><a href="ch-bf.html#cb860-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">dlnorm</span>(<span class="dt">x =</span> b, <span class="dt">meanlog =</span> <span class="dv">0</span>, <span class="dt">sdlog =</span> <span class="dv">100</span>)</span>
<span id="cb860-5"><a href="ch-bf.html#cb860-5" aria-hidden="true"></a>}</span>
<span id="cb860-6"><a href="ch-bf.html#cb860-6" aria-hidden="true"></a><span class="co"># Compute marginal likelihood by applying integrate twice</span></span>
<span id="cb860-7"><a href="ch-bf.html#cb860-7" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="cf">function</span>(b) {</span>
<span id="cb860-8"><a href="ch-bf.html#cb860-8" aria-hidden="true"></a>  <span class="kw">integrate</span>(<span class="cf">function</span>(a) <span class="kw">plik3</span>(a, b), <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value</span>
<span id="cb860-9"><a href="ch-bf.html#cb860-9" aria-hidden="true"></a>  }</span>
<span id="cb860-10"><a href="ch-bf.html#cb860-10" aria-hidden="true"></a><span class="co"># integrate requires a vectorized function:</span></span>
<span id="cb860-11"><a href="ch-bf.html#cb860-11" aria-hidden="true"></a>(MargLik3 &lt;-<span class="st"> </span><span class="kw">integrate</span>(<span class="kw">Vectorize</span>(f), <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value)</span></code></pre></div>
<pre><code>## [1] 0.00000707</code></pre>
<p>The results show that this third model has an even smaller marginal likelihood compared to the first two (<span class="math inline">\(0.00000707\)</span>). With its two parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, this third model has a lot of flexibility to explain a lot of different patterns of observed empirical results. However, again, this increased flexibility comes at a cost, and the simple pattern of observed data does not seem to require such complex model assumptions. The small value for the marginal likelihood indicates that this complex model has less support from the data.</p>
<p>That is, for this present simple example case, we would prefer model 1 over the other two, since it has the largest marginal likelihood (<span class="math inline">\(0.02\)</span>), and we would prefer model 2 over model 3, since the marginal likelihood of model 2 (<span class="math inline">\(0.0099\)</span>) is larger than that of model 3 (<span class="math inline">\(0.00000707\)</span>). The decision about which model is preferred is based on comparing the marginal likelihoods.</p>
</div>
<div id="bayes-factor" class="section level3 hasAnchor" number="15.1.2">
<h3><span class="header-section-number">15.1.2</span> Bayes factor<a href="ch-bf.html#bayes-factor" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The  Bayes factor is a measure of  relative evidence, the comparison of the  predictive performance of one model against another one. This comparison is a  ratio of marginal likelihoods:</p>
<p><span class="math display">\[\begin{equation}
BF_{12} = \frac{P(\boldsymbol{y} \mid \mathcal{M}_1)}{P(\boldsymbol{y} \mid \mathcal{M}_2)}
\end{equation}\]</span></p>
<p><span class="math inline">\(BF_{12}\)</span> indicates the extent to which the data are more likely under <span class="math inline">\(\mathcal{M}_1\)</span> over <span class="math inline">\(\mathcal{M}_2\)</span>, or in other words, the relative evidence that we have for <span class="math inline">\(\mathcal{M}_1\)</span> over <span class="math inline">\(\mathcal{M}_2\)</span>. Values larger than one indicate evidence in favor of <span class="math inline">\(\mathcal{M}_1\)</span>, smaller than one indicate evidence in favor of <span class="math inline">\(\mathcal{M}_2\)</span>, and values close to one indicate that the evidence is inconclusive. This model comparison does not depend on a specific parameter value. Instead, all possible prior parameter values are taken into account simultaneously. This is in contrast with the likelihood ratio test, as it is explained in Box <a href="ch-bf.html#thm:likR">15.1</a>.</p>
<div class="extra">
<div class="theorem">
<p><span id="thm:likR" class="theorem"><strong>Box 15.1  </strong></span><strong>The likelihood ratio vs Bayes Factor.</strong></p>
</div>
<p>The  likelihood ratio test is a very similar, but  frequentist, approach to model comparison and hypothesis testing, which also compares the likelihood for the data given two different models. We show this here to highlight the similarities and differences between frequentist and Bayesian hypothesis testing. In contrast to the Bayes factor, the likelihood ratio test depends on the “best” (i.e., the maximum likelihood) estimate for the model parameter(s), that is, the model parameter <span class="math inline">\(\theta\)</span> occurs on the right side of the semi-colon in the equation for each likelihood. (An aside: we do not use a  conditional statement, i.e., the vertical bar, when talking about likelihood in the frequentist context; instead, we use a semi-colon. This is because the statement <span class="math inline">\(f(y\mid \theta)\)</span> is a conditional statement, implying that <span class="math inline">\(\theta\)</span> has a probability density function associated with it; in the frequentist framework, parameters cannot have a pdf associated with them, they are assumed to have fixed, point values.)</p>
<p><span class="math display">\[\begin{equation}
LikRat = \frac{P(\boldsymbol{y} ; \boldsymbol{\hat{\Theta}_1}, \mathcal{M}_1)}{P(\boldsymbol{y} ; \boldsymbol{\hat{\Theta}_2}, \mathcal{M}_2)}
\end{equation}\]</span></p>
<p>That means that in the likelihood ratio test, each model is tested on its ability to explain the data using this “best” estimate for the model parameter (here, the  maximum likelihood estimate <span class="math inline">\(\hat{\theta}\)</span>). That is, the likelihood ratio test reduces the full range of possible parameter values to a  point value, leading to overfitting the model to the maximum likelihood estimate  (MLE). If the MLE badly misestimates the true value of the parameter (point value), due to  Type M error <span class="citation">(Gelman and Carlin <a href="#ref-gelmancarlin" role="doc-biblioref">2014</a>)</span>, we could end up with a “significant” effect that is just a consequence of this misestimation <span class="citation">(it will not be consistently replicable; see Vasishth et al. <a href="#ref-VasishthMertzenJaegerGelman2018" role="doc-biblioref">2018</a> for an example)</span>. By contrast, the Bayes factor involves range hypotheses, which are implemented via  integrals over the model parameter; that is, it uses marginal likelihoods that are averaged across all possible prior values of the model parameter(s). Thus, if, due to Type M error, the best point estimate (the MLE) for the model parameter(s) is not very representative of the possible values for the model parameter(s), then the Bayes factors will be superior to the frequentist likelihood ratio test (see exercise <a href="ch-bf.html#exr:bf-logn">15.2</a>). An additional difference, of course, is that Bayes factors rely on priors for estimating each model’s parameter(s), whereas the frequentist likelihood ratio test does not (and cannot) consider priors in the estimation of the best-fitting model parameter(s). As we show in this chapter, this has far-reaching consequences for Bayes factor-based model comparison.
</p>
</div>
<p>A scale (see Table <a href="ch-bf.html#tab:BFs">15.1</a>) has been proposed to interpret Bayes factors according to the strength of evidence in favor of one model (corresponding to some hypothesis) over another <span class="citation">(Jeffreys <a href="#ref-jeffreys1939theory" role="doc-biblioref">1939</a>)</span>; but this scale should not be regarded as a hard and fast rule with clear boundaries.</p>
<table>
<caption><span id="tab:BFs">TABLE 15.1: </span> The  Bayes factor scale as proposed by <span class="citation">Jeffreys (<a href="#ref-jeffreys1939theory" role="doc-biblioref">1939</a>)</span>. This scale should not be regarded as a hard and fast rule.</caption>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(BF_{12}\)</span></th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><span class="math inline">\(&gt;100\)</span></td>
<td align="left">Extreme evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(30-100\)</span></td>
<td align="left">Very strong evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(10-30\)</span></td>
<td align="left">Strong evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(3-10\)</span></td>
<td align="left">Moderate evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(1-3\)</span></td>
<td align="left">Anecdotal evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(1\)</span></td>
<td align="left">No evidence.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(\frac{1}{1}-\frac{1}{3}\)</span></td>
<td align="left">Anecdotal evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\frac{1}{3}-\frac{1}{10}\)</span></td>
<td align="left">Moderate evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(\frac{1}{10}-\frac{1}{30}\)</span></td>
<td align="left">Strong evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\frac{1}{30}-\frac{1}{100}\)</span></td>
<td align="left">Very strong evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(&lt;\frac{1}{100}\)</span></td>
<td align="left">Extreme evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
</tbody>
</table>
<p>So if we go back to our previous example, we can calculate <span class="math inline">\(BF_{12}\)</span>, <span class="math inline">\(BF_{13}\)</span>, and <span class="math inline">\(BF_{23}\)</span>. The subscript represents the order in which the models are compared; for example, <span class="math inline">\(BF_{21}\)</span> is simply <span class="math inline">\(\frac{1}{BF_{12}}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
BF_{12} = \frac{marginal \; likelihood \; model \; 1}{marginal \; likelihood \; model \; 2} = \frac{MargLik1}{MargLik2} = 2
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
BF_{13} = \frac{MargLik1}{MargLik3}=  2825.4
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
BF_{32} = \frac{MargLik3}{MargLik2} =  0.001 = \frac{1}{BF_{23}} =  \frac{1}{1399.9 }
\end{equation}\]</span></p>
<p>However, if we want to know, given the data <span class="math inline">\(y\)</span>, what the probability for model <span class="math inline">\(\mathcal{M}_1\)</span> is, or how much more probable model <span class="math inline">\(\mathcal{M}_1\)</span> is than model <span class="math inline">\(\mathcal{M}_2\)</span>, then we need the  prior odds, that is, we need to specify how probable <span class="math inline">\(\mathcal{M}_1\)</span> is compared to <span class="math inline">\(\mathcal{M}_2\)</span> <em>a priori</em>.</p>
<p><span class="math display">\[\begin{align}
\frac{p(\mathcal{M}_1 \mid y)}{p(\mathcal{M}_2 \mid y)} =&amp; \frac{p(\mathcal{M}_1)}{p(\mathcal{M}_2)} \times \frac{P(y \mid \mathcal{M}_1)}{P(y \mid \mathcal{M}_2)}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\text{Posterior odds}_{12} = &amp; \text{Prior odds}_{12} \times BF_{12}
\end{align}\]</span></p>
<p></p>
<p>The Bayes factor indicates the amount by which we should update our  relative belief between the two models in light of the data and priors.
However, <strong>the Bayes factor alone cannot tell us which one of the models is the most probable</strong>. Given our priors for the models and the Bayes factor, we can calculate the odds between the models.</p>
<p>Here we compute posterior model probabilities for the case where we compare two models against each other. However,  posterior model probabilities can also be computed for the more general case, where more than two models are considered:</p>
<p><span class="math display">\[\begin{equation}
p(\mathcal{M}_1 \mid \boldsymbol{y}) = \frac{p(\boldsymbol{y} \mid \mathcal{M}_1) p(\mathcal{M}_1)}{\sum_n p(\boldsymbol{y} \mid \mathcal{M}_n) p(\mathcal{M}_n)}
\end{equation}\]</span></p>
<p>For simplicity, we mostly constrain ourselves to two models. (However, the sensitivity analyses we carry out below compare more than two models.)</p>
<p>Bayes factors (and posterior model probabilities) tell us how much evidence the data (and priors) provide in favor of one model or another.
Put differently, they enable us to draw conclusions about the  model space, that is, to determine the degree to which each hypothesis agrees with the available data.</p>
<p>A completely different issue, however, is the question of how to perform (discrete) decisions based on continuous evidence. The question here is: which hypothesis should one choose to maximize utility? While Bayes factors have a clear rationale and justification in terms of the (continuous) evidence they provide, there is not a clear and direct mapping from inferences to how to perform decisions based on them. To derive decisions based on posterior model probabilities,  utility functions are needed. Indeed, the utility of different possible actions (i.e., to accept and act based on one hypothesis or another) can differ quite dramatically in different situations.
Erroneously rejecting a novel therapy could have significant negative consequences for a researcher attempting to implement life-saving treatment, but adopting the therapy incorrectly might have less of an impact.
By contrast, erroneously claiming a new discovery in fundamental research may have bad consequences (low utility), whereas erroneously missing a new  discovery claim may be less problematic if further evidence can be accumulated. Thus, Bayesian evidence (in the form of Bayes factors or posterior model probabilities) must be combined with utility functions in order to perform decisions based on them. For example, this could imply specifying the utility of a  true discovery and the utility of a  false discovery.  Calibration (i.e., simulations) can then be used to derive decisions that maximize overall utility.<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a></p>
<p>The question now is: how do we extend the Bayes factor calculation to models that we care about, i.e., to models that represent more realistic data analysis situations. In cognitive science, we typically fit fairly complex hierarchical models with many variance components. The major problem is that we won’t be able to calculate the marginal likelihood for hierarchical models (or any other complex model) either analytically or just using the R functions shown above. There are two very useful methods for calculating the Bayes factor for complex models: the  Savage–Dickey density ratio method <span class="citation">(Dickey and Lientz <a href="#ref-DickeyLientz1970" role="doc-biblioref">1970</a>; Wagenmakers et al. <a href="#ref-wagenmakers2010BayesianHypothesisTesting" role="doc-biblioref">2010</a>)</span> and  bridge sampling <span class="citation">(Bennett <a href="#ref-bennettEfficientEstimationFree1976" role="doc-biblioref">1976</a>; Meng and Wong <a href="#ref-mengSimulatingRatiosNormalizing1996" role="doc-biblioref">1996</a>)</span>.</p>
<p>The Savage–Dickey density ratio method has the advantage that is a straightforward way to compute the Bayes factor, but it is limited to nested models, i.e., models with point null hypotheses (e.g., <span class="math inline">\(\mu = 0\)</span>). At the time of writing, the implementation of the Savage–Dickey method in <code>brms</code> can be unstable, especially in cases where the posterior is far away from zero. Despite this instability in some special cases, in many situations it can make sense to use this approach instead of bridge sampling <span class="citation">(Oberauer, Musfeld, and Aust <a href="#ref-Aust2024" role="doc-biblioref">2024</a>)</span>.</p>
<p>Bridge sampling is in general a much more powerful method, but it requires many more  effective samples than what is normally required for parameter estimation, and it may be difficult to obtain stable Bayes factors with very large data sets. In this chapter, we will primarily use bridge sampling from the  <code>bridgesampling</code> package <span class="citation">(Gronau et al. <a href="#ref-gronauTutorialBridgeSampling2017" role="doc-biblioref">2017</a>; Gronau, Singmann, and Wagenmakers <a href="#ref-gronauBridgesamplingPackageEstimating2017" role="doc-biblioref">2017</a>)</span> with the function  <code>bayes_factor()</code> to calculate the Bayes factor in the first examples.</p>
</div>
</div>
<div id="sec-N400BF" class="section level2 hasAnchor" number="15.2">
<h2><span class="header-section-number">15.2</span> Examining the N400 effect with Bayes factor<a href="ch-bf.html#sec-N400BF" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In section <a href="ch-hierarchical.html#sec-N400hierarchical">5.2</a> we estimated the effect of  cloze probability on the  N400 average signal. This yielded a posterior credible interval for the effect of cloze probability. It is certainly possible to check whether, e.g., the 95% posterior credible interval overlaps with zero or not. However, such estimation cannot really answer the following question: How much evidence do we have in support for an effect? A 95% credible interval that doesn’t overlap with zero, or a high probability mass away from zero may hint that the predictor may be needed to explain the data, but it is not really answering how much evidence we have in favor of an effect <span class="citation">(see Box <a href="ch-comparison.html#thm:null">14.1</a>, and for discussion, see Royall <a href="#ref-Royall" role="doc-biblioref">1997</a>; Wagenmakers et al. <a href="#ref-wagenmakersPrinciplePredictiveIrrelevance2020" role="doc-biblioref">2020</a>; Rouder, Haaf, and Vandekerckhove <a href="#ref-rouder2018bayesian" role="doc-biblioref">2018</a>)</span>. The Bayes factor answers this question about the evidence in favor of an effect by explicitly conducting a model comparison. We will compare a model that assumes the presence of an effect, with a null model that assumes no effect.</p>
<p>As we saw before, the Bayes factor is highly sensitive to the priors. In the example presented above, both models are identical except for the effect of interest, <span class="math inline">\(\beta\)</span>, and so the prior on this parameter will play a major role in the calculation of the Bayes factor.</p>
<p>Next, we will run a hierarchical model which includes group-level (or random) intercepts and slopes by items and by subjects. We will use regularizing priors on all the parameters–this speeds up computation and implies realistic expectations about the parameters. However, the  prior on <span class="math inline">\(\beta\)</span> will be crucial for the calculation of the Bayes factor.</p>
<p>One possible way to build a good prior for the parameter <span class="math inline">\(\beta\)</span>, which estimates the influence of cloze probability, is as follows (see chapter <a href="ch-priors.html#ch-priors">6</a> for a more detailed discussion on prior selection). The reasoning below is based on domain knowledge; but there is room for differences of opinion here. In a realistic data analysis situation, we would carry out a sensitivity analysis using a range of priors to determine the extent of influence of the priors.</p>
<ol style="list-style-type: decimal">
<li>One may want to be agnostic regarding the direction of the effect; that means that we will  center the prior of <span class="math inline">\(\beta\)</span> on zero by specifying that the mean of the prior distribution is zero. However, we are still not sure about the variance of the prior on <span class="math inline">\(\beta\)</span>.</li>
<li>One would need to know a bit about the variation on the dependent variable that we are analyzing. After re-analyzing the data from a couple of EEG experiments available from osf.io, we can say that for N400 averages, the standard deviation of the signal is between 8-15 microvolts <span class="citation">(Nicenboim, Vasishth, and Rösler <a href="#ref-NicenboimPreactivation2019" role="doc-biblioref">2020</a>)</span>.</li>
<li>Based on published estimates of effects in psycholinguistics, we can conclude that they are generally rather small, often representing between 5%-30% of the standard deviation of the dependent variable.</li>
<li>The effect of noun predictability on the N400 is one of the most reliable and strongest effects in neurolinguistics (together with the P600 that might even be stronger), and the slope <span class="math inline">\(\beta\)</span> represents the average change in voltage when moving from a cloze probability of zero to one–the strongest prediction effect.</li>
</ol>
<p>An additional way to obtain good priors is to perform prior predictive checks <span class="citation">(Schad, Betancourt, and Vasishth <a href="#ref-schad2020toward" role="doc-biblioref">2020</a>, also see chapter <a href="ch-workflow.html#ch-workflow">7</a>, which presents a principled Bayesian workflow)</span>. Here, the idea is to simulate data from the model and the priors, and then to analyze the simulated data using summary statistics. For example, it would be possible to compute the summary statistic of the difference in the N400 between high versus low cloze probability. The simulations would yield a distribution of differences. Arguably, this distribution of differences, that is, the data analyses of the simulated data, are much easier to judge for plausibility than the prior parameters specifying prior distributions. That is, we might find it easier to judge whether a difference in voltage between high and low cloze probability is plausible rather than judging the parameters of the model. For reasons of brevity, we skip this step here.</p>
<p>Instead, we will start with the prior <span class="math inline">\(\beta \sim \mathit{Normal}(0,5)\)</span> (since 5 microvolts is roughly 30% of 15, which is the upper bound of the expected standard deviation of the EEG signal).</p>
<div class="sourceCode" id="cb862"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb862-1"><a href="ch-bf.html#cb862-1" aria-hidden="true"></a>priors1 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb862-2"><a href="ch-bf.html#cb862-2" aria-hidden="true"></a>             <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="dt">class =</span> b),</span>
<span id="cb862-3"><a href="ch-bf.html#cb862-3" aria-hidden="true"></a>             <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">10</span>, <span class="dv">5</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb862-4"><a href="ch-bf.html#cb862-4" aria-hidden="true"></a>             <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sd),</span>
<span id="cb862-5"><a href="ch-bf.html#cb862-5" aria-hidden="true"></a>             <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">4</span>), <span class="dt">class =</span> cor))</span></code></pre></div>
<p>We load the data set on N400 amplitudes, which has data on cloze probabilities <span class="citation">(Nieuwland et al. <a href="#ref-nieuwlandLargescaleReplicationStudy2018" role="doc-biblioref">2018</a>)</span>. The cloze probability measure is mean-centered to make the intercept and the random intercepts easier to interpret (i.e., after centering, they represent the grand mean and the average variability around the grand mean across subjects or items).</p>
<div class="sourceCode" id="cb863"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb863-1"><a href="ch-bf.html#cb863-1" aria-hidden="true"></a><span class="kw">data</span>(df_eeg)</span>
<span id="cb863-2"><a href="ch-bf.html#cb863-2" aria-hidden="true"></a>df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span></span>
<span id="cb863-3"><a href="ch-bf.html#cb863-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cloze =</span> cloze <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cloze))</span></code></pre></div>
<p>A large number of effective samples will be needed to be able to get stable estimates of the Bayes factor with bridge sampling. For this reason a large number of sampling iterations (<code>n = 20000</code>) is specified. The parameter <code>adapt_delta</code> is set to <span class="math inline">\(0.9\)</span> to ensure that the posterior sampler is working correctly. For Bayes factors analyses, it’s necessary to set the argument  <code>save_pars = save_pars(all = TRUE)</code>. This setting is a precondition for performing bridge sampling to compute the Bayes factor.</p>
<div class="sourceCode" id="cb864"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb864-1"><a href="ch-bf.html#cb864-1" aria-hidden="true"></a>fit_N400_h_linear &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span></span>
<span id="cb864-2"><a href="ch-bf.html#cb864-2" aria-hidden="true"></a><span class="st">                           </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</span>
<span id="cb864-3"><a href="ch-bf.html#cb864-3" aria-hidden="true"></a>                         <span class="dt">prior =</span> priors1,</span>
<span id="cb864-4"><a href="ch-bf.html#cb864-4" aria-hidden="true"></a>                         <span class="dt">warmup =</span> <span class="dv">2000</span>,</span>
<span id="cb864-5"><a href="ch-bf.html#cb864-5" aria-hidden="true"></a>                         <span class="dt">iter =</span> <span class="dv">20000</span>,</span>
<span id="cb864-6"><a href="ch-bf.html#cb864-6" aria-hidden="true"></a>                         <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</span>
<span id="cb864-7"><a href="ch-bf.html#cb864-7" aria-hidden="true"></a>                         <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</span>
<span id="cb864-8"><a href="ch-bf.html#cb864-8" aria-hidden="true"></a>                         <span class="dt">data =</span> df_eeg)</span></code></pre></div>
<p>Next, take a look at the population-level (or fixed) effects from the Bayesian modeling.</p>
<div class="sourceCode" id="cb865"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb865-1"><a href="ch-bf.html#cb865-1" aria-hidden="true"></a><span class="kw">fixef</span>(fit_N400_h_linear)</span></code></pre></div>
<pre><code>##           Estimate Est.Error Q2.5 Q97.5
## Intercept     3.65      0.45 2.77  4.54
## c_cloze       2.33      0.64 1.05  3.58</code></pre>
<p>We can now take a look at the estimates and at the credible intervals. The effect of cloze probability (<code>c_cloze</code>) is <span class="math inline">\(2.33\)</span> microvolts with a 95% credible interval ranging from <span class="math inline">\(1.05\)</span> to <span class="math inline">\(3.58\)</span> microvolts. While this provides an initial hint that highly probable words may elicit a stronger N400 compared to low probable words, by just looking at the posterior there is no way to quantify evidence for the question whether this effect is different from zero. Model comparison is needed to answer this question.</p>
<p>To this end, we run the model again, now without the parameter of interest, i.e., the null model. This is a model where our prior for <span class="math inline">\(\beta\)</span> is that it is exactly zero.</p>
<div class="sourceCode" id="cb867"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb867-1"><a href="ch-bf.html#cb867-1" aria-hidden="true"></a>fit_N400_h_null &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span></span>
<span id="cb867-2"><a href="ch-bf.html#cb867-2" aria-hidden="true"></a><span class="st">                         </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</span>
<span id="cb867-3"><a href="ch-bf.html#cb867-3" aria-hidden="true"></a>                       <span class="dt">prior =</span> priors1[priors1<span class="op">$</span>class <span class="op">!=</span><span class="st"> &quot;b&quot;</span>, ],</span>
<span id="cb867-4"><a href="ch-bf.html#cb867-4" aria-hidden="true"></a>                       <span class="dt">warmup =</span> <span class="dv">2000</span>,</span>
<span id="cb867-5"><a href="ch-bf.html#cb867-5" aria-hidden="true"></a>                       <span class="dt">iter =</span> <span class="dv">20000</span>,</span>
<span id="cb867-6"><a href="ch-bf.html#cb867-6" aria-hidden="true"></a>                       <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</span>
<span id="cb867-7"><a href="ch-bf.html#cb867-7" aria-hidden="true"></a>                       <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</span>
<span id="cb867-8"><a href="ch-bf.html#cb867-8" aria-hidden="true"></a>                       <span class="dt">data =</span> df_eeg)</span></code></pre></div>
<p>Now everything is ready to compute the log  marginal likelihood, that is, the likelihood of the data given the model, after integrating out the model parameters. In the toy examples shown above, we had used the R-function <code>integrate()</code> to perform this integration. This is not possible for the more realistic and more complex models that are considered here because the integrals that have to be solved are too high-dimensional and complex for these simple functions to do the job. Instead, a standard approach to deal with realistic complex models is to use  bridge sampling <span class="citation">(Gronau et al. <a href="#ref-gronauTutorialBridgeSampling2017" role="doc-biblioref">2017</a>; Gronau, Singmann, and Wagenmakers <a href="#ref-gronauBridgesamplingPackageEstimating2017" role="doc-biblioref">2017</a>)</span>. We perform this integration using the function  <code>bridge_sampler()</code> for each of the two models:</p>
<div class="sourceCode" id="cb868"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb868-1"><a href="ch-bf.html#cb868-1" aria-hidden="true"></a>margLogLik_linear &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_N400_h_linear, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span>
<span id="cb868-2"><a href="ch-bf.html#cb868-2" aria-hidden="true"></a>margLogLik_null &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_N400_h_null, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>This gives us the marginal log likelihoods for each of the models. From these, we can compute the Bayes factors. The  <code>bayes_factor()</code> function is a convenient tool for calculating the Bayes factor.</p>
<div class="sourceCode" id="cb869"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb869-1"><a href="ch-bf.html#cb869-1" aria-hidden="true"></a>(BF_ln &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(margLogLik_linear, margLogLik_null))</span></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 50.96782</code></pre>
<p>Alternatively, the Bayes factor can be computed manually as well. First, compute the difference in marginal log likelihoods, then transform this difference in log likelihoods to the likelihood scale (using <code>exp()</code>). A difference in the exponential scale is a ratio: <span class="math inline">\(exp(a-b) = exp(a)/exp(b)\)</span>. This computation yields the Bayes factor. However, the values <code>exp(ml1)</code> and <code>exp(ml2)</code> are too small to be represented accurately by R. Therefore, for numerical reasons, it is important to take the difference first and only then compute the exponential <span class="math inline">\(exp(a-b)\)</span>, i.e., <code>exp(margLogLik_linear$logml - margLogLik_null$logml)</code>, which yields the same result as the <code>bayes_factor()</code> command.</p>
<div class="sourceCode" id="cb871"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb871-1"><a href="ch-bf.html#cb871-1" aria-hidden="true"></a><span class="kw">exp</span>(margLogLik_linear<span class="op">$</span>logml <span class="op">-</span><span class="st"> </span>margLogLik_null<span class="op">$</span>logml)</span></code></pre></div>
<pre><code>## [1] 51</code></pre>
<p>The Bayes factor is quite large in this example, and furnishes strong evidence for the alternative model, which includes a coefficient representing the effect of cloze probability. That is, under the criteria shown in Table <a href="ch-bf.html#tab:BFs">15.1</a>, the Bayes factor furnishes strong evidence for an effect of cloze probability.</p>
<p>In this example, there was good prior information about the model parameter <span class="math inline">\(\beta\)</span>.
What transpires, though, if we are unsure of the prior for the model parameter? Because our prior for <span class="math inline">\(\beta\)</span> is inappropriate, it is possible that we will compare the null model with an extremely “bad” alternative model.</p>
<p>For example, assuming that we do not know much about N400 effects, or that we do not want to make strong assumptions, we might be inclined to use an  uninformative prior. For example, these could look as follows (where all the priors except for <code>b</code> remain unchanged):</p>
<div class="sourceCode" id="cb873"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb873-1"><a href="ch-bf.html#cb873-1" aria-hidden="true"></a>priors_vague &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb873-2"><a href="ch-bf.html#cb873-2" aria-hidden="true"></a>                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">500</span>), <span class="dt">class =</span> b),</span>
<span id="cb873-3"><a href="ch-bf.html#cb873-3" aria-hidden="true"></a>                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">10</span>, <span class="dv">5</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb873-4"><a href="ch-bf.html#cb873-4" aria-hidden="true"></a>                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sd),</span>
<span id="cb873-5"><a href="ch-bf.html#cb873-5" aria-hidden="true"></a>                  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">4</span>), <span class="dt">class =</span> cor))</span></code></pre></div>
<p>We can use these uninformative priors in the Bayesian model:</p>
<div class="sourceCode" id="cb874"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb874-1"><a href="ch-bf.html#cb874-1" aria-hidden="true"></a>fit_N400_h_linear_vague &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span></span>
<span id="cb874-2"><a href="ch-bf.html#cb874-2" aria-hidden="true"></a><span class="st">                                 </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</span>
<span id="cb874-3"><a href="ch-bf.html#cb874-3" aria-hidden="true"></a>                               <span class="dt">prior =</span> priors_vague,</span>
<span id="cb874-4"><a href="ch-bf.html#cb874-4" aria-hidden="true"></a>                               <span class="dt">warmup =</span> <span class="dv">2000</span>,</span>
<span id="cb874-5"><a href="ch-bf.html#cb874-5" aria-hidden="true"></a>                               <span class="dt">iter =</span> <span class="dv">20000</span>,</span>
<span id="cb874-6"><a href="ch-bf.html#cb874-6" aria-hidden="true"></a>                               <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</span>
<span id="cb874-7"><a href="ch-bf.html#cb874-7" aria-hidden="true"></a>                               <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</span>
<span id="cb874-8"><a href="ch-bf.html#cb874-8" aria-hidden="true"></a>                               <span class="dt">data =</span> df_eeg)</span></code></pre></div>
<p>Interestingly, we can still estimate the effect of cloze probability fairly well:</p>
<div class="sourceCode" id="cb875"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb875-1"><a href="ch-bf.html#cb875-1" aria-hidden="true"></a><span class="kw">posterior_summary</span>(fit_N400_h_linear_vague, <span class="dt">variable =</span> <span class="st">&quot;b_c_cloze&quot;</span>)</span></code></pre></div>
<pre><code>##           Estimate Est.Error Q2.5 Q97.5
## b_c_cloze     2.37     0.646 1.08  3.63</code></pre>
<p>Next, we again perform the bridge sampling for the alternative model.</p>
<div class="sourceCode" id="cb877"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb877-1"><a href="ch-bf.html#cb877-1" aria-hidden="true"></a>margLogLik_linear_vague &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_N400_h_linear_vague,</span>
<span id="cb877-2"><a href="ch-bf.html#cb877-2" aria-hidden="true"></a>                                          <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>We compute the Bayes factor for the alternative over the null model, <span class="math inline">\(BF_{10}\)</span>:</p>
<div class="sourceCode" id="cb878"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb878-1"><a href="ch-bf.html#cb878-1" aria-hidden="true"></a>(BF_lnVague &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(margLogLik_linear_vague,</span>
<span id="cb878-2"><a href="ch-bf.html#cb878-2" aria-hidden="true"></a>                            margLogLik_null))</span></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 0.56000</code></pre>
<p>This is easier to read as the evidence for null model over the alternative:</p>
<div class="sourceCode" id="cb880"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb880-1"><a href="ch-bf.html#cb880-1" aria-hidden="true"></a><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>BF_lnVague[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 1.79</code></pre>
<p>The result is inconclusive: there is no evidence in favor of or against the effect of cloze probability. The reason for that is that priors are never uninformative when it comes to Bayes factors. The  wide prior specifies that both very small and very large effect sizes are possible (with some considerable probability), but there is relatively little evidence in the data for such large effect sizes.</p>
<p>The above example is related to a criticism of Bayes factors by Uri Simonsohn, that Bayes factors can provide evidence in favor of the null and against a very specific alternative model, when the researchers only know the direction of the effect (see <a href="https://datacolada.org/78a" class="uri">https://datacolada.org/78a</a>). This can happen when an uninformative prior is used.</p>
<p>One way to overcome this problem is to actually try to learn about the effect size that we are investigating. This can be done by first running an exploratory experiment and analysis without computing any Bayes factor, and then use the posterior distribution derived from this first experiment to calibrate the priors for the next confirmatory experiment where we do use the Bayes factor <span class="citation">(see Verhagen and Wagenmakers <a href="#ref-verhagenBayesianTestsQuantify2014" role="doc-biblioref">2014</a> for a Bayes Factor test calibrated to investigate replication success)</span>.</p>
<p>Another possibility is to examine a lot of different alternative models, where each model uses different prior assumptions.
In this manner, the degree to which the Bayes factor results rely on or are sensitive to the prior assumptions can be examined.
This is an instance of a sensitivity analysis. Recall that the model is the likelihood <em>and</em> the priors. We can therefore compare models that only differ in the prior <span class="citation">(for an example involving EEG and predictability effects, see Nicenboim, Vasishth, and Rösler <a href="#ref-NicenboimPreactivation2019" role="doc-biblioref">2020</a>)</span>.</p>
<div id="sensitivity-analysis-1" class="section level3 hasAnchor" number="15.2.1">
<h3><span class="header-section-number">15.2.1</span> Sensitivity analysis<a href="ch-bf.html#sensitivity-analysis-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we perform a  sensitivtiy analysis by examining Bayes factors for several models. Each model has the same likelihood but a different prior for <span class="math inline">\(\beta\)</span>. For all of the priors we assume a normal distribution with a mean of zero. Assuming a mean of zero asserts that we do not make any assumption a priori about the direction of the effect. The standard deviations of the various priors vary from one another.
That is, what differs is the amount of uncertainty about the  effect size that we allow for in the prior.
While a small standard deviation indicates that we expect the effect to be not very large, a large standard deviation permits very large effect sizes.
Although a model with a wide prior (i.e., large standard deviation) also allocates prior probability to small effect sizes, it allocates much less probability to small effect sizes compared to a model with a narrow prior. Thus, if the effect size is in reality small, then a model with a narrow prior (small standard deviation) will have a better chance of detecting the effect.</p>
<p>Next, we try out a range of standard deviations, ranging from 1 to a much wider prior that has a standard deviation of 100. In practice, for the experiment method we are discussing here, it would not be a good idea to define very large standard deviations such as 100 microvolts, since they imply unrealistically large effect sizes. However, we include such a large value here just for illustration. Such a sensitivity analysis takes a very long time: here, we are running 11 models, where each model involves a lot of iterations to obtain stable Bayes factor estimates.</p>
<div class="sourceCode" id="cb882"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb882-1"><a href="ch-bf.html#cb882-1" aria-hidden="true"></a>prior_sd &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dv">2</span>, <span class="fl">2.5</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">50</span>, <span class="dv">100</span>)</span>
<span id="cb882-2"><a href="ch-bf.html#cb882-2" aria-hidden="true"></a>BF &lt;-<span class="st"> </span><span class="kw">c</span>()</span>
<span id="cb882-3"><a href="ch-bf.html#cb882-3" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(prior_sd)) {</span>
<span id="cb882-4"><a href="ch-bf.html#cb882-4" aria-hidden="true"></a>  psd &lt;-<span class="st"> </span>prior_sd[i]</span>
<span id="cb882-5"><a href="ch-bf.html#cb882-5" aria-hidden="true"></a>  <span class="co"># for each prior we fit the model</span></span>
<span id="cb882-6"><a href="ch-bf.html#cb882-6" aria-hidden="true"></a>  fit &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</span>
<span id="cb882-7"><a href="ch-bf.html#cb882-7" aria-hidden="true"></a>    <span class="dt">prior =</span></span>
<span id="cb882-8"><a href="ch-bf.html#cb882-8" aria-hidden="true"></a>      <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb882-9"><a href="ch-bf.html#cb882-9" aria-hidden="true"></a>        <span class="kw">set_prior</span>(<span class="kw">paste0</span>(<span class="st">&quot;normal(0,&quot;</span>, psd, <span class="st">&quot;)&quot;</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb882-10"><a href="ch-bf.html#cb882-10" aria-hidden="true"></a>        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">10</span>, <span class="dv">5</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb882-11"><a href="ch-bf.html#cb882-11" aria-hidden="true"></a>        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sd),</span>
<span id="cb882-12"><a href="ch-bf.html#cb882-12" aria-hidden="true"></a>        <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">4</span>), <span class="dt">class =</span> cor)),</span>
<span id="cb882-13"><a href="ch-bf.html#cb882-13" aria-hidden="true"></a>    <span class="dt">warmup =</span> <span class="dv">2000</span>,</span>
<span id="cb882-14"><a href="ch-bf.html#cb882-14" aria-hidden="true"></a>    <span class="dt">iter =</span> <span class="dv">20000</span>,</span>
<span id="cb882-15"><a href="ch-bf.html#cb882-15" aria-hidden="true"></a>    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</span>
<span id="cb882-16"><a href="ch-bf.html#cb882-16" aria-hidden="true"></a>    <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</span>
<span id="cb882-17"><a href="ch-bf.html#cb882-17" aria-hidden="true"></a>    <span class="dt">data =</span> df_eeg)</span>
<span id="cb882-18"><a href="ch-bf.html#cb882-18" aria-hidden="true"></a>  <span class="co"># for each model we run a brigde sampler</span></span>
<span id="cb882-19"><a href="ch-bf.html#cb882-19" aria-hidden="true"></a>  lml_linear_beta &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span>
<span id="cb882-20"><a href="ch-bf.html#cb882-20" aria-hidden="true"></a>  <span class="co"># we store the Bayes factor compared to the null model</span></span>
<span id="cb882-21"><a href="ch-bf.html#cb882-21" aria-hidden="true"></a>  BF &lt;-<span class="st"> </span><span class="kw">c</span>(BF, <span class="kw">bayes_factor</span>(lml_linear_beta, lml_null)<span class="op">$</span>bf)</span>
<span id="cb882-22"><a href="ch-bf.html#cb882-22" aria-hidden="true"></a>}</span>
<span id="cb882-23"><a href="ch-bf.html#cb882-23" aria-hidden="true"></a>BFs &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">beta_sd =</span> prior_sd, BF)</span></code></pre></div>
<p>For each model, we run bridge sampling and we compute the Bayes factor of the model against our baseline or null model, which does not contain a population-level effect of cloze probability (<span class="math inline">\(BF_{10}\)</span>). Next, we need a way to visualize all the Bayes factors. We plot them in Figure <a href="ch-bf.html#fig:BFpriorsX">15.3</a> as a function of the prior width.</p>
<div class="figure"><span style="display:block;" id="fig:BFpriorsX"></span>
<img src="bookdown_files/figure-html/BFpriorsX-1.svg" alt="Prior sensitivity analysis for the Bayes factor." width="672" />
<p class="caption">
FIGURE 15.3: Prior sensitivity analysis for the Bayes factor.
</p>
</div>
<p></p>
<p>This figure clearly shows that the Bayes factor provides evidence for the alternative model; that is, it provides evidence that the population-level (or fixed) effect cloze probability is needed to explain the data. This can be seen as the Bayes factor is quite large for a range of different values for the prior standard deviation. The Bayes factor is largest for a prior standard deviation of <span class="math inline">\(2.5\)</span>, suggesting a rather small size of the effect of cloze probability. If we assume gigantic effect sizes a priori (e.g., standard deviations of 50 or 100), then the evidence for the alternative model is weaker. Conceptually, the data do not fully support such big effect sizes, but start to favor the null model relatively more, when such big effect sizes are tested against the null. Overall, we can conclude that the data provide evidence for a not too large but robust influence of cloze probability on the N400 amplitude.</p>
</div>
<div id="sec-BFnonnested" class="section level3 hasAnchor" number="15.2.2">
<h3><span class="header-section-number">15.2.2</span>  Non-nested models<a href="ch-bf.html#sec-BFnonnested" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One important advantage of Bayes factors is that they can be used to compare models that are not nested. In nested models, the simpler model is a special case of the more complex and general model. For example, our previous model of cloze probability was a general model, allowing different influences of cloze probability on the N400. We compared this to a simpler, more specific null model, where the influence of cloze probability was not included, which means that the regression coefficient (population level or fixed effect) for cloze probability was assumed to be set to zero. Such nested models can be also compared using frequentist methods such as the likelihood ratio test (ANOVA).</p>
<p>By contrast, the Bayes factor also makes it possible to compare non-nested models. An example of a non-nested model would be a case where we log-transform the cloze probability variable before using it as a predictor. A model with log cloze probability as a predictor is not a special case of a model with linear cloze probability as predictor. These are just different, alternative models. With Bayes factors, we can compare these non-nested models with each other to determine which receives more evidence from the data. (For clarity: one can only apply model comparison if the same dependent variables is modeled. Hence, we can use the Bayes factor to compare a model containing log-transformed predictors with a model containing untransformed predictors, but we cannot compare a model using an untransformed dependent variable with a model using a log-transformed dependent variable.)</p>
<p>To do so, we first log-transform the cloze probability variable. Some cloze probabilities in the data set are equal to zero. This creates a problem when taking logs, since the log of zero is minus infinity, a value that we cannot use. We are going to overcome this problem by  “smoothing” the cloze probability in this example. We use  additive smoothing <span class="citation">(also called Laplace or  Lidstone smoothing; Lidstone <a href="#ref-Lidstone1920" role="doc-biblioref">1920</a>; Chen and Goodman <a href="#ref-ChenGoodman1999" role="doc-biblioref">1999</a>)</span> with pseudocounts set to one, this means that the smoothed probability is calculated as the number of responses with a given gender plus one divided by the total number of responses plus two.</p>
<div class="sourceCode" id="cb883"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb883-1"><a href="ch-bf.html#cb883-1" aria-hidden="true"></a>df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span></span>
<span id="cb883-2"><a href="ch-bf.html#cb883-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">scloze =</span> (cloze_ans <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(N <span class="op">+</span><span class="st"> </span><span class="dv">2</span>),</span>
<span id="cb883-3"><a href="ch-bf.html#cb883-3" aria-hidden="true"></a>         <span class="dt">c_logscloze =</span> <span class="kw">log</span>(scloze) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(<span class="kw">log</span>(scloze)))</span></code></pre></div>
<p>Next, we center the predictor variable, and we scale it to the same standard deviation as the linear cloze probabilities. To implement this scaling, first divide the centered smoothed log cloze probability variable by its standard deviation (effectively creating <span class="math inline">\(z\)</span>-scaled values). As a next step, multiply the <span class="math inline">\(z\)</span>-scaled values by the standard deviation of the non-transformed cloze probability variable. This way, both predictors (log cloze and cloze) have the same standard deviation. We therefore expect them to have a similar impact on the N400. As a result of this transformation, the same priors can be used for both variables (given that we currently have no specific information about the effect of log cloze probability versus linear cloze probability):</p>
<!--TO-DO: explain here the scaling-->
<!-- If you don't scale the predictors, then when you use raw cloze probability, beta is the change in EEG from cloze 0 to cloze 1, right? So that's more than the entire range of cloze which is from 0.1 to 0.9 (or something like this).
But when you use log_2(cloze), with one unit of change of log_2(cloze) you barely move in cloze scale, since this corresponds 2^-10 to 2^-9, or from 2^-9 to 2^-8, etc.  This is not a big issue, but the two betas will be in very different scales. If we know that the difference in N400 for very low (~.1) and very high (~.9) cloze corresponds to ~ 1microvolt from the previous lit, the beta in the first case (raw cloze) will be a bit larger than 1microvolt (I guess 1.2?), but the beta in the case of log_2 cloze will be much larger.

If one wants to put them in the same scale, to be able to use the same priors, one possibility is to make that for both of them 1 unit corresponds to the entire range (or the same range), so you can make that 0.9-0.1 would be one unit, and also log_2(.9)-log_2(.1). (I think this is what I did). The two betas won't have the same interpretation, but at least they are more comparable, and one can assign the same prior.
-->
<div class="sourceCode" id="cb884"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb884-1"><a href="ch-bf.html#cb884-1" aria-hidden="true"></a>df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span></span>
<span id="cb884-2"><a href="ch-bf.html#cb884-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_logscloze =</span> <span class="kw">scale</span>(c_logscloze) <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(c_cloze))</span></code></pre></div>
<p>Then, run a linear mixed-effects model with log cloze probability instead of linear cloze probability, and again carry out bridge sampling.</p>
<div class="sourceCode" id="cb885"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb885-1"><a href="ch-bf.html#cb885-1" aria-hidden="true"></a>fit_N400_h_log &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_logscloze <span class="op">+</span></span>
<span id="cb885-2"><a href="ch-bf.html#cb885-2" aria-hidden="true"></a><span class="st">                        </span>(c_logscloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_logscloze <span class="op">|</span><span class="st"> </span>item),</span>
<span id="cb885-3"><a href="ch-bf.html#cb885-3" aria-hidden="true"></a>                      <span class="dt">prior =</span> priors1,</span>
<span id="cb885-4"><a href="ch-bf.html#cb885-4" aria-hidden="true"></a>                      <span class="dt">warmup =</span> <span class="dv">2000</span>,</span>
<span id="cb885-5"><a href="ch-bf.html#cb885-5" aria-hidden="true"></a>                      <span class="dt">iter =</span> <span class="dv">20000</span>,</span>
<span id="cb885-6"><a href="ch-bf.html#cb885-6" aria-hidden="true"></a>                      <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</span>
<span id="cb885-7"><a href="ch-bf.html#cb885-7" aria-hidden="true"></a>                      <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</span>
<span id="cb885-8"><a href="ch-bf.html#cb885-8" aria-hidden="true"></a>                      <span class="dt">data =</span> df_eeg)</span></code></pre></div>
<div class="sourceCode" id="cb886"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb886-1"><a href="ch-bf.html#cb886-1" aria-hidden="true"></a>margLogLik_log &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_N400_h_log, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>Next, compare the linear and the log model to each other using Bayes factors.</p>
<div class="sourceCode" id="cb887"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb887-1"><a href="ch-bf.html#cb887-1" aria-hidden="true"></a>(BF_log_lin &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(margLogLik_log, margLogLik_linear))</span></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 6.04762</code></pre>
<p>The results show a Bayes factor of <span class="math inline">\(6\)</span> of the log model over the linear model. This shows some evidence that log cloze probability is a better predictor of N400 amplitudes than linear cloze probability. This analysis demonstrates that model comparisons using Bayes factor are not limited to nested models, but can also be used for non-nested models. Here, a specific benefit of the Bayes factor is that it considers the functional shape or complexity of the models, instead of merely counting the number of parameters (such as BIC and AIC).</p>
</div>
</div>
<div id="the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest" class="section level2 hasAnchor" number="15.3">
<h2><span class="header-section-number">15.3</span> The influence of the priors on Bayes factors: beyond the effect of interest<a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We saw above that the width (or standard deviation) of the prior distribution for the effect of interest had a strong impact on the results from Bayes factor analyses. Thus, one question is whether only the prior for the effect of interest is important, or whether priors for other model parameters can also impact the resulting Bayes factors in an analysis. It turns out that priors for other model parameters can also be important and impact Bayes factors, especially when there are non-linear components in the model, such as in  generalized linear mixed effects models. We investigate this issue by using a simulated data set on a variable that has a  Bernoulli distribution; in each trial, subjects can perform either successfully (<code>pDV = 1</code>) on a task, or not (<code>pDV = 0</code>). The simulated data is from a factorial experimental design, with one between-subject factor <span class="math inline">\(F\)</span> with 2 levels (<span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>), and Table <a href="ch-bf.html#tab:cTabXMeans">15.2</a> shows success probabilities for each of the experimental conditions.</p>
<div class="sourceCode" id="cb889"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb889-1"><a href="ch-bf.html#cb889-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_BF&quot;</span>)</span>
<span id="cb889-2"><a href="ch-bf.html#cb889-2" aria-hidden="true"></a><span class="kw">str</span>(df_BF)</span></code></pre></div>
<pre><code>## tibble [100 × 3] (S3: tbl_df/tbl/data.frame)
##  $ F  : Factor w/ 2 levels &quot;F1&quot;,&quot;F2&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ pDV: int [1:100] 1 1 1 1 1 1 1 1 1 1 ...
##  $ id : int [1:100] 1 2 3 4 5 6 7 8 9 10 ...</code></pre>
<table>
<caption><span id="tab:cTabXMeans">TABLE 15.2: </span>Summary statistics per condition for the simulated data.</caption>
<thead>
<tr class="header">
<th align="left">Factor A</th>
<th align="left">N data</th>
<th align="left">Means</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">F1</td>
<td align="left"><span class="math inline">\(50\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">F2</td>
<td align="left"><span class="math inline">\(50\)</span></td>
<td align="left"><span class="math inline">\(0.70\)</span></td>
</tr>
</tbody>
</table>
<p>Our question now is whether there is evidence for a difference in success probabilities between groups <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>.
As contrasts for the factor <span class="math inline">\(F\)</span>, we use scaled sum coding <span class="math inline">\((-0.5, +0.5)\)</span>.</p>
<div class="sourceCode" id="cb891"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb891-1"><a href="ch-bf.html#cb891-1" aria-hidden="true"></a><span class="kw">contrasts</span>(df_BF<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">+0.5</span>)</span></code></pre></div>
<p>Next, we proceed to specify our priors. For the difference between groups (<span class="math inline">\(F1\)</span> versus <span class="math inline">\(F2\)</span>), define a normally distributed prior with a mean of <span class="math inline">\(0\)</span> and a standard deviation of <span class="math inline">\(0.5\)</span>. Thus, we do not specify a direction of the difference a priori, and assume not too large effect sizes. Now run two logistic <code>brms</code> models, one with the group factor <span class="math inline">\(F\)</span> included, and one without the factor <span class="math inline">\(F\)</span>, and compute Bayes factors using bridge sampling to obtain the evidence that the data provide for the alternative hypothesis that a group difference exists between levels <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>.</p>
<p>So far, we have only specified the prior for the effect size. The question we are asking now is whether priors on other model parameters can impact the Bayes factor computations for testing the group effect. Specifically, can the prior for the intercept influence the Bayes factor for the group difference? The results show that the answer is yes, in some situations. Let’s have a look at this in more detail. Let’s assume that we compare two different priors for the intercept. We specify each as a normal distribution with a standard deviation of <span class="math inline">\(0.1\)</span>, thus, specifying relatively high certainty a priori where the intercept of the data will fall. The only difference that we now specify, is that one time, the prior mean (on the latent logistic scale) is set to <span class="math inline">\(0\)</span>, corresponding to a prior mean probability of <span class="math inline">\(0.5\)</span>. In the other condition, we specify a prior mean of <span class="math inline">\(2\)</span>, corresponding to a prior mean probability of <span class="math inline">\(0.88\)</span>. When we look at the data (see Table <a href="ch-bf.html#tab:cTabXMeans">15.2</a>) we see that the prior mean of <span class="math inline">\(0\)</span> (i.e., prior probability for the intercept of <span class="math inline">\(0.5\)</span>) is not very compatible with the data, whereas the prior mean of <span class="math inline">\(2\)</span> (i.e., a prior probability for the intercept of <span class="math inline">\(0.88\)</span>) is quite closely aligned with the actual data.</p>
<p>We now compute Bayes factors for the group difference (<span class="math inline">\(F1\)</span> versus <span class="math inline">\(F2\)</span>) by using these different priors for the intercept. Thus, we first fit a null (<span class="math inline">\(M0\)</span>) and alternative (<span class="math inline">\(M1\)</span>) model under a prior that is misaligned with the data (a narrow distribution centered in zero), and perform bridge sampling for these models:</p>
<div class="sourceCode" id="cb892"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb892-1"><a href="ch-bf.html#cb892-1" aria-hidden="true"></a><span class="co"># set prior</span></span>
<span id="cb892-2"><a href="ch-bf.html#cb892-2" aria-hidden="true"></a>priors_logit1 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.1</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb892-3"><a href="ch-bf.html#cb892-3" aria-hidden="true"></a>                   <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> b))</span>
<span id="cb892-4"><a href="ch-bf.html#cb892-4" aria-hidden="true"></a><span class="co"># Bayesian GLM: M0</span></span>
<span id="cb892-5"><a href="ch-bf.html#cb892-5" aria-hidden="true"></a>fit_pDV_H0 &lt;-<span class="st"> </span><span class="kw">brm</span>(pDV <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb892-6"><a href="ch-bf.html#cb892-6" aria-hidden="true"></a>                  <span class="dt">data =</span> df_BF,</span>
<span id="cb892-7"><a href="ch-bf.html#cb892-7" aria-hidden="true"></a>                  <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb892-8"><a href="ch-bf.html#cb892-8" aria-hidden="true"></a>                  <span class="dt">prior =</span> priors_logit1[<span class="op">-</span><span class="dv">2</span>, ],</span>
<span id="cb892-9"><a href="ch-bf.html#cb892-9" aria-hidden="true"></a>                  <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>))</span>
<span id="cb892-10"><a href="ch-bf.html#cb892-10" aria-hidden="true"></a><span class="co"># Bayesian GLM: M1</span></span>
<span id="cb892-11"><a href="ch-bf.html#cb892-11" aria-hidden="true"></a>fit_pDV_H1 &lt;-<span class="st"> </span><span class="kw">brm</span>(pDV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</span>
<span id="cb892-12"><a href="ch-bf.html#cb892-12" aria-hidden="true"></a>                  <span class="dt">data =</span> df_BF,</span>
<span id="cb892-13"><a href="ch-bf.html#cb892-13" aria-hidden="true"></a>                  <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb892-14"><a href="ch-bf.html#cb892-14" aria-hidden="true"></a>                  <span class="dt">prior =</span> priors_logit1,</span>
<span id="cb892-15"><a href="ch-bf.html#cb892-15" aria-hidden="true"></a>                  <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>))</span>
<span id="cb892-16"><a href="ch-bf.html#cb892-16" aria-hidden="true"></a><span class="co"># bridge sampling</span></span>
<span id="cb892-17"><a href="ch-bf.html#cb892-17" aria-hidden="true"></a>mLL_binom_H0 &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pDV_H0, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span>
<span id="cb892-18"><a href="ch-bf.html#cb892-18" aria-hidden="true"></a>mLL_binom_H1 &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pDV_H1, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>Next, get ready for computing Bayes factors by again running the null (<span class="math inline">\(M0\)</span>) and the alternative (<span class="math inline">\(M1\)</span>) model, now assuming a more realistic prior for the intercept (prior mean <span class="math inline">\(= 2\)</span>).</p>
<div class="sourceCode" id="cb893"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb893-1"><a href="ch-bf.html#cb893-1" aria-hidden="true"></a>priors_logit2 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="fl">0.1</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb893-2"><a href="ch-bf.html#cb893-2" aria-hidden="true"></a>                   <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> b))</span>
<span id="cb893-3"><a href="ch-bf.html#cb893-3" aria-hidden="true"></a><span class="co"># Bayesian GLM: M0</span></span>
<span id="cb893-4"><a href="ch-bf.html#cb893-4" aria-hidden="true"></a>fit_pDV_H0_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">brm</span>(pDV <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb893-5"><a href="ch-bf.html#cb893-5" aria-hidden="true"></a>                    <span class="dt">data =</span> df_BF,</span>
<span id="cb893-6"><a href="ch-bf.html#cb893-6" aria-hidden="true"></a>                    <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb893-7"><a href="ch-bf.html#cb893-7" aria-hidden="true"></a>                    <span class="dt">prior =</span> priors_logit2[<span class="op">-</span><span class="dv">2</span>, ],</span>
<span id="cb893-8"><a href="ch-bf.html#cb893-8" aria-hidden="true"></a>                    <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>))</span>
<span id="cb893-9"><a href="ch-bf.html#cb893-9" aria-hidden="true"></a><span class="co"># Bayesian GLM: M1</span></span>
<span id="cb893-10"><a href="ch-bf.html#cb893-10" aria-hidden="true"></a>fit_pDV_H1_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">brm</span>(pDV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</span>
<span id="cb893-11"><a href="ch-bf.html#cb893-11" aria-hidden="true"></a>                    <span class="dt">data =</span> df_BF,</span>
<span id="cb893-12"><a href="ch-bf.html#cb893-12" aria-hidden="true"></a>                    <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb893-13"><a href="ch-bf.html#cb893-13" aria-hidden="true"></a>                    <span class="dt">prior =</span> priors_logit2,</span>
<span id="cb893-14"><a href="ch-bf.html#cb893-14" aria-hidden="true"></a>                    <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>))</span>
<span id="cb893-15"><a href="ch-bf.html#cb893-15" aria-hidden="true"></a><span class="co"># bridge sampling</span></span>
<span id="cb893-16"><a href="ch-bf.html#cb893-16" aria-hidden="true"></a>mLL_binom_H0_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pDV_H0_<span class="dv">2</span>, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span>
<span id="cb893-17"><a href="ch-bf.html#cb893-17" aria-hidden="true"></a>mLL_binom_H1_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pDV_H1_<span class="dv">2</span>, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>Based on these models and bridge samples, we can now compute the Bayes factors in support for <span class="math inline">\(M1\)</span> (i.e., in support of a group-difference between <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>). We can do so for the unrealistic prior for the intercept (prior mean of <span class="math inline">\(0\)</span>) and the more realistic prior for the intercept (prior mean of <span class="math inline">\(2\)</span>).</p>
<div class="sourceCode" id="cb894"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb894-1"><a href="ch-bf.html#cb894-1" aria-hidden="true"></a>(BF_binom_H1_H0 &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(mLL_binom_H1, mLL_binom_H0))</span></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 7.18805</code></pre>
<div class="sourceCode" id="cb896"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb896-1"><a href="ch-bf.html#cb896-1" aria-hidden="true"></a>(BF_binom_H1_H0_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(mLL_binom_H1_<span class="dv">2</span>, mLL_binom_H0_<span class="dv">2</span>))</span></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 29.58150</code></pre>
<p>The results show that with the realistic prior for the intercept (prior mean <span class="math inline">\(= 2\)</span>), the evidence for the <span class="math inline">\(M1\)</span> is quite strong, with a Bayes factor of <span class="math inline">\(BF_{10} =\)</span> 29.6. With the unrealistic prior for the intercept (i.e., prior mean <span class="math inline">\(= 0\)</span>), by contrast, the evidence for the <span class="math inline">\(M1\)</span> is much reduced, <span class="math inline">\(BF_{10} =\)</span> 7.2, and now only modest.</p>
<p>Thus, when performing Bayes factor analyses, not only can the priors for the effect of interest (here the group difference) impact the results, under certain circumstances priors for other model parameters can too, such as the prior mean for the intercept here. Such an influence will not always be strong, and can sometimes be negligible. There may be many situations, where the exact specification of the intercept does not have much of an effect on the Bayes factor for a group difference. However, such influences can in principle occur, especially in models with non-linear components. Therefore, it is very important to be careful in specifying realistic priors for all model parameters, also including the intercept. A good way to judge whether prior assumptions are realistic and plausible is prior predictive checks, where we simulate data based on the priors and the model and judge whether the simulated data is plausible and realistic.</p>
</div>
<div id="sec-stanBF" class="section level2 hasAnchor" number="15.4">
<h2><span class="header-section-number">15.4</span>  Bayes factor in Stan<a href="ch-bf.html#sec-stanBF" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The package  <code>bridgesampling</code> allows for a straightforward calculation of Bayes factor for Stan models as well. All the limitations and caveats of Bayes factor discussed in this chapter apply to Stan code as much as they apply to <code>brms</code> code. The sampling notation (<code>~</code>) should not be used; see Box <a href="ch-introstan.html#thm:tilde">10.2</a>.</p>
<p>An advantage of using Stan in comparison with <code>brms</code> is Stan’s flexibility. We revisit the model implemented before in section <a href="ch-introstan.html#sec-interstan">10.4.2</a>. We want to assess the evidence for a <em>positive</em> effect of  attentional load on  pupil size against a similar model that assumes no effect. To do this, assume the following likelihood:</p>
<p><span class="math display">\[\begin{equation}
p\_size_n \sim \mathit{Normal}(\alpha + c\_load_n \cdot \beta_1 + c\_trial \cdot \beta_2 + c\_load \cdot c\_trial \cdot \beta_3, \sigma)
\end{equation}\]</span></p>
<p>Define priors for all the <span class="math inline">\(\beta\)</span>s as before, with the difference that <span class="math inline">\(\beta_1\)</span> can only have positive values:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(1000, 500) \\
\beta_1 &amp;\sim \mathit{Normal}_+(0, 100) \\
\beta_2 &amp;\sim \mathit{Normal}(0, 100) \\
\beta_3 &amp;\sim \mathit{Normal}(0, 100) \\
\sigma &amp;\sim \mathit{Normal}_+(0, 1000)
\end{aligned}
\end{equation}\]</span></p>
<p>The following Stan model is the direct translation of the new priors and likelihood.</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] c_load;
  vector[N] c_trial;
  vector[N] p_size;
}
parameters {
  real alpha;
  real&lt;lower = 0&gt; beta1;
  real beta2;
  real beta3;
  real&lt;lower = 0&gt; sigma;
}
model {
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta1 | 0, 100) -
    normal_lccdf(0 | 0, 100);
  target += normal_lpdf(beta2 | 0, 100);
  target += normal_lpdf(beta3 | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  target += normal_lpdf(p_size | alpha + c_load * beta1 +
                                 c_trial * beta2 +
                                 c_load .* c_trial * beta3, sigma);
}</code></pre>
<p>Fit the model with 20000 iterations to ensure that the Bayes factor is stable, and increase the <code>adapt_delta</code> parameter to avoid warnings:</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb899-1"><a href="ch-bf.html#cb899-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_pupil&quot;</span>)</span>
<span id="cb899-2"><a href="ch-bf.html#cb899-2" aria-hidden="true"></a>df_pupil &lt;-<span class="st"> </span>df_pupil <span class="op">%&gt;%</span></span>
<span id="cb899-3"><a href="ch-bf.html#cb899-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_load =</span> load <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(load),</span>
<span id="cb899-4"><a href="ch-bf.html#cb899-4" aria-hidden="true"></a>         <span class="dt">c_trial =</span> trial <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(trial))</span>
<span id="cb899-5"><a href="ch-bf.html#cb899-5" aria-hidden="true"></a>ls_pupil &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">p_size =</span> df_pupil<span class="op">$</span>p_size,</span>
<span id="cb899-6"><a href="ch-bf.html#cb899-6" aria-hidden="true"></a>                 <span class="dt">c_load =</span> df_pupil<span class="op">$</span>c_load,</span>
<span id="cb899-7"><a href="ch-bf.html#cb899-7" aria-hidden="true"></a>                 <span class="dt">c_trial =</span> df_pupil<span class="op">$</span>c_trial,</span>
<span id="cb899-8"><a href="ch-bf.html#cb899-8" aria-hidden="true"></a>                 <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil))</span>
<span id="cb899-9"><a href="ch-bf.html#cb899-9" aria-hidden="true"></a>pupil_pos &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb899-10"><a href="ch-bf.html#cb899-10" aria-hidden="true"></a>                         <span class="st">&quot;pupil_pos.stan&quot;</span>,</span>
<span id="cb899-11"><a href="ch-bf.html#cb899-11" aria-hidden="true"></a>                         <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb899-12"><a href="ch-bf.html#cb899-12" aria-hidden="true"></a>fit_pupil_int_pos &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> pupil_pos,</span>
<span id="cb899-13"><a href="ch-bf.html#cb899-13" aria-hidden="true"></a>                          <span class="dt">data =</span> ls_pupil,</span>
<span id="cb899-14"><a href="ch-bf.html#cb899-14" aria-hidden="true"></a>                          <span class="dt">warmup =</span> <span class="dv">1000</span>,</span>
<span id="cb899-15"><a href="ch-bf.html#cb899-15" aria-hidden="true"></a>                          <span class="dt">iter =</span> <span class="dv">20000</span>,</span>
<span id="cb899-16"><a href="ch-bf.html#cb899-16" aria-hidden="true"></a>                          <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">.95</span>))</span></code></pre></div>
<p>The null model that we defined has <span class="math inline">\(\beta_1 = 0\)</span> and is written in Stan as follows:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] c_load;
  vector[N] c_trial;
  vector[N] p_size;
}
parameters {
  real alpha;
  real beta2;
  real beta3;
  real&lt;lower = 0&gt; sigma;
}
model {
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta2 | 0, 100);
  target += normal_lpdf(beta3 | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  target += normal_lpdf(p_size | alpha + c_trial * beta2 +
                                 c_load .* c_trial * beta3, sigma);
}
generated quantities{</code></pre>
<div class="sourceCode" id="cb901"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb901-1"><a href="ch-bf.html#cb901-1" aria-hidden="true"></a>pupil_null &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb901-2"><a href="ch-bf.html#cb901-2" aria-hidden="true"></a>                          <span class="st">&quot;pupil_null.stan&quot;</span>,</span>
<span id="cb901-3"><a href="ch-bf.html#cb901-3" aria-hidden="true"></a>                          <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb901-4"><a href="ch-bf.html#cb901-4" aria-hidden="true"></a>fit_pupil_int_null &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> pupil_null,</span>
<span id="cb901-5"><a href="ch-bf.html#cb901-5" aria-hidden="true"></a>                           <span class="dt">data =</span> ls_pupil,</span>
<span id="cb901-6"><a href="ch-bf.html#cb901-6" aria-hidden="true"></a>                           <span class="dt">warmup =</span> <span class="dv">1000</span>,</span>
<span id="cb901-7"><a href="ch-bf.html#cb901-7" aria-hidden="true"></a>                           <span class="dt">iter =</span> <span class="dv">20000</span>)</span></code></pre></div>
<p>Compare the models with bridge sampling:</p>
<div class="sourceCode" id="cb902"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb902-1"><a href="ch-bf.html#cb902-1" aria-hidden="true"></a>lml_pupil &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pupil_int_pos, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span>
<span id="cb902-2"><a href="ch-bf.html#cb902-2" aria-hidden="true"></a>lml_pupil_null &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pupil_int_null, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</span>
<span id="cb902-3"><a href="ch-bf.html#cb902-3" aria-hidden="true"></a>BF_att &lt;-<span class="st"> </span>bridgesampling<span class="op">::</span><span class="kw">bf</span>(lml_pupil, lml_pupil_null)</span></code></pre></div>
<div class="sourceCode" id="cb903"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb903-1"><a href="ch-bf.html#cb903-1" aria-hidden="true"></a>BF_att</span></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of lml_pupil over lml_pupil_null: 25.28019</code></pre>
<p>We find that the data is 25.28 more likely under a model that assumes a positive effect of load than under a model that assumes no effect.</p>
</div>
<div id="bayes-factors-in-theory-and-in-practice" class="section level2 hasAnchor" number="15.5">
<h2><span class="header-section-number">15.5</span> Bayes factors in theory and in practice<a href="ch-bf.html#bayes-factors-in-theory-and-in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="bayes-factors-in-theory-stability-and-accuracy" class="section level3 hasAnchor" number="15.5.1">
<h3><span class="header-section-number">15.5.1</span> Bayes factors in theory: Stability and  accuracy<a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One question that we can ask here is how stable and accurate the estimates of Bayes factors are. The bridge sampling algorithm needs a lot of posterior samples to obtain stable estimates of the Bayes factor. Running bridge sampling based on a too small an effective sample size will yield unstable estimates of the Bayes factor, such that repeated computations will yield radically different Bayes factor values. Moreover, even if the Bayes factor is approximated in a stable way, it is unclear whether this approximate Bayes factor is equal to the true Bayes factor, or whether there is  bias in the computation such that the  approximate Bayes factor has a wrong value. We show this below.</p>
<div id="instability-due-to-the-effective-number-of-posterior-samples" class="section level4 hasAnchor" number="15.5.1.1">
<h4><span class="header-section-number">15.5.1.1</span> Instability due to the effective number of posterior samples<a href="ch-bf.html#instability-due-to-the-effective-number-of-posterior-samples" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The  number of iterations, which in turn affects the total  number of posterior samples can have a strong impact on the  robustness of the results of the bridge sampling algorithm (i.e., on the resulting Bayes factor) and there are no good theoretical guarantees that bridge sample will yield accurate estimates of Bayes factors. In the analyses presented above, we set the number of iterations to a very large number of <span class="math inline">\(n = 20000\)</span>. The sensitivity analysis therefore took a considerable amount of time. Indeed, the results from this analysis were stable, as shown below.</p>
<p>Running the same analysis with less iterations will induce some instability in the Bayes factor estimates based on the bridge sampling, such that running the same analysis twice would yield different results. Furthermore, due to variations in starting values, bridge sampling itself may be unstable and yield inconsistent results for successive runs on the same posterior samples. This is highly concerning because if the number of effective sample sizes is not large enough, the results published in a paper may not be stable.
Indeed, the default number of iterations in <code>brms</code> is set as <code>iter = 2000</code> (and the default number of warmup iterations is <code>warmup = 1000</code>). These defaults were not set to support  bridge sampling, i.e., they were not defined for computation of densities to support Bayes factors. Instead, they are valid for posterior inference on expectations (e.g., posterior means) for models that are not too complex. However, when using these defaults for estimation of densities and the computation of Bayes factors,  instabilities can arise.</p>
<p>As an illustration, we perform the same  sensitivity analysis again, now using the default number of <span class="math inline">\(2000\)</span> iterations in <code>brms</code>. The posterior sampling process now runs much quicker. Moreover, we check the stability of the Bayes factors in the sensitivity analyses by repeating both sensitivity analyses (with <span class="math inline">\(n = 20000\)</span> iterations and with the default number of <span class="math inline">\(n = 2000\)</span> iterations) a second time, to see whether the results for Bayes factors are stable.</p>
<div class="figure"><span style="display:block;" id="fig:BFpriors2"></span>
<img src="bookdown_files/figure-html/BFpriors2-1.svg" alt="The effect of the number of samples on a prior sensitivity analysis for the Bayes factor. Black lines show 2 runs with 20,000 iterations. Grey lines show 20 runs with default number of iterations (2000)." width="672" />
<p class="caption">
FIGURE 15.4: The effect of the number of samples on a prior sensitivity analysis for the Bayes factor. Black lines show 2 runs with 20,000 iterations. Grey lines show 20 runs with default number of iterations (2000).
</p>
</div>
<p>The results displayed in Figure <a href="ch-bf.html#fig:BFpriors2">15.4</a> show that the Bayes factors are highly unstable when the number of iterations is low. They clearly deviate from the Bayes factors estimated with <span class="math inline">\(20000\)</span> iterations, resulting in very  unstable estimates. By contrast, the analyses using <span class="math inline">\(20000\)</span> iterations provide nearly the same results in both analyses. The two lines lie virtually directly on top of each other; the points are jittered horizontally for better visibility.</p>
<p>This demonstrates that it is necessary to use a large number of iterations when computing Bayes factors using bridge sampling. In practice, one should compute the sensitivity analysis (or at least one of the models or priors) twice (as we did here) to make sure that the results are stable and sufficiently similar, in order to provide a good basis for reporting results.</p>
<p>By contrast, as mentioned earlier as well, Bayes factors based on the  Savage0–Dickey method (as implemented in <code>brms</code>) can be unstable even when using a large number of posterior samples. This problem can arise especially when the posterior is very far from zero, and thus very large or very small Bayes factors are obtained. (In such cases, the precise value of the Bayes factor may sometimes not matter in practical applications.) However, because of this instability of the Savage-Dickey method in <code>brms</code>, it is a good idea to use bridge sampling, and to check the stability of the estimates using more than one method.</p>
</div>
<div id="inaccuracy-of-bayes-factor-estimates-does-the-estimate-approximate-the-true-bayes-factor-well" class="section level4 hasAnchor" number="15.5.1.2">
<h4><span class="header-section-number">15.5.1.2</span> Inaccuracy of Bayes factor estimates: Does the estimate approximate the true  Bayes factor well?<a href="ch-bf.html#inaccuracy-of-bayes-factor-estimates-does-the-estimate-approximate-the-true-bayes-factor-well" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An important point about approximate estimates of Bayes factors using bridge sampling is that there are no strong guarantees for their  accuracy. That is, even if we can show that the  approximated Bayes factor estimate using bridge sampling is stable (i.e., when using sufficient effective samples, see the analyses above), even then it remains unclear whether the Bayes factor estimate actually is close to the true Bayes factor.
The stably estimated Bayes factors based on bridge sampling may, in theory, be biased, meaning that they may not be very close to the true (correct) Bayes factor.
The technique of  simulation-based calibration  <span class="citation">(SBC; Talts et al. <a href="#ref-talts2018validating" role="doc-biblioref">2018</a>; Schad, Betancourt, and Vasishth <a href="#ref-schad2020toward" role="doc-biblioref">2020</a>)</span> can be used to investigate this question (SBC is also discussed in section <a href="ch-custom.html#sec-validSBC">12.2</a> in chapter <a href="ch-custom.html#ch-custom">12</a>). We investigate this question next.<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a></p>
<p>In the SBC approach, the priors are used to simulate data. Then, posterior inference is done on the simulated data, and the posterior can be compared to the prior. If the posteriors are equal to the priors, then this supports accurate computations. Applied to Bayes factor analyses, one defines a prior on the hypothesis space, i.e., one defines the prior probabilities for a null and an alternative model, specifying how likely each model is a priori. From these priors, one can randomly draw one hypothesis (model), e.g., <span class="math inline">\(nsim = 500\)</span> times. Thus, in each of <span class="math inline">\(500\)</span> draws one randomly chooses one model (either <span class="math inline">\(M0\)</span> or <span class="math inline">\(M1\)</span>), with the probabilities given by the model priors. For each draw, one first samples model parameters from their prior distributions, and then uses these sampled model parameters to simulate data. For each simulated data set, one can then compute marginal likelihoods and Bayes factor estimates using posterior samples and bridge sampling, and one can then compute the posterior probabilities for each hypothesis (i.e., how likely each model is a posteriori). As the last, and critical step in SBC, one can then compare the posterior model probabilities to the prior model probabilities. A key result in SBC is that if the computation of marginal likelihoods and posterior model probabilities is performed accurately (without bias) by the bridge sampling procedure; that is, if the Bayes factor estimate is close to the true Bayes factor, then the posterior model probabilities should be the same as the prior model probabilities.</p>
<p>Here, we perform this SBC approach. Across the <span class="math inline">\(500\)</span> simulations, we systematically vary the prior model probability from zero to one. For each of the <span class="math inline">\(500\)</span> simulations we sample a model (hypothesis) from the model prior, then sample parameters from the priors over parameters, use the sampled parameters to simulate fake data, fit the null and the alternative model on the simulated data, perform bridge sampling for each model, compute the Bayes factor estimate between them, and compute posterior model probabilities. If the bridge sampling works accurately, then the posterior model probabilities should be the same as the prior model probabilities. Given that we varied the prior model probabilities from zero to one, the posterior model probabilities should also vary from zero to one. In Figure <a href="ch-bf.html#fig:SBC3plot">15.5</a>, we plot the posterior model probabilities as a function of the prior model probabilities. If the posterior probabilities are the same as the priors, then the local regression line and all points should lie on the diagonal.</p>
<div class="figure"><span style="display:block;" id="fig:SBC3plot"></span>
<img src="bookdown_files/figure-html/SBC3plot-1.svg" alt="The posterior probabilities for M0 are plotted as a function of prior probabilities for M0. If the approximation of the Bayes factor using bridge sampling is unbiased, then the data should be aligned along the diagonal (see dashed black line). The thick black line is a prediction from a local regression analysis. The points are average posterior probabilities as a function of a priori selected hypotheses for 50 simulation runs each. Error bars represent 95 percent confidence intervals." width="576" />
<p class="caption">
FIGURE 15.5: The posterior probabilities for M0 are plotted as a function of prior probabilities for M0. If the approximation of the Bayes factor using bridge sampling is unbiased, then the data should be aligned along the diagonal (see dashed black line). The thick black line is a prediction from a local regression analysis. The points are average posterior probabilities as a function of a priori selected hypotheses for 50 simulation runs each. Error bars represent 95 percent confidence intervals.
</p>
</div>
<p>The results of this analysis in Figure <a href="ch-bf.html#fig:SBC3plot">15.5</a> show that the local regression line is very close to the diagonal, and that the data points (each summarizing results from 50 simulations, with means and confidence intervals) also lie close to the diagonal. This importantly demonstrates that the estimated posterior model probabilities are close to their a priori values. This result shows that posterior model probabilities, which are based on the Bayes factor estimates from the bridge sampling, are unbiased for a large range of different a priori model probabilities.</p>
<p>This result is very important as it shows one example case where the Bayes factor approximation is accurate. However, this demonstration is valid only for this one specific application case, i.e., with a particular data set, particular models, specific priors for the parameters, and a specific comparison between nested models. Strictly speaking, if one wants to be sure that the Bayes factor estimate is accurate for a particular data analysis, then such a SBC validation analysis would have to be computed for every data analysis. For details, including code, on how to perform such an SBC, see <a href="https://osf.io/y354c/" class="uri">https://osf.io/y354c/</a>. However, the fact that the SBC yields such promising results for this first application case also gives some hope that the bridge sampling may be accurate also for other comparable data analysis situations.</p>
<p>Based on these results on the average theoretical performance of Bayes factor estimation, we next turn to a different issue: how Bayes factors depend on and vary with varying data, leading to bad performance in individual cases despite good average performance.</p>
</div>
</div>
<div id="sec-BFvar" class="section level3 hasAnchor" number="15.5.2">
<h3><span class="header-section-number">15.5.2</span> Bayes factors in practice: Variability with the data<a href="ch-bf.html#sec-BFvar" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="variation-associated-with-the-data-subjects-items-and-residual-noise" class="section level4 hasAnchor" number="15.5.2.1">
<h4><span class="header-section-number">15.5.2.1</span> Variation associated with the data (subjects, items, and residual noise)<a href="ch-bf.html#variation-associated-with-the-data-subjects-items-and-residual-noise" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A second, and very different, source limiting robustness of Bayes factor estimates derives from the variability that is observed with the data, i.e., among subjects, items, and residual noise. Thus, when repeating an experiment a second time in a replication analysis, using different subjects and items, will lead to different outcomes of the statistical analysis every time a new replication run is conducted.
The  “dance of <span class="math inline">\(p\)</span>-values” <span class="citation">(Cumming <a href="#ref-cumming2014new" role="doc-biblioref">2014</a>)</span> illustrates this well-known limit to robustness in frequentist analyses, where <span class="math inline">\(p\)</span>-values are not consistently significant across studies over repeated replication attempts. Instead, every time a study is repeated, the outcomes produce wildly disparate <span class="math inline">\(p\)</span>-values.
This can also be observed when simulating data from some known truth and re-running analyses on simulated data sets.</p>
<p>Additionally, Bayesian analyses ought to incorporate this same kind of variability (also refer to <a href="https://daniellakens.blogspot.com/2016/07/dance-of-bayes-factors.html" class="uri">https://daniellakens.blogspot.com/2016/07/dance-of-bayes-factors.html</a>). Here we show this type of variability in Bayes factor analyses by looking at a new example data analysis: We look at research on sentence comprehension, and specifically on effects of cue-based retrieval interference <span class="citation">(Lewis and Vasishth <a href="#ref-lewisvasishth:cogsci05" role="doc-biblioref">2005</a>; Van Dyke and McElree <a href="#ref-van2011cue" role="doc-biblioref">2011</a>)</span>.</p>
</div>
<div id="an-example-the-facilitatory-interference-effect" class="section level4 hasAnchor" number="15.5.2.2">
<h4><span class="header-section-number">15.5.2.2</span> An example: The facilitatory interference effect<a href="ch-bf.html#an-example-the-facilitatory-interference-effect" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The experiments investigating the cognitive processes behind a well-researched phenomenon in sentence comprehension will be examined next. The  agreement attraction configuration below serves as the example for this discussion. In it, the grammatically incorrect sentence (2) appears more grammatical than the equally grammatically incorrect sentence (1):</p>
<ol style="list-style-type: decimal">
<li>The key to the cabinet are in the kitchen.</li>
<li>The key to the cabinets are in the kitchen.</li>
</ol>
<p>Both sentences are ungrammatical because the subject (“key”) does not agree with the verb in number (“are”).
When compared to (1), sentences like (2) are frequently found to have shorter reading times at (or immediately after) the verb (“are”); for a meta-analysis, see <span class="citation">Jäger, Engelmann, and Vasishth (<a href="#ref-JaegerEngelmannVasishth2017" role="doc-biblioref">2017</a>)</span>. These shorter reading times are sometimes called  “facilitatory interference” <span class="citation">(Dillon <a href="#ref-dillon2011structured" role="doc-biblioref">2011</a>)</span>; facilitatory in this context simply refers to the fact that reading times at the relevant word are shorter in (2) compared to (1), without necessarily implying that processing is easier. One explanation for the shorter reading times is that there is an  illusion of grammaticality because the attractor word (cabinets in this case) agrees locally in number with the verb.
This is an interesting phenomenon because the plural versus singular feature of the attractor noun (“cabinet/s”) is not the subject, and therefore, under the rules of English grammar, is not supposed to agree with the number marking on the verb. That agreement attraction effects are consistently observed indicates that some  non-compositional processes are taking place.</p>
<p>Using a computational implementation <span class="citation">(formulated in the ACT-R framework, Anderson et al. <a href="#ref-abbl02" role="doc-biblioref">2004</a>)</span>, an account of agreement attraction effects in language processing explains how  retrieval-based working memory mechanisms lead to such agreement attraction effects in non-grammatical sentences <span class="citation">(Engelmann, Jäger, and Vasishth <a href="#ref-EngelmannJaegerVasishth2019" role="doc-biblioref">2020</a>; see also Hammerly, Staub, and Dillon <a href="#ref-hammerly2019grammaticality" role="doc-biblioref">2019</a>; and Yadav et al. <a href="#ref-YadavetalJML2022" role="doc-biblioref">2023</a>)</span>. Numerous studies have examined agreement attraction in grammatically incorrect sentences using comparable experimental setups and various dependent measures, including eye tracking and self-paced reading.
It is generally believed to be a robust empirical phenomenon, and we choose it for analysis here for that reason.</p>
<p>Here, we look at a self-paced reading study on agreement attraction in Spanish by <span class="citation">Lago et al. (<a href="#ref-lago2015agreement" role="doc-biblioref">2015</a>)</span>.
For the experimental condition agreement attraction (<span class="math inline">\(x\)</span>; sentence type), we estimate a population-level effect against a null model in which the sentence type population-level effect is not included.
For the agreement attraction effect of sentence type, we use sum contrast coding (i.e., -1 and +1). We run a hierarchical model with the following formula in <code>brms</code>: <code>rt ~ x + (x | subj) + (x | item)</code>, where <code>rt</code> is reading time, we have random variation associated with subjects and with items, and we assume that reading times follow a log-normal distribution: <code>family = lognormal()</code>.</p>
<p>First, load the data:</p>
<div class="sourceCode" id="cb905"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb905-1"><a href="ch-bf.html#cb905-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_lagoE1&quot;</span>)</span>
<span id="cb905-2"><a href="ch-bf.html#cb905-2" aria-hidden="true"></a><span class="kw">head</span>(df_lagoE1)</span></code></pre></div>
<pre><code>##     subj item  rt  int  x   expt
## 2     S1   I1 588  low -1 lagoE1
## 22    S1  I10 682 high  1 lagoE1
## 77    S1  I13 226  low -1 lagoE1
## 92    S1  I14 580 high  1 lagoE1
## 136   S1  I17 549  low -1 lagoE1
## 153   S1  I18 458 high  1 lagoE1</code></pre>
<p>As a next step, determine priors for the analysis of these data.</p>
</div>
<div id="determine-priors-using-meta-analysis" class="section level4 hasAnchor" number="15.5.2.3">
<h4><span class="header-section-number">15.5.2.3</span> Determine priors using meta-analysis<a href="ch-bf.html#determine-priors-using-meta-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One good way to obtain priors for Bayesian analyses, and specifically for Bayes factor analyses, is to use results from meta-analyses on the subject. Here, we take the prior for the experimental manipulation of agreement attraction from a published meta-analysis <span class="citation">(Jäger, Engelmann, and Vasishth <a href="#ref-JaegerEngelmannVasishth2017" role="doc-biblioref">2017</a>)</span>.<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a></p>
<p>The mean effect size (difference in reading time between the two experimental conditions) in the meta-analysis is <span class="math inline">\(-22\)</span> milliseconds (ms), with <span class="math inline">\(95\% \;CI = [-36 \; -9]\)</span> <span class="citation">(Jäger, Engelmann, and Vasishth <a href="#ref-JaegerEngelmannVasishth2017" role="doc-biblioref">2017</a>, Table 4)</span>. This means that on average, the target word (i.e., the verb) in sentences such as (2) is on average read <span class="math inline">\(22\)</span> milliseconds faster than in sentences such as (1). The size of the effect is measured on the millisecond scale, assuming a normal distribution of effect sizes across studies.</p>
<p>However, individual reading times usually do not follow a normal distribution. Instead, a better assumption about the distribution of reading times is a  log-normal distribution. This is what we will assume in the <code>brms</code> model. Therefore, to use the prior from the meta-analysis in the Bayesian analysis, we have to transform the prior values from the millisecond scale to log millisecond scale.</p>
<p>We have performed this transformation in the published version of the arXiv paper. Based on these calculations, the prior for the experimental factor of interference effects is set to a normal distribution with mean <span class="math inline">\(= -0.03\)</span> and standard deviation = <span class="math inline">\(0.009\)</span>. For the other model parameters, we use  principled priors.</p>
<div class="sourceCode" id="cb907"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb907-1"><a href="ch-bf.html#cb907-1" aria-hidden="true"></a>priors &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb907-2"><a href="ch-bf.html#cb907-2" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="op">-</span><span class="fl">0.03</span>, <span class="fl">0.009</span>), <span class="dt">class =</span> b),</span>
<span id="cb907-3"><a href="ch-bf.html#cb907-3" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> sd),</span>
<span id="cb907-4"><a href="ch-bf.html#cb907-4" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb907-5"><a href="ch-bf.html#cb907-5" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor))</span></code></pre></div>
</div>
<div id="running-a-hierarchical-bayesian-analysis" class="section level4 hasAnchor" number="15.5.2.4">
<h4><span class="header-section-number">15.5.2.4</span> Running a hierarchical Bayesian analysis<a href="ch-bf.html#running-a-hierarchical-bayesian-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Next, run a <code>brms</code> model on the data. We use a large number of iterations (<code>iter = 10000</code>) with bridge sampling to estimate the Bayes factor of the “full” model, which includes a population-level effect for the experimental condition agreement attraction (<code>x</code>; i.e., sentence type). As mentioned above, for the agreement attraction effect of sentence type, we use sum contrast coding (i.e., <span class="math inline">\(-1\)</span> and <span class="math inline">\(+1\)</span>).</p>
<p>We first show the population-level (or fixed) effects from the posterior analyses:</p>
<div class="sourceCode" id="cb908"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb908-1"><a href="ch-bf.html#cb908-1" aria-hidden="true"></a><span class="kw">fixef</span>(m1_lagoE1)</span></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept     6.02      0.06  5.90  6.13
## x            -0.03      0.01 -0.04 -0.01</code></pre>
<p>They show that for the population-level effect <code>x</code>, capturing the agreement attraction effect, the 95% credible interval does not overlap with zero. This indicates that there is some hint that the effect may have the expected negative direction, reflecting shorter reading times in the plural condition. As mentioned earlier, this does not provide a direct test of the hypothesis that the effect exists and is not zero. This is not tested here, because we did not specify the null hypothesis of zero effect explicitly. We can, however, draw inferences about this null hypothesis by using the Bayes factor.</p>
<p>Estimate Bayes factors between a full model, where the effect of agreement attraction is included, and a null model, where the effect of agreement attraction is absent, using the command <code>bayes_factor(lml_m1_lagoE1, lml_m0_lagoE1)</code>.
The Bayes factor <span class="math inline">\(BF_{10}\)</span>, or the strength of the alternative over the null, is calculated by the function.</p>
<div class="sourceCode" id="cb910"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb910-1"><a href="ch-bf.html#cb910-1" aria-hidden="true"></a>h_lagoE1<span class="op">$</span>bf</span></code></pre></div>
<pre><code>## [1] 6.29</code></pre>
<p>With a Bayes factor of <span class="math inline">\(6\)</span>, the output indicates that the alternative model–which incorporates the population-level effect of agreement attraction–may have some merit.
That is, this provides evidence for the alternative hypothesis that there is a difference between the experimental conditions, i.e., a facilitatory effect in the plural condition of the size derived from the meta-analysis.</p>
<p>As discussed earlier, the <code>bayes_factor</code> command should be run several times to check the stability of the Bayes factor calculation.</p>
</div>
<div id="variability-of-the-bayes-factor-posterior-simulations" class="section level4 hasAnchor" number="15.5.2.5">
<h4><span class="header-section-number">15.5.2.5</span> Variability of the Bayes factor:  Posterior simulations<a href="ch-bf.html#variability-of-the-bayes-factor-posterior-simulations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One way to investigate how variable the outcome of Bayes factor analyses can be (given that the Bayes factor is computed in a stable and accurate way), is to run prior predictive simulations. A key question then is how to set priors that yield realistic simulated data sets. Here, we choose the priors based on the posterior from a previous real empirical data set.
<!--One way to investigate how variable the outcome of Bayes factor analyses can be (given that the Bayes factor is computed in a stable and accurate way), is to run posterior predictive simulations based on a fitted model.-->
That is, one can use the posterior from the model above, and use it as a prior in future prior predictive simulations. Computing the Bayes factor analysis again on the simulated data can provide some insight into how variable the Bayes factor will be.</p>
<p>Here, we perform the prior predictive simulations. To determine the priors, we use a single given data set and model <span class="citation">(namely the Bayesian hierarchical model fitted to the data from Lago et al. <a href="#ref-lago2015agreement" role="doc-biblioref">2015</a>)</span>, and use the posterior distributions from this model as the prior for our analysis.
In these simulations, one takes posterior samples for the model parameters (i.e., <span class="math inline">\(p(\boldsymbol{\Theta} \mid \boldsymbol{y})\)</span>), and for each posterior sample of the model parameters, one can simulate new data <span class="math inline">\(\tilde{\boldsymbol{y}}\)</span> from the model <span class="math inline">\(p(\tilde{\boldsymbol{y}} \mid \boldsymbol{\Theta})\)</span>.</p>
<div class="sourceCode" id="cb912"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb912-1"><a href="ch-bf.html#cb912-1" aria-hidden="true"></a>pred_lagoE1 &lt;-<span class="st"> </span><span class="kw">posterior_predict</span>(m1_lagoE1)</span></code></pre></div>
<p>The question that we are interested in here now is, how much information is contained in this simulated data. That is, we can run Bayesian models on this simulated data and compute Bayes factors to test whether in the simulated data there is evidence for agreement attraction effects.
The interesting question here is how variable the outcomes of these Bayes factor analyses will be among various simulated replications of the same study.</p>
<p>Now, using <span class="math inline">\(50\)</span> different data sets simulated from the posterior/prior predictive distribution, we carry out this analysis.
For each of these data sets, we can proceed in exactly the same way as we did for the original observed experimental data. That is, we again fit the same <code>brms</code> model <span class="math inline">\(50\)</span> times, now to the simulated data, and using the same prior as before. For each simulated data set, we use bridge sampling to compute the Bayes factor of the alternative model compared to a null model where the agreement attraction effect (population-level effect predictor of sentence type, <code>x</code>) is set to <span class="math inline">\(0\)</span>. For each simulated predictive data set, we store the resulting Bayes factor. We again use the prior from the meta-analysis.</p>
</div>
<div id="visualize-distribution-of-bayes-factors" class="section level4 hasAnchor" number="15.5.2.6">
<h4><span class="header-section-number">15.5.2.6</span> Visualize distribution of Bayes factors<a href="ch-bf.html#visualize-distribution-of-bayes-factors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can now visualize the distribution of Bayes factors (<span class="math inline">\(BF_{10}\)</span>) across prior predictive distributions by plotting a histogram.
In this histogram, values greater than one support the alternative model (M1) that there are agreement attraction effects (i.e., the sentence type effect differs from zero), and values of the Bayes factor less than one support the null model (M0), which states that there is no agreement attraction effect (i.e., there is no difference in reading times between experimental conditions).</p>
<div class="figure"><span style="display:block;" id="fig:plotBFdistrn"></span>
<img src="bookdown_files/figure-html/plotBFdistrn-1.svg" alt="Estimates of the retrieval interference facilitatory effect and the 95% credible intervals for all simulations (solid lines) and the empirically observed data (dashed line) are shown in the left panel. An illustration of the alternative model's Bayes factors (BF10) over the null model in 50 simulated data sets is shown in the right panel. The horizontal error bar displays 95% of all Bayes factors, the dashed line displays the Bayes factor calculated from the empirical data, and the vertical solid black line displays equal evidence for both hypotheses." width="672" />
<p class="caption">
FIGURE 15.6: Estimates of the retrieval interference facilitatory effect and the 95% credible intervals for all simulations (solid lines) and the empirically observed data (dashed line) are shown in the left panel. An illustration of the alternative model’s Bayes factors (BF10) over the null model in 50 simulated data sets is shown in the right panel. The horizontal error bar displays 95% of all Bayes factors, the dashed line displays the Bayes factor calculated from the empirical data, and the vertical solid black line displays equal evidence for both hypotheses.
</p>
</div>
<p>The results show that the Bayes factors are quite variable. The Bayes factor results differ in that they either provide strong evidence for the alternative model (<span class="math inline">\(BF_{10} &gt; 10\)</span>) or moderate evidence for the null model (<span class="math inline">\(BF_{10} &lt; 1/3\)</span>), despite the fact that all data sets are simulated from the same posterior predictive distribution. The majority of the simulated data sets support the alternative model with moderate to weak evidence. In other words, this analysis reveals a  “dance of the Bayes factors” with simulated repetitions of the same study, similar to the “dance of <span class="math inline">\(p\)</span>-values” <span class="citation">(Cumming <a href="#ref-cumming2014new" role="doc-biblioref">2014</a>)</span>. The variation in these findings demonstrates a very important point that is not widely appreciated: the evidence we get from a particular Bayes factors calculation may not hold up if the same study is replicated. Just obtaining a large Bayes factor alone is not necessarily informative; the variation in the Bayes factor under (hypothetical or actual) repeated sampling needs to be considered as well.</p>
<p>Why are there variations in the Bayes factors amongst the simulated data sets? The difference between the two sentence types’ reading times, and thus the experimental effect from which we want to draw conclusions, could vary depending on the noise and uncertainty in the posterior predictive simulations. This is one obvious explanation for why the results could be so different. Plotting the Bayes factors from this simulated data set against the estimated difference in simulated reading times between the two sentence types (as determined by the Bayesian model) therefore provides an interesting perspective. In other words, we take the population-level effects of the Bayesian model and extract the estimated mean difference in reading times at the verb between plural and singular attractor conditions. Then, we plot the Bayes factor as a function of this difference (along with 95% credible intervals).</p>
<div class="figure"><span style="display:block;" id="fig:BFregression"></span>
<img src="bookdown_files/figure-html/BFregression-1.svg" alt="The Bayes factor (BF10) as a function of the estimate (with 95 percent credible intervals) of the facilitatory effect of retrieval interference across 50 simulated data sets. The prior is from a meta-analysis." width="576" />
<p class="caption">
FIGURE 15.7: The Bayes factor (BF10) as a function of the estimate (with 95 percent credible intervals) of the facilitatory effect of retrieval interference across 50 simulated data sets. The prior is from a meta-analysis.
</p>
</div>
<p>The findings (illustrated in Figure <a href="ch-bf.html#fig:BFregression">15.7</a>) demonstrate that there are significant variations in the average difference in reading times between experimental conditions amongst posterior predictive simulations. This suggests that there is little information about the effect of interest in the experimental data and design.
Of course, if the data is noisy, Bayes factor analyses based on this simulated data cannot be stable across simulations either.
Therefore, as evident from Figure <a href="ch-bf.html#fig:BFregression">15.7</a>, one of the main factors influencing the Bayes factor computations is, in fact, the variation in mean reading times between experimental conditions <span class="citation">(other model parameters don’t show a close association; see the published version of the arXiv article Schad et al. <a href="#ref-SchadEtAlBF" role="doc-biblioref">2021</a>)</span>.</p>
<p>The Bayes factor BF10 in Figure <a href="ch-bf.html#fig:BFregression">15.7</a> increases with the difference in reading times, meaning that the more quickly the plural noun condition (in this case, “cabinets” in example sentence 2) is read in comparison to the singular noun condition (i.e., “cabinet”; example sentence 1), the stronger the evidence is in favor of the alternative model.
Conversely, when the difference in reading times becomes less negative, that is, when the plural condition (sentence 2) is not read noticeably faster than the singular condition (sentence 1), the Bayes factor BF10 drops to values smaller than 1.
Crucially, this behavior arises from the fact that we are utilizing informative priors from the meta-analysis, where the agreement attraction effect’s prior mean has a negative value (i.e., a prior mean of <span class="math inline">\(-0.03\)</span>) instead of being centered at a mean of zero. A null model of no effect is therefore more consistent with reading time differences that are less negative or more positive than this prior mean. This also leads to the startling conclusion that, in contrast to the much more variable Bayes factor results, the 95% credible intervals are fairly consistent and do not all overlap with zero. This should alarm researchers who use the 95% credible interval to decide whether an effect is present or not, i.e., to make a discovery claim.</p>
<p>The precise question of whether the data provide more support for the effect size found in the meta-analysis than the absence of any effect is addressed by computing Bayes factors for such a prior with a non-zero mean.</p>
<p>The important lesson to learn from this analysis is that Bayes factors can be quite variable for different data sets assessing the same phenomenon. Individual data sets in the cognitive sciences often do not contain a lot of information about the phenomenon of interest, even when–as is the case here with agreement attraction–the phenomenon is thought to be a relatively robust phenomenon. For a more detailed investigation of how Bayes factors can vary with data, in both simulated and real replication studies, we refer the reader to the published version of the arXiv article, and <span class="citation">Vasishth, Yadav, et al. (<a href="#ref-SampleSizeCBB2021" role="doc-biblioref">2022</a>)</span>.</p>
</div>
</div>
<div id="sec-caution" class="section level3 hasAnchor" number="15.5.3">
<h3><span class="header-section-number">15.5.3</span> A cautionary note about Bayes factors<a href="ch-bf.html#sec-caution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Just like frequentist <span class="math inline">\(p\)</span>-values <span class="citation">(Wasserstein and Lazar <a href="#ref-pvals" role="doc-biblioref">2016</a>)</span>, Bayes factors are easy to misuse and misinterpret, and have the potential to mislead the scientist if used in an automated manner. A recent article <span class="citation">(Tendeiro et al. <a href="#ref-tendeiro" role="doc-biblioref">2023</a>)</span> reviews many of the  misuses of Bayes factors analyses in psychology and related areas. As discussed in this chapter, Bayes factors (and Bayesian analysis in general) require a great deal of thought; there is no substitute for sensitivity analyses, and the development of sensible priors. Using default priors and deriving black and white conclusions from Bayes factor analyses is never a good idea.</p>
</div>
</div>
<div id="sample-size-determination-using-bayes-factors" class="section level2 hasAnchor" number="15.6">
<h2><span class="header-section-number">15.6</span> Sample size determination using Bayes factors<a href="ch-bf.html#sample-size-determination-using-bayes-factors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section contains text adapted from <span class="citation">Vasishth, Yadav, et al. (<a href="#ref-SampleSizeCBB2021" role="doc-biblioref">2022</a>)</span>.</p>
<p>When planning a new experiment, it is possible to take what is a fundamentally frequentist approach to work out what sample size one would need in order to cross a certain Bayes factor threshold of evidence.</p>
<p>It may sound surprising to Bayesian modelers that sample size planning is even something to plan for: One of the many advantages of Bayesian modeling is that it is straightforward to plan an experiment without necessarily specifying the sample size in advance <span class="citation">(e.g., Spiegelhalter, Abrams, and Myles <a href="#ref-spiegelhalter2004bayesian" role="doc-biblioref">2004</a>)</span>. Indeed, in our own research, running an experiment until some precision criterion in the posterior distribution is reached <span class="citation">(Freedman, Lowe, and Macaskill <a href="#ref-Freedman1984" role="doc-biblioref">1984</a>; Spiegelhalter, Freedman, and Parmar <a href="#ref-spiegelhalter1994bayesian" role="doc-biblioref">1994</a>; Kruschke <a href="#ref-kruschke2014doing" role="doc-biblioref">2014</a>; Kruschke and Liddell <a href="#ref-kruschke2018bayesian" role="doc-biblioref">2018</a>)</span> is our method of choice <span class="citation">(Jäger et al. <a href="#ref-JaegerMertzenVanDykeVasishth2019" role="doc-biblioref">2020</a>; Vasishth et al. <a href="#ref-VasishthMertzenJaegerGelman2018" role="doc-biblioref">2018</a>; Stone et al. <a href="#ref-stoneNOL2021" role="doc-biblioref">2023</a>)</span>. This approach is possible to implement if one has sufficient financial resources (and time) to keep running an experiment till a particular precision criterion is reached.</p>
<p>However, even when planning a Bayesian analysis, there can be situations where one needs to determine sample size in advance. One important situation where this becomes necessary is when one applies for research funding. In a funding proposal, one obviously has to specify the sample size in advance in order to ask for the necessary funds for conducting the study. Other situations where sample size planning is needed is in the design of clinical trials, the design of replication trials, and when pre-registering experiments and/or preparing registered reports.</p>
<p>There exist good proposals on how to work out sample sizes in advance, specifically in the case of Bayesian analyses. For example, <span class="citation">Wang and Gelfand (<a href="#ref-wang2002simulation" role="doc-biblioref">2002</a>)</span> aims to ensure that the researcher obtains strong evidence for the effect being estimated.</p>
<p>In the present paper, we unpack the approach taken in <span class="citation">Wang and Gelfand (<a href="#ref-wang2002simulation" role="doc-biblioref">2002</a>)</span>. The approach is important because it provides an easy-to-implement workflow for doing sample size calculations using complex hierarchical models of the type we discuss in the present book.</p>
<p>The <span class="citation">Wang and Gelfand (<a href="#ref-wang2002simulation" role="doc-biblioref">2002</a>)</span> approach is as follows. We have adapted the procedure outlined below slightly for our purposes, but the essential ideas are due to these authors.</p>
<ol style="list-style-type: decimal">
<li>Decide on a distribution for the effect sizes you wish to detect.</li>
<li>Choose a criterion that counts as a threshold for a decision. This can be a Bayes factor of, say, 10 <span class="citation">(Jeffreys <a href="#ref-jeffreys1939theory" role="doc-biblioref">1939</a>)</span>.<a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a></li>
<li>Then do the following for increasing sample sizes <span class="math inline">\(n\)</span>:
<ol style="list-style-type: decimal">
<li>Simulate prior predictive data <span class="math inline">\(niter\)</span> times (say, <span class="math inline">\(niter=100\)</span>) for sample size <span class="math inline">\(n\)</span>; use informative priors <span class="citation">(these are referred to as sampling priors in Wang and Gelfand <a href="#ref-wang2002simulation" role="doc-biblioref">2002</a>)</span>.</li>
<li>Fit the model to the simulated data using uninformative priors <span class="citation">(these are called fitting priors in Wang and Gelfand <a href="#ref-wang2002simulation" role="doc-biblioref">2002</a>)</span>, and derive the posterior distribution each time, and compute the Bayes factor using a null model that assumes a zero effect for the parameter of interest.</li>
<li>Display, in one plot, the <span class="math inline">\(niter\)</span> posterior distributions and the Bayes factors. If the chosen decision criterion is met reasonably well under repeated sampling for a given sample size, choose that sample size.</li>
</ol></li>
</ol>
<p>In <span class="citation">Vasishth, Yadav, et al. (<a href="#ref-SampleSizeCBB2021" role="doc-biblioref">2022</a>)</span>, we show how this approach can be adapted for the
types of models we discuss in the present chapter.</p>
</div>
<div id="summary-12" class="section level2 hasAnchor" number="15.7">
<h2><span class="header-section-number">15.7</span> Summary<a href="ch-bf.html#summary-12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayes factors are a very important tool in Bayesian data analysis. They allow the researcher to quantify the evidence in favor of certain effects in the data by comparing a full model, which contains a parameter corresponding to the effect of interest, with a null model, that does not contain that parameter. We saw that Bayes factor analyses are highly sensitive to priors specified for the parameters; this is true both for the parameter corresponding to the effect of interest, but also sometimes for priors relating to other parameters in the model, such as the intercept. It is therefore very important to perform prior predictive checks to select good and plausible priors. Moreover, sensitivity analyses, where Bayes factors are investigated for differing prior assumptions, should be standardly reported in any analysis involving Bayes factors. We studied theoretical aspects of Bayes factors and saw that bridge sampling requires a very large effective sample size in order to obtain stable results for approximate Bayes factors. Therefore, one should always perform a Bayes factor analysis at least twice to ensure that the results are stable. Bridge sampling comes with no strong guarantees concerning its accuracy, and we saw that simulation-based calibration can be used to evaluate the accuracy of Bayes factor estimates. Last, we learned that Bayes factors can strongly vary with the data. In the cognitive sciences, the data are–even for relatively robust effects–often not stable due to small effect sizes and limited sample size. Therefore, also the resulting Bayes factors can strongly vary with the data. As a consequence, only large effect sizes, large sample studies, and/or replication studies can lead to reliable inferences from empirical data in the cognitive sciences.</p>
<p>One topic that was not discussed in detail in this chapter is data aggregation. In repeated measures data, null hypothesis Bayes factor analyses can be performed on the raw data, i.e., without aggregation, by using Bayesian hierarchical models. In an alternative approach, the data are first aggregated by taking the mean per subject and condition, before running null hypothesis Bayes factor analyses on the aggregated data. Inferences / Bayes factors based on aggregated data can be biased, when either (i) item variability is present in addition to subject variability, or (ii) when the sphericity assumption (inherent in repeated measures ANOVA) is violated <span class="citation">(Schad, Nicenboim, and Vasishth <a href="#ref-schad2022data" role="doc-biblioref">2023</a>)</span>. In these cases, aggregated analyses can lead to biased results and should not be used. By contrast, non-aggregated analyses are robust also in these cases and yield accurate Bayes factor estimates.</p>
</div>
<div id="further-reading-12" class="section level2 hasAnchor" number="15.8">
<h2><span class="header-section-number">15.8</span> Further reading<a href="ch-bf.html#further-reading-12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A detailed explanation on how bridge sampling works can be found in <span class="citation">Gronau et al. (<a href="#ref-gronauTutorialBridgeSampling2017" role="doc-biblioref">2017</a>)</span>, and more details about the bridgesampling package can be found in <span class="citation">Gronau, Singmann, and Wagenmakers (<a href="#ref-gronauBridgesamplingPackageEstimating2017" role="doc-biblioref">2017</a>)</span>. <span class="citation">Wagenmakers et al. (<a href="#ref-wagenmakers2010BayesianHypothesisTesting" role="doc-biblioref">2010</a>)</span> provides a complete tutorial and the mathematical proof of the Savage–Dickey method; also see <span class="citation">O’Hagan and Forster (<a href="#ref-kendall2004" role="doc-biblioref">2004</a>)</span> and <a href="https://statproofbook.github.io/P/bf-sddr.html" class="uri">https://statproofbook.github.io/P/bf-sddr.html</a>. Some limitations of the Savage–Dickey approach under certain conditions are discussed in <span class="citation">Heck (<a href="#ref-heck2019caveat" role="doc-biblioref">2019</a>)</span>. The package <code>bayestestr</code> <span class="citation">(Makowski, Ben-Shachar, and Lüdecke <a href="#ref-makowski2019bayestestr" role="doc-biblioref">2019</a>)</span> can also be used for Bayes factor computations using the Savage–Dickey method. For a Bayes Factor Test calibrated to investigate replication success, see <span class="citation">Verhagen and Wagenmakers (<a href="#ref-verhagenBayesianTestsQuantify2014" role="doc-biblioref">2014</a>)</span>. A special issue on hierarchical modeling and Bayes factors appears in the journal Computational Brain and Behavior in response to an article by <span class="citation">van Doorn et al. (<a href="#ref-van2021bayes" role="doc-biblioref">2021</a>)</span>. <span class="citation">Kruschke and Liddell (<a href="#ref-kruschke2018bayesian" role="doc-biblioref">2018</a>)</span> discuss alternatives to Bayes factors for hypothesis testing. An argument against null hypothesis testing with Bayes Factors appears in this blog post by Andrew Gelman: <a href="https://statmodeling.stat.columbia.edu/2019/09/10/i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing/" class="uri">https://statmodeling.stat.columbia.edu/2019/09/10/i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing/</a>.
An argument in favor of null hypothesis testing with Bayes Factor as an approximation (but assuming realistic effects) appears in: <a href="https://statmodeling.stat.columbia.edu/2018/03/10/incorporating-bayes-factor-understanding-scientific-information-replication-crisis/" class="uri">https://statmodeling.stat.columbia.edu/2018/03/10/incorporating-bayes-factor-understanding-scientific-information-replication-crisis/</a>.
A visualization of the distinction between Bayes factor and k-fold cross-validation is in a blog post by Fabian Dablander, <a href="https://tinyurl.com/47n5cte4" class="uri">https://tinyurl.com/47n5cte4</a>. Decision theory, which was only mentioned in passing in this chapter, is discussed in <span class="citation">Parmigiani and Inoue (<a href="#ref-parmigiani2009decision" role="doc-biblioref">2009</a>)</span>.
Hypothesis testing in its different flavors is discussed in <span class="citation">Robert (<a href="#ref-robert202250" role="doc-biblioref">2022</a>)</span>.
Alternative ways to test the accuracy of Bayes factors are discussed in <span class="citation">Sekulovski, Marsman, and Wagenmakers (<a href="#ref-sekulovskigood" role="doc-biblioref">2024</a>)</span>.
When planning studies, Bayes-factor based power calculations can be carried out; an example of such power computations in the context of psycholinguistics, which uses the software tools discussed in the present book, is <span class="citation">Vasishth, Yadav, et al. (<a href="#ref-SampleSizeCBB2021" role="doc-biblioref">2022</a>)</span> (also see the references cited there).</p>
</div>
<div id="exercises-2" class="section level2 hasAnchor" number="15.9">
<h2><span class="header-section-number">15.9</span> Exercises<a href="ch-bf.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:bysubjects" class="exercise"><strong>Exercise 15.1  </strong></span>Is there evidence for differences in the effect of cloze probability among the subjects?</p>
</div>
<p>Use Bayes factor to compare the log cloze probability model that we examined in section <a href="ch-bf.html#sec-BFnonnested">15.2.2</a> with a similar model but that incorporates the strong assumption of no difference between subjects for the effect of cloze (<span class="math inline">\(\tau_{u_2}=0\)</span>).</p>
<div class="exercise">
<p><span id="exr:bf-logn" class="exercise"><strong>Exercise 15.2  </strong></span>Is there evidence for the claim that English subject relative clauses are easier to process than object relative clauses?</p>
</div>
<p>Consider again the reading time data coming from Experiment 1 of <span class="citation">Grodner and Gibson (<a href="#ref-grodner" role="doc-biblioref">2005</a>)</span> presented in exercise <a href="ch-hierarchical.html#exr:hierarchical-logn">5.2</a>:</p>
<div class="sourceCode" id="cb913"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb913-1"><a href="ch-bf.html#cb913-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_gg05_rc&quot;</span>)</span>
<span id="cb913-2"><a href="ch-bf.html#cb913-2" aria-hidden="true"></a>df_gg05_rc</span></code></pre></div>
<pre><code>## # A tibble: 672 × 7
##    subj  item condition    RT residRT qcorrect experiment
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;     
## 1     1     1 objgap      320   -21.4        0 tedrg3    
## 2     1     2 subjgap     424    74.7        1 tedrg2    
## 3     1     3 objgap      309   -40.3        0 tedrg3    
## # ℹ 669 more rows</code></pre>
<p>As in exercise <a href="ch-hierarchical.html#exr:hierarchical-logn">5.2</a>, you should use a sum coding for the predictors. Here, object relative clauses (<code>"objgaps"</code>) are coded <span class="math inline">\(+1/2\)</span>, and subject relative clauses as <span class="math inline">\(-1/2\)</span>.</p>
<div class="sourceCode" id="cb915"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb915-1"><a href="ch-bf.html#cb915-1" aria-hidden="true"></a>df_gg05_rc &lt;-<span class="st"> </span>df_gg05_rc <span class="op">%&gt;%</span></span>
<span id="cb915-2"><a href="ch-bf.html#cb915-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cond =</span> <span class="kw">if_else</span>(condition <span class="op">==</span><span class="st"> &quot;objgap&quot;</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">-1</span><span class="op">/</span><span class="dv">2</span>))</span></code></pre></div>
<p>Using the Bayes factors function shown in this chapter, quantify the evidence against the null model (no population-level reading time difference between SRC and ORC) relative to the following alternative models:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, 1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, 0.1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, 0.01)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}_+(0, 1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}_+(0, 0.1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}_+(0, 0.01)\)</span></li>
</ol>
<p>(A <span class="math inline">\(\mathit{Normal}_+(.)\)</span> prior can be set in <code>brms</code> by defining a lower boundary as <span class="math inline">\(0\)</span>, with the argument <code>lb = 0</code>.)</p>
<p>What are the Bayes factors in favor of the alternative models a-f, compared to the null model?</p>
<p>Now carry out a standard frequentist likelihood ratio test using the <code>anova()</code> function that is used with the <code>lmer()</code> function. The commands
for doing this comparison would be:</p>
<div class="sourceCode" id="cb916"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb916-1"><a href="ch-bf.html#cb916-1" aria-hidden="true"></a>m_full &lt;-<span class="st"> </span><span class="kw">lmer</span>(<span class="kw">log</span>(RT) <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span></span>
<span id="cb916-2"><a href="ch-bf.html#cb916-2" aria-hidden="true"></a><span class="st">                 </span>(c_cond <span class="op">||</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">||</span><span class="st"> </span>item),</span>
<span id="cb916-3"><a href="ch-bf.html#cb916-3" aria-hidden="true"></a>               df_gg05_rc)</span>
<span id="cb916-4"><a href="ch-bf.html#cb916-4" aria-hidden="true"></a>m_null &lt;-<span class="st"> </span><span class="kw">lmer</span>(<span class="kw">log</span>(RT) <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(c_cond<span class="op">||</span>subj) <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">||</span><span class="st"> </span>item),</span>
<span id="cb916-5"><a href="ch-bf.html#cb916-5" aria-hidden="true"></a>               df_gg05_rc)</span>
<span id="cb916-6"><a href="ch-bf.html#cb916-6" aria-hidden="true"></a><span class="kw">anova</span>(m_null, m_full)</span></code></pre></div>
<p>How do the conclusions from the Bayes factor analyses compare with the conclusion we obtain from the frequentist model comparison?</p>
<div class="exercise">
<p><span id="exr:bf-logistic" class="exercise"><strong>Exercise 15.3  </strong></span>In the Grodner and Gibson 2005 data, in question-response accuracies, is there evidence for the claim that sentences with subject relative clauses are easier to comprehend?</p>
</div>
<p>Consider the question response accuracy of the data of Experiment 1 of <span class="citation">Grodner and Gibson (<a href="#ref-grodner" role="doc-biblioref">2005</a>)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Compare a model that assumes that RC type affects question accuracy on the population level and with the effect varying by-subjects and by-items with <em>a null model</em> that assumes that there is no population-level effect present.</li>
<li>Compare a model that assumes that RC type affects question accuracy on the population level and with the effect varying by-subjects and by-items with <em>another null model</em> that assumes that there is no population-level or group-level effect present, that is no by-subject or by-item effects. What’s the meaning of the results of the Bayes factor analysis?</li>
</ol>
<p>Assume that for the effect of RC on question accuracy, <span class="math inline">\(\beta \sim \mathit{Normal}(0, 0.1)\)</span> is a reasonable prior, and that for all the variance components, the same prior, <span class="math inline">\(\tau \sim \mathit{Normal}_{+}(0, 1)\)</span>, is a reasonable prior.</p>
<div class="exercise">
<p><span id="exr:lognstan" class="exercise"><strong>Exercise 15.4  </strong></span>Bayes factor and bounded parameters using Stan.</p>
</div>
<p>Re-fit the data of a single subject pressing a button repeatedly from <a href="ch-reg.html#sec-trial">4.2</a> from <code>data("df_spacebar")</code>, coding the model in Stan.</p>
<p>Start by assuming the following likelihood and priors:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim \mathit{LogNormal}(\alpha + c\_trial_n \cdot \beta,\sigma)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(6, 1.5) \\
\beta &amp;\sim \mathit{Normal}_+(0, 0.1)\\
\sigma &amp;\sim \mathit{Normal}_+(0, 1)
\end{aligned}
\end{equation}\]</span></p>
<p>Use the Bayes factor to answer the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li>Is there evidence for any effect of trial number in comparison with no effect?</li>
<li>Is there evidence for a positive effect of trial number (as the subject reads further, they slowdown) in comparison with no effect?</li>
<li>Is there evidence for a negative effect of trial number (as the subject reads further, they speedup) in comparison with no effect?</li>
<li>Is there evidence for a positive effect of trial number in comparison with a negative effect?</li>
</ol>
<p>(Expect very large Bayes factors in this exercise.)</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references hanging-indent">
<div id="ref-abbl02">
<p>Anderson, John R., Dan Bothell, Michael D Byrne, Scott Douglass, Christian Lebiere, and Yulin Qin. 2004. “An Integrated Theory of the Mind.” <em>Psychological Review</em> 111 (4): 1036–60.</p>
</div>
<div id="ref-bennettEfficientEstimationFree1976">
<p>Bennett, Charles H. 1976. “Efficient Estimation of Free Energy Differences from Monte Carlo Data.” <em>Journal of Computational Physics</em> 22 (2): 245–68. <a href="https://doi.org/10.1016/0021-9991(76)90078-4">https://doi.org/10.1016/0021-9991(76)90078-4</a>.</p>
</div>
<div id="ref-bishop2006pattern">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.</p>
</div>
<div id="ref-ChenGoodman1999">
<p>Chen, Stanley F., and Joshua Goodman. 1999. “An Empirical Study of Smoothing Techniques for Language Modeling.” <em>Computer Speech &amp; Language</em> 13 (4): 359–94. <a href="https://doi.org/https://doi.org/10.1006/csla.1999.0128">https://doi.org/https://doi.org/10.1006/csla.1999.0128</a>.</p>
</div>
<div id="ref-cumming2014new">
<p>Cumming, Geoff. 2014. “The New Statistics: Why and How.” <em>Psychological Science</em> 25 (1): 7–29.</p>
</div>
<div id="ref-DickeyLientz1970">
<p>Dickey, James M., and B. P. Lientz. 1970. “The Weighted Likelihood Ratio, Sharp Hypotheses About Chances, the Order of a Markov Chain.” <em>The Annals of Mathematical Statistics</em> 41 (1): 214–26. <a href="https://www.jstor.org/stable/2239734">https://www.jstor.org/stable/2239734</a>.</p>
</div>
<div id="ref-dillon2011structured">
<p>Dillon, Brian W. 2011. “Structured Access in Sentence Comprehension.” PhD thesis, University of Maryland.</p>
</div>
<div id="ref-EngelmannJaegerVasishth2019">
<p>Engelmann, Felix, Lena A. Jäger, and Shravan Vasishth. 2020. “The Effect of Prominence and Cue Association in Retrieval Processes: A Computational Account.” <em>Cognitive Science</em> 43 (12): e12800. <a href="https://doi.org/10.1111/cogs.12800">https://doi.org/10.1111/cogs.12800</a>.</p>
</div>
<div id="ref-Freedman1984">
<p>Freedman, Laurence S., D. Lowe, and P. Macaskill. 1984. “Stopping Rules for Clinical Trials Incorporating Clinical Opinion.” <em>Biometrics</em> 40 (3): 575–86.</p>
</div>
<div id="ref-gelmancarlin">
<p>Gelman, Andrew, and John B. Carlin. 2014. “Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors.” <em>Perspectives on Psychological Science</em> 9 (6): 641–51. <a href="https://doi.org/https://doi.org/10.1177/1745691614551642">https://doi.org/https://doi.org/10.1177/1745691614551642</a>.</p>
</div>
<div id="ref-grodner">
<p>Grodner, Daniel, and Edward Gibson. 2005. “Consequences of the Serial Nature of Linguistic Input.” <em>Cognitive Science</em> 29: 261–90. <a href="https://doi.org/https://doi.org/10.1207/s15516709cog0000_7">https://doi.org/https://doi.org/10.1207/s15516709cog0000_7</a>.</p>
</div>
<div id="ref-gronauTutorialBridgeSampling2017">
<p>Gronau, Quentin F., Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Marsman, David S. Leslie, Jonathan J. Forster, Eric-Jan Wagenmakers, and Helen Steingroever. 2017. “A Tutorial on Bridge Sampling.” <em>Journal of Mathematical Psychology</em> 81: 80–97. <a href="https://doi.org/10.1016/j.jmp.2017.09.005">https://doi.org/10.1016/j.jmp.2017.09.005</a>.</p>
</div>
<div id="ref-gronauBridgesamplingPackageEstimating2017">
<p>Gronau, Quentin F., Henrik Singmann, and Eric-Jan Wagenmakers. 2017. “Bridgesampling: An R Package for Estimating Normalizing Constants.” <em>arXiv Preprint arXiv:1710.08162</em>. <a href="http://arxiv.org/abs/1710.08162">http://arxiv.org/abs/1710.08162</a>.</p>
</div>
<div id="ref-hammerly2019grammaticality">
<p>Hammerly, Christopher, Adrian Staub, and Brian W. Dillon. 2019. “The Grammaticality Asymmetry in Agreement Attraction Reflects Response Bias: Experimental and Modeling Evidence.” <em>Cognitive Psychology</em> 110: 70–104. <a href="https://doi.org/https://doi.org/10.1016/j.cogpsych.2019.01.001">https://doi.org/https://doi.org/10.1016/j.cogpsych.2019.01.001</a>.</p>
</div>
<div id="ref-heck2019caveat">
<p>Heck, Daniel W. 2019. “A Caveat on the Savage–Dickey Density Ratio: The Case of Computing Bayes Factors for Regression Parameters.” <em>British Journal of Mathematical and Statistical Psychology</em> 72 (2): 316–33.</p>
</div>
<div id="ref-JaegerEngelmannVasishth2017">
<p>Jäger, Lena A., Felix Engelmann, and Shravan Vasishth. 2017. “Similarity-Based Interference in Sentence Comprehension: Literature review and Bayesian meta-analysis.” <em>Journal of Memory and Language</em> 94: 316–39. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004">https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004</a>.</p>
</div>
<div id="ref-JaegerMertzenVanDykeVasishth2019">
<p>Jäger, Lena A., Daniela Mertzen, Julie A. Van Dyke, and Shravan Vasishth. 2020. “Interference Patterns in Subject-Verb Agreement and Reflexives Revisited: A Large-Sample Study.” <em>Journal of Memory and Language</em> 111. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2019.104063">https://doi.org/https://doi.org/10.1016/j.jml.2019.104063</a>.</p>
</div>
<div id="ref-jeffreys1939theory">
<p>Jeffreys, Harold. 1939. <em>Theory of Probability</em>. Oxford: Clarendon Press.</p>
</div>
<div id="ref-kass1995bayes">
<p>Kass, Robert E., and Adrian E. Raftery. 1995. “Bayes Factors.” <em>Journal of the American Statistical Association</em> 90 (430): 773–95. <a href="https://doi.org/10.1080/01621459.1995.10476572">https://doi.org/10.1080/01621459.1995.10476572</a>.</p>
</div>
<div id="ref-kruschke2014doing">
<p>Kruschke, John K. 2014. <em>Doing Bayesian Data Analysis: A tutorial with R, JAGS, and Stan</em>. Academic Press.</p>
</div>
<div id="ref-kruschke2018bayesian">
<p>Kruschke, John K., and Torrin M. Liddell. 2018. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” <em>Psychonomic Bulletin &amp; Review</em> 25 (1): 178–206. <a href="https://doi.org/https://doi.org/10.3758/s13423-016-1221-4">https://doi.org/https://doi.org/10.3758/s13423-016-1221-4</a>.</p>
</div>
<div id="ref-lago2015agreement">
<p>Lago, Sol, Diego Shalom, Mariano Sigman, Ellen F. Lau, and Colin Phillips. 2015. “Agreement Processes in Spanish Comprehension.” <em>Journal of Memory and Language</em> 82: 133–49. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2015.02.002">https://doi.org/https://doi.org/10.1016/j.jml.2015.02.002</a>.</p>
</div>
<div id="ref-lewisvasishth:cogsci05">
<p>Lewis, Richard L., and Shravan Vasishth. 2005. “An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval.” <em>Cognitive Science</em> 29: 1–45. <a href="https://doi.org/%2010.1207/s15516709cog0000_25">https://doi.org/ 10.1207/s15516709cog0000_25</a>.</p>
</div>
<div id="ref-Lidstone1920">
<p>Lidstone, George James. 1920. “Note on the General Case of the Bayes-Laplace Formula for Inductive or a Posteriori Probabilities.” <em>Transactions of the Faculty of Actuaries</em> 8 (182-192): 13.</p>
</div>
<div id="ref-mackay">
<p>MacKay, David J. C. 2003. <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge, UK: Cambridge University Press.</p>
</div>
<div id="ref-makowski2019bayestestr">
<p>Makowski, Dominique, Mattan S. Ben-Shachar, and Daniel Lüdecke. 2019. “BayestestR: Describing Effects and Their Uncertainty, Existence and Significance Within the Bayesian Framework.” <em>Journal of Open Source Software</em> 4 (40): 1541. <a href="https://doi.org/https://doi.org/10.21105/joss.01541">https://doi.org/https://doi.org/10.21105/joss.01541</a>.</p>
</div>
<div id="ref-mengSimulatingRatiosNormalizing1996">
<p>Meng, Xiao-li, and Wing Hung Wong. 1996. “Simulating Ratios of Normalizing Constants via a Simple Identity: A Theoretical Exploration.” <em>Statistica Sinica</em>, 831–60. <a href="https://doi.org/http://www.jstor.org/stable/24306045">https://doi.org/http://www.jstor.org/stable/24306045</a>.</p>
</div>
<div id="ref-navarro2015learning">
<p>Navarro, Danielle J. 2015. <em>Learning Statistics with R</em>. https://learningstatisticswithr.com.</p>
</div>
<div id="ref-NicenboimPreactivation2019">
<p>Nicenboim, Bruno, Shravan Vasishth, and Frank Rösler. 2020. “Are Words Pre-Activated Probabilistically During Sentence Comprehension? Evidence from New Data and a Bayesian Random-Effects Meta-Analysis Using Publicly Available Data.” <em>Neuropsychologia</em> 142. <a href="https://doi.org/10.1016/j.neuropsychologia.2020.107427">https://doi.org/10.1016/j.neuropsychologia.2020.107427</a>.</p>
</div>
<div id="ref-nieuwlandLargescaleReplicationStudy2018">
<p>Nieuwland, Mante S., Stephen Politzer-Ahles, Evelien Heyselaar, Katrien Segaert, Emily Darley, Nina Kazanina, Sarah Von Grebmer Zu Wolfsthurn, et al. 2018. “Large-Scale Replication Study Reveals a Limit on Probabilistic Prediction in Language Comprehension.” <em>eLife</em> 7. <a href="https://doi.org/10.7554/eLife.33468">https://doi.org/10.7554/eLife.33468</a>.</p>
</div>
<div id="ref-Aust2024">
<p>Oberauer, Klaus, Philipp Musfeld, and Frederik Aust. 2024. “The Stability of Bayes-Factor Estimates for Testing Effects in Generalized Linear Models: Bridge Sampling and Savage–Dickey Ratio.”</p>
</div>
<div id="ref-ohagan2006uncertain">
<p>O’Hagan, Anthony, Caitlin E. Buck, Alireza Daneshkhah, J. Richard Eiser, Paul H. Garthwaite, David J. Jenkinson, Jeremy E. Oakley, and Tim Rakow. 2006. <em>Uncertain Judgements: Eliciting Experts’ Probabilities</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-kendall2004">
<p>O’Hagan, Anthony, and Jonathan J. Forster. 2004. “Kendall’s Advanced Theory of Statistics, Vol. 2B: Bayesian Inference.” Wiley.</p>
</div>
<div id="ref-parmigiani2009decision">
<p>Parmigiani, Giovanni, and Lurdes Inoue. 2009. <em>Decision Theory: Principles and Approaches</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-robert202250">
<p>Robert, Christian P. 2022. “50 Shades of Bayesian Testing of Hypotheses.” In <em>Advancements in Bayesian Methods and Implementation</em>, edited by Arni S. R. Srinivasa Rao, G. Alastair Young, and C. R. Rao, 47:103–20. Handbook of Statistics. Elsevier. <a href="https://doi.org/https://doi.org/10.1016/bs.host.2022.06.003">https://doi.org/https://doi.org/10.1016/bs.host.2022.06.003</a>.</p>
</div>
<div id="ref-rouder2018bayesian">
<p>Rouder, Jeffrey N, Julia M. Haaf, and Joachim Vandekerckhove. 2018. “Bayesian Inference for Psychology, Part IV: Parameter Estimation and Bayes Factors.” <em>Psychonomic Bulletin &amp; Review</em> 25 (1): 102–13. <a href="https://doi.org/https://doi.org/10.3758/s13423-017-1420-7">https://doi.org/https://doi.org/10.3758/s13423-017-1420-7</a>.</p>
</div>
<div id="ref-rouder2009bayesian">
<p>Rouder, Jeffrey N, Paul L Speckman, Dongchu Sun, Richard D Morey, and Geoffrey Iverson. 2009. “Bayesian t Tests for Accepting and Rejecting the Null Hypothesis.” <em>Psychonomic Bulletin &amp; Review</em> 16 (2): 225–37. <a href="https://doi.org/https://doi.org/10.3758/PBR.16.2.225">https://doi.org/https://doi.org/10.3758/PBR.16.2.225</a>.</p>
</div>
<div id="ref-Royall">
<p>Royall, Richard. 1997. <em>Statistical Evidence: A Likelihood Paradigm</em>. New York: Chapman; Hall, CRC Press.</p>
</div>
<div id="ref-schad2020toward">
<p>Schad, Daniel J., Michael J. Betancourt, and Shravan Vasishth. 2019. “Toward a Principled Bayesian Workflow in Cognitive Science.” <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/ARXIV.1904.12765">https://doi.org/10.48550/ARXIV.1904.12765</a>.</p> 2020. “Toward a Principled Bayesian Workflow in Cognitive Science.” <em>Psychological Methods</em> 26 (1): 103–26. <a href="https://doi.org/https://doi.org/10.1037/met0000275">https://doi.org/https://doi.org/10.1037/met0000275</a>.</p>
</div>
<div id="ref-SchadEtAlBF">
<p>Schad, Daniel J., Bruno Nicenboim, Paul-Christian Bürkner, Michael J. Betancourt, and Shravan Vasishth. 2021. “Workflow Techniques for the Robust Use of Bayes Factors.” <em>arXiv Preprint arXiv:2103.08744</em>.</p>
</div>
<div id="ref-schad2022data">
<p>Schad, Daniel J., Bruno Nicenboim, and Shravan Vasishth. 2023. “Data Aggregation Can Lead to Biased Inferences in Bayesian Linear Mixed Models and Bayesian Analysis of Variance.” <em>Psychological Methods</em>. <a href="https://doi.org/https://doi.org/10.1037/met0000621">https://doi.org/https://doi.org/10.1037/met0000621</a>.</p>
</div>
<div id="ref-schonbrodt2018bayes">
<p>Schönbrodt, Felix D., and Eric-Jan Wagenmakers. 2018. “Bayes Factor Design Analysis: Planning for Compelling Evidence.” <em>Psychonomic Bulletin &amp; Review</em> 25 (1): 128–42. <a href="https://doi.org/https://doi.org/10.3758/s13423-017-1230-y">https://doi.org/https://doi.org/10.3758/s13423-017-1230-y</a>.</p>
</div>
<div id="ref-sekulovskigood">
<p>Sekulovski, Nikola, Maarten Marsman, and Eric-Jan Wagenmakers. 2024. “A Good Check on the Bayes Factor,” March. <a href="https://doi.org/10.31234/osf.io/59gj8">https://doi.org/10.31234/osf.io/59gj8</a>.</p>
</div>
<div id="ref-spiegelhalter2004bayesian">
<p>Spiegelhalter, David J., Keith R. Abrams, and Jonathan P. Myles. 2004. <em>Bayesian Approaches to Clinical Trials and Health-Care Evaluation</em>. Vol. 13. John Wiley &amp; Sons.</p>
</div>
<div id="ref-spiegelhalter1994bayesian">
<p>Spiegelhalter, David J., Laurence S. Freedman, and Mahesh K. B. Parmar. 1994. “Bayesian Approaches to Randomized Trials.” <em>Journal of the Royal Statistical Society. Series A (Statistics in Society)</em> 157 (3): 357–416.</p>
</div>
<div id="ref-stoneNOL2021">
<p>Stone, Kate, Bruno Nicenboim, Shravan Vasishth, and Frank Roesler. 2023. “Understanding the Effects of Constraint and Predictability in ERP.” <em>Neurobiology of Language</em>. <a href="https://doi.org/10.1162/nol_a_00094">https://doi.org/10.1162/nol_a_00094</a>.</p>
</div>
<div id="ref-talts2018validating">
<p>Talts, Sean, Michael J. Betancourt, Daniel P. Simpson, Aki Vehtari, and Andrew Gelman. 2018. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” <em>arXiv Preprint arXiv:1804.06788</em>.</p>
</div>
<div id="ref-tendeiro">
<p>Tendeiro, Jorge N., Henk A. L. Kiers, Rink Hoekstra, Tsz Keung Wong, and Richard D Morey. 2023. “Diagnosing the Misuse of the Bayes Factor in Applied Research.” <em>Advances in Methods and Practices in Psychological Science</em>.</p>
</div>
<div id="ref-van2021bayes">
<p>van Doorn, Johnny, Frederik Aust, Julia M. Haaf, Angelika Stefan, and Eric-Jan Wagenmakers. 2021. “Bayes Factors for Mixed Models.” <em>Computational Brain and Behavior</em>. <a href="https://doi.org/https://doi.org/10.1007/s42113-021-00113-2">https://doi.org/https://doi.org/10.1007/s42113-021-00113-2</a>.</p>
</div>
<div id="ref-van2011cue">
<p>Van Dyke, Julie A., and Brian McElree. 2011. “Cue-Dependent Interference in Comprehension.” <em>Journal of Memory and Language</em> 65 (3): 247–63. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2011.05.002">https://doi.org/https://doi.org/10.1016/j.jml.2011.05.002</a>.</p>
</div>
<div id="ref-VasishthMertzenJaegerGelman2018">
<p>Vasishth, Shravan, Daniela Mertzen, Lena A. Jäger, and Andrew Gelman. 2018. “The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability.” <em>Journal of Memory and Language</em> 103: 151–75. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2018.07.004">https://doi.org/https://doi.org/10.1016/j.jml.2018.07.004</a>.</p>
</div>
<div id="ref-SampleSizeCBB2021">
<p>Vasishth, Shravan, Himanshu Yadav, Daniel J. Schad, and Bruno Nicenboim. 2022. “Sample Size Determination for Bayesian Hierarchical Models Commonly Used in Psycholinguistics.” <em>Computational Brain and Behavior</em>. <a href="https://doi.org/https://doi.org/10.1007/s42113-021-00125-y">https://doi.org/https://doi.org/10.1007/s42113-021-00125-y</a>.</p>
</div>
<div id="ref-verhagenBayesianTestsQuantify2014">
<p>Verhagen, Josine, and Eric-Jan Wagenmakers. 2014. “Bayesian Tests to Quantify the Result of a Replication Attempt.” <em>Journal of Experimental Psychology: General</em> 143 (4): 1457–75. <a href="https://doi.org/10.1037/a0036731">https://doi.org/10.1037/a0036731</a>.</p>
</div>
<div id="ref-wagenmakersPrinciplePredictiveIrrelevance2020">
<p>Wagenmakers, Eric-Jan, Michael D Lee, Jeffrey N Rouder, and Richard D Morey. 2020. “The Principle of Predictive Irrelevance or Why Intervals Should Not Be Used for Model Comparison Featuring a Point Null Hypothesis.” In <em>The Theory of Statistics in Psychology: Applications, Use, and Misunderstandings</em>, edited by Craig W. Gruber, 111–29. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-48043-1_8">https://doi.org/10.1007/978-3-030-48043-1_8</a>.</p>
</div>
<div id="ref-wagenmakers2010BayesianHypothesisTesting">
<p>Wagenmakers, Eric-Jan, Tom Lodewyckx, Himanshu Kuriyal, and Raoul P. P. P. Grasman. 2010. “Bayesian Hypothesis Testing for Psychologists: A Tutorial on the Savage–Dickey Method.” <em>Cognitive Psychology</em> 60 (3): 158–89. <a href="https://doi.org/https://doi.org/10.1016/j.cogpsych.2009.12.001">https://doi.org/https://doi.org/10.1016/j.cogpsych.2009.12.001</a>.</p>
</div>
<div id="ref-wang2002simulation">
<p>Wang, Fei, and Alan E. Gelfand. 2002. “A Simulation-Based Approach to Bayesian Sample Size Determination for Performance Under a Given Model and for Separating Models.” <em>Statistical Science</em>, 193–208. <a href="https://doi.org/http://www.jstor.org/stable/3182824">https://doi.org/http://www.jstor.org/stable/3182824</a>.</p>
</div>
<div id="ref-pvals">
<p>Wasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” <em>The American Statistician</em> 70 (2): 129–33. <a href="https://doi.org/https://doi.org/10.1080/00031305.2016.1154108">https://doi.org/https://doi.org/10.1080/00031305.2016.1154108</a>.</p>
</div>
<div id="ref-YadavetalJML2022">
<p>Yadav, Himanshu, Garrett Smith, Sebastian Reich, and Shravan Vasishth. 2023. “Number Feature Distortion Modulates Cue-Based Retrieval in Reading.” <em>Journal of Memory and Language</em> 129. <a href="https://doi.org/10.1016/j.jml.2022.104400">https://doi.org/10.1016/j.jml.2022.104400</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="51">
<li id="fn51"><p>See Schad, D. J., Nicenboim, B., Bürkner, P. C., Betancourt, M., and Vasishth, S. (2022). Workflow techniques for the robust use of Bayes factors. Psychological Methods.<a href="ch-bf.html#fnref51" class="footnote-back">↩︎</a></p></li>
<li id="fn52"><p>For details, see Schad, D. J., Nicenboim, B., Bürkner, P. C., Betancourt, M., and Vasishth, S. (2022). Workflow techniques for the robust use of Bayes factors. Psychological Methods.<a href="ch-bf.html#fnref52" class="footnote-back">↩︎</a></p></li>
<li id="fn53"><p>This meta-analysis already includes the data that we want to make inference about; thus, this meta-analysis estimate is not really the right estimate to use, since it involves using the data twice. We ignore this detail here because our goal is simply to illustrate the approach.<a href="ch-bf.html#fnref53" class="footnote-back">↩︎</a></p></li>
<li id="fn54"><p>The Bayes factor is just one of many possible performance criteria; see <span class="citation">Wang and Gelfand (<a href="#ref-wang2002simulation" role="doc-biblioref">2002</a>)</span> for some other alternatives.<a href="ch-bf.html#fnref54" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-comparison.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-cv.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
