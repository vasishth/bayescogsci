<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Bayes factors | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Bayes factors | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Bayes factors | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2023-02-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-comparison.html"/>
<link rel="next" href="ch-cv.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b> Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b> Bayesian Regression Models using Stan: brms</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b> Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b> Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a><ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
<li class="chapter" data-level="3.11" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect finger tapping times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b> No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a><ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#model-building"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#exemplary-data-analysis"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-workflow.html"><a href="ch-workflow.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-workflow.html"><a href="ch-workflow.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a><ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-7"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-9"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparameterization</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-10"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a><ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b> Validation of a computed posterior distribution</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#simulation-based-calibration-revealing-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> Simulation-based calibration revealing a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-and-limitation-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues and limitation of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-re-implementing-the-exponential-distribution-manually"><i class="fa fa-check"></i><b>12.3</b> Another custom distribution: Re-implementing the exponential distribution manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-11"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-12"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a><ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a><ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a><ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a><ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="15.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>15.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#summary-13"><i class="fa fa-check"></i><b>15.6</b> Summary</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.7</b> Further reading</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a><ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#issues-with-cross-validation"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a><ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-14"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a><ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-15"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
<li class="chapter" data-level="18.5" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a><ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a><ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-16"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a><ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a><ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-17"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-bf" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 15</span> Bayes factors<a href="ch-bf.html#ch-bf" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter is based on a longer manuscript available on arXiv: <span class="citation">Schad et al. (<a href="#ref-SchadEtAlBF">2021</a>)</span>. Bayesian approaches provide tools for different aspects of data analysis. A key contribution of Bayesian data analysis to cognitive science is that it provides probabilistic ways to quantify the evidence that data provide in support of one model or another. Models provide ways to implement scientific hypotheses; as a consequence, model comparison and hypothesis testing are closely related. There are two kinds of hypotheses: point hypotheses, which hypothesize that a model parameter has a specific point value–such as e.g., zero. By contrast, range hypotheses specify that a parameter exists and is needed to explain the data, but they do not specify the parameter value, which can be estimated from data. Bayesian hypothesis testing of range hypotheses is implemented using Bayes factors <!-- @ly2016harold; @mulder2016editors; --><span class="citation">(Rouder, Haaf, and Vandekerckhove <a href="#ref-rouder2018bayesian">2018</a>; Schönbrodt and Wagenmakers <a href="#ref-schonbrodt2018bayes">2018</a>; Wagenmakers et al. <a href="#ref-wagenmakers2010BayesianHypothesisTesting">2010</a>; Kass and Raftery <a href="#ref-kass1995bayes">1995</a>; Gronau et al. <a href="#ref-gronau2017tutorial">2017</a><a href="#ref-gronau2017tutorial">a</a>; Jeffreys <a href="#ref-jeffreys1939theory">1939</a>)</span>, which quantify evidence in favor of one statistical (or computational) model over another. Point hypotheses are the norm in frequentist hypothesis testing, and can be implemented in Bayesian analyses using posterior density ratios. This chapter will focus on Bayes factors as the way to compare models and to obtain evidence about (range) hypotheses.</p>
<p>There are subtleties associated with Bayes factors that are not widely appreciated. For example, the results of Bayes factor analyses are highly sensitive to and crucially depend on prior assumptions about model parameters (we will illustrate this below), which can vary between experiments/research problems and even differ subjectively between different researchers. Many authors use or recommend so-called default prior distributions, where the prior parameters are fixed, and are independent of the scientific problem in question <span class="citation">(Hammerly, Staub, and Dillon <a href="#ref-hammerly2019grammaticality">2019</a>; Navarro <a href="#ref-navarro2015learning">2015</a>)</span>. However, default priors result in an overly simplistic perspective on Bayesian hypothesis testing, and can be misleading. For this reason, even though leading experts in the use of Bayes factor, such as <span class="citation">Rouder et al. (<a href="#ref-rouder2009bayesian">2009</a>)</span>, often provide default priors for computing Bayes factors, they also make it clear that: “simply put, principled inference is a thoughtful process that cannot be performed by rigid adherence to defaults” <span class="citation">(Rouder et al. <a href="#ref-rouder2009bayesian">2009</a>, 235)</span>. However, this observation does not seem to have had much impact on how Bayes factors are used in fields like psychology and psycholinguistics; the use of default priors when computing Bayes factor seems to be widespread.</p>
<p>Given the key influence of priors on Bayes factors, defining priors becomes a central issue when using Bayes factors. The priors determine which models will be compared.</p>
<p>In this chapter, we demonstrate how Bayes factors should be used in practical settings in cognitive science. In doing so, we demonstrate the strength of this approach and some important pitfalls that researchers should be aware of.</p>
<div id="hypothesis-testing-using-the-bayes-factor" class="section level2 hasAnchor">
<h2><span class="header-section-number">15.1</span> Hypothesis testing using the Bayes factor<a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="marginal-likelihood" class="section level3 hasAnchor">
<h3><span class="header-section-number">15.1.1</span> Marginal likelihood<a href="ch-bf.html#marginal-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bayes’ rule can be written with reference to a specific statistical model <span class="math inline">\(\mathcal{M}_1\)</span>.</p>
<p><span class="math display">\[\begin{equation}
p(\boldsymbol{\Theta} \mid \boldsymbol{y}, \mathcal{M}_1) = \frac{p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)}{p(\boldsymbol{y} \mid \mathcal{M}_1)}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol{y}\)</span> refers to the data and <span class="math inline">\(\boldsymbol{\Theta}\)</span> is a vector of parameters; for example, this vector could include the intercept, slope, and variance component in a linear regression model.</p>
<p>The denominator <span class="math inline">\(p(\boldsymbol{y} \mid \mathcal{M}_1)\)</span> is the marginal likelihood, and is a single number that gives us the likelihood of the observed data <span class="math inline">\(\boldsymbol{y}\)</span> given the model <span class="math inline">\(\mathcal{M}_1\)</span> (and only in the discrete case, it gives us the probability of the observed data <span class="math inline">\(\boldsymbol{y}\)</span> given the model; see section <a href="ch-intro.html#sec-marginal">1.7</a>). Because in general it’s not a probability, it should be interpreted relative to another marginal likelihood (evaluated at the same <span class="math inline">\(\boldsymbol{y}\)</span>).</p>
<p>In frequentist statistics, it’s also common to quantify evidence for the model by determining the maximum likelihood, that is, the likelihood of the data given the best-fitting model parameter. Thus, the data is used twice: once for fitting the parameter, and then for evaluating the likelihood. Importantly, this inference completely hinges upon this best-fitting parameter to be a meaningful value that represents well what we know about the parameter, and doesn’t take the uncertainty of the estimates into account. Bayesian inference quantifies the uncertainty that is associated with a parameter, that is, one accepts that the knowledge about the parameter value is uncertain. Computing the marginal likelihood entails computing the likelihood given all plausible values for the model parameter.</p>
<p>One difficulty in the above equation showing Bayes’ rule is that the marginal likelihood <span class="math inline">\(p(\boldsymbol{y} \mid \mathcal{M}_1)\)</span> in the denominator cannot be easily computed in Bayes’ rule:</p>
<p><span class="math display">\[\begin{equation}
p(\boldsymbol{\Theta} \mid \boldsymbol{y}, \mathcal{M}_1)  = \frac{p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)}{p(\boldsymbol{y} \mid \mathcal{M}_1)}
\end{equation}\]</span></p>
<p>The marginal likelihood does not depend on the model parameters <span class="math inline">\(\Theta\)</span>; the parameters are “marginalized” or integrated out:</p>
<p><span class="math display" id="eq:marginall">\[\begin{equation}
p(\boldsymbol{y} \mid \mathcal{M}_1) = \int p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1) d \boldsymbol{\Theta}
\tag{15.1}
\end{equation}\]</span></p>
<p>The likelihood is evaluated for every possible parameter value, weighted by the prior plausibility of the parameter values. The product <span class="math inline">\(p(\boldsymbol{y} \mid \boldsymbol{\Theta}, \mathcal{M}_1) p(\boldsymbol{\Theta} \mid \mathcal{M}_1)\)</span> is then summed up (that is what the integral does).</p>
<p>For this reason, the prior is as important as the likelihood. Equation <a href="ch-bf.html#eq:marginall">(15.1)</a> also looks almost identical to the prior predictive distribution from section <a href="ch-compbda.html#sec-priorpred">3.3</a> (that is, the predictions that the model makes before seeing any data). The prior predictive distribution is repeated below for convenience:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p(\boldsymbol{y_{pred}}) &amp;= p(y_{pred_1},\dots,y_{pred_n})\\
&amp;= \int_{\boldsymbol{\Theta}} p(y_{pred_1}|\boldsymbol{\Theta})\cdot p(y_{pred_2}|\boldsymbol{\Theta})\cdots p(y_{pred_N}|\boldsymbol{\Theta}) p(\boldsymbol{\Theta}) \, d\boldsymbol{\Theta} 
\end{aligned}
\end{equation}\]</span></p>
<p>However, while the prior predictive distribution describes possible observations, the marginal likelihood is evaluated on the actually observed data.</p>
<p>Let’s compute the Bayes factor for a very simple example case. We assume a study where we assess the number of “successes” observed in a fixed number of trials. For example, suppose that we have 80 “successes” out of 100 trials. A simple model of this data can be built by assuming, as we did in section <a href="ch-intro.html#sec-binomialcloze">1.4</a>, that the data are distributed according to a binomial distribution.
In a binomial distribution, <span class="math inline">\(n\)</span> independent experiments are performed, where the result of each experiment is either a “success” or “no success” with probability <span class="math inline">\(\theta\)</span>. The binomial distribution is the probability distribution of the number of successes <span class="math inline">\(k\)</span> (number of “success” responses) in this situation for a given sample of experiments <span class="math inline">\(X\)</span>.</p>
<p>Suppose now that we have prior information about the probability parameter <span class="math inline">\(\theta\)</span>. As we explained in section <a href="ch-introBDA.html#sec-analytical">2.2</a>, a typical prior distribution for <span class="math inline">\(\theta\)</span> is a beta distribution. The beta distribution defines a probability distribution on the interval <span class="math inline">\([0, 1]\)</span>, which is the interval on which the probability <span class="math inline">\(\theta\)</span> is defined. It has two parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, which determine the shape of the distribution. The prior parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> can be interpreted as the a priori number of “successes” versus “failures.” These could be based on previous evidence, or on the researcher’s beliefs, drawing on their domain knowledge <span class="citation">(O’Hagan et al. <a href="#ref-ohagan2006uncertain">2006</a>)</span>.</p>
<p>Here, to illustrate the calculation of the Bayes factor, we assume that the parameters of the beta distribution are <span class="math inline">\(a=4\)</span> and <span class="math inline">\(b=2\)</span>. As mentioned above, these parameters can be interpreted as representing “success” (<span class="math inline">\(4\)</span> prior observations representing success), and “no success” (<span class="math inline">\(2\)</span> prior observations representing “no success”). The resulting prior distribution is visualized in Figure <a href="ch-bf.html#fig:beta24">15.1</a>. A <span class="math inline">\(\mathit{Beta}(a=4,b=2)\)</span> prior on <span class="math inline">\(\theta\)</span> amounts to a regularizing prior with some, but no clear prior evidence for more than 50% of success.</p>
<div class="figure"><span style="display:block;" id="fig:beta24"></span>
<img src="bookdown_files/figure-html/beta24-1.svg" alt="beta distribution with parameters a = 4 and b = 2." width="672" />
<p class="caption">
FIGURE 15.1: beta distribution with parameters a = 4 and b = 2.
</p>
</div>
<p>To compute the marginal likelihood, equation <a href="ch-bf.html#eq:marginall">(15.1)</a> shows that we need to multiply the likelihood with the prior. The marginal likelihood is then the area under the curve, that is, the likelihood averaged across all possible values for the model parameter (the probability of success).</p>
<p>Based on this data, likelihood, and prior we can calculate the marginal likelihood, that is, this area under the curve, in the following way using R:<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a></p>
<div class="sourceCode" id="cb864"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb864-1" data-line-number="1"><span class="co"># First we multiply the likelihood with the prior</span></a>
<a class="sourceLine" id="cb864-2" data-line-number="2">plik1 &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</a>
<a class="sourceLine" id="cb864-3" data-line-number="3">  <span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">80</span>, <span class="dt">size =</span> <span class="dv">100</span>, <span class="dt">prob =</span> theta) <span class="op">*</span></a>
<a class="sourceLine" id="cb864-4" data-line-number="4"><span class="st">    </span><span class="kw">dbeta</span>(<span class="dt">x =</span> theta, <span class="dt">shape1 =</span> <span class="dv">4</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb864-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb864-6" data-line-number="6"><span class="co"># Then we integrate (compute the area under the curve):</span></a>
<a class="sourceLine" id="cb864-7" data-line-number="7">(MargLik1 &lt;-<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> plik1, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value)</a></code></pre></div>
<pre><code>## [1] 0.02</code></pre>
<p>One would prefer a model that gives a higher marginal likelihood, i.e., a higher likelihood of observing the data after integrating out the influence of the model parameter(s) (here: <span class="math inline">\(\theta\)</span>). A model will yield a high marginal likelihood if it makes a high proportion of good predictions <span class="citation">(i.e., model 2 in Figure <a href="ch-bf.html#fig:OccamFactor">15.2</a>; the figure is adapted from Bishop <a href="#ref-bishop2006pattern">2006</a>)</span>.
Model predictions are normalized, that is, the total probability that models assign to different expected data patterns is the same for all models.
Models that are too flexible (model 3 in Figure <a href="ch-bf.html#fig:OccamFactor">15.2</a>) will divide their prior predictive probability density across all of their predictions. Such models can predict many different outcomes. Thus, they likely can also predict the actually observed outcome. However, due to the normalization, they cannot predict it with high probability, because they also predict all kinds of other outcomes. This is true for both models with priors that are too wide or for models with too many parameters. Bayesian model comparison automatically penalizes such complex models, which is called the “Occam factor” <span class="citation">(MacKay <a href="#ref-mackay">2003</a>)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:OccamFactor"></span>
<img src="images/OccamFactorBW.png" alt="Shown are the schematic marginal likelihoods that each of three models assigns to different possible data sets. The total probability each model assigns to the data is equal to one, i.e., the areas under the curves of all three models are the same. Model 1 (black), the low complexity model, assigns all the probability to a narrow range of possible data, and can predict these possible data sets with high likelihood. Model 3 (light grey) assigns its probability to a large range of different possible outcomes, but predicts each individual observed data set with low likelihood (high complexity model). Model 2 (dark grey) takes an intermediate position (intermediate complexity). The vertical dashed line (dark grey) illustrates where the actual empirically observed data fall. The data most support model 2, since this model predicts the data with highest likelihood. The figure is closely based on Figure 3.13 in Bishop (2006)." width="100%" />
<p class="caption">
FIGURE 15.2: Shown are the schematic marginal likelihoods that each of three models assigns to different possible data sets. The total probability each model assigns to the data is equal to one, i.e., the areas under the curves of all three models are the same. Model 1 (black), the low complexity model, assigns all the probability to a narrow range of possible data, and can predict these possible data sets with high likelihood. Model 3 (light grey) assigns its probability to a large range of different possible outcomes, but predicts each individual observed data set with low likelihood (high complexity model). Model 2 (dark grey) takes an intermediate position (intermediate complexity). The vertical dashed line (dark grey) illustrates where the actual empirically observed data fall. The data most support model 2, since this model predicts the data with highest likelihood. The figure is closely based on Figure 3.13 in Bishop (2006).
</p>
</div>
<p>By contrast, good models (Figure <a href="ch-bf.html#fig:OccamFactor">15.2</a>, model 2) will make very specific predictions, where the specific predictions are consistent with the observed data. Here, all the predictive probability density is located at the “location” where the observed data fall, and little probability density is located at other places, providing good support for the model. Of course, specific predictions can also be wrong, when expectations differ from what the observed data actually look like (Figure <a href="ch-bf.html#fig:OccamFactor">15.2</a>, model 1).</p>
<p>Having a natural Occam factor is good for posterior inference, i.e., for assessing how much (continuous) evidence there is for one model or another. However, it doesn’t necessarily imply good decision making or hypothesis testing, i.e., to make discrete decisions about which model explains the data best, or on which model to base further actions.</p>
<p>Here, we provide two examples of more flexible models. First, the following model assumes the same likelihood and the same distribution function for the prior. However, we assume a flat, uninformative prior, with prior parameters <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = 1\)</span> (i.e., only one prior “success” and one prior “failure”), which provides more prior spread than the first model. Again, we can formulate our model as multiplying the likelihood with the prior, and integrate out the influence of the parameter <span class="math inline">\(\theta\)</span>:</p>
<div class="sourceCode" id="cb866"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb866-1" data-line-number="1">plik2 &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</a>
<a class="sourceLine" id="cb866-2" data-line-number="2">  <span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">80</span>, <span class="dt">size =</span> <span class="dv">100</span>, <span class="dt">prob =</span> theta) <span class="op">*</span></a>
<a class="sourceLine" id="cb866-3" data-line-number="3"><span class="st">    </span><span class="kw">dbeta</span>(<span class="dt">x =</span> theta, <span class="dt">shape1 =</span> <span class="dv">1</span>, <span class="dt">shape2 =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb866-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb866-5" data-line-number="5">(MargLik2 &lt;-<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f =</span> plik2, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value)</a></code></pre></div>
<pre><code>## [1] 0.0099</code></pre>
<p>We can see that this second model is more flexible: due to the more spread-out prior, it is compatible with a larger range of possible observed data patterns. However, when we integrate out the <span class="math inline">\(\theta\)</span> parameter to obtain the marginal likelihood, we can see that this flexibility also comes with a cost: the model has a smaller marginal likelihood (<span class="math inline">\(0.0099\)</span>) than the first model (<span class="math inline">\(0.02\)</span>). Thus, on average (averaged across all possible values of <span class="math inline">\(\theta\)</span>) the second model performs worse in explaining the specific data that we observed compared to the first model, and has less support from the data.</p>
<p>A model might be more “complex” because it has a more spread-out prior, or alternatively because it has a more complex likelihood function, which uses a larger number of parameters to explain the same data. Here we implement a third model, which assumes a more complex likelihood by using a beta-binomial distribution. The beta-binomial distribution is similar to the binomial distribution, with one important difference: In the binomial distribution the probability of success <span class="math inline">\(\theta\)</span> is fixed across trials. In the beta-binomial distribution, the probability of success is fixed for each trial, but is drawn from a beta distribution across trials. Thus, <span class="math inline">\(\theta\)</span> can differ between trials. In the beta-binomial distribution, we thus assume that the likelihood function is a combination of a binomial distribution and a beta distribution of the probability <span class="math inline">\(\theta\)</span>, which yields:</p>
<p><span class="math display">\[\begin{equation}
p(X = k \mid a, b) = \frac{B(k+a, n-k+b)}{B(a,b)}
\end{equation}\]</span></p>
<p>What is important here is that this more complex distribution has two parameters (<span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>; rather than one, <span class="math inline">\(\theta\)</span>) to explain the same data. We assume log-normally distributed priors for the <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> parameters, with location zero and scale <span class="math inline">\(100\)</span>. The likelihood of this combined beta-binomial distribution is given by the R-function <code>dbbinom()</code> in the package <code>extraDistr</code>. We can now write down the likelihood times the priors (given as log-normal densities, <code>dlnorm()</code>), and integrate out the influence of the two free model parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> using numerical integration (applying <code>integrate</code> twice):</p>
<div class="sourceCode" id="cb868"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb868-1" data-line-number="1">plik3 &lt;-<span class="st"> </span><span class="cf">function</span>(a, b) {</a>
<a class="sourceLine" id="cb868-2" data-line-number="2">  <span class="kw">dbbinom</span>(<span class="dt">x =</span> <span class="dv">80</span>, <span class="dt">size =</span> <span class="dv">100</span>, <span class="dt">alpha =</span> a, <span class="dt">beta =</span> b) <span class="op">*</span></a>
<a class="sourceLine" id="cb868-3" data-line-number="3"><span class="st">    </span><span class="kw">dlnorm</span>(<span class="dt">x =</span> a, <span class="dt">meanlog =</span> <span class="dv">0</span>, <span class="dt">sdlog =</span> <span class="dv">100</span>) <span class="op">*</span></a>
<a class="sourceLine" id="cb868-4" data-line-number="4"><span class="st">    </span><span class="kw">dlnorm</span>(<span class="dt">x =</span> b, <span class="dt">meanlog =</span> <span class="dv">0</span>, <span class="dt">sdlog =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb868-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb868-6" data-line-number="6"><span class="co"># Compute marginal likelihood by applying integrate twice</span></a>
<a class="sourceLine" id="cb868-7" data-line-number="7">f &lt;-<span class="st"> </span><span class="cf">function</span>(b) {</a>
<a class="sourceLine" id="cb868-8" data-line-number="8">  <span class="kw">integrate</span>(<span class="cf">function</span>(a) <span class="kw">plik3</span>(a, b), <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value</a>
<a class="sourceLine" id="cb868-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb868-10" data-line-number="10"><span class="co"># integrate requires a vectorized function:</span></a>
<a class="sourceLine" id="cb868-11" data-line-number="11">(MargLik3 &lt;-<span class="st"> </span><span class="kw">integrate</span>(<span class="kw">Vectorize</span>(f), <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value)</a></code></pre></div>
<pre><code>## [1] 0.00000707</code></pre>
<p>The results show that this third model has an even smaller marginal likelihood compared to the first two (<span class="math inline">\(0.00000707\)</span>). With its two parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, this third model has a lot of flexibility to explain a lot of different patterns of observed empirical results. However, again, this increased flexibility comes at a cost, and the simple pattern of observed data does not seem to require such complex model assumptions. The small value for the marginal likelihood indicates that this complex model has less support from the data.</p>
<p>That is, for this present simple example case, we would prefer model 1 over the other two, since it has the largest marginal likelihood (<span class="math inline">\(0.02\)</span>), and we would prefer model 2 over model 3, since the marginal likelihood of model 2 (<span class="math inline">\(0.0099\)</span>) is larger than that of model 3 (<span class="math inline">\(0.00000707\)</span>). The decision about which model is preferred is based on comparing the marginal likelihoods.</p>
</div>
<div id="bayes-factor" class="section level3 hasAnchor">
<h3><span class="header-section-number">15.1.2</span> Bayes factor<a href="ch-bf.html#bayes-factor" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Bayes factor is a measure of relative evidence, the comparison of the predictive performance of one model against another one. This comparison is a ratio of marginal likelihoods:</p>
<p><span class="math display">\[\begin{equation}
BF_{12} = \frac{P(\boldsymbol{y} \mid \mathcal{M}_1)}{P(\boldsymbol{y} \mid \mathcal{M}_2)}
\end{equation}\]</span></p>
<p><span class="math inline">\(BF_{12}\)</span> indicates the extent to which the data are more likely under <span class="math inline">\(\mathcal{M}_1\)</span> over <span class="math inline">\(\mathcal{M}_2\)</span>, or in other words, which of the two models is more likely to have generated the data, or the relative evidence that we have for <span class="math inline">\(\mathcal{M}_1\)</span> over <span class="math inline">\(\mathcal{M}_2\)</span>. Values larger than one indicate evidence in favor of <span class="math inline">\(\mathcal{M}_1\)</span>, smaller than one indicate evidence in favor of <span class="math inline">\(\mathcal{M}_2\)</span>, and values close to one indicate that the evidence is inconclusive. This model comparison does not depend on a specific parameter value. Instead, all possible prior parameter values are taken into account simultaneously. This is in contrast with the likelihood ratio test, as it is explained in Box <a href="ch-bf.html#thm:likR">15.1</a></p>

<div class="extra">
<div class="theorem">
<p><span id="thm:likR" class="theorem"><strong>Box 15.1  </strong></span><strong>The likelihood ratio vs Bayes Factor.</strong></p>
</div>
<p>The likelihood ratio test is a very similar, but frequentist, approach to model comparison and hypothesis testing, which also compares the likelihood for the data given two different models. We show this here to highlight the similarities and differences between frequentist and Bayesian hypothesis testing. In contrast to the Bayes factor, the likelihood ratio test depends on the “best” (i.e., the maximum likelihood) estimate for the model parameter(s), that is, the model parameter <span class="math inline">\(\theta\)</span> occurs on the right side of the semi-colon in the equation for each likelihood. (An aside: we do not use a conditional statement, i.e., the vertical bar, when talking about likelihood in the frequentist context; instead, we use a semi-colon. This is because the statement <span class="math inline">\(f(y\mid \theta)\)</span> is a conditional statement, implying that <span class="math inline">\(\theta\)</span> has a probability density function associated with it; in the frequentist framework, parameters cannot have a pdf associated with them, they are assumed to have fixed, point values.)</p>
<p><span class="math display">\[\begin{equation}
LikRat = \frac{P(\boldsymbol{y} ; \boldsymbol{\hat{\Theta}_1}, \mathcal{M}_1)}{P(\boldsymbol{y} ; \boldsymbol{\hat{\Theta}_2}, \mathcal{M}_2)}
\end{equation}\]</span></p>
<p>That means that in the likelihood ratio test, each model is tested on its ability to explain the data using this “best” estimate for the model parameter (here, the maximum likelihood estimate <span class="math inline">\(\hat{\theta}\)</span>). That is, the likelihood ratio test reduces the full range of possible parameter values to a point value, leading to overfitting the model to the maximum likelihood estimate (MLE). If the MLE badly misestimates the true value of the parameter, due to Type M/S error <span class="citation">(Gelman and Carlin <a href="#ref-gelmancarlin">2014</a>)</span>, we could end up with a “significant” effect that is just a consequence of this misestimation (it will not be consistently replicable; see <span class="citation">Vasishth, Mertzen, Jäger, et al. (<a href="#ref-VasishthMertzenJaegerGelman2018">2018</a><a href="#ref-VasishthMertzenJaegerGelman2018">a</a>)</span> for an example). By contrast, the Bayes factor involves range hypotheses, which are implemented via integrals over the model parameter; that is, it uses marginal likelihoods that are averaged across all possible posterior values of the model parameter(s). Thus, if, due to Type M error, the best point estimate (the MLE) for the model parameter(s) is not very representative of the possible values for the model parameter(s), then Bayes factors will be superior to the likelihood ratio test. An additional difference, of course, is that Bayes factors rely on priors for estimating each model’s parameter(s), whereas the frequentist likelihood ratio test does not (and cannot) consider priors in the estimation of the best-fitting model parameter(s). As we show in this chapter, this has far-reaching consequences for Bayes factor-based model comparison; for a more extensive exposition, see <span class="citation">Schad et al. (<a href="#ref-schad2022workflow">2022</a>)</span> and <span class="citation">Vasishth, Yadav, et al. (<a href="#ref-SampleSizeCBB2021">2022</a>)</span>.</p>
</div>

<p>For the Bayes factor, a scale (see Table <a href="ch-bf.html#tab:BFs">15.1</a>) has been proposed to interpret Bayes factors according to the strength of evidence in favor of one model (corresponding to some hypothesis) over another <span class="citation">(Jeffreys <a href="#ref-jeffreys1939theory">1939</a>)</span>; but this scale should not be regarded as a hard and fast rule with clear boundaries.</p>
<table>
<caption><span id="tab:BFs">TABLE 15.1: </span> The Bayes factor scale as proposed by <span class="citation">Jeffreys (<a href="#ref-jeffreys1939theory">1939</a>)</span>. This scale should not be regarded as a hard and fast rule.</caption>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(BF_{12}\)</span></th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><span class="math inline">\(&gt;100\)</span></td>
<td align="left">Extreme evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(30-100\)</span></td>
<td align="left">Very strong evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(10-30\)</span></td>
<td align="left">Strong evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(3-10\)</span></td>
<td align="left">Moderate evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(1-3\)</span></td>
<td align="left">Anecdotal evidence for <span class="math inline">\(\mathcal{M}_1\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(1\)</span></td>
<td align="left">No evidence.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(\frac{1}{1}-\frac{1}{3}\)</span></td>
<td align="left">Anecdotal evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\frac{1}{3}-\frac{1}{10}\)</span></td>
<td align="left">Moderate evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(\frac{1}{10}-\frac{1}{30}\)</span></td>
<td align="left">Strong evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\frac{1}{30}-\frac{1}{100}\)</span></td>
<td align="left">Very strong evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(&lt;\frac{1}{100}\)</span></td>
<td align="left">Extreme evidence for <span class="math inline">\(\mathcal{M}_2\)</span>.</td>
</tr>
</tbody>
</table>
<p>So if we go back to our previous example, we can calculate <span class="math inline">\(BF_{12}\)</span>, <span class="math inline">\(BF_{13}\)</span>, and <span class="math inline">\(BF_{23}\)</span>. The subscript represents the order in which the models are compared; for example, <span class="math inline">\(BF_{21}\)</span> is simply <span class="math inline">\(\frac{1}{BF_{12}}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
BF_{12} = \frac{marginal \; likelihood \; model \; 1}{marginal \; likelihood \; model \; 2} = \frac{MargLik1}{MargLik2} = 2 
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
BF_{13} = \frac{MargLik1}{MargLik3}=  2825.4 
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
BF_{32} = \frac{MargLik3}{MargLik2} =  0.001 = \frac{1}{BF_{23}} =  \frac{1}{1399.9 }
\end{equation}\]</span></p>
<p>However, if we want to know, given the data <span class="math inline">\(y\)</span>, what the probability for model <span class="math inline">\(\mathcal{M}_1\)</span> is, or how much more probable model <span class="math inline">\(\mathcal{M}_1\)</span> is than model <span class="math inline">\(\mathcal{M}_2\)</span>, then we need the prior odds, that is, we need to specify how probable <span class="math inline">\(\mathcal{M}_1\)</span> is compared to <span class="math inline">\(\mathcal{M}_2\)</span> <em>a priori</em>.</p>
<p><span class="math display">\[\begin{align}
\frac{p(\mathcal{M}_1 \mid y)}{p(\mathcal{M}_2 \mid y)} =&amp; \frac{p(\mathcal{M}_1)}{p(\mathcal{M}_2)} \times \frac{P(y \mid \mathcal{M}_1)}{P(y \mid \mathcal{M}_2)}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\text{Posterior odds}_{12} = &amp; \text{Prior odds}_{12} \times BF_{12}
\end{align}\]</span></p>
<p>The Bayes factor tells us, given the data and the priors, by how much we need to update our relative belief between the two models. However, <strong>the Bayes factor alone cannot tell us which one of the models is the most probable</strong>. Given our priors for the models and the Bayes factor, we can calculate the odds between the models.</p>
<p>Here we compute posterior model probabilities for the case where we compare two models against each other. However, posterior model probabilities can also be computed for the more general case, where more than two models are considered:</p>
<p><span class="math display">\[\begin{equation}
p(\mathcal{M}_1 \mid \boldsymbol{y}) = \frac{p(\boldsymbol{y} \mid \mathcal{M}_1) p(\mathcal{M}_1)}{\sum_n p(\boldsymbol{y} \mid \mathcal{M}_n) p(\mathcal{M}_n)}
\end{equation}\]</span></p>
<p>For simplicity, we mostly constrain ourselves to two models. (However, the sensitivity analyses we carry out below compare more than two models.)</p>
<p>Bayes factors (and posterior model probabilities) tell us how much evidence the data (and priors) provide in favor of one model or another. That is, they allow us to perform inferences on the model space, i.e., to learn how much each hypothesis is consistent with the data.</p>
<p>A completely different issue, however, is the question of how to perform (discrete) decisions based on continuous evidence. The question here is: which hypothesis should one choose to maximize utility? While Bayes factors have a clear rationale and justification in terms of the (continuous) evidence they provide, there is not a clear and direct mapping from inferences to how to perform decisions based on them. To derive decisions based on posterior model probabilities, utility functions are needed. Indeed, the utility of different possible actions (i.e., to accept and act based on one hypothesis or another) can differ quite dramatically in different situations. For example, for a researcher trying to implement a life-saving therapy, erroneously rejecting this new therapy could have high negative utility, whereas erroneously adopting the new therapy may have little negative consequences. By contrast, erroneously claiming a new discovery in fundamental research may have bad consequences (low utility), whereas erroneously missing a new discovery claim may be less problematic if further evidence can be accumulated. Thus, Bayesian evidence (in the form of Bayes factors or posterior model probabilities) must be combined with utility functions in order to perform decisions based on them. For example, this could imply specifying the utility of a true discovery (<span class="math inline">\(U_{TD}\)</span>) and the utility of a false discovery (<span class="math inline">\(U_{FD}\)</span>). Calibration (i.e., simulations) can then be used to derive decisions that maximize overall utility <span class="citation">(see Schad et al. <a href="#ref-schad2022workflow">2022</a>)</span>.</p>
<p>The question now is how do we extend this method to models that we care about, i.e., that represent more realistic data analysis situations. In cognitive science, we typically fit fairly complex hierarchical models with many variance components. The major problem is that we won’t be able to calculate the marginal likelihood for hierarchical models (or any other complex model) either analytically or just using the R functions shown above. There are two very useful methods for calculating the Bayes factor for complex models: the Savage–Dickey density ratio method <span class="citation">(Dickey, Lientz, and others <a href="#ref-DickeyLientz1970">1970</a>; Wagenmakers et al. <a href="#ref-wagenmakers2010BayesianHypothesisTesting">2010</a>)</span> and bridge sampling <span class="citation">(Bennett <a href="#ref-bennettEfficientEstimationFree1976">1976</a>; Meng and Wong <a href="#ref-mengSimulatingRatiosNormalizing1996">1996</a>)</span>. The Savage–Dickey density ratio method is a straightforward way to compute the Bayes factor, but it is limited to nested models.
The current implementation of the Savage–Dickey method in <code>brms</code> can be unstable, especially in cases where the posterior is far away from zero. <!-- We will revisit this instability later in this chapter. --> Bridge sampling is a much more powerful method, but it requires many more effective samples than what is normally required for parameter estimation. We will use bridge sampling from the <code>bridgesampling</code> package <span class="citation">(Gronau et al. <a href="#ref-gronauTutorialBridgeSampling2017">2017</a><a href="#ref-gronauTutorialBridgeSampling2017">b</a>; Gronau, Singmann, and Wagenmakers <a href="#ref-gronauBridgesamplingPackageEstimating2017">2017</a>)</span> with the function <code>bayes_factor()</code> to calculate the Bayes factor in the first examples.</p>
</div>
</div>
<div id="sec-N400BF" class="section level2 hasAnchor">
<h2><span class="header-section-number">15.2</span> Examining the N400 effect with Bayes factor<a href="ch-bf.html#sec-N400BF" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- One popular measure of semantic processing is the N400 effect in the electro-encephalogram [EEG, @kutas1980reading]. This is a negative deflection in event-related potentials (ERP), which are average EEG signals time-locked to some experimental event, such as the presentation of a particular word. The N400 is a negative deflection that occurs roughly 400 ms after word onset. It occIn more detail:urs in situations where an (implicit) semantic expectation is violated, and the strength of the N400 reflects the degree to which the expectation is violated [@kutas1980reading; @rabovsky2018modelling]. For example, this can be assessed as an effect of cloze probability, where low probability words elicit a stronger N400 compared to high probability words [@kutas1984brain]. -->
<p>In section <a href="ch-hierarchical.html#sec-N400hierarchical">5.2</a> we estimated the effect of cloze probability on the N400 average signal. This yielded a posterior credible interval for the effect of cloze probability. It is certainly possible to check whether e.g., the 95% posterior credible interval overlaps with zero or not. However, such estimation cannot really answer the following question: How much evidence do we have in support for an effect? A 95% credible interval that doesn’t overlap with zero, or a high probability mass away from zero may hint that the predictor may be needed to explain the data, but it is not really answering how much evidence we have in favor of an effect <span class="citation">(for discussion, see Royall <a href="#ref-Royall">1997</a>; Wagenmakers et al. <a href="#ref-wagenmakersPrinciplePredictiveIrrelevance2020">2020</a>; Rouder, Haaf, and Vandekerckhove <a href="#ref-rouder2018bayesian">2018</a>)</span>.</p>
<p>This is a very important point, and is often overlooked in the literature. Many papers misuse 95% posterior credible intervals to argue that there is evidence for or against an effect. In the past, we have also misused posterior credible intervals in this way <span class="citation">(and even recommended this incorrect interpretation in, for example, Nicenboim and Vasishth <a href="#ref-NicenboimVasishth2016">2016</a>)</span>.</p>
<p>The reason why the 95% posterior credible interval does not answer the question about evidence for the alternative model <span class="math inline">\(M1\)</span> or the null model <span class="math inline">\(M0\)</span> is that we do not explicitly consider and quantify the possibility that the parameter estimate is zero: we do not quantify the likelihood of the data under the assumption that the effect is absent; see also Box <a href="ch-comparison.html#thm:null">14.1</a>.
The Bayes factor answers this question about the evidence in favor of an effect by explicitly conducting a model comparison. We will compare a model that assumes the presence of an effect, with a null model that assumes no effect.</p>
<p>As we saw before, the Bayes factor is highly sensitive to the priors. In the example presented above, both models are identical except for the effect of interest, <span class="math inline">\(\beta\)</span>, and so the prior on this parameter will play a major role in the calculation of the Bayes factor.</p>
<p>Next, we will run a hierarchical model which includes random intercepts and slopes by items and by subjects. We will use regularizing priors on all the parameters–this speeds up computation and implies realistic expectations about the parameters. However, the prior on <span class="math inline">\(\beta\)</span> will be <strong>crucial</strong> for the calculation of the Bayes factor.</p>
<p>One possible way we can build a good prior for the parameter <span class="math inline">\(\beta\)</span> estimating the influence of cloze probability here is the following (see chapter <a href="ch-priors.html#ch-priors">6</a> for an extended discussion about prior selection). The reasoning below is based on domain knowledge; but there is room for differences of opinion here. In a realistic data analysis situation, we would carry out a sensitivity analysis using a range of priors to determine the extent of influence of the priors.</p>
<ol style="list-style-type: decimal">
<li>One may want to be agnostic regarding the direction of the effect; that means that we will center the prior of <span class="math inline">\(\beta\)</span> on zero by specifying that the mean of the prior distribution is zero. However, we are still not sure about the variance of the prior on <span class="math inline">\(\beta\)</span>.</li>
<li>One would need to know a bit about the variation on the dependent variable that we are analyzing. After re-analyzing the data from a couple of EEG experiments available from osf.io, we can say that for N400 averages, the standard deviation of the signal is between 8-15 microvolts <span class="citation">(Nicenboim, Vasishth, and Rösler <a href="#ref-nicenboim2020words">2020</a><a href="#ref-nicenboim2020words">c</a>)</span>.</li>
<li>Based on published estimates of effects in psycholinguistics, we can conclude that they are generally rather small, often representing between 5%-30% of the standard deviation of the dependent variable.</li>
<li>The effect of noun predictability on the N400 is one of the most reliable and strongest effects in neurolinguistics (together with the P600 that might even be stronger), and the slope <span class="math inline">\(\beta\)</span> represents the average change in voltage when moving from a cloze probability of zero to one–the strongest prediction effect.</li>
</ol>
<p>An additional and highly recommended way to obtain good priors <span class="citation">(Schad, Betancourt, and Vasishth <a href="#ref-schad2020toward">2020</a>, also see chapter <a href="ch-workflow.html#ch-workflow">7</a>, which presents a principled Bayesian workflow)</span> is to perform prior predictive checks. Here, the idea is to simulate data from the model and the priors, and then to analyze the simulated data using summary statistics. For example, it would be possible to compute the summary statistic of the difference in the N400 between high versus low cloze probability. The simulations would yield a distribution of differences. Arguably, this distribution of differences, that is, the data analyses of the simulated data, are much easier to judge for plausibility than the prior parameters specifying prior distributions. That is, we might find it easier to judge whether a difference in voltage between high and low cloze probability is plausible rather than judging the parameters of the model. For reasons of brevity, we skip this steps here.</p>
<p>Instead, we will start with the prior <span class="math inline">\(\beta \sim \mathit{Normal}(0,5)\)</span> (since 5 microvolts is roughly 30% of 15, which is the upper bound of the expected standard deviation of the EEG signal).</p>
<div class="sourceCode" id="cb870"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb870-1" data-line-number="1">priors1 &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb870-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb870-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb870-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">10</span>, <span class="dv">5</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb870-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb870-6" data-line-number="6">  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">4</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb870-7" data-line-number="7">)</a></code></pre></div>
<p>We load the data set on N400 amplitudes, which has data on cloze probabilities <span class="citation">(Nieuwland et al. <a href="#ref-nieuwlandLargescaleReplicationStudy2018">2018</a>)</span>. We mean-center the cloze probability measure to make the intercept and the random intercepts easier to interpret (i.e., after centering, they represent the grand mean and the average variability around the grand mean across subjects or items).</p>
<div class="sourceCode" id="cb871"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb871-1" data-line-number="1"><span class="kw">data</span>(df_eeg)</a>
<a class="sourceLine" id="cb871-2" data-line-number="2">df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">c_cloze =</span> cloze <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cloze))</a></code></pre></div>
<p>We will need a large number of effective samples to be able to get stable estimates of the Bayes factor with bridge sampling, for this reason a large number of sampling iterations (<code>n = 20000</code>) is specified. Because otherwise we see warnings, we also set <code>adapt_delta = 0.9</code> to ensure that the posterior sampler is working correctly. For Bayes factors analyses, it’s necessary to set the argument <code>save_pars = save_pars(all = TRUE)</code>. This setting is a precondition for later performing bridge sampling for computing the Bayes factor.</p>
<div class="sourceCode" id="cb872"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb872-1" data-line-number="1">fit_N400_h_linear &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span></a>
<a class="sourceLine" id="cb872-2" data-line-number="2"><span class="st">  </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb872-3" data-line-number="3">  <span class="dt">prior =</span> priors1,</a>
<a class="sourceLine" id="cb872-4" data-line-number="4">  <span class="dt">warmup =</span> <span class="dv">2000</span>,</a>
<a class="sourceLine" id="cb872-5" data-line-number="5">  <span class="dt">iter =</span> <span class="dv">20000</span>,</a>
<a class="sourceLine" id="cb872-6" data-line-number="6">  <span class="dt">cores =</span> <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb872-7" data-line-number="7">  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</a>
<a class="sourceLine" id="cb872-8" data-line-number="8">  <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb872-9" data-line-number="9">  <span class="dt">data =</span> df_eeg)</a></code></pre></div>
<p>Next, take a look at the population-level (or fixed) effects from the Bayesian modeling.</p>
<div class="sourceCode" id="cb873"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb873-1" data-line-number="1"><span class="kw">fixef</span>(fit_N400_h_linear)</a></code></pre></div>
<pre><code>##           Estimate Est.Error Q2.5 Q97.5
## Intercept     3.65      0.45 2.76  4.53
## c_cloze       2.33      0.65 1.05  3.59</code></pre>
<p>We can now take a look at the estimates and at the credible intervals. The effect of cloze probability (<code>c_cloze</code>) is <span class="math inline">\(2.33\)</span> with a 95% credible interval ranging from <span class="math inline">\(1.05\)</span> to <span class="math inline">\(3.59\)</span>. While this provides an initial hint that highly probable words may elicit a stronger N400 compared to low probable words, by just looking at the posterior there is no way to quantify evidence for the question whether this effect is different from zero. Model comparison is needed to answer this question.</p>
<p>To this end, we run the model again, now without the parameter of interest, i.e., the null model. This is a model where our prior for <span class="math inline">\(\beta\)</span> is that it is exactly zero.</p>
<div class="sourceCode" id="cb875"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb875-1" data-line-number="1">fit_N400_h_null &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span></a>
<a class="sourceLine" id="cb875-2" data-line-number="2"><span class="st">  </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb875-3" data-line-number="3"><span class="dt">prior =</span> priors1[priors1<span class="op">$</span>class <span class="op">!=</span><span class="st"> &quot;b&quot;</span>, ],</a>
<a class="sourceLine" id="cb875-4" data-line-number="4"><span class="dt">warmup =</span> <span class="dv">2000</span>,</a>
<a class="sourceLine" id="cb875-5" data-line-number="5"><span class="dt">iter =</span> <span class="dv">20000</span>,</a>
<a class="sourceLine" id="cb875-6" data-line-number="6"><span class="dt">cores =</span> <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb875-7" data-line-number="7"><span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</a>
<a class="sourceLine" id="cb875-8" data-line-number="8"><span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb875-9" data-line-number="9"><span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb875-10" data-line-number="10">)</a></code></pre></div>
<p>Now everything is ready to compute the log marginal likelihood, that is, the likelihood of the data given the model, after integrating out the model parameters. In the toy examples shown above, we had used the R-function <code>integrate()</code> to perform this integration. This is not possible for the more realistic and more complex models that are considered here because the integrals that have to be solved are too high-dimensional and complex for these simple functions to do the job. Instead, a standard approach to sampling realistic complex models is to use bridge sampling <span class="citation">(Gronau et al. <a href="#ref-gronauTutorialBridgeSampling2017">2017</a><a href="#ref-gronauTutorialBridgeSampling2017">b</a>; Gronau, Singmann, and Wagenmakers <a href="#ref-gronauBridgesamplingPackageEstimating2017">2017</a>)</span>. We perform this integration using the function <code>bridge_sampler()</code> for each of the two models:</p>
<div class="sourceCode" id="cb876"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb876-1" data-line-number="1">margLogLik_linear &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_N400_h_linear, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb876-2" data-line-number="2">margLogLik_null &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_N400_h_null, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>This gives us the marginal log likelihoods for each of the models. From these, we can compute the Bayes factors. The function <code>bayes_factor()</code> is a convenient function to calculate the Bayes factor.</p>
<div class="sourceCode" id="cb877"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb877-1" data-line-number="1">(BF_ln &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(margLogLik_linear, margLogLik_null))</a></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 50.96782</code></pre>
<p>Alternatively, the Bayes factor can be computed manually as well. First, we compute the difference in marginal log likelihoods, then we transform this difference in log likelihoods to the likelihood scale (using <code>exp()</code>). A difference in the exponential scale a ratio: <code>exp(a-b) = exp(a)/exp(b)</code>, that is, it computes the Bayes factor. However, the values <code>exp(ml1)</code> and <code>exp(ml2)</code> are too small to be represented accurately by R. Therefore, for numerical reasons, it is important to take the difference first and to compute the exponential afterwards <code>exp(a-b)</code>, i.e., <code>exp(margLogLik_linear$logml - margLogLik_null$logml)</code>, which yields the same result as the <code>bayes_factor()</code> command.</p>
<div class="sourceCode" id="cb879"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb879-1" data-line-number="1"><span class="kw">exp</span>(margLogLik_linear<span class="op">$</span>logml <span class="op">-</span><span class="st"> </span>margLogLik_null<span class="op">$</span>logml)</a></code></pre></div>
<pre><code>## [1] 51</code></pre>
<p>The Bayes factor is quite large in this example, and furnishes strong evidence for the alternative model, which includes a coefficient representing the effect of cloze probability. That is, under the criteria shown in Table <a href="ch-bf.html#tab:BFs">15.1</a>, the Bayes factor furnish strong evidence for an effect of cloze probability.</p>
<p>In this example, there was good prior information about the model parameter <span class="math inline">\(\beta\)</span>. However, what happens if we are not sure about the prior for the model parameter? It might happen that we compare the null model with a very “bad” alternative model, because our prior for <span class="math inline">\(\beta\)</span> is not appropriate.</p>
<p>For example, assuming that we do not know much about N400 effects, or that we do not want to make strong assumptions, we might be inclined to use an uninformative prior. For example, these could look as follows (where all the priors except for <code>b</code> remain unchanged):</p>
<div class="sourceCode" id="cb881"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb881-1" data-line-number="1">priors_vague &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb881-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb881-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">500</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb881-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">10</span>, <span class="dv">5</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb881-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb881-6" data-line-number="6">  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">4</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb881-7" data-line-number="7">)</a></code></pre></div>
<p>We can use these uninformative priors in the Bayesian model:</p>
<div class="sourceCode" id="cb882"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb882-1" data-line-number="1">fit_N400_h_linear_vague &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span></a>
<a class="sourceLine" id="cb882-2" data-line-number="2"><span class="st">  </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb882-3" data-line-number="3"><span class="dt">prior =</span> priors_vague,</a>
<a class="sourceLine" id="cb882-4" data-line-number="4"><span class="dt">warmup =</span> <span class="dv">2000</span>,</a>
<a class="sourceLine" id="cb882-5" data-line-number="5"><span class="dt">iter =</span> <span class="dv">20000</span>,</a>
<a class="sourceLine" id="cb882-6" data-line-number="6"><span class="dt">cores =</span> <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb882-7" data-line-number="7"><span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</a>
<a class="sourceLine" id="cb882-8" data-line-number="8"><span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb882-9" data-line-number="9"><span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb882-10" data-line-number="10">)</a></code></pre></div>
<p>Interestingly, we can still estimate the effect of cloze probability fairly well:</p>
<div class="sourceCode" id="cb883"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb883-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_N400_h_linear_vague, <span class="dt">variable =</span> <span class="st">&quot;b_c_cloze&quot;</span>)</a></code></pre></div>
<pre><code>##           Estimate Est.Error Q2.5 Q97.5
## b_c_cloze     2.37     0.652 1.07  3.64</code></pre>
<p>Next, we again perform the bridge sampling for the alternative model.</p>
<div class="sourceCode" id="cb885"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb885-1" data-line-number="1">margLogLik_linear_vague &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_N400_h_linear_vague,</a>
<a class="sourceLine" id="cb885-2" data-line-number="2">                                          <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>We compute the Bayes factor for the alternative over the null model, <span class="math inline">\(BF_{10}\)</span>:</p>
<div class="sourceCode" id="cb886"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb886-1" data-line-number="1">(BF_lnVague &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(margLogLik_linear_vague, margLogLik_null))</a></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 0.56000</code></pre>
<p>This is easier to read as the evidence for null model over the alternative:</p>
<div class="sourceCode" id="cb888"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb888-1" data-line-number="1"><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>BF_lnVague[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## [1] 1.79</code></pre>
<p>The result is inconclusive: there is no evidence in favor of or against the effect of cloze probability. The reason for that is that priors are never uninformative when it comes to Bayes factors. The wide prior specifies that both very small and very large effect sizes are possible (with some considerable probability), but there is relatively little evidence in the data for such large effect sizes.</p>
<p>The above example is related to a criticism of Bayes factors by Uri Simonsohn, that Bayes factors can provide evidence in favor of the null and against a very specific alternative model, when the researchers only know the direction of the effect (see <a href="https://datacolada.org/78a" class="uri">https://datacolada.org/78a</a>). This can happen when an uninformative prior is used.</p>
<p>One way to overcome this problem is to actually try to learn about the effect size that we are investigating. This can be done by first running an exploratory experiment and analysis without computing any Bayes factor, and then use the posterior distribution derived from this first experiment to calibrate the priors for the next confirmatory experiment where we do use the Bayes factor <span class="citation">(see Verhagen and Wagenmakers <a href="#ref-verhagenBayesianTestsQuantify2014">2014</a> for a Bayes Factor test calibrated to investigate replication success)</span>.</p>
<p>Another possibility is to examine a lot of different alternative models, where each model uses different prior assumptions. This way, it’s possible to investigate the extent to which the Bayes factor results depend on, or are sensitive to, the prior assumptions. This is an instance of a sensitivity analysis. Recall that the model is the likelihood <em>and</em> the priors. We can therefore compare models that only differ in the prior <span class="citation">(for an example involving EEG and predictability effects, see Nicenboim, Vasishth, and Rösler <a href="#ref-nicenboim2020words">2020</a><a href="#ref-nicenboim2020words">c</a>)</span>.</p>
<div id="sensitivity-analysis-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">15.2.1</span> Sensitivity analysis<a href="ch-bf.html#sensitivity-analysis-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we perform a sensitivtiy analysis by examining Bayes factors for several models. Each model has the same likelihood but a different prior for <span class="math inline">\(\beta\)</span>. For all of the priors we assume a normal distribution with a mean of zero. Assuming a mean of zero asserts that we do not make any assumption a priori that the effect differs from zero. If the effect should differ from zero, we want the data to tell us that. What differs between the different priors is their standard deviation. That is, what differs is the amount of uncertainty about the effect size that we allow for in the prior. A large standard deviation allows for very large effect sizes, whereas a small standard deviation asserts that we expect the effect not to be very large. Although a model with a wide prior (i.e., large standard deviation) also allocates prior probability to small effect sizes, it allocates much less probability to small effect sizes compared to a model with a narrow prior. Thus, if the effect size is in reality small, then a model with a narrow prior (small standard deviation) will have a better chance of detecting the effect.</p>
<p>Next, we try out a range of standard deviations, ranging from 1 to a much wider prior that has a standard deviation of 100. In practice, for the experiment method we are discussing here, it would not be a good idea to define very large standard deviations such as 100 microvolts, since they imply unrealistically large effect sizes. However, we include such a large value here just for illustration. Such a sensitivity analysis takes a very long time: here, we are running 11 models, where each model involves a lot of iterations to obtain stable Bayes factor estimates.</p>
<div class="sourceCode" id="cb890"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb890-1" data-line-number="1">prior_sd &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dv">2</span>, <span class="fl">2.5</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">50</span>, <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb890-2" data-line-number="2">BF &lt;-<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb890-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(prior_sd)) {</a>
<a class="sourceLine" id="cb890-4" data-line-number="4">  psd &lt;-<span class="st"> </span>prior_sd[i]</a>
<a class="sourceLine" id="cb890-5" data-line-number="5">  <span class="co"># for each prior we fit the model</span></a>
<a class="sourceLine" id="cb890-6" data-line-number="6">  fit &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb890-7" data-line-number="7">    <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb890-8" data-line-number="8">      <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb890-9" data-line-number="9">        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb890-10" data-line-number="10">        <span class="kw">set_prior</span>(<span class="kw">paste0</span>(<span class="st">&quot;normal(0,&quot;</span>, psd, <span class="st">&quot;)&quot;</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</a>
<a class="sourceLine" id="cb890-11" data-line-number="11">        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">10</span>, <span class="dv">5</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb890-12" data-line-number="12">        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb890-13" data-line-number="13">        <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">4</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb890-14" data-line-number="14">      ),</a>
<a class="sourceLine" id="cb890-15" data-line-number="15">    <span class="dt">warmup =</span> <span class="dv">2000</span>,</a>
<a class="sourceLine" id="cb890-16" data-line-number="16">    <span class="dt">iter =</span> <span class="dv">20000</span>,</a>
<a class="sourceLine" id="cb890-17" data-line-number="17">    <span class="dt">cores =</span> <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb890-18" data-line-number="18">    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</a>
<a class="sourceLine" id="cb890-19" data-line-number="19">    <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb890-20" data-line-number="20">    <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb890-21" data-line-number="21">  )</a>
<a class="sourceLine" id="cb890-22" data-line-number="22">  <span class="co"># for each model we run a brigde sampler</span></a>
<a class="sourceLine" id="cb890-23" data-line-number="23">  lml_linear_beta &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb890-24" data-line-number="24">  <span class="co"># we store the Bayes factor compared to the null model</span></a>
<a class="sourceLine" id="cb890-25" data-line-number="25">  BF &lt;-<span class="st"> </span><span class="kw">c</span>(BF, <span class="kw">bayes_factor</span>(lml_linear_beta, lml_null)<span class="op">$</span>bf)</a>
<a class="sourceLine" id="cb890-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb890-27" data-line-number="27">BFs &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">beta_sd =</span> prior_sd, BF)</a></code></pre></div>
<p>For each model, we run bridge sampling and we compute the Bayes factor of the model against our baseline or null model, which does not contain a population-level effect of cloze probability (<span class="math inline">\(BF_{10}\)</span>). Next, we need a way to visualize all the Bayes factors. We plot them in Figure <a href="ch-bf.html#fig:BFpriorsX">15.3</a> as a function of the prior width.</p>
<div class="figure"><span style="display:block;" id="fig:BFpriorsX"></span>
<img src="bookdown_files/figure-html/BFpriorsX-1.svg" alt="Prior sensitivity analysis for the Bayes factor." width="672" />
<p class="caption">
FIGURE 15.3: Prior sensitivity analysis for the Bayes factor.
</p>
</div>
<p>This figure clearly shows that the Bayes factor provides evidence for the alternative model; that is, it provides evidence that the fixed effect cloze probability is needed to explain the data. This can be seen as the Bayes factor is quite large for a range of different values for the prior standard deviation. The Bayes factor is largest for a prior standard deviation of <span class="math inline">\(2.5\)</span>, suggesting a rather small size of the effect of cloze probability. If we assume gigantic effect sizes a priori (e.g., standard deviations of 50 or 100), then the evidence for the alternative model is weaker. Conceptually, the data do not fully support such big effect sizes, but start to favor the null model relatively more, when such big effect sizes are tested against the null. Overall, we can conclude that the data provide evidence for a not too large but robust influence of cloze probability on the N400 amplitude.</p>
</div>
<div id="sec-BFnonnested" class="section level3 hasAnchor">
<h3><span class="header-section-number">15.2.2</span> Non-nested models<a href="ch-bf.html#sec-BFnonnested" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One important advantage of Bayes factors is that they can be used to compare models that are not nested. In nested models, the simpler model is a special case of the more complex and general model. For example, our previous model of cloze probability was a general model, allowing different influences of cloze probability on the N400. We compared this to a simpler, more specific null model, where the influence of cloze probability was not included, which means that the regression coefficient (fixed effect) for cloze probability was assumed to be set to zero. Such nested models can be also compared using frequentist methods such as the likelihood ratio test (ANOVA).</p>
<p>By contrast, the Bayes factor also makes it possible to compare non-nested models. An example of a non-nested model would be a case where we log-transform the cloze probability variable before using it as a predictor. A model with log cloze probability as a predictor is not a special case of a model with linear cloze probability as predictor. These are just different, alternative models. With Bayes factors, we can compare these non-nested models with each other to determine which receives more evidence from the data.</p>
<p>To do so, we first log-transform the cloze probability variable. Some cloze probabilities in the data set are equal to zero. This creates a problem when taking logs, since the log of zero is minus infinity, a value that we cannot use. We are going to overcome this problem by “smoothing” the cloze probability in this example. We use additive smoothing <span class="citation">(also called Laplace or Lidstone smoothing; Lidstone <a href="#ref-Lidstone1920">1920</a>; Chen and Goodman <a href="#ref-ChenGoodman1999">1999</a>)</span> with pseudocounts set to one, this means that the smoothed probability is calculated as the number of responses with a given gender plus one divided by the total number of responses plus two.</p>
<div class="sourceCode" id="cb891"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb891-1" data-line-number="1">df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb891-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb891-3" data-line-number="3">    <span class="dt">scloze =</span> (cloze_ans <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(N <span class="op">+</span><span class="st"> </span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb891-4" data-line-number="4">    <span class="dt">c_logscloze =</span> <span class="kw">log</span>(scloze) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(<span class="kw">log</span>(scloze))</a>
<a class="sourceLine" id="cb891-5" data-line-number="5">  )</a></code></pre></div>
<p>Next, we center the predictor variable, and we scale it to the same standard deviation as the linear cloze probabilities. To implement this scaling, first divide the centered smoothed log cloze probability variable by its standard deviation (effectively creating <span class="math inline">\(z\)</span>-scaled values). As a next step, multiply the <span class="math inline">\(z\)</span>-scaled values by the standard deviation of the non-transformed cloze probability variable. This way, both predictors (log cloze and cloze) have the same standard deviation. We therefore expect them to have a similar impact on the N400. As a result of this transformation, the same priors can be used for both variables (given that we currently have no specific information about the effect of log cloze probability versus linear cloze probability):</p>
<!--TO-DO: explain here the scaling-->
<!-- If you don't scale the predictors, then when you use raw cloze probability, beta is the change in EEG from cloze 0 to cloze 1, right? So that's more than the entire range of cloze which is from 0.1 to 0.9 (or something like this). 
But when you use log_2(cloze), with one unit of change of log_2(cloze) you barely move in cloze scale, since this corresponds 2^-10 to 2^-9, or from 2^-9 to 2^-8, etc.  This is not a big issue, but the two betas will be in very different scales. If we know that the difference in N400 for very low (~.1) and very high (~.9) cloze corresponds to ~ 1microvolt from the previous lit, the beta in the first case (raw cloze) will be a bit larger than 1microvolt (I guess 1.2?), but the beta in the case of log_2 cloze will be much larger.

If one wants to put them in the same scale, to be able to use the same priors, one possibility is to make that for both of them 1 unit corresponds to the entire range (or the same range), so you can make that 0.9-0.1 would be one unit, and also log_2(.9)-log_2(.1). (I think this is what I did). The two betas won't have the same interpretation, but at least they are more comparable, and one can assign the same prior.
-->
<div class="sourceCode" id="cb892"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb892-1" data-line-number="1">df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb892-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_logscloze =</span> <span class="kw">scale</span>(c_logscloze) <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(c_cloze))</a></code></pre></div>
<p>Then, run a linear mixed-effects model with log cloze probability instead of linear cloze probability, and we again carry out bridge sampling.</p>
<div class="sourceCode" id="cb893"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb893-1" data-line-number="1">fit_N400_h_log &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_logscloze <span class="op">+</span></a>
<a class="sourceLine" id="cb893-2" data-line-number="2"><span class="st">  </span>(c_logscloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_logscloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb893-3" data-line-number="3"><span class="dt">prior =</span> priors1,</a>
<a class="sourceLine" id="cb893-4" data-line-number="4"><span class="dt">warmup =</span> <span class="dv">2000</span>,</a>
<a class="sourceLine" id="cb893-5" data-line-number="5"><span class="dt">iter =</span> <span class="dv">20000</span>,</a>
<a class="sourceLine" id="cb893-6" data-line-number="6"><span class="dt">cores =</span> <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb893-7" data-line-number="7"><span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</a>
<a class="sourceLine" id="cb893-8" data-line-number="8"><span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb893-9" data-line-number="9"><span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb893-10" data-line-number="10">)</a></code></pre></div>
<div class="sourceCode" id="cb894"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb894-1" data-line-number="1">margLogLik_log &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_N400_h_log, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>Next, compare the linear and the log model to each other using Bayes factors.</p>
<div class="sourceCode" id="cb895"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb895-1" data-line-number="1">(BF_log_lin &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(margLogLik_log, margLogLik_linear))</a></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 6.04762</code></pre>
<p>The results show a Bayes factor of <span class="math inline">\(6\)</span> of the log model over the linear model. This shows some evidence that log cloze probability is a better predictor of N400 amplitudes than linear cloze probability. Importantly, this analysis demonstrates that model comparisons using Bayes factor are not limited to nested models, but can also be used for non-nested models.</p>
</div>
</div>
<div id="the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest" class="section level2 hasAnchor">
<h2><span class="header-section-number">15.3</span> The influence of the priors on Bayes factors: beyond the effect of interest<a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We saw above that the width (or standard deviation) of the prior distribution for the effect of interest had a strong impact on the results from Bayes factor analyses. Thus, one question is whether only the prior for the effect of interest is important, or whether priors for other model parameters can also impact the resulting Bayes factors in an analysis. It turns out that priors for other model parameters can also be important and impact Bayes factors, especially when there are non-linear components in the model, such as in generalized linear mixed effects models. We investigate this issue by using a simulated data set on a variable that has a Bernoulli distribution; in each trial, subjects can perform either successfully (<code>pDV = 1</code>) on a task, or not (<code>pDV = 0</code>). The simulated data is from a factorial experimental design, with one between-subject factor <span class="math inline">\(F\)</span> with 2 levels (<span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>), and Table <a href="ch-bf.html#tab:cTabXMeans">15.2</a> shows success probabilities for each of the experimental conditions.</p>
<div class="sourceCode" id="cb897"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb897-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_BF&quot;</span>)</a>
<a class="sourceLine" id="cb897-2" data-line-number="2"><span class="kw">str</span>(df_BF)</a></code></pre></div>
<pre><code>## tibble [100 × 3] (S3: tbl_df/tbl/data.frame)
##  $ F  : Factor w/ 2 levels &quot;F1&quot;,&quot;F2&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ pDV: int [1:100] 1 1 1 1 1 1 1 1 1 1 ...
##  $ id : int [1:100] 1 2 3 4 5 6 7 8 9 10 ...</code></pre>
<table>
<caption>
<span id="tab:cTabXMeans">TABLE 15.2: </span>Summary statistics per condition for the simulated data.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Factor A
</th>
<th style="text-align:left;">
N data
</th>
<th style="text-align:left;">
Means
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
F1
</td>
<td style="text-align:left;">
<span class="math inline">\(50\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.98\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
F2
</td>
<td style="text-align:left;">
<span class="math inline">\(50\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.70\)</span>
</td>
</tr>
</tbody>
</table>
<p>Our question now is whether there is evidence for a difference in success probabilities between groups <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>.
As contrasts for the factor <span class="math inline">\(F\)</span>, we use scaled sum coding <span class="math inline">\((-0.5, +0.5)\)</span>.</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb899-1" data-line-number="1"><span class="kw">contrasts</span>(df_BF<span class="op">$</span>F) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">+0.5</span>)</a></code></pre></div>
<p>Next, we proceed to specify our priors. For the difference between groups (<span class="math inline">\(F1\)</span> versus <span class="math inline">\(F2\)</span>), define a normally distributed prior with a mean of <span class="math inline">\(0\)</span> and a standard deviation of <span class="math inline">\(0.5\)</span>. Thus, we do not specify a direction of the difference a priori, and assume not too large effect sizes. Now run two logistic <code>brms</code> models, one with the group factor <span class="math inline">\(F\)</span> included, and one without the factor <span class="math inline">\(F\)</span>, and compute Bayes factors using bridge sampling to obtain the evidence that the data provide for the alternative hypothesis that a group difference exists between levels <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>.</p>
<p>So far, we have only specified the prior for the effect size. The question we are asking now is whether priors on other model parameters can impact the Bayes factor computations for testing the group effect. Specifically, can the prior for the intercept influence the Bayes factor for the group difference? The results show that yes, such an influence can take place in some situations. Let’s have a look at this in more detail. Let’s assume that we compare two different priors for the intercept. We specify each as a normal distribution with a standard deviation of <span class="math inline">\(0.1\)</span>, thus, specifying relatively high certainty a priori where the intercept of the data will fall. The only difference that we now specify, is that one time, the prior mean (on the latent logistic scale) is set to <span class="math inline">\(0\)</span>, corresponding to a prior mean probability of <span class="math inline">\(0.5\)</span>. In the other condition, we specify a prior mean of <span class="math inline">\(2\)</span>, corresponding to a prior mean probability of <span class="math inline">\(0.88\)</span>. When we look at the data (see Table <a href="ch-bf.html#tab:cTabXMeans">15.2</a>) we see that the prior mean of <span class="math inline">\(0\)</span> (i.e., prior probability for the intercept of <span class="math inline">\(0.5\)</span>) is not very compatible with the data, whereas the prior mean of <span class="math inline">\(2\)</span> (i.e., a prior probability for the intercept of <span class="math inline">\(0.88\)</span>) is quite closely aligned with the actual data.</p>
<p>We now compute Bayes factors for the group difference (<span class="math inline">\(F1\)</span> versus <span class="math inline">\(F2\)</span>) by using these different priors for the intercept. Thus, we first fit a null (<span class="math inline">\(M0\)</span>) and alternative (<span class="math inline">\(M1\)</span>) model under the assumption of a false prior believe (mean <span class="math inline">\(= 0\)</span>), and perform bridge sampling for these models:</p>
<div class="sourceCode" id="cb900"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb900-1" data-line-number="1"><span class="co"># set prior</span></a>
<a class="sourceLine" id="cb900-2" data-line-number="2">priors_logit1 &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb900-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.1</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb900-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb900-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb900-6" data-line-number="6"><span class="co"># Bayesian GLM: M0</span></a>
<a class="sourceLine" id="cb900-7" data-line-number="7">fit_pDV_H0 &lt;-<span class="st"> </span><span class="kw">brm</span>(pDV <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb900-8" data-line-number="8">  <span class="dt">data =</span> df_BF,</a>
<a class="sourceLine" id="cb900-9" data-line-number="9">  <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>),</a>
<a class="sourceLine" id="cb900-10" data-line-number="10">  <span class="dt">prior =</span> priors_logit1[<span class="op">-</span><span class="dv">2</span>, ],</a>
<a class="sourceLine" id="cb900-11" data-line-number="11">  <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb900-12" data-line-number="12">)</a>
<a class="sourceLine" id="cb900-13" data-line-number="13"><span class="co"># Bayesian GLM: M1</span></a>
<a class="sourceLine" id="cb900-14" data-line-number="14">fit_pDV_H1 &lt;-<span class="st"> </span><span class="kw">brm</span>(pDV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb900-15" data-line-number="15">  <span class="dt">data =</span> df_BF,</a>
<a class="sourceLine" id="cb900-16" data-line-number="16">  <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>),</a>
<a class="sourceLine" id="cb900-17" data-line-number="17">  <span class="dt">prior =</span> priors_logit1,</a>
<a class="sourceLine" id="cb900-18" data-line-number="18">  <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb900-19" data-line-number="19">)</a>
<a class="sourceLine" id="cb900-20" data-line-number="20"><span class="co"># bridge sampling</span></a>
<a class="sourceLine" id="cb900-21" data-line-number="21">mLL_binom_H0 &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pDV_H0, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb900-22" data-line-number="22">mLL_binom_H1 &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pDV_H1, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>Next, we again prepare computation of Bayes factors, by again running the null (<span class="math inline">\(M0\)</span>) and the alternative (<span class="math inline">\(M1\)</span>) model, now assuming a more realistic prior for the intercept (prior mean <span class="math inline">\(= 2\)</span>).</p>
<div class="sourceCode" id="cb901"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb901-1" data-line-number="1">priors_logit2 &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb901-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">2</span>, <span class="fl">0.1</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb901-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> b)</a>
<a class="sourceLine" id="cb901-4" data-line-number="4">)</a>
<a class="sourceLine" id="cb901-5" data-line-number="5"><span class="co"># Bayesian GLM: M0</span></a>
<a class="sourceLine" id="cb901-6" data-line-number="6">fit_pDV_H0_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">brm</span>(pDV <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb901-7" data-line-number="7">  <span class="dt">data =</span> df_BF,</a>
<a class="sourceLine" id="cb901-8" data-line-number="8">  <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>),</a>
<a class="sourceLine" id="cb901-9" data-line-number="9">  <span class="dt">prior =</span> priors_logit2[<span class="op">-</span><span class="dv">2</span>, ],</a>
<a class="sourceLine" id="cb901-10" data-line-number="10">  <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb901-11" data-line-number="11">)</a>
<a class="sourceLine" id="cb901-12" data-line-number="12"><span class="co"># Bayesian GLM: M1</span></a>
<a class="sourceLine" id="cb901-13" data-line-number="13">fit_pDV_H1_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">brm</span>(pDV <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>F,</a>
<a class="sourceLine" id="cb901-14" data-line-number="14">  <span class="dt">data =</span> df_BF,</a>
<a class="sourceLine" id="cb901-15" data-line-number="15">  <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>),</a>
<a class="sourceLine" id="cb901-16" data-line-number="16">  <span class="dt">prior =</span> priors_logit2,</a>
<a class="sourceLine" id="cb901-17" data-line-number="17">  <span class="dt">save_pars =</span> <span class="kw">save_pars</span>(<span class="dt">all =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb901-18" data-line-number="18">)</a>
<a class="sourceLine" id="cb901-19" data-line-number="19"><span class="co"># bridge sampling</span></a>
<a class="sourceLine" id="cb901-20" data-line-number="20">mLL_binom_H0_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pDV_H0_<span class="dv">2</span>, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb901-21" data-line-number="21">mLL_binom_H1_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pDV_H1_<span class="dv">2</span>, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>Based on these models and bridge samples, we can now compute the Bayes factors in support for <span class="math inline">\(M1\)</span> (i.e., in support of a group-difference between <span class="math inline">\(F1\)</span> and <span class="math inline">\(F2\)</span>). We can do so for the unrealistic prior for the intercept (prior mean of <span class="math inline">\(0\)</span>) and the more realistic prior for the intercept (prior mean of <span class="math inline">\(2\)</span>).</p>
<div class="sourceCode" id="cb902"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb902-1" data-line-number="1">(BF_binom_H1_H0 &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(mLL_binom_H1, mLL_binom_H0))</a></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 7.19449</code></pre>
<div class="sourceCode" id="cb904"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb904-1" data-line-number="1">(BF_binom_H1_H0_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">bayes_factor</span>(mLL_binom_H1_<span class="dv">2</span>, mLL_binom_H0_<span class="dv">2</span>))</a></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of x1 over x2: 29.60257</code></pre>
<p>The results show that with the realistic prior for the intercept (prior mean <span class="math inline">\(= 2\)</span>), the evidence for the <span class="math inline">\(M1\)</span> is quite strong, with a Bayes factor of <span class="math inline">\(BF_{10} =\)</span> 29.6. With the unrealistic prior for the intercept (i.e., prior mean <span class="math inline">\(= 0\)</span>), by contrast, the evidence for the <span class="math inline">\(M1\)</span> is much reduced, <span class="math inline">\(BF_{10} =\)</span> 7.2, and now only modest.</p>
<p>Thus, when performing Bayes factor analyses, not only can the priors for the effect of interest (here the group difference) impact the results, under certain circumstances priors for other model parameters can too, such as the prior mean for the intercept here. Such an influence will not always be strong, and can sometimes be negligible. There may be many situations, where the exact specification of the intercept does not have much of an effect on the Bayes factor for a group difference. However, such influences can in principle occur, especially in models with non-linear components. Therefore, it is very important to be careful in specifying realistic priors for all model parameters, also including the intercept. A good way to judge whether prior assumptions are realistic and plausible is prior predictive checks, where we simulate data based on the priors and the model and judge whether the simulated data is plausible and realistic.</p>
</div>
<div id="sec-stanBF" class="section level2 hasAnchor">
<h2><span class="header-section-number">15.4</span> Bayes factor in Stan<a href="ch-bf.html#sec-stanBF" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The package <code>bridgesampling</code> allows for a straightforward calculation of Bayes factor for Stan models as well. All the limitations and caveats of Bayes factor discussed in this chapter apply to Stan code as much as they apply to <code>brms</code> code. Importantly, the sampling notation (<code>~</code>) should not be used; see Box <a href="ch-introstan.html#thm:tilde">10.2</a>.</p>
<p>An advantage of using Stan in comparison with <code>brms</code> is Stan’s flexibility. We revisit the model implemented before in section <a href="ch-introstan.html#sec-interstan">10.4.2</a>. We want to assess the evidence for a <em>positive</em> effect of attentional load on pupil size against a similar model that assumes no effect. To do this, assume the following likelihood:</p>
<p><span class="math display">\[\begin{equation}
p\_size_n \sim \mathit{Normal}(\alpha + c\_load_n \cdot \beta_1 + c\_trial \cdot \beta_2 + c\_load \cdot c\_trial \cdot \beta_3, \sigma)
\end{equation}\]</span></p>
<p>Define priors for all the <span class="math inline">\(\beta\)</span>s as before, with the difference that <span class="math inline">\(\beta_1\)</span> can only have positive values:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(1000, 500) \\
\beta_1 &amp;\sim \mathit{Normal}_+(0, 100) \\
\beta_2 &amp;\sim \mathit{Normal}(0, 100) \\
\beta_3 &amp;\sim \mathit{Normal}(0, 100) \\
\sigma &amp;\sim \mathit{Normal}_+(0, 1000)
\end{aligned}
\end{equation}\]</span></p>
<p>The following Stan model is the direct translation of the new priors and likelihood.</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] c_load;
  vector[N] c_trial;
  vector[N] p_size;
}
parameters {
  real alpha;
  real&lt;lower = 0&gt; beta1;
  real beta2;
  real beta3;
  real&lt;lower = 0&gt; sigma;
}
model {
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta1 | 0, 100) -
    normal_lccdf(0 | 0, 100);
  target += normal_lpdf(beta2 | 0, 100);
  target += normal_lpdf(beta3 | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  target += normal_lpdf(p_size | alpha + c_load * beta1 +
                                 c_trial * beta2 +
                                 c_load .* c_trial * beta3, sigma);
}</code></pre>
<p>Fit the model with 20000 iterations to ensure that the Bayes factor is stable, and increase the <code>adapt_delta</code> parameter to avoid warnings:</p>
<div class="sourceCode" id="cb907"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb907-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_pupil&quot;</span>)</a>
<a class="sourceLine" id="cb907-2" data-line-number="2">df_pupil &lt;-<span class="st"> </span>df_pupil <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb907-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb907-4" data-line-number="4">    <span class="dt">c_load =</span> load <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(load),</a>
<a class="sourceLine" id="cb907-5" data-line-number="5">    <span class="dt">c_trial =</span> trial <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(trial)</a>
<a class="sourceLine" id="cb907-6" data-line-number="6">  )</a>
<a class="sourceLine" id="cb907-7" data-line-number="7">ls_pupil &lt;-<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb907-8" data-line-number="8">  <span class="dt">p_size =</span> df_pupil<span class="op">$</span>p_size,</a>
<a class="sourceLine" id="cb907-9" data-line-number="9">  <span class="dt">c_load =</span> df_pupil<span class="op">$</span>c_load,</a>
<a class="sourceLine" id="cb907-10" data-line-number="10">  <span class="dt">c_trial =</span> df_pupil<span class="op">$</span>c_trial,</a>
<a class="sourceLine" id="cb907-11" data-line-number="11">  <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil)</a>
<a class="sourceLine" id="cb907-12" data-line-number="12">)</a>
<a class="sourceLine" id="cb907-13" data-line-number="13">pupil_pos &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb907-14" data-line-number="14">                         <span class="st">&quot;pupil_pos.stan&quot;</span>,</a>
<a class="sourceLine" id="cb907-15" data-line-number="15">                         <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</a>
<a class="sourceLine" id="cb907-16" data-line-number="16">fit_pupil_int_pos &lt;-<span class="st"> </span><span class="kw">stan</span>(</a>
<a class="sourceLine" id="cb907-17" data-line-number="17">  <span class="dt">file =</span> pupil_pos,</a>
<a class="sourceLine" id="cb907-18" data-line-number="18">  <span class="dt">data =</span> ls_pupil,</a>
<a class="sourceLine" id="cb907-19" data-line-number="19">  <span class="dt">warmup =</span> <span class="dv">1000</span>,</a>
<a class="sourceLine" id="cb907-20" data-line-number="20">  <span class="dt">iter =</span> <span class="dv">20000</span>,</a>
<a class="sourceLine" id="cb907-21" data-line-number="21">  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">.95</span>))</a></code></pre></div>
<p>The null model that we defined has <span class="math inline">\(\beta_1 = 0\)</span> and is written in Stan as follows:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] c_load;
  vector[N] c_trial;
  vector[N] p_size;
}
parameters {
  real alpha;
  real beta2;
  real beta3;
  real&lt;lower = 0&gt; sigma;
}
model {
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta2 | 0, 100);
  target += normal_lpdf(beta3 | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  target += normal_lpdf(p_size | alpha + c_trial * beta2 +
                                 c_load .* c_trial * beta3, sigma);
}
generated quantities{</code></pre>
<div class="sourceCode" id="cb909"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb909-1" data-line-number="1">pupil_null &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</a>
<a class="sourceLine" id="cb909-2" data-line-number="2">  <span class="st">&quot;pupil_null.stan&quot;</span>,</a>
<a class="sourceLine" id="cb909-3" data-line-number="3">  <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span></a>
<a class="sourceLine" id="cb909-4" data-line-number="4">)</a>
<a class="sourceLine" id="cb909-5" data-line-number="5">fit_pupil_int_null &lt;-<span class="st"> </span><span class="kw">stan</span>(</a>
<a class="sourceLine" id="cb909-6" data-line-number="6">  <span class="dt">file =</span> pupil_null,</a>
<a class="sourceLine" id="cb909-7" data-line-number="7">  <span class="dt">data =</span> ls_pupil,</a>
<a class="sourceLine" id="cb909-8" data-line-number="8">  <span class="dt">warmup =</span> <span class="dv">1000</span>,</a>
<a class="sourceLine" id="cb909-9" data-line-number="9">  <span class="dt">iter =</span> <span class="dv">20000</span></a>
<a class="sourceLine" id="cb909-10" data-line-number="10">)</a></code></pre></div>
<p>Compare the models with bridge sampling:</p>
<div class="sourceCode" id="cb910"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb910-1" data-line-number="1">lml_pupil &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pupil_int_pos, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb910-2" data-line-number="2">lml_pupil_null &lt;-<span class="st"> </span><span class="kw">bridge_sampler</span>(fit_pupil_int_null, <span class="dt">silent =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb910-3" data-line-number="3">BF_att &lt;-<span class="st"> </span>bridgesampling<span class="op">::</span><span class="kw">bf</span>(lml_pupil, lml_pupil_null)</a></code></pre></div>
<div class="sourceCode" id="cb911"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb911-1" data-line-number="1">BF_att</a></code></pre></div>
<pre><code>## Estimated Bayes factor in favor of lml_pupil over lml_pupil_null: 25.17274</code></pre>
<p>We find that the data is 25.173 more likely under a model that assumes a positive effect of load than under a model that assumes no effect.</p>
</div>
<div id="bayes-factors-in-theory-and-in-practice" class="section level2 hasAnchor">
<h2><span class="header-section-number">15.5</span> Bayes factors in theory and in practice<a href="ch-bf.html#bayes-factors-in-theory-and-in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="bayes-factors-in-theory-stability-and-accuracy" class="section level3 hasAnchor">
<h3><span class="header-section-number">15.5.1</span> Bayes factors in theory: Stability and accuracy<a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One question that we can ask here is how stable and accurate the estimates of Bayes factors are. Importantly, the bridge sampling algorithm needs a lot of posterior samples to obtain stable estimates of the Bayes factor. Running bridge sampling based on a too small an effective sample size (related to the number of posterior samples) will yield unstable estimates of the Bayes factor, such that repeated computations will yield radically different Bayes factor values. Moreover, even if the Bayes factor is approximated in a stable way, it is unclear whether this approximate Bayes factor is equal to the true Bayes factor, or whether there is bias in the computation such that the approximate Bayes factor has a wrong value. We show this below.</p>
<div id="instability-due-to-the-effective-number-of-posterior-samples" class="section level4 hasAnchor">
<h4><span class="header-section-number">15.5.1.1</span> Instability due to the effective number of posterior samples<a href="ch-bf.html#instability-due-to-the-effective-number-of-posterior-samples" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The number of iterations, which in turn affects the total number of posterior samples can have a strong impact on the robustness of the results of the bridge sampling algorithm (i.e., on the resulting Bayes factor) and there are no good theoretical guarantees that bridge sample will yield accurate estimates of Bayes factors. In the analyses presented above, we set the number of iterations to a very large number of <span class="math inline">\(n = 20000\)</span>. The sensitivity analysis therefore took a considerable amount of time. Indeed, the results from this analysis were stable, as shown below.</p>
<p>Running the same analysis with less iterations will induce some instability in the Bayes factor estimates based on the bridge sampling, such that running the same analysis twice would yield different results for the Bayes factor. Moreover, bridge sampling in itself may be unstable and may return different results for different runs on the same posterior samples (just because of different starting values). This is very concerning, as the results reported in a paper might not be stable if the number of effective sample size is not large enough. Indeed, the default number of iterations in <code>brms</code> is set as <code>iter = 2000</code> (and the default number of warmup iterations is <code>warmup = 1000</code>). These defaults were not set to support bridge sampling, i.e., they were not defined for computation of densities to support Bayes factors. Instead, they are valid for posterior inference on expectations (e.g., posterior means) for models that are not too complex. However, when using these defaults for estimation of densities and the computation of Bayes factors, instabilities can arise.</p>
<p>As an illustration, we perform the same sensitivity analysis again, now using the default number of <span class="math inline">\(2000\)</span> iterations in <code>brms</code>. The posterior sampling process now runs much quicker. Moreover, we check the stability of the Bayes factors in the sensitivity analyses by repeating both sensitivity analyses (with <span class="math inline">\(n = 20000\)</span> iterations and with the default number of <span class="math inline">\(n = 2000\)</span> iterations) a second time, to see whether the results for Bayes factors are stable.</p>
<div class="figure"><span style="display:block;" id="fig:BFpriors2"></span>
<img src="bookdown_files/figure-html/BFpriors2-1.svg" alt="The effect of the number of samples on a prior sensitivity analysis for the Bayes factor. Grey lines show 20 runs with default number of iterations (2000)." width="672" />
<p class="caption">
FIGURE 15.4: The effect of the number of samples on a prior sensitivity analysis for the Bayes factor. Grey lines show 20 runs with default number of iterations (2000).
</p>
</div>
<p>The results displayed in Figure <a href="ch-bf.html#fig:BFpriors2">15.4</a> show that the resulting Bayes factors are highly unstable when the number of iterations is low. They clearly deviate from the Bayes factors estimated with <span class="math inline">\(20000\)</span> iterations, resulting in very unstable estimates. By contrast, the analyses using <span class="math inline">\(20000\)</span> iterations provide nearly the same results in both analyses. The two lines lie virtually directly on top of each other; the points are jittered horizontally for better visibility.</p>
<p>This result demonstrates that it is necessary to use a large number of iterations when computing Bayes factors using <code>brms</code> and <code>bridge_sampler()</code>. In practice, one should compute the sensitivity analysis (or at least one of the models or priors) twice (as we did here) to make sure that the results are stable and sufficiently similar, in order to provide a good basis for reporting results.</p>
<p>By contrast, Bayes factors based on the Savage-Dickey method (as implemented in <code>brms</code>) can be unstable even when using a large number of posterior samples. This problem can arise especially when the posterior is very far from zero, and thus very large or very small Bayes factors are obtained. Because of this instability of the Savage-Dickey method in <code>brms</code>, it is a good idea to use bridge sampling, and to check the stability of the estimates.</p>
</div>
<div id="inaccuracy-of-bayes-factor-estimates-does-the-estimate-approximate-the-true-bayes-factor-well" class="section level4 hasAnchor">
<h4><span class="header-section-number">15.5.1.2</span> Inaccuracy of Bayes factor estimates: Does the estimate approximate the true Bayes factor well?<a href="ch-bf.html#inaccuracy-of-bayes-factor-estimates-does-the-estimate-approximate-the-true-bayes-factor-well" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An important point about approximate estimates of Bayes factors using bridge sampling is that there are no strong guarantees for their accuracy. That is, even if we can show that the approximated Bayes factor estimate using bridge sampling is stable (i.e., when using sufficient effective samples, see the analyses above), even then it remains unclear whether the Bayes factor estimate actually is close to the true Bayes factor. In principle, it could very well be that the stably estimated Bayes factors based on bridge sampling are in fact biased, i.e., that they are not close to the correct (true) Bayes factor, but that the estimation exhibits bias and yields a different value. The technique of simulation-based calibration <span class="citation">(SBC; Talts et al. <a href="#ref-talts2018validating">2018</a>; Schad, Betancourt, and Vasishth <a href="#ref-schad2020toward">2020</a>)</span> can be used to investigate this question (SBC is also discussed in section <a href="ch-custom.html#sec-validSBC">12.2</a> in chapter <a href="ch-custom.html#ch-custom">12</a>). We ask and investigate this question next <span class="citation">(for details, see Schad et al. <a href="#ref-schad2022workflow">2022</a>)</span>.</p>
<p>In the SBC approach, the priors are used to simulate data. Then, posterior inference is done on the simulated data, and the posterior can be compared to the prior. If the posteriors are equal to the priors, then this supports accurate computations. Applied to Bayes factor analyses, one defines a prior on the hypothesis space, i.e., one defines the prior probabilities for a null and an alternative model, specifying how likely each model is a priori. From these priors, one can randomly draw one hypothesis (model), e.g., <span class="math inline">\(nsim = 500\)</span> times. Thus, in each of <span class="math inline">\(500\)</span> draws one randomly chooses one model (either <span class="math inline">\(M0\)</span> or <span class="math inline">\(M1\)</span>), with the probabilities given by the model priors. For each draw, one first samples model parameters from their prior distributions, and then uses these sampled model parameters to simulate data. For each simulated data set, one can then compute marginal likelihoods and Bayes factor estimates using posterior samples and bridge sampling, and one can then compute the posterior probabilities for each hypothesis (i.e., how likely each model is a posteriori). As the last, and critical step in SBC, one can then compare the posterior model probabilities to the prior model probabilities. A key result in SBC is that if the computation of marginal likelihoods and posterior model probabilities is performed accurately (without bias) by the bridge sampling procedure; that is, if the Bayes factor estimate is close to the true Bayes factor, then the posterior model probabilities should be the same as the prior model probabilities.</p>
<p>Here, we perform this SBC approach. Across the <span class="math inline">\(500\)</span> simulations, we systematically vary the prior model probability from zero to one. For each of the <span class="math inline">\(500\)</span> simulations we sample a model (hypothesis) from the model prior, then sample parameters from the priors over parameters, use the sampled parameters to simulate fake data, fit the null and the alternative model on the simulated data, perform bridge sampling for each model, compute the Bayes factor estimate between them, and compute posterior model probabilities. If the bridge sampling works accurately, then the posterior model probabilities should be the same as the prior model probabilities. Given that we varied the prior model probabilities from zero to one, the posterior model probabilities should also vary from zero to one. In Figure <a href="ch-bf.html#fig:SBC3plot">15.5</a>, we plot the posterior model probabilities as a function of the prior model probabilities. If the posterior probabilities are the same as the priors, then the local regression line and all points should lie on the diagonal.</p>
<div class="figure"><span style="display:block;" id="fig:SBC3plot"></span>
<img src="bookdown_files/figure-html/SBC3plot-1.svg" alt="The posterior probabilities for M0 are plotted as a function of prior probabilities for M0. If the approximation of the Bayes factor using bridge sampling is unbiased, then the data should be aligned along the diagonal (see dashed black line). The thick black line is a prediction from a local regression analysis. The points are average posterior probabilities as a function of a priori selected hypotheses for 50 simulation runs each. Error bars represent 95 percent confidence intervals." width="576" />
<p class="caption">
FIGURE 15.5: The posterior probabilities for M0 are plotted as a function of prior probabilities for M0. If the approximation of the Bayes factor using bridge sampling is unbiased, then the data should be aligned along the diagonal (see dashed black line). The thick black line is a prediction from a local regression analysis. The points are average posterior probabilities as a function of a priori selected hypotheses for 50 simulation runs each. Error bars represent 95 percent confidence intervals.
</p>
</div>
<p>The results of this analysis in Figure <a href="ch-bf.html#fig:SBC3plot">15.5</a> show that the local regression line is very close to the diagonal, and that the data points (each summarizing results from 50 simulations, with means and confidence intervals) also lie close to the diagonal. This importantly demonstrates that the estimated posterior model probabilities are close to their a priori values. This result shows that posterior model probabilities, which are based on the Bayes factor estimates from the bridge sampling, are unbiased for a large range of different a priori model probabilities.</p>
<p>This result is very important as it shows one example case where the Bayes factor approximation is accurate. Importantly, however, of course this demonstration is valid only for this one specific application case, i.e., with a particular data set, particular models, specific priors for the parameters, and a specific comparison between nested models. Strictly speaking, if one wants to be sure that the Bayes factor estimate is accurate for a particular data analysis, then such a SBC validation analysis would have to be computed for every data analysis. For details, including code, on how to perform such an SBC, see <span class="citation">Schad et al. (<a href="#ref-schad2022workflow">2022</a>)</span>. However, the fact that the SBC yields such promising results for this first application case also gives some hope that the bridge sampling may be accurate also for other comparable data analysis situations.</p>
<p>Based on these results on the average theoretical performance of Bayes factor estimation, we next turn to a different issue: how Bayes factors depend on and vary with varying data, leading to bad performance in individual cases despite good average performance.</p>
</div>
</div>
<div id="sec-BFvar" class="section level3 hasAnchor">
<h3><span class="header-section-number">15.5.2</span> Bayes factors in practice: Variability with the data<a href="ch-bf.html#sec-BFvar" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="variation-associated-with-the-data-subjects-items-and-residual-noise" class="section level4 hasAnchor">
<h4><span class="header-section-number">15.5.2.1</span> Variation associated with the data (subjects, items, and residual noise)<a href="ch-bf.html#variation-associated-with-the-data-subjects-items-and-residual-noise" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A second, and very different, source limiting robustness of Bayes factor estimates derives from the variability that is observed with the data, i.e., among subjects, items, and residual noise. Thus, when repeating an experiment a second time in a replication analysis, using different subjects and items, will lead to different outcomes of the statistical analysis every time a new replication run is conducted. This limit to robustness is well known in frequentist analyses, as the “dance of p-values” <span class="citation">(Cumming <a href="#ref-cumming2014new">2014</a>)</span>, where over repeated replication attempts, p-values are not consistently significant across studies. Instead, the results yield highly different p-values each time a study is re-run. This can also be observed when simulating data from some known truth and re-running analyses on simulated data sets.</p>
<p>This same type of variability should also be present in Bayesian analyses (also see <a href="https://daniellakens.blogspot.com/2016/07/dance-of-bayes-factors.html" class="uri">https://daniellakens.blogspot.com/2016/07/dance-of-bayes-factors.html</a>). <!--Here, we investigate into how variable outcomes for sensitivity analyses can be for empirical replication data for a given experimental effect.-->Here we show this type of variability in Bayes factor analyses by looking at a new example data analysis: We look at research on sentence comprehension, and specifically on effects of cue-based retrieval interference <span class="citation">(Lewis and Vasishth <a href="#ref-lewisvasishth:cogsci05">2005</a>; Van Dyke and McElree <a href="#ref-van2011cue">2011</a>)</span>.</p>
</div>
<div id="example-facilitatory-interference-effects" class="section level4 hasAnchor">
<h4><span class="header-section-number">15.5.2.2</span> Example: Facilitatory interference effects<a href="ch-bf.html#example-facilitatory-interference-effects" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the following, we will look at experimental studies that investigated cognitive mechanisms underlying a well-studied phenomenon in sentence comprehension. The example we consider here is the agreement attraction configuration below, where the ungrammatical sentence (2) seems more grammatical than the equally ungrammatical sentence (1):</p>
<ol style="list-style-type: decimal">
<li>The key to the cabinet are in the kitchen.</li>
<li>The key to the cabinets are in the kitchen.</li>
</ol>
<p>Both sentences are ungrammatical because the subject (“key”) does not agree with the verb in number (“are”). Sentences such as (2) are often found to have shorter reading times at (or just following) the verb (“are”) compared to (1) <span class="citation">(for a meta-analysis see Jäger, Engelmann, and Vasishth <a href="#ref-JaegerEngelmannVasishth2017">2017</a>)</span>. Such shorter reading times are sometimes referred to as “facilitatory interference” <span class="citation">(Dillon <a href="#ref-dillon2011structured">2011</a>)</span>; facilitatory here does not necessarily mean that processing is easier, it just means that reading times at the relevant word are shorter in (2) vs. (1). One proposal explaining the shorter reading times is that the attractor word (here, cabinets) agrees locally in number with the verb, leading to an illusion of grammaticality. This is an interesting phenomenon because the plural versus singular feature of the attractor noun (“cabinet/s”) is not the subject, and therefore, under the rules of English grammar, is not supposed to agree with the number marking on the verb. That agreement attraction effects are consistently observed indicates that some non-compositional processes are taking place.</p>
<p>An account of agreement attraction effects in language processing that is based on a full computational implementation <span class="citation">(which is in the ACT-R framework, Anderson et al. <a href="#ref-abbl02">2004</a>)</span>, explains such agreement attraction effects in ungrammatical sentences as a result of retrieval-based working memory mechanisms <span class="citation">(Engelmann, Jäger, and Vasishth <a href="#ref-EngelmannJaegerVasishth2019">2020</a>; cf. Hammerly, Staub, and Dillon <a href="#ref-hammerly2019grammaticality">2019</a>; and Yadav, Smith, et al. <a href="#ref-YadavetalJML2022">2022</a>)</span>. Agreement attraction in ungrammatical sentences has been investigated many times in similar experimental setups with different dependent measures such as self-paced reading and eye-tracking. It is generally believed to be a robust empirical phenomenon, and we choose it for analysis here for that reason.</p>
<p>Here, we look at a self-paced reading study on agreement attraction in Spanish by <span class="citation">Lago et al. (<a href="#ref-lago2015agreement">2015</a>)</span>. We estimate a population-level effect for the experimental condition agreement attraction (<code>x</code>; i.e., sentence type), against a null model where the population-level effect of sentence type is excluded. For the agreement attraction effect of sentence type, we use sum contrast coding (i.e., -1 and +1). We run a hierarchical model with the following formula in <code>brms</code>: <code>rt ~ 1+ x + (1+ x | subj) + (1 + x | item)</code>, where <code>rt</code> is reading time, we have random variation associated with subjects and with items, and we assume that reading times follow a log-normal distribution: <code>family = lognormal()</code>.</p>
<p>First, load the data:</p>
<div class="sourceCode" id="cb913"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb913-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_lagoE1&quot;</span>)</a>
<a class="sourceLine" id="cb913-2" data-line-number="2"><span class="kw">head</span>(df_lagoE1)</a></code></pre></div>
<pre><code>##     subj item  rt  int  x   expt
## 2     S1   I1 588  low -1 lagoE1
## 22    S1  I10 682 high  1 lagoE1
## 77    S1  I13 226  low -1 lagoE1
## 92    S1  I14 580 high  1 lagoE1
## 136   S1  I17 549  low -1 lagoE1
## 153   S1  I18 458 high  1 lagoE1</code></pre>
<p>As a next step, determine priors for the analysis of these data.</p>
</div>
<div id="determine-priors-using-meta-analysis" class="section level4 hasAnchor">
<h4><span class="header-section-number">15.5.2.3</span> Determine priors using meta-analysis<a href="ch-bf.html#determine-priors-using-meta-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One good way to obtain priors for Bayesian analyses, and specifically for Bayes factor analyses, is to use results from meta-analyses on the subject. Here, we take the prior for the experimental manipulation of agreement attraction from a published meta-analysis <span class="citation">(Jäger, Engelmann, and Vasishth <a href="#ref-JaegerEngelmannVasishth2017">2017</a>)</span>.<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a></p>
<p>The mean effect size (difference in reading time between the two experimental conditions) in the meta-analysis is <span class="math inline">\(-22\)</span> milliseconds (ms), with <span class="math inline">\(95\% \;CI = [-36 \; -9]\)</span> <span class="citation">(Jäger, Engelmann, and Vasishth <a href="#ref-JaegerEngelmannVasishth2017">2017</a>, Table 4)</span>. This means that on average, the target word (i.e., the verb) in sentences such as (2) is on average read <span class="math inline">\(22\)</span> milliseconds faster than in sentences such as (1). The size of the effect is measured on the millisecond scale, assuming a normal distribution of effect sizes across studies.</p>
<p>However, individual reading times usually do not follow a normal distribution. Instead, a better assumption about the distribution of reading times is a log-normal distribution. This is what we will assume in the <code>brms</code> model. Therefore, to use the prior from the meta-analysis in the Bayesian analysis, we have to transform the prior values from the millisecond scale to log millisecond scale.</p>
<p>We have performed this transformation in <span class="citation">Schad et al. (<a href="#ref-schad2022workflow">2022</a>)</span>. Based on these calculations, the prior for the experimental factor of interference effects is set to a normal distribution with mean <span class="math inline">\(= -0.03\)</span> and standard deviation = <span class="math inline">\(0.009\)</span>. For the other model parameters, we use principled priors.</p>
<div class="sourceCode" id="cb915"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb915-1" data-line-number="1">priors &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb915-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb915-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="op">-</span><span class="fl">0.03</span>, <span class="fl">0.009</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb915-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb915-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb915-6" data-line-number="6">  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb915-7" data-line-number="7">)</a></code></pre></div>
</div>
<div id="running-a-hierarchical-bayesian-analysis" class="section level4 hasAnchor">
<h4><span class="header-section-number">15.5.2.4</span> Running a hierarchical Bayesian analysis<a href="ch-bf.html#running-a-hierarchical-bayesian-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Next, run a <code>brms</code> model on the data. We use a large number of iterations (<code>iter = 10000</code>) with bridge sampling to estimate the Bayes factor of the “full” model, which includes a population-level effect for the experimental condition agreement attraction (<code>x</code>; i.e., sentence type). As mentioned above, for the agreement attraction effect of sentence type, we use sum contrast coding (i.e., <span class="math inline">\(-1\)</span> and <span class="math inline">\(+1\)</span>).</p>
<p>We first show the population-level effects from the posterior analyses:</p>
<div class="sourceCode" id="cb916"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb916-1" data-line-number="1"><span class="kw">fixef</span>(m1_lagoE1)</a></code></pre></div>
<pre><code>##           Estimate Est.Error  Q2.5 Q97.5
## Intercept     6.02      0.06  5.90  6.13
## x            -0.03      0.01 -0.04 -0.01</code></pre>
<p>They show that for the population-level effect <code>x</code>, capturing the agreement attraction effect, the 95% credible interval does not overlap with zero. This indicates that there is some hint that the effect may have the expected negative direction, reflecting shorter reading times in the plural condition. As mentioned earlier, this does not provide a direct test of the hypothesis that the effect exists and is not zero. This is not tested here, because we did not specify the null hypothesis of zero effect explicitly. We can, however, draw inferences about this null hypothesis by using the Bayes factor.</p>
<p>Estimate Bayes factors between a full model, where the effect of agreement attraction is included, and a null model, where the effect of agreement attraction is absent, using the command <code>bayes_factor(lml_m1_lagoE1, lml_m0_lagoE1)</code>. The function computes the Bayes factor <span class="math inline">\(BF_{10}\)</span>, that is, the evidence of the alternative over the null.</p>
<div class="sourceCode" id="cb918"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb918-1" data-line-number="1">h_lagoE1<span class="op">$</span>bf</a></code></pre></div>
<pre><code>## [1] 6.29</code></pre>
<p>The output shows a Bayes factor of <span class="math inline">\(6\)</span>, suggesting that there is some support for the alternative model, which includes the population-level effect of agreement attraction. That is, this provides evidence for the alternative hypothesis that there is a difference between the experimental conditions, i.e., a facilitatory effect in the plural condition of the size derived from the meta-analysis.</p>
<p>The <code>bayes_factor</code> command should be run several times to check the stability of the Bayes factor calculation.</p>
</div>
<div id="variability-of-the-bayes-factor-posterior-simulations" class="section level4 hasAnchor">
<h4><span class="header-section-number">15.5.2.5</span> Variability of the Bayes factor: Posterior simulations<a href="ch-bf.html#variability-of-the-bayes-factor-posterior-simulations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One way to investigate how variable the outcome of Bayes factor analyses can be (given that the Bayes factor is computed in a stable and accurate way), is to run posterior simulations based on a fitted model. That is, one can assume that the truth is approximately known (as approximated by the posterior model fit), and that based on this “truth” several data sets are simulated. Computing the Bayes factor analysis again on the simulated data can provide some insight into how variable the Bayes factor will be in a situation where the “true” data generating process is always the same, and where variations in Bayes factor results have to be attributed to random noise in subjects, items, residual variation, and to uncertainty about the precise true parameter values.</p>
<p>We can take the Bayesian hierarchical model fitted to the data from <span class="citation">Lago et al. (<a href="#ref-lago2015agreement">2015</a>)</span>, and run posterior predictive simulations. In these simulations, one takes posterior samples for the model parameters (i.e., <span class="math inline">\(p(\boldsymbol{\Theta} \mid \boldsymbol{y})\)</span>), and for each posterior sample of the model parameters, one can simulate new data <span class="math inline">\(\tilde{\boldsymbol{y}}\)</span> from the model <span class="math inline">\(p(\tilde{\boldsymbol{y}} \mid \boldsymbol{\Theta})\)</span>.</p>
<div class="sourceCode" id="cb920"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb920-1" data-line-number="1">pred_lagoE1 &lt;-<span class="st"> </span><span class="kw">posterior_predict</span>(m1_lagoE1)</a></code></pre></div>
<p>The question that we are interested in here now is, how much information is contained in this posterior simulated data. That is, we can run Bayesian models on this posterior simulated data and compute Bayes factors to test whether in the simulated data there is evidence for agreement attraction effects. Of great interest to us is then the question of how variable the results of these Bayes factor analyses will be across different simulated replications of the same study.</p>
<p>We now perform this analysis for <span class="math inline">\(50\)</span> different data sets simulated from the posterior predictive distribution. For each of these data sets, we can proceed in exactly the same way as we did for the real observed experimental data. That is, we again fit the same <code>brms</code> model <span class="math inline">\(50\)</span> times, now to the simulated data, and using the same prior as before. For each simulated data set, we use bridge sampling to compute the Bayes factor of the alternative model compared to a null model where the agreement attraction effect (population-level effect predictor of sentence type, <code>x</code>) is set to <span class="math inline">\(0\)</span>. For each simulated posterior predictive data set, we store the resulting Bayes factor. We again use the prior from the meta-analysis.</p>
</div>
<div id="visualize-distribution-of-bayes-factors" class="section level4 hasAnchor">
<h4><span class="header-section-number">15.5.2.6</span> Visualize distribution of Bayes factors<a href="ch-bf.html#visualize-distribution-of-bayes-factors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can now visualize the distribution of Bayes factors (<span class="math inline">\(BF_{10}\)</span>) across posterior predictive distributions by plotting a histogram. Values larger than one in this histogram indicate evidence for the alternative model (M1) that agreement attraction effects exist (i.e., the sentence type effect is different from zero), and Bayes factor values smaller than one indicate evidence for the null model (M0) that no agreement attraction effect exists (i.e., the difference in reading times between experimental conditions is zero).</p>
<div class="figure"><span style="display:block;" id="fig:plotBFdistrn"></span>
<img src="bookdown_files/figure-html/plotBFdistrn-1.svg" alt="Left panel: A histogram of Bayes factors (BF10) of the alternative model over the null model in 50 simulated data sets. The vertical solid black line shows equal evidence for both hypotheses; the dashed line shows the Bayes factor computed from the empirical data; the horizontal error bar shows 95 percent of all Bayes factors. Right panel: Estimates of the facilitatory effect of retrieval interference and 95 percent credible intervals across all simulations (solid lines) and the empirically observed data (dashed line)." width="960" />
<p class="caption">
FIGURE 15.6: Left panel: A histogram of Bayes factors (BF10) of the alternative model over the null model in 50 simulated data sets. The vertical solid black line shows equal evidence for both hypotheses; the dashed line shows the Bayes factor computed from the empirical data; the horizontal error bar shows 95 percent of all Bayes factors. Right panel: Estimates of the facilitatory effect of retrieval interference and 95 percent credible intervals across all simulations (solid lines) and the empirically observed data (dashed line).
</p>
</div>
<p>The results show that the Bayes factors are quite variable. Although all data sets are simulated from the same posterior predictive distribution, the Bayes factor results are as different as providing moderate evidence for the null model (<span class="math inline">\(BF_{10} &lt; 1/3\)</span>) or providing strong evidence for the alternative model (<span class="math inline">\(BF_{10} &gt; 10\)</span>). The bulk of the simulated data sets provide moderate or anecdotal evidence for the alternative model. That is, much like the “dance of p-values” <span class="citation">(Cumming <a href="#ref-cumming2014new">2014</a>)</span>, this analysis reveals a “dance of the Bayes factors” with simulated repetitions of the same study. The variability in these results shows that a typical cognitive or psycholinguistic data set is not necessarily highly informative for drawing firm conclusions about the hypotheses in question.</p>
<p>What is driving these differences in the Bayes factors between simulated data sets? One obvious reason why the outcomes may be so different is that the difference in reading times between the two sentence types, that is, the experimental effect that we wish to make inferences about, may vary based on the noise and uncertainty in the posterior predictive simulations. It is therefore interesting to plot the Bayes factors from this simulated data set as a function of the difference in simulated reading times between the two sentence types as estimated in the Bayesian model. That is, we extract the estimated mean difference in reading times at the verb between plural and singular attractor conditions from the population-level effects of the Bayesian model, and plot the Bayes factor as a function of this difference (together with 95% credible intervals).</p>
<div class="figure"><span style="display:block;" id="fig:BFregression"></span>
<img src="bookdown_files/figure-html/BFregression-1.svg" alt="The Bayes factor (BF10) as a function of the estimate (with 95 percent credible intervals) of the facilitatory effect of retrieval interference across 50 simulated data sets. The prior is from a meta-analysis." width="576" />
<p class="caption">
FIGURE 15.7: The Bayes factor (BF10) as a function of the estimate (with 95 percent credible intervals) of the facilitatory effect of retrieval interference across 50 simulated data sets. The prior is from a meta-analysis.
</p>
</div>
<p>The results (displayed in Figure <a href="ch-bf.html#fig:BFregression">15.7</a>) show that the mean difference in reading times between experimental conditions varies dramatically across posterior predictive simulations. This indicates that the experimental data and design contain a limited amount of information about the effect of interest. Of course, if the data is noisy, Bayes factor analyses based on this simulated data cannot be stable across simulations either. Accordingly, as is clear from Figure <a href="ch-bf.html#fig:BFregression">15.7</a>, the difference in mean reading times between experimental conditions is indeed a major driving force for the Bayes factor calculations <span class="citation">(other model parameters don’t show a close association; Schad et al. <a href="#ref-schad2022workflow">2022</a>)</span>.</p>
<p>In Figure <a href="ch-bf.html#fig:BFregression">15.7</a>, as the difference between reading times becomes more negative,
that is, the faster the plural noun condition (i.e., “cabinets” in the example; sentence 2) is read compared to the singular noun condition (i.e., “cabinet”; example sentence 1), the larger the Bayes factor BF10 becomes, indicating that the evidence in favor of the alternative model increases. By contrast, when the difference between reading times becomes less negative, i.e., the plural condition (sentence 2) is not read much faster than the singular condition (sentence 1), then the Bayes factor BF10 decreases to values smaller than 1. Importantly, this behavior occurs because we are using an informative priors from the meta-analysis, where the prior mean for the agreement attraction effect is not centered at a mean of zero, but has a negative value (i.e., a prior mean of <span class="math inline">\(-0.03\)</span>). Therefore, differences in reading times that are less negative / more positive than this prior mean are more in line with a null model of no effect. This also leads to the striking observation that the 95% credible intervals are quite consistent and all do not overlap with zero, whereas the Bayes factor results are far more variable. This should alarm researchers who use the 95% credible interval to decide whether an effect is present or not, i.e., to make a discovery claim.</p>
<p>Computing Bayes factors for such a prior with a non-zero mean asks the very specific question of whether the data provide more evidence for the effect size obtained from the meta-analysis compared to the absence of any effect.</p>
<p>The important lesson to learn from this analysis is that Bayes factors can be quite variable for different data sets assessing the same phenomenon. Individual data sets in the cognitive sciences often do not contain a lot of information about the phenomenon of interest, even when–as is the case here with agreement attraction–the phenomenon is thought to be a relatively robust phenomenon. For a more detailed investigation of how Bayes factors can vary with data, in both simulated and real replication studies, we refer the reader to <span class="citation">Schad et al. (<a href="#ref-schad2022workflow">2022</a>)</span> and <span class="citation">Vasishth, Yadav, et al. (<a href="#ref-SampleSizeCBB2021">2022</a>)</span>.</p>
</div>
</div>
<div id="sec-caution" class="section level3 hasAnchor">
<h3><span class="header-section-number">15.5.3</span> A cautionary note about Bayes factors<a href="ch-bf.html#sec-caution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Just like frequentist p-values <span class="citation">(Wasserstein and Lazar <a href="#ref-pvals">2016</a>)</span>, Bayes factors are easy to misuse and misinterpret, and have the potential to mislead the scientist if used in an automated manner. A recent article <span class="citation">(Tendeiro <a href="#ref-tendiero">2022</a>)</span> reviews many of the misuses of Bayes factors analyses in psychology and related areas. As discussed in this chapter, Bayes factors (and Bayesian analysis in general) require a great deal of thought; there is no substitute for sensitivity analyses, and the development of sensible priors. Using default priors and deriving black and white conclusions from Bayes factors analyses is never a good idea.</p>
</div>
</div>
<div id="summary-13" class="section level2 hasAnchor">
<h2><span class="header-section-number">15.6</span> Summary<a href="ch-bf.html#summary-13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayes factors are a very important tool in Bayesian data analysis. They allow the researcher to quantify the evidence in favor of certain effects in the data by comparing a full model, which contains a parameter corresponding to the effect of interest, with a null model, that does not contain that parameter. We saw that Bayes factor analyses are highly sensitive to priors specified for the parameters; this is true both for the parameter corresponding to the effect of interest, but also sometimes for priors relating to other parameters in the model, such as the intercept. It is therefore very important to perform prior predictive checks to select good and plausible priors. Moreover, sensitivity analyses, where Bayes factors are investigated for differing prior assumptions, should be standardly reported in any analysis involving Bayes factors. We studied theoretical aspects of Bayes factors and saw that bridge sampling requires a very large effective sample size in order to obtain stable results for approximate Bayes factors. Therefore, one should always perform a Bayes factor analysis at least twice to ensure that the results are stable. Bridge sampling comes with no strong guarantees concerning its accuracy, and we saw that simulation-based calibration can be used to evaluate the accuracy of Bayes factor estimates. Last, we learned that Bayes factors can strongly vary with the data. In the cognitive sciences, the data are–even for relatively robust effects–often not stable due to small effect sizes and limited sample size. Therefore, also the resulting Bayes factors can strongly vary with the data. As a consequence, only large effect sizes, large sample studies, and/or replication studies can lead to reliable inferences from empirical data in the cognitive sciences.</p>
<p>One topic that was not discussed in detail in this chapter is data aggregation. In repeated measures data, null hypothesis Bayes factor analyses can be performed on the raw data, i.e., without aggregation, by using Bayesian hierarchical models. In an alternative approach, the data are first aggregated by taking the mean per subject and condition, before running null hypothesis Bayes factor analyses on the aggregated data. Importantly, inferences / Bayes factors based on aggregated data can be biased, when either (i) item variability is present in addition to subject variability, or (ii) when the sphericity assumption (inherent in repeated measures ANOVA) is violated <span class="citation">(Schad, Nicenboim, and Vasishth <a href="#ref-schad2022data">2022</a>)</span>. In these cases, aggregated analyses provide biased results and should not be used. By contrast, non-aggregated analyses are robust also in these cases and yield accurate Bayes factor estimates.</p>
<p>Another issue not discussed here is sample size determination using Bayes factors when planning a study. <span class="citation">Wang and Gelfand (<a href="#ref-wang2002simulation">2002</a>)</span> is an important paper in this connection; also see <span class="citation">Vasishth, Yadav, et al. (<a href="#ref-SampleSizeCBB2021">2022</a>)</span> for an example involving a psycholinguistic experiment design.</p>
</div>
<div id="further-reading-12" class="section level2 hasAnchor">
<h2><span class="header-section-number">15.7</span> Further reading<a href="ch-bf.html#further-reading-12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A detailed explanation on how bridge sampling works can be found in <span class="citation">Gronau et al. (<a href="#ref-gronauTutorialBridgeSampling2017">2017</a><a href="#ref-gronauTutorialBridgeSampling2017">b</a>)</span>, and more details about the bridgesampling package can be found in <span class="citation">Gronau, Singmann, and Wagenmakers (<a href="#ref-gronauBridgesamplingPackageEstimating2017">2017</a>)</span>. <span class="citation">Wagenmakers et al. (<a href="#ref-wagenmakers2010BayesianHypothesisTesting">2010</a>)</span> provides a complete tutorial and the mathematical proof of the Savage-Dickey method; also see <span class="citation">O’Hagan and Forster (<a href="#ref-kendall2004">2004</a>)</span>.
For a Bayes Factor Test calibrated to investigate replication success, see <span class="citation">Verhagen and Wagenmakers (<a href="#ref-verhagenBayesianTestsQuantify2014">2014</a>)</span>. A special issue on hierarchical modeling and Bayes factors appears in the journal Computational Brain and Behavior in response to an article by <span class="citation">van Doorn et al. (<a href="#ref-van2021bayes">2021</a>)</span>. <span class="citation">Kruschke and Liddell (<a href="#ref-kruschke2018bayesian">2018</a>)</span> discuss alternatives to Bayes factors for hypothesis testing. An argument against null hypothesis testing with Bayes Factors appears in this blog post by Andrew Gelman: <a href="https://statmodeling.stat.columbia.edu/2019/09/10/i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing/" class="uri">https://statmodeling.stat.columbia.edu/2019/09/10/i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing/</a>,
An argument in favor of null hypothesis testing with Bayes Factor as an approximation (but assuming realistic effects) appears in: <a href="https://statmodeling.stat.columbia.edu/2018/03/10/incorporating-bayes-factor-understanding-scientific-information-replication-crisis/" class="uri">https://statmodeling.stat.columbia.edu/2018/03/10/incorporating-bayes-factor-understanding-scientific-information-replication-crisis/</a>.
A visualization of the distinction between Bayes factor and k-fold cross-validation is in a blog post by Fabian Dablander, <a href="https://tinyurl.com/47n5cte4" class="uri">https://tinyurl.com/47n5cte4</a>. Decision theory, which was only mentioned in passing in this chapter, is discussed in <span class="citation">Parmigiani and Inoue (<a href="#ref-parmigiani2009decision">2009</a>)</span>.
Hypothesis testing in its different flavors is discussed in <span class="citation">Robert (<a href="#ref-robert202250">2022</a>)</span>.</p>
</div>
<div id="exercises-2" class="section level2 hasAnchor">
<h2><span class="header-section-number">15.8</span> Exercises<a href="ch-bf.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:bysubjects" class="exercise"><strong>Exercise 15.1  </strong></span>Is there evidence for differences in the effect of cloze probability among the subjects?</p>
</div>

<p>Use Bayes factor to compare the log cloze probability model that we examined in section <a href="ch-bf.html#sec-BFnonnested">15.2.2</a> with a similar model but that incorporates the strong assumption of no difference between subjects for the effect of cloze (<span class="math inline">\(\tau_{u_2}=0\)</span>).</p>
<div class="exercise">
<p><span id="exr:bf-logn" class="exercise"><strong>Exercise 15.2  </strong></span>Is there evidence for the claim that English subject relative clause are easier to process than object relative clauses?</p>
</div>
<p>Consider again the reading time data coming from Experiment 1 of <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span> presented in exercise <a href="ch-hierarchical.html#exr:hierarchical-logn">5.2</a>. Try to quantify the evidence against the null model (no population-level reading times difference between SRC and ORC) relative to the following alternative models:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, 1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, .1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, .01)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}_+(0, 1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}_+(0, .1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}_+(0, .01)\)</span></li>
</ol>
<p>(A <span class="math inline">\(\mathit{Normal}_+(.)\)</span> prior can be set in <code>brms</code> by defining a lower boundary as <span class="math inline">\(0\)</span>, with the argument <code>lb = 0</code>.)</p>
<p>What are the Bayes factor in favor of the alternative models a-f, compared to the null model?</p>
<div class="exercise">
<p><span id="exr:bf-logistic" class="exercise"><strong>Exercise 15.3  </strong></span>Is there evidence for the claim that sentences with subject relative clauses are easier to comprehend?</p>
</div>

<p>Consider now the question response accuracy of the data of Experiment 1 of <span class="citation">Grodner and Gibson (<a href="#ref-grodner">2005</a>)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Compare a model that assumes that RC type affects question accuracy on the population and by-subjects and by-items with <em>a null model</em> that assumes that there is no population-level present.</li>
<li>Compare a model that assumes that RC type affects question accuracy on the population and by-subjects and by-items with <em>another null model</em> that assumes that there is no population-level or group-level present, that is no by-subject or by-item effect. What’s the meaning of the results of the Bayes factor analysis.</li>
</ol>
<p>Assume that for the effect of RC on question accuracy, <span class="math inline">\(\beta \sim \mathit{Normal}(0, .1)\)</span> is a reasonable prior, and that for all the variance components, the same prior, <span class="math inline">\(\tau \sim \mathit{Normal}_{+}(0, 1)\)</span>, is a reasonable prior.</p>
<div class="exercise">
<p><span id="exr:lognstan" class="exercise"><strong>Exercise 15.4  </strong></span>Bayes factor and bounded parameters using Stan.</p>
</div>
<p>Re-fit the data of a single subject pressing a button repeatedly from <a href="ch-reg.html#sec-trial">4.2</a> from <code>data(&quot;df_spacebar&quot;)</code>, coding the model in Stan.</p>
<p>Start by assuming the following likelihood and priors:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim \mathit{LogNormal}(\alpha + c\_trial_n \cdot \beta,\sigma)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(6, 1.5) \\
\beta &amp;\sim \mathit{Normal}_+(0, .1)\\
\sigma &amp;\sim \mathit{Normal}_+(0, 1)
\end{aligned}
\end{equation}\]</span></p>
<p>Use the Bayes factor to answer the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li>Is there evidence for any effect of trial number in comparison with no effect?</li>
<li>Is there evidence for a positive effect of trial number (as the subject reads further, they slowdown) in comparison with no effect?</li>
<li>Is there evidence for a negative effect of trial number (as the subject reads further, they speedup) in comparison with no effect?</li>
<li>Is there evidence for a positive effect of trial number in comparison with a negative effect?</li>
</ol>
<p>(Expect very large Bayes factors in this exercise.)</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-abbl02">
<p>Anderson, John R., Dan Bothell, Michael D. Byrne, Scott Douglass, Christian Lebiere, and Yulin Qin. 2004. “An Integrated Theory of the Mind.” <em>Psychological Review</em> 111 (4): 1036–60.</p>
</div>
<div id="ref-bennettEfficientEstimationFree1976">
<p>Bennett, Charles H. 1976. “Efficient Estimation of Free Energy Differences from Monte Carlo Data.” <em>Journal of Computational Physics</em> 22 (2): 245–68. <a href="https://doi.org/10.1016/0021-9991(76)90078-4" class="uri">https://doi.org/10.1016/0021-9991(76)90078-4</a>.</p>
</div>
<div id="ref-bishop2006pattern">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.</p>
</div>
<div id="ref-ChenGoodman1999">
<p>Chen, Stanley F, and Joshua Goodman. 1999. “An Empirical Study of Smoothing Techniques for Language Modeling.” <em>Computer Speech &amp; Language</em> 13 (4): 359–94. <a href="https://doi.org/https://doi.org/10.1006/csla.1999.0128" class="uri">https://doi.org/https://doi.org/10.1006/csla.1999.0128</a>.</p>
</div>
<div id="ref-cumming2014new">
<p>Cumming, Geoff. 2014. “The New Statistics: Why and How.” <em>Psychological Science</em> 25 (1): 7–29.</p>
</div>
<div id="ref-DickeyLientz1970">
<p>Dickey, James M, BP Lientz, and others. 1970. “The Weighted Likelihood Ratio, Sharp Hypotheses About Chances, the Order of a Markov Chain.” <em>The Annals of Mathematical Statistics</em> 41 (1). Institute of Mathematical Statistics: 214–26.</p>
</div>
<div id="ref-dillon2011structured">
<p>Dillon, Brian William. 2011. “Structured Access in Sentence Comprehension.” PhD thesis.</p>
</div>
<div id="ref-EngelmannJaegerVasishth2019">
<p>Engelmann, Felix, Lena A. Jäger, and Shravan Vasishth. 2020. “The Effect of Prominence and Cue Association in Retrieval Processes: A Computational Account.” <em>Cognitive Science</em> 43 (12): e12800. <a href="https://doi.org/10.1111/cogs.12800" class="uri">https://doi.org/10.1111/cogs.12800</a>.</p>
</div>
<div id="ref-gelmancarlin">
<p>Gelman, Andrew, and John B. Carlin. 2014. “Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors.” <em>Perspectives on Psychological Science</em> 9 (6). SAGE Publications: 641–51.</p>
</div>
<div id="ref-grodner">
<p>Grodner, Daniel, and Edward Gibson. 2005. “Consequences of the Serial Nature of Linguistic Input.” <em>Cognitive Science</em> 29: 261–90.</p>
</div>
<div id="ref-gronau2017tutorial">
<p>Gronau, Quentin F., Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Marsman, David S Leslie, Jonathan J Forster, Eric-Jan Wagenmakers, and Helen Steingroever. 2017a. “A Tutorial on Bridge Sampling.” <em>Journal of Mathematical Psychology</em> 81. Elsevier: 80–97.</p>
</div>
<div id="ref-gronauTutorialBridgeSampling2017">
<p>Gronau, Quentin F., Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Marsman, David S Leslie, Jonathan J Forster, Eric-Jan Wagenmakers, and Helen Steingroever. 2017a. “A Tutorial on Bridge Sampling.” <em>Journal of Mathematical Psychology</em> 81. Elsevier: 80–97.</p> 2017b. “A Tutorial on Bridge Sampling.” <em>Journal of Mathematical Psychology</em> 81: 80–97. <a href="https://doi.org/10.1016/j.jmp.2017.09.005" class="uri">https://doi.org/10.1016/j.jmp.2017.09.005</a>.</p>
</div>
<div id="ref-gronauBridgesamplingPackageEstimating2017">
<p>Gronau, Quentin F., Henrik Singmann, and Eric-Jan Wagenmakers. 2017. “Bridgesampling: An R Package for Estimating Normalizing Constants.” <em>Arxiv</em>. <a href="http://arxiv.org/abs/1710.08162" class="uri">http://arxiv.org/abs/1710.08162</a>.</p>
</div>
<div id="ref-hammerly2019grammaticality">
<p>Hammerly, Christopher, Adrian Staub, and Brian Dillon. 2019. “The Grammaticality Asymmetry in Agreement Attraction Reflects Response Bias: Experimental and Modeling Evidence.” <em>Cognitive Psychology</em> 110: 70–104.</p>
</div>
<div id="ref-JaegerEngelmannVasishth2017">
<p>Jäger, Lena A., Felix Engelmann, and Shravan Vasishth. 2017. “Similarity-Based Interference in Sentence Comprehension: Literature review and Bayesian meta-analysis.” <em>Journal of Memory and Language</em> 94: 316–39. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004" class="uri">https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004</a>.</p>
</div>
<div id="ref-jeffreys1939theory">
<p>Jeffreys, Harold. 1939. <em>Theory of Probability</em>. Oxford: Clarendon Press.</p>
</div>
<div id="ref-kass1995bayes">
<p>Kass, Robert E, and Adrian E Raftery. 1995. “Bayes Factors.” <em>Journal of the American Statistical Association</em> 90 (430). Taylor &amp; Francis: 773–95.</p>
</div>
<div id="ref-kruschke2018bayesian">
<p>Kruschke, John, and Torrin M Liddell. 2018. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” <em>Psychonomic Bulletin &amp; Review</em> 25 (1). Springer: 178–206.</p>
</div>
<div id="ref-lago2015agreement">
<p>Lago, Sol, Diego Shalom, Mariano Sigman, Ellen F Lau, and Colin Phillips. 2015. “Agreement Processes in Spanish Comprehension.” <em>Journal of Memory and Language</em> 82: 133–49.</p>
</div>
<div id="ref-lewisvasishth:cogsci05">
<p>Lewis, Richard L., and Shravan Vasishth. 2005. “An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval.” <em>Cognitive Science</em> 29: 1–45.</p>
</div>
<div id="ref-Lidstone1920">
<p>Lidstone, George James. 1920. “Note on the General Case of the Bayes-Laplace Formula for Inductive or a Posteriori Probabilities.” <em>Transactions of the Faculty of Actuaries</em> 8 (182-192): 13.</p>
</div>
<div id="ref-mackay">
<p>MacKay, David JC. 2003. <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge, UK: Cambridge University Press.</p>
</div>
<div id="ref-mengSimulatingRatiosNormalizing1996">
<p>Meng, Xiao-li, and Wing Hung Wong. 1996. “Simulating Ratios of Normalizing Constants via a Simple Identity: A Theoretical Exploration.” <em>Statistica Sinica</em>, 831–60.</p>
</div>
<div id="ref-navarro2015learning">
<p>Navarro, Daniel. 2015. <em>Learning Statistics with R</em>. https://learningstatisticswithr.com.</p>
</div>
<div id="ref-NicenboimVasishth2016">
<p>Nicenboim, Bruno, and Shravan Vasishth. 2016. “Statistical methods for linguistic research: Foundational Ideas - Part II.” <em>Language and Linguistics Compass</em> 10 (11): 591–613. <a href="https://doi.org/10.1111/lnc3.12207" class="uri">https://doi.org/10.1111/lnc3.12207</a>.</p>
</div>
<div id="ref-nicenboim2020words">
<p>Nicenboim, Bruno, Shravan Vasishth, and Frank Rösler. 2020c. “Are Words Pre-Activated Probabilistically During Sentence Comprehension? Evidence from New Data and a Bayesian Random-Effects Meta-Analysis Using Publicly Available Data.” <em>Neuropsychologia</em>, 107427.</p>
</div>
<div id="ref-nieuwlandLargescaleReplicationStudy2018">
<p>Nieuwland, Mante S, Stephen Politzer-Ahles, Evelien Heyselaar, Katrien Segaert, Emily Darley, Nina Kazanina, Sarah Von Grebmer Zu Wolfsthurn, et al. 2018. “Large-Scale Replication Study Reveals a Limit on Probabilistic Prediction in Language Comprehension.” <em>eLife</em> 7. <a href="https://doi.org/10.7554/eLife.33468" class="uri">https://doi.org/10.7554/eLife.33468</a>.</p>
</div>
<div id="ref-ohagan2006uncertain">
<p>O’Hagan, Anthony, Caitlin E Buck, Alireza Daneshkhah, J Richard Eiser, Paul H Garthwaite, David J Jenkinson, Jeremy E Oakley, and Tim Rakow. 2006. <em>Uncertain Judgements: Eliciting Experts’ Probabilities</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-kendall2004">
<p>O’Hagan, Antony, and Jonathan Forster. 2004. “Kendall’s Advanced Theory of Statistics, Vol. 2B: Bayesian Inference.” Wiley.</p>
</div>
<div id="ref-parmigiani2009decision">
<p>Parmigiani, Giovanni, and Lurdes Inoue. 2009. <em>Decision Theory: Principles and Approaches</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-robert202250">
<p>Robert, Christian P. 2022. “50 Shades of Bayesian Testing of Hypotheses.” <em>arXiv Preprint arXiv:2206.06659</em>.</p>
</div>
<div id="ref-rouder2018bayesian">
<p>Rouder, Jeffrey N., Julia M Haaf, and Joachim Vandekerckhove. 2018. “Bayesian Inference for Psychology, Part IV: Parameter Estimation and Bayes Factors.” <em>Psychonomic Bulletin &amp; Review</em> 25 (1): 102–13.</p>
</div>
<div id="ref-rouder2009bayesian">
<p>Rouder, Jeffrey N., Paul L Speckman, Dongchu Sun, Richard D Morey, and Geoffrey Iverson. 2009. “Bayesian T Tests for Accepting and Rejecting the Null Hypothesis.” <em>Psychonomic Bulletin &amp; Review</em> 16 (2): 225–37.</p>
</div>
<div id="ref-Royall">
<p>Royall, Richard. 1997. <em>Statistical Evidence: A Likelihood Paradigm</em>. New York: Chapman; Hall, CRC Press.</p>
</div>
<div id="ref-schad2020toward">
<p>Schad, Daniel J., Michael J. Betancourt, and Shravan Vasishth. 2020. “Toward a Principled Bayesian Workflow in Cognitive Science.” <em>Psychological Methods</em> 26 (1). American Psychological Association: 103–26.</p>
</div>
<div id="ref-SchadEtAlBF">
<p>Schad, Daniel J., Bruno Nicenboim, Paul-Christian Bürkner, Michael J. Betancourt, and Shravan Vasishth. 2021. “Workflow Techniques for the Robust Use of Bayes Factors.”</p>
</div>
<div id="ref-schad2022workflow">
<p>Schad, Daniel J, Bruno Nicenboim, Paul-Christian Bürkner, Michael Betancourt, and Shravan Vasishth. 2022. “Workflow Techniques for the Robust Use of Bayes Factors.” <em>Psychological Methods</em>. American Psychological Association.</p>
</div>
<div id="ref-schad2022data">
<p>Schad, Daniel J, Bruno Nicenboim, and Shravan Vasishth. 2022. “Data Aggregation Can Lead to Biased Inferences in Bayesian Linear Mixed Models.” <em>arXiv Preprint arXiv:2203.02361</em>.</p>
</div>
<div id="ref-schonbrodt2018bayes">
<p>Schönbrodt, Felix D, and Eric-Jan Wagenmakers. 2018. “Bayes Factor Design Analysis: Planning for Compelling Evidence.” <em>Psychonomic Bulletin &amp; Review</em> 25 (1): 128–42.</p>
</div>
<div id="ref-talts2018validating">
<p>Talts, Sean, Michael J. Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2018. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” <em>arXiv Preprint arXiv:1804.06788</em>.</p>
</div>
<div id="ref-tendiero">
<p>Tendeiro, Kiers, J. 2022. “Diagnosing the Use of the Bayes Factor in Applied Research.”</p>
</div>
<div id="ref-van2021bayes">
<p>van Doorn, Johnny, Frederik Aust, Julia M Haaf, Angelika Stefan, and Eric-Jan Wagenmakers. 2021. “Bayes Factors for Mixed Models.” <em>Computational Brain and Behavior</em>. <a href="https://doi.org/https://doi.org/10.1007/s42113-021-00113-2" class="uri">https://doi.org/https://doi.org/10.1007/s42113-021-00113-2</a>.</p>
</div>
<div id="ref-van2011cue">
<p>Van Dyke, Julie A, and Brian McElree. 2011. “Cue-Dependent Interference in Comprehension.” <em>Journal of Memory and Language</em> 65 (3). Elsevier: 247–63.</p>
</div>
<div id="ref-VasishthMertzenJaegerGelman2018">
<p>Vasishth, Shravan, Daniela Mertzen, Lena A. Jäger, and Andrew Gelman. 2018a. “The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability.” <em>Journal of Memory and Language</em> 103: 151–75. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2018.07.004" class="uri">https://doi.org/https://doi.org/10.1016/j.jml.2018.07.004</a>.</p>
</div>
<div id="ref-SampleSizeCBB2021">
<p>Vasishth, Shravan, Himanshu Yadav, Daniel J. Schad, and Bruno Nicenboim. 2022. “Sample Size Determination for Bayesian Hierarchical Models Commonly Used in Psycholinguistics.” <em>Computational Brain and Behavior</em>.</p>
</div>
<div id="ref-verhagenBayesianTestsQuantify2014">
<p>Verhagen, Josine, and Eric-Jan Wagenmakers. 2014. “Bayesian Tests to Quantify the Result of a Replication Attempt.” <em>Journal of Experimental Psychology: General</em> 143 (4): 1457–75. <a href="https://doi.org/10.1037/a0036731" class="uri">https://doi.org/10.1037/a0036731</a>.</p>
</div>
<div id="ref-wagenmakersPrinciplePredictiveIrrelevance2020">
<p>Wagenmakers, Eric-Jan, Michael D. Lee, Jeffrey N. Rouder, and Richard D. Morey. 2020. “The Principle of Predictive Irrelevance or Why Intervals Should Not Be Used for Model Comparison Featuring a Point Null Hypothesis.” In <em>The Theory of Statistics in Psychology: Applications, Use, and Misunderstandings</em>, edited by Craig W. Gruber, 111–29. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-48043-1_8" class="uri">https://doi.org/10.1007/978-3-030-48043-1_8</a>.</p>
</div>
<div id="ref-wagenmakers2010BayesianHypothesisTesting">
<p>Wagenmakers, Eric-Jan, Tom Lodewyckx, Himanshu Kuriyal, and Raoul Grasman. 2010. “Bayesian Hypothesis Testing for Psychologists: A Tutorial on the Savage–Dickey Method.” <em>Cognitive Psychology</em> 60 (3). Elsevier: 158–89.</p>
</div>
<div id="ref-wang2002simulation">
<p>Wang, Fei, and Alan E Gelfand. 2002. “A Simulation-Based Approach to Bayesian Sample Size Determination for Performance Under a Given Model and for Separating Models.” <em>Statistical Science</em>. JSTOR, 193–208.</p>
</div>
<div id="ref-pvals">
<p>Wasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” <em>The American Statistician</em> 70 (2). Taylor &amp; Francis: 129–33.</p>
</div>
<div id="ref-YadavetalJML2022">
<p>Yadav, Himanshu, Garrett Smith, Sebastian Reich, and Shravan Vasishth. 2022. “Number Feature Distortion Modulates Cue-Based Retrieval in Reading.” <em>Journal of Memory and Language</em>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="42">
<li id="fn42"><p>Given that the posterior is analytically available for beta-distributed priors for the binomial distribution, we could alternatively compute the posterior first, and then integrate out the probability <span class="math inline">\(\theta\)</span>.<a href="ch-bf.html#fnref42" class="footnote-back">↩</a></p></li>
<li id="fn43"><p>This meta-analysis already includes the data that we want to make inference about; thus, this meta-analysis estimate is not really the right estimate to use, since it involves using the data twice. We ignore this detail here because our goal is simply to illustrate the approach.<a href="ch-bf.html#fnref43" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-comparison.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-cv.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
