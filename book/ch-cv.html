<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 16 Cross-validation | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 16 Cross-validation | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Cross-validation | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2024-08-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-bf.html"/>
<link rel="next" href="ch-cogmod.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b>  Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The  law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b>  Discrete random variables: An example using the  binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b>  Continuous random variables: An example using the  normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1:  Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2:  Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate  simulated bivariate  (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The  marginal likelihood  (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b>  Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the  posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a  likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a  prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The  posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the  posterior through  sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b>  Bayesian Regression Models using Stan:  brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b>  Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors:  sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b>  Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b>  Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b>  Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b>  Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b>  Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The  log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
<li class="chapter" data-level="3.11" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first  linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b>  Likelihood and  priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b>  Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b>  Logistic regression: Does  set size affect  free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b>  Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b>  Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b>  No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b>  Varying intercepts and  varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A  hierarchical log-normal model: The  Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes  log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of  Prior Elicitation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English  relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the  variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b>  Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from  meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’  posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
<li class="chapter" data-level="6.7" data-path="ch-priors.html"><a href="ch-priors.html#sec-priorsexercises"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#building-a-model"><i class="fa fa-check"></i><b>7.1</b>  Building a model</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-to-ask-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions to ask on a model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#checking-whether-assumptions-are-consistent-with-domain-expertise-prior-predictive-checks"><i class="fa fa-check"></i><b>7.2.1</b>  Checking whether assumptions are consistent with  domain expertise: Prior predictive checks</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#testing-for-correct-posterior-approximations-checks-of-computational-faithfulness"><i class="fa fa-check"></i><b>7.2.2</b>  Testing for correct posterior approximations: Checks of computational faithfulness</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#sensitivity-of-the-model"><i class="fa fa-check"></i><b>7.2.3</b>  Sensitivity of the model</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#does-the-model-adequately-capture-the-dataposterior-predictive-checks"><i class="fa fa-check"></i><b>7.2.4</b>  Does the model adequately capture the data?–Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b>  Contrast coding</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding:  Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b>  Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b>  Cell means parameterization and  posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b>  Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The  hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The  <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor of four levels</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b>  Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b>  Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or  model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b>  Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts:  Monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b>  Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b>  Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the  intercept in  non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-6"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial  <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b>  Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b>  Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one  covariate</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a  group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-7"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan:  Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example:  Cloze probability with Stan with the  binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b>  Regression models in Stan</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first  linear regression in Stan: Does attentional load affect  pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b>  Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-8"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Hierarchical models and reparameterization </a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated  varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b>  Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-9"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b>  Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The  simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> An example where simulation-based calibration reveals a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues with and limitations of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-the-exponential-distribution-implemented-manually"><i class="fa fa-check"></i><b>12.3</b> Another  custom distribution: The exponential distribution  implemented manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-10"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b>  Meta-analysis and  measurement error models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b>  Measurement-error models</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-11"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#prior-predictive-vs.-posterior-predictive-model-comparison"><i class="fa fa-check"></i><b>14.1</b> Prior predictive vs. posterior predictive model comparison</a></li>
<li class="chapter" data-level="14.2" data-path="ch-comparison.html"><a href="ch-comparison.html#some-important-points-to-consider-when-comparing-models"><i class="fa fa-check"></i><b>14.2</b> Some important points to consider when comparing models</a></li>
<li class="chapter" data-level="14.3" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b>  Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b>  Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and  accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="15.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>15.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#sample-size-determination-using-bayes-factors"><i class="fa fa-check"></i><b>15.6</b> Sample size determination using Bayes factors</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#summary-12"><i class="fa fa-check"></i><b>15.7</b> Summary</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.8</b> Further reading</a></li>
<li class="chapter" data-level="15.9" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b>  Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#sec-issuesCV"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b>  PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-13"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to cognitive modeling</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#what-characterizes-a-computational-cognitive-model"><i class="fa fa-check"></i><b>17.1</b> What characterizes a computational cognitive model?</a></li>
<li class="chapter" data-level="17.2" data-path="ch-cogmod.html"><a href="ch-cogmod.html#some-advantages-of-taking-the-latent-variable-modeling-approach"><i class="fa fa-check"></i><b>17.2</b> Some advantages of taking the latent-variable modeling approach</a></li>
<li class="chapter" data-level="17.3" data-path="ch-cogmod.html"><a href="ch-cogmod.html#types-of-computational-cognitive-model"><i class="fa fa-check"></i><b>17.3</b> Types of computational cognitive model</a></li>
<li class="chapter" data-level="17.4" data-path="ch-cogmod.html"><a href="ch-cogmod.html#summary-14"><i class="fa fa-check"></i><b>17.4</b> Summary</a></li>
<li class="chapter" data-level="17.5" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling  multiple categorical responses</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A  hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-15"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
<li class="chapter" data-level="18.5" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A  multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A  hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-16"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with  contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-17"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="ch-closing.html"><a href="ch-closing.html"><i class="fa fa-check"></i><b>21</b> In closing</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-cv" class="section level1 hasAnchor" number="16">
<h1><span class="header-section-number">Chapter 16</span> Cross-validation<a href="ch-cv.html#ch-cv" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>A popular way to evaluate and compare models is to investigate their ability to make predictions for  “out-of-sample data”, that is, to use what we learned from the observed data to predict future or  unseen observations.  Cross-validation is used to test which of the models under consideration is/are able to learn the most from our data in order to make better predictions. In cognitive science, the goal is only rarely to predict future observations, but rather to compare how well different models fare in accounting for the observed data. Nevertheless, there are situations in cognitive science where evaluating the predictive performance of models becomes important.</p>
<p>The objective of cross-validation is to avoid  over-optimistic predictions; such over-optimistic predictions would arise if we were to use the data to estimate the parameters of our model, and then use these estimates to predict the same data. That amounts to using the data twice. The basic idea behind cross-validation is that the models are fit with a large subset of the data, the  <em>training set</em>, and then the fitted models are used to predict a smaller, unseen part of the data, the  <em>held-out set</em>.
In order to treat the entire data set as a held-out set and evaluate the predictive accuracy  using every observation, one successively changes what constitutes the training set and the held-out set by refitting the model and evaluating it in different folds. This ensures that the predictions of the model are tested over the entire data set.</p>
<div id="the-expected-log-predictive-density-of-a-model" class="section level2 hasAnchor" number="16.1">
<h2><span class="header-section-number">16.1</span> The expected log predictive density of a model<a href="ch-cv.html#the-expected-log-predictive-density-of-a-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In order to compare the quality of the posterior predictions of two models, a  <em>utility</em> function or a  <em>scoring rule</em> is used <span class="citation">(see Gneiting and Raftery <a href="#ref-GneitingRaftery2007" role="doc-biblioref">2007</a> for a review on scoring rules)</span>. The  logarithmic score rule <span class="citation">(Good <a href="#ref-Good1952" role="doc-biblioref">1952</a>)</span>, shown in Equation <a href="ch-cv.html#eq:lscore">(16.1)</a>, has been proposed as a reasonable way to assess the  posterior predictive distribution of a candidate model <span class="math inline">\(\mathcal{M}_1\)</span> given the data <span class="math inline">\(y\)</span>. This approach is reasonable because it takes into account the uncertainty of the predictions (unlike, for example, using the mean square error). If new observations are well-accounted by the posterior predictive distribution, then the density of the posterior predictive distribution is high and so is its logarithm.</p>
<p><span class="math display" id="eq:lscore">\[\begin{equation}
u( \mathcal{M}_1, y_{pred}) = \log p(y_{pred}| y, \mathcal{M}_1)
\tag{16.1}
\end{equation}\]</span></p>
<p>Unlike the Bayes factor, the prior is absent from Equation <a href="ch-cv.html#eq:lscore">(16.1)</a>. However, the prior does have a role here: The posterior predictive distribution is based on the posterior distribution <span class="math inline">\(p(\Theta\mid y)\)</span> (where <span class="math inline">\(\Theta\)</span> is a vector of all the parameters of the model), which, according to Bayes’ rule, depends on both priors and likelihood together. Recall Equation <a href="ch-compbda.html#eq:postpp">(3.8)</a> in section <a href="ch-compbda.html#sec-ppd">3.6</a>, repeated here for convenience:</p>
<p><span class="math display" id="eq:postpp3">\[\begin{equation}
p(y_{pred}\mid y )=\int_\Theta p(y_{pred}\mid \Theta) p(\Theta\mid y)\, d\Theta
\tag{16.2}
\end{equation}\]</span></p>
<p>In Equation <a href="ch-cv.html#eq:postpp3">(16.2)</a>, we are implicitly conditioning on the model under consideration:</p>
<p><span class="math display" id="eq:postpp2">\[\begin{equation}
p(y_{pred}\mid y,  \mathcal{M}_1)=\int_\Theta p(y_{pred}\mid \Theta, \mathcal{M}_1) p(\Theta\mid y, \mathcal{M}_1)\, d\Theta
\tag{16.3}
\end{equation}\]</span></p>
<p>The predicted data, <span class="math inline">\(y_{pred}\)</span>, are unknown to the utility function, so the utility function as presented in Equation <a href="ch-cv.html#eq:lscore">(16.1)</a> cannot be evaluated. For this reason, we marginalize over <em>all possible future data</em> (calculating <span class="math inline">\(E[\log p(y_{pred}| y, \mathcal{M}_1)]\)</span>); this expression is called the  expected log predictive density of model <span class="math inline">\(\mathcal{M}_1\)</span>:</p>
<p><span class="math display" id="eq:elpd">\[\begin{equation}
elpd = u(\mathcal{M}_1) = \int_{y_{pred}} p_t(y_{pred}) \log p(y_{pred} \mid y, \mathcal{M}_1) \, dy_{pred}
\tag{16.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p_t\)</span> is the  true data generating distribution. If we consider a set of models, the model with the highest  <span class="math inline">\(elpd\)</span> is the model with the predictions that are the closest to those of the true data generating process.<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a> The intuition behind Equation <a href="ch-cv.html#eq:elpd">(16.4)</a> is that we are evaluating the predictive distribution of <span class="math inline">\(\mathcal{M}_1\)</span> over all possible future data weighted by how likely the future data are according to their true distribution. This means that observations that are very likely according to the true model will have a higher weight than unlikely ones.</p>
<p>But we don’t know the true data-generating distribution, <span class="math inline">\(p_t\)</span>! If we knew it, we wouldn’t be looking for the best model, since we would already know what the best model is.</p>
<p>We can use the  observed data distribution as a proxy for the true data generating distribution. So instead of weighting the predictive distribution by the true density of all possible future data, we just use the <span class="math inline">\(N\)</span> observations that we have. We can do that because our observations are presumed to be samples from the true distribution of the data: Under this assumption, observations with higher likelihood according to the true distribution of the data will also be obtained more often. This means that instead of integrating, we sum the posterior predictive density of the observations and we give to each observation the same weight; this is valid because observations that are appearing more often in the data should presumably also appear more often in the future (see also Box <a href="ch-cv.html#thm:integral">16.1</a>). This quantity is called  <em>log pointwise predictive density</em> or  <span class="math inline">\(lpd\)</span> <span class="citation">(Vehtari, Gelman, and Gabry <a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">2017</a><a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">b</a> write this without the <span class="math inline">\(1/N\)</span>)</span>:</p>
<p><span class="math display" id="eq:elpdapprox">\[\begin{equation}
lpd = \frac{1}{N} \sum_{n=1}^{N} \log p(y_n|y, \mathcal{M}_1)
\tag{16.5}
\end{equation}\]</span></p>
<p>The <span class="math inline">\(lpd\)</span> is an overestimate of <em>elpd</em> for actual future data, because the parameters of the posterior predictive distribution are estimated with the same observations that we are considering out-of-sample. Incidentally, this also explains why posterior predictive checks are generally optimistic and good fits cannot be taken too seriously. But they do serve the purpose of identifying very strong  model misspecifications.<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a></p>
<p>However, we can obtain a more conservative estimate of the predictive performance of a model using cross-validation <span class="citation">(Geisser and Eddy <a href="#ref-GeisserEddy1979" role="doc-biblioref">1979</a>)</span>. This is explained next. <span class="citation">(As an aside, we mention here that there are also other alternatives to cross-validation; these are presented in Vehtari and Ojanen <a href="#ref-VehtariOjanen2012" role="doc-biblioref">2012</a>)</span>.</p>
<div class="extra">
<div class="theorem">
<p><span id="thm:integral" class="theorem"><strong>Box 16.1  </strong></span><strong>How do we get rid of the integral in the approximation of elpd?</strong></p>
</div>
<p>As an example, imagine that there are <span class="math inline">\(N\)</span> observations in an experiment. Suppose also that the  true generative process (which is normally always unknown to us) is a  Beta distribution:</p>
<p><span class="math display">\[\begin{equation}
p_t(y) = \mathit{ Beta}(y | 1, 3)
\end{equation}\]</span></p>
<p>Set <span class="math inline">\(N\)</span> and observe some simulated data <span class="math inline">\(y\)</span>:</p>
<div class="sourceCode" id="cb917"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb917-1"><a href="ch-cv.html#cb917-1" aria-hidden="true"></a>N &lt;-<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb917-2"><a href="ch-cv.html#cb917-2" aria-hidden="true"></a>y_data &lt;-<span class="st"> </span><span class="kw">rbeta</span>(N, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb917-3"><a href="ch-cv.html#cb917-3" aria-hidden="true"></a><span class="kw">head</span>(y_data)</span></code></pre></div>
<pre><code>## [1] 0.0888 0.2795 0.5947 0.2078 0.2744 0.2803</code></pre>
<p>Let’s say that we fit the Bayesian model <span class="math inline">\(\mathcal{M}_{1}\)</span>, and somehow, after getting the posterior distribution, we are able to derive the analytical form of its posterior predictive distribution for the model:</p>
<p><span class="math display">\[\begin{equation}
p(y_{pred} | y, \mathcal{M}_1) = \mathit{Beta}(y_{pred} | 2, 2)
\end{equation}\]</span></p>
<p>This distribution will tell us how likely different future observations will be, and it also entails that our future observations will be bounded by <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. (Any observation outside this range will have a probability density of zero).</p>
<p>Imagine that we could know the true distribution of the data, <span class="math inline">\(p_t\)</span>, which is conveniently close to our posterior predictive distribution. This means that Equation <a href="ch-cv.html#eq:elpd">(16.4)</a>, repeated below, is simple enough, and we know all its terms:</p>
<p><span class="math display">\[\begin{equation}
elpd = u(\mathcal{M}_1) = \int_{y_{pred}} p_t(y_{pred}) \log p(y_{pred} \mid y, \mathcal{M}_1)\, dy_{pred}
\end{equation}\]</span></p>
<p>We can compute this quantity in R. Notice that we don’t introduce the data at any point. However, the data had to be used when <code>p</code>, the posterior predictive distribution, was derived; we skipped that step here.</p>
<div class="sourceCode" id="cb919"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb919-1"><a href="ch-cv.html#cb919-1" aria-hidden="true"></a><span class="co"># True distribution:</span></span>
<span id="cb919-2"><a href="ch-cv.html#cb919-2" aria-hidden="true"></a>p_t &lt;-<span class="st"> </span><span class="cf">function</span>(y) <span class="kw">dbeta</span>(y, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb919-3"><a href="ch-cv.html#cb919-3" aria-hidden="true"></a><span class="co"># Predictive distribution:</span></span>
<span id="cb919-4"><a href="ch-cv.html#cb919-4" aria-hidden="true"></a>p &lt;-<span class="st"> </span><span class="cf">function</span>(y) <span class="kw">dbeta</span>(y, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb919-5"><a href="ch-cv.html#cb919-5" aria-hidden="true"></a><span class="co"># Integration:</span></span>
<span id="cb919-6"><a href="ch-cv.html#cb919-6" aria-hidden="true"></a>integrand &lt;-<span class="st"> </span><span class="cf">function</span>(y) <span class="kw">p_t</span>(y) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">p</span>(y))</span>
<span id="cb919-7"><a href="ch-cv.html#cb919-7" aria-hidden="true"></a><span class="kw">integrate</span>(<span class="dt">f =</span> integrand, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">1</span>)<span class="op">$</span>value</span></code></pre></div>
<pre><code>## [1] -0.375</code></pre>
<p>Because we will never know <code>p_t</code>, this integral can be approximated using the data, <code>y_data</code>. It is possible to approximate the integration without any reference to <code>p_t</code>; see Equation <a href="ch-cv.html#eq:elpdapprox">(16.5)</a>:</p>
<div class="sourceCode" id="cb921"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb921-1"><a href="ch-cv.html#cb921-1" aria-hidden="true"></a><span class="dv">1</span><span class="op">/</span>N <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">p</span>(y_data)))</span></code></pre></div>
<pre><code>## [1] -0.38</code></pre>
<p>The main problem with this approach is that we are using <code>y_data</code> twice, once to derive <code>p</code>, the predictive posterior distribution, and once for the approximation of <span class="math inline">\(elpd\)</span>. We’ll see that cross-validation approaches rely on deriving the posterior predictive distribution with part of the data, and estimating the approximation to <span class="math inline">\(elpd\)</span> with unseen data. (Don’t worry that we don’t know the analytical form of the posterior predictive distribution: we saw that we could generate samples from that distribution based on the distribution we use as the likelihood and our posterior samples.)</p>
</div>
</div>
<div id="k-fold-and-leave-one-out-cross-validation" class="section level2 hasAnchor" number="16.2">
<h2><span class="header-section-number">16.2</span> K-fold and leave-one-out cross-validation<a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The basic idea of  K-fold cross-validation  (K-fold-CV) is to split the <span class="math inline">\(N\)</span> observations of our data into K subsets, such that each subset is used as a  validation (or held-out) set, <span class="math inline">\(D_k\)</span>, while the remaining set (the  training set), <span class="math inline">\(D_{-k}\)</span> is used for estimating the parameters and approximating <span class="math inline">\(p_t\)</span>, the true data distribution. The  leave-one-out cross-validation  (LOO-CV) method represents a special case of K-fold-CV where, <span class="math inline">\(K=N\)</span> and the training set only excludes one observation. For the general case, K-fold-CV, we estimate <span class="math inline">\(elpd\)</span> as follows.</p>
<p><span class="math display" id="eq:approxelpd">\[\begin{equation}
\widehat{elpd} =  \frac{1}{N} \sum_{n=1}^{N} \log  p(y_n| D_{-k}, \mathcal{M}_1) \text{ with }
y_n \in D_k \tag{16.6}
\end{equation}\]</span></p>
<p>In Equation <a href="ch-cv.html#eq:approxelpd">(16.6)</a>, each observation, <span class="math inline">\(y_n\)</span>, belongs to a certain  “validation” fold, <span class="math inline">\(D_k\)</span>, and the predictive accuracy of <span class="math inline">\(y_n\)</span> is evaluated based on a posterior predictive model trained on the set, <span class="math inline">\(D_{-k}\)</span>, which is the complete data set excluding the validation fold that contains the <span class="math inline">\(n\)</span>-th observation. This means that the posterior predictive distribution used to evaluate <span class="math inline">\(y_{n}\)</span> was derived without having information from that <span class="math inline">\(y_n\)</span>-th observation (in other words, the model was trained without that observation in the subset of the data, <span class="math inline">\(D_{-k}\)</span>).</p>
<p>In K-fold-CV, several observations are held out in same (validation) fold. This means that the held-out observations are split among K folds, and <span class="math inline">\(D_{-k}\)</span>, the data used to derive the posterior predictive distribution, contain only a proportion of the observations; this proportion is <span class="math inline">\((1 - 1/K)\)</span>. By contrast, in leave-one-out cross-validation, the held-out data set includes only one observation. That is, <span class="math inline">\(D_{-k}\)</span> contains the entire data set except for one data point, <span class="math inline">\(y_n\)</span>, with <span class="math inline">\(n=1,\dots,N\)</span>, that is <span class="math inline">\(D_{-n}\)</span>. Box <a href="ch-cv.html#thm:CV-alg">16.2</a> explains the algorithm in detail.</p>
<p>For the general case of K-fold-CV, <span class="citation">Vehtari, Gelman, and Gabry (<a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">2017</a><a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">b</a>)</span> define the  expected log <em>pointwise</em> predictive density of the observation <span class="math inline">\(y_n\)</span> as follows:</p>
<p><span class="math display">\[\begin{equation}
\widehat{elpd}_{n} =  \log  p(y_n| D_{- k} , \mathcal{M}_1) \text{ with } y_n \in D_k
\end{equation}\]</span></p>
<p>This quantity indicates the predictive accuracy of the model <span class="math inline">\(\mathcal{M}_1\)</span> for a single observation, and it is reported in the package  <code>loo</code> and also in  <code>brms</code>. In addition, the <code>loo</code> package uses the sum of the expected log pointwise predictive density, <span class="math inline">\(\sum \widehat{elpd}_n\)</span> (Equation <a href="ch-cv.html#eq:approxelpd">(16.6)</a> without <span class="math inline">\(\frac{1}{N}\)</span>) as a measure of predictive accuracy (this is referred to as  <code>elpd_loo</code> or  <code>elpd_kfold</code> in the packages <code>loo</code> and <code>brms</code>). For model comparison, the difference between the <span class="math inline">\(\sum \widehat{elpd}_n\)</span> of competing models can be computed, including the standard deviation of the sampling distribution of the difference. It’s important to notice that we are calculating an approximation to the expectation that we actually want to compute, <span class="math inline">\(elpd\)</span>, and thus we always need to consider its inherent randomness (due to having a limited amount of data), which is quantified by the standard error <span class="citation">(Vehtari et al. <a href="#ref-vehtariLimitationsLimitationsBayesian2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>Unlike what is common with information criterion methods (such as the  Akaike Information Criterion, AIC, and the  Deviance Information Criterion,  DIC), a higher  <span class="math inline">\(\widehat{elpd}\)</span> means higher predictive accuracy. An alternative to using <span class="math inline">\(\widehat{elpd}\)</span> is to examine <span class="math inline">\(-2\times \widehat{elpd}\)</span>, which is equivalent to deviance, and is called the  LOO Information Criterion  (LOOIC) <span class="citation">(see section 22 of Vehtari <a href="#ref-FAQCV" role="doc-biblioref">2022</a>)</span>.</p>
<p>The approximation to the true data generating distribution is worse when fewer observations are used, and thus ideally we would set <span class="math inline">\(K =N\)</span>, and thus compute LOO-CV rather than K-fold-CV. The main advantage of LOO-CV is its  robustness, since the training set is as similar as possible to the observed data, and the same observations are never used simultaneously for training and evaluating the predictions. A major disadvantage is the  computational burden <span class="citation">(Vehtari and Ojanen <a href="#ref-VehtariOjanen2012" role="doc-biblioref">2012</a>)</span>, since we need to fit a model as many times as the number of observations. The package <code>loo</code> provides an approximation to LOO-CV,  Pareto smoothed importance sampling leave-one-out <span class="citation">( PSIS-LOO; Vehtari et al. <a href="#ref-VehtariGelman2015Pareto" role="doc-biblioref">2024</a>; Vehtari, Gelman, and Gabry <a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">2017</a><a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">b</a>)</span> which, as we show next, is relatively straightforward to use in <code>brms</code> and in Stan models (see <a href="https://mc-stan.org/loo/articles/loo2-with-rstan.html" class="uri">https://mc-stan.org/loo/articles/loo2-with-rstan.html</a>). However, in some cases, its estimates can be unreliable; this is indicated by the estimated  shape parameter <span class="math inline">\(\hat{k}\)</span> of the  generalized Pareto distribution (<a href="https://mc-stan.org/loo/reference/pareto-k-diagnostic.html" class="uri">https://mc-stan.org/loo/reference/pareto-k-diagnostic.html</a>). The value <span class="math inline">\(\hat{k}\)</span> (which is unrelated to the <span class="math inline">\(k\)</span> in K-fold-CV) estimates how far an approximated individual leave-one-out distribution is from the full distribution. If leaving out an observation substantially alters the posterior, then importance sampling cannot provide a reliable estimate. Very high <span class="math inline">\(\hat{k}\)</span> values often indicate model misspecification, outliers, or mistakes in data processing. The threshold for <span class="math inline">\(\hat{k}\)</span> depends on the sample size <span class="citation">(Vehtari et al. <a href="#ref-VehtariGelman2015Pareto" role="doc-biblioref">2024</a>)</span> and it is reported by the <code>loo()</code> function. In cases, where one or several pointwise predictive density have associated large <span class="math inline">\(\hat{k}\)</span>, either (i) the problematic predictions can be refitted with  exact LOO-CV, (ii) one can try some additional computations using the existing posterior sample based on the  moment matching approximation <span class="citation">(see <a href="https://mc-stan.org/loo/articles/loo2-moment-matching.html" class="uri" role="doc-biblioref">https://mc-stan.org/loo/articles/loo2-moment-matching.html</a> and Paananen et al. <a href="#ref-Paananen_2021" role="doc-biblioref">2021</a>)</span>, or (iii) one can abandon PSIS-LOO-CV and use K-fold-CV, with K typically set to 10.</p>
<p>One of the main disadvantages of cross-validation (in comparison with Bayes factor at least) is that the numerical difference in predictive accuracy is hard to interpret. As a rule of thumb, it has been suggested that if the <code>elpd</code> difference ( <code>elpd_diff</code> in the <code>loo</code> package) is less than 4, the difference is small, and if it is larger than 4, one should compare that difference to its standard error  (<code>se_diff</code>) <span class="citation">(see section 16 of Vehtari <a href="#ref-FAQCV" role="doc-biblioref">2022</a>)</span>.</p>
<div class="extra">
<div class="theorem">
<p><span id="thm:CV-alg" class="theorem"><strong>Box 16.2  </strong></span><strong>The cross-validation algorithm</strong></p>
</div>
<p>Here we spell out the Bayesian  cross-validation algorithm in detail:</p>
<ol style="list-style-type: decimal">
<li><p>Split the data pseudo-randomly into <span class="math inline">\(K\)</span> held-out or validation sets <span class="math inline">\(D_k\)</span>, (where <span class="math inline">\(k=1,\dots,K\)</span>) that are a fraction of the original data, and <span class="math inline">\(K\)</span> training sets, <span class="math inline">\(D_{-k}\)</span>. The length of the held-out data vector <span class="math inline">\(D_k\)</span> is approximately <span class="math inline">\(1/K\)</span>-th the size of the full data set. It is common to use <span class="math inline">\(K=10\)</span> for K-fold-CV. For LOO-CV, K should be set to the number of observations.</p></li>
<li><p>Fit <span class="math inline">\(K\)</span> models using each of the <span class="math inline">\(K\)</span> training sets, and obtain posterior distributions <span class="math inline">\(p_{-k} (\Theta) = p(\Theta\mid D_{-k})\)</span>, where <span class="math inline">\(\Theta\)</span> is the vector of model parameters.</p></li>
<li><p>Each posterior distribution <span class="math inline">\(p(\Theta\mid D_{-k})\)</span> is used to compute the predictive accuracy for each held-out data-point <span class="math inline">\(y_n\)</span> in the vector <span class="math inline">\(D_{k}\)</span>:</p></li>
</ol>
<p><span class="math display">\[\begin{equation}
    \widehat{elpd}_n = \log p(y_n \mid D_{-k}) \text{ with } y_n \in D_k
  \end{equation}\]</span></p>
<p>Given that the posterior distribution <span class="math inline">\(p(\Theta\mid D_{-k})\)</span> is summarized by <span class="math inline">\(S\)</span> samples, the log predictive density for each data point <span class="math inline">\(y_n\)</span> in a data vector <span class="math inline">\(D_k\)</span> can be approximated as follows:</p>
<p><span class="math display" id="eq:pwkfold">\[\begin{equation}
    \widehat{elpd}_n = \log \left(\frac{1}{S} \sum_{s=1}^S p(y_n\mid \Theta^{k,s})\right)
    \tag{16.7}
  \end{equation}\]</span></p>
<p>where <span class="math inline">\(\Theta^{k,s}\)</span> corresponds to the sample <span class="math inline">\(s\)</span> of the posterior of the model fit to the training set <span class="math inline">\(D_-k\)</span>.</p>
<ol start="5" style="list-style-type: decimal">
<li>We obtain the <span class="math inline">\(elpd_{kfold}\)</span> (or <span class="math inline">\(elpd_{loo}\)</span>) for all the held-out data points by summing up the <span class="math inline">\(\widehat{elpd}_n\)</span>:</li>
</ol>
<p><span class="math display" id="eq:totalkfold">\[\begin{equation}
    elpd_{kfold} = \sum_{n=1}^N \widehat{elpd}_n
    \tag{16.8}
  \end{equation}\]</span></p>
<p>The  standard deviation of the sampling distribution (the  standard error) can be computed by multiplying the standard deviation (or square root of variance) of the <span class="math inline">\(N\)</span> components by <span class="math inline">\(\sqrt{N}\)</span>. Letting <span class="math inline">\(\widehat{ELPD}\)</span> be the vector <span class="math inline">\(\widehat{elpd}_1,\dots,\widehat{elpd}_N\)</span>, the standard error is computed as follows:</p>
<p><span class="math display" id="eq:sekfold">\[\begin{equation}
se(\widehat{elpd}) = \sqrt{N \mathit{Var}(\widehat{ELPD})}
\tag{16.9}
\end{equation}\]</span></p>
<p>The difference between the <span class="math inline">\(elpd_{kfold}\)</span> of two competing models, <span class="math inline">\(\mathcal{M}_1\)</span> and <span class="math inline">\(\mathcal{M}_2\)</span> , is a measure of  relative predictive performance. The standard error of their difference can be computed using the formula discussed in <span class="citation">Vehtari, Gelman, and Gabry (<a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">2017</a><a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">b</a>)</span>:</p>
<p><span class="math display" id="eq:sekfolddiff">\[\begin{equation}
se(\widehat{elpd}_{\mathcal{M}1} - \widehat{elpd}_{\mathcal{M}2}) = \sqrt{N \mathit{Var}(\widehat{ELPD_{\mathcal{M}1}} - \widehat{ELPD_{\mathcal{M}2}})}
\tag{16.10}
\end{equation}\]</span></p>
</div>
</div>
<div id="testing-the-n400-effect-using-cross-validation" class="section level2 hasAnchor" number="16.3">
<h2><span class="header-section-number">16.3</span> Testing the N400 effect using cross-validation<a href="ch-cv.html#testing-the-n400-effect-using-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As in section <a href="ch-bf.html#sec-N400BF">15.2</a> with the Bayes factor, let us revisit section <a href="ch-hierarchical.html#sec-N400hierarchical">5.2</a>, where the effect of cloze probability on the  N400 average signal was estimated. Consider two models here, a model that includes the effect of  cloze probability, such as <code>fit_N400_sih</code> from section <a href="ch-hierarchical.html#sec-sih">5.2.5</a>, and a null model.</p>
<p>Verify the model that was fit; this is a hierarchical model that includes an effect of cloze probability:</p>
<div class="sourceCode" id="cb923"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb923-1"><a href="ch-cv.html#cb923-1" aria-hidden="true"></a><span class="kw">formula</span>(fit_N400_sih)</span></code></pre></div>
<pre><code>## n400 ~ c_cloze + (c_cloze | subj) + (c_cloze | item)</code></pre>
<p>In contrast to the situation with Bayes factor, priors are less critical for cross-validation. Priors are only important in cross-validation to the extent that they affect parameter estimation: As discussed earlier, very narrow priors can bias the posterior; and unrealistically wide priors can lead to convergence problems. The  number of samples is also less critical than with Bayes factor; most of the uncertainty in the estimates of the <span class="math inline">\(\widehat{elpd}\)</span> is due to the number of observations. However, a very small number of samples can affect the <span class="math inline">\(\widehat{elpd}\)</span> because the posterior estimation will be affected by the small sample size.
We update our previous formula to define a null model as follows:</p>
<div class="sourceCode" id="cb925"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb925-1"><a href="ch-cv.html#cb925-1" aria-hidden="true"></a>fit_N400_sih_null &lt;-<span class="st"> </span><span class="kw">update</span>(fit_N400_sih, <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>c_cloze)</span></code></pre></div>
<div id="cross-validation-with-psis-loo" class="section level3 hasAnchor" number="16.3.1">
<h3><span class="header-section-number">16.3.1</span> Cross-validation with PSIS-LOO<a href="ch-cv.html#cross-validation-with-psis-loo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Estimating <span class="math inline">\(elpd\)</span> using  PSIS-LOO is very straightforward with  <code>brms</code>, which uses the package  <code>loo</code> as a back-end. There is no need to refit the model, and <code>loo</code> takes care of applying the PSIS approximation to derive estimates and standard errors.</p>
<div class="sourceCode" id="cb926"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb926-1"><a href="ch-cv.html#cb926-1" aria-hidden="true"></a>(loo_sih &lt;-<span class="st"> </span><span class="kw">loo</span>(fit_N400_sih))</span></code></pre></div>
<pre><code>## 
## Computed from 4000 by 2863 log-likelihood matrix.
## 
##          Estimate   SE
## elpd_loo -11093.0 46.7
## p_loo        81.8  2.8
## looic     22186.0 93.4
## ------
## MCSE of elpd_loo is 0.1.
## MCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.8]).
## 
## All Pareto k estimates are good (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode" id="cb928"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb928-1"><a href="ch-cv.html#cb928-1" aria-hidden="true"></a>(loo_sih_null &lt;-<span class="st"> </span><span class="kw">loo</span>(fit_N400_sih_null))</span></code></pre></div>
<pre><code>## 
## Computed from 4000 by 2863 log-likelihood matrix.
## 
##          Estimate   SE
## elpd_loo -11095.5 46.5
## p_loo        89.3  3.0
## looic     22190.9 93.0
## ------
## MCSE of elpd_loo is 0.2.
## MCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.8]).
## 
## All Pareto k estimates are good (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>The function <code>loo</code> reports three quantities with their standard error:</p>
<ol style="list-style-type: decimal">
<li> <code>elpd_loo</code> is the  sum of pointwise predictive accuracy (a larger, less negative number indicates better predictions).</li>
<li> <code>p_loo</code> is an estimate of  effective complexity of the model; asymptotically and under certain regularity conditions, <code>p_loo</code> can be interpreted as the effective number of parameters. If <code>p_loo</code> is larger than the number of data points or parameters, this may indicate a severe model misspecification.</li>
<li> <code>looic</code> is simply <code>-2*elpd_loo</code>, the <span class="math inline">\(elpd\)</span> on the  deviance scale. This is called the  information criterion, and is mainly provided for historical reasons: other information criteria like the AIC (Akaike Information Criterion) and the DIC (Deviance Information Criterion) are commonly used in model selection <span class="citation">(Venables and Ripley <a href="#ref-venablesripley" role="doc-biblioref">2002</a>; Lunn et al. <a href="#ref-lunn2012bugs" role="doc-biblioref">2012</a>)</span>.</li>
</ol>
<p>It’s important to bear in mind that the PSIS-LOO approximation to LOO can only be trusted if there are no large  Pareto (<span class="math inline">\(\hat{k}\)</span>) estimates. To compare the models, take a look at the difference between <code>elpd_loo</code> and the standard error of that difference:</p>
<div class="sourceCode" id="cb930"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb930-1"><a href="ch-cv.html#cb930-1" aria-hidden="true"></a><span class="kw">loo_compare</span>(loo_sih, loo_sih_null)</span></code></pre></div>
<pre><code>##                   elpd_diff se_diff
## fit_N400_sih       0.0       0.0   
## fit_N400_sih_null -2.5       2.5</code></pre>
<p>Although the model that includes cloze probability as a predictor has higher predictive accuracy, the difference is smaller than 4 and it’s smaller than two SE. This means that from the perspective of LOO-CV, both models are almost indistinguishable! In fact, the same will happen if the model is compared using logarithmic predictability to the linear or null model; see exercise <a href="ch-cv.html#exr:logcv">16.1</a>.</p>
<p>It is also possible to check whether the alternative model is making good predictions for some range of values by examining the difference in pointwise predictive accuracy as a function of, for example, cloze probability. In the following plot, we subtract the predictive accuracy of the alternative model from the accuracy of the null model; larger differences can be interpreted as an advantage for the alternative model. However, as far as posterior predictive accuracy goes, both models are quite similar. Figure <a href="ch-cv.html#fig:diffpredacc">16.1</a> shows that the difference in predictive accuracy is symmetrical with respect to the zero; as we go further from the mean cloze (which is around <span class="math inline">\(0.5\)</span>), the differences in predictions are larger but they span positive and negative values.</p>
<p>The following code stores the difference in predictive accuracy of the models in a variable and plots it in Figure <a href="ch-cv.html#fig:diffpredacc">16.1</a>.</p>
<div class="sourceCode" id="cb932"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb932-1"><a href="ch-cv.html#cb932-1" aria-hidden="true"></a>df_eeg &lt;-<span class="st"> </span><span class="kw">mutate</span>(df_eeg,</span>
<span id="cb932-2"><a href="ch-cv.html#cb932-2" aria-hidden="true"></a>                 <span class="dt">diff_elpd =</span> loo_sih<span class="op">$</span>pointwise[, <span class="st">&quot;elpd_loo&quot;</span>] <span class="op">-</span></span>
<span id="cb932-3"><a href="ch-cv.html#cb932-3" aria-hidden="true"></a><span class="st">                   </span>loo_sih_null<span class="op">$</span>pointwise[, <span class="st">&quot;elpd_loo&quot;</span>])</span>
<span id="cb932-4"><a href="ch-cv.html#cb932-4" aria-hidden="true"></a><span class="kw">ggplot</span>(df_eeg, <span class="kw">aes</span>(<span class="dt">x =</span> cloze, <span class="dt">y =</span> diff_elpd)) <span class="op">+</span></span>
<span id="cb932-5"><a href="ch-cv.html#cb932-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.4</span>, <span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">w =</span> <span class="fl">.001</span>, <span class="dt">h =</span> <span class="dv">0</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:diffpredacc"></span>
<img src="bookdown_files/figure-html/diffpredacc-1.svg" alt="The difference in predictive accuracy between a model including the effect of cloze and a null model. A larger (more positive) difference indicates an advantage for the model that includes the effect of cloze." width="672" />
<p class="caption">
FIGURE 16.1: The difference in predictive accuracy between a model including the effect of cloze and a null model. A larger (more positive) difference indicates an advantage for the model that includes the effect of cloze.
</p>
</div>
<p>The expectation was that, similar to the Bayes factor, cross-validation techniques will also show that a model that includes cloze probability as a predictor is superior to a model without it. It is unsettling that the above result does not show this: the effect of cloze probability on the N400 has been replicated in numerous studies.</p>
<p>Before discussing why a large difference was not observed, let us check what K-fold-CV yields.</p>
</div>
<div id="cross-validation-with-k-fold" class="section level3 hasAnchor" number="16.3.2">
<h3><span class="header-section-number">16.3.2</span> Cross-validation with K-fold<a href="ch-cv.html#cross-validation-with-k-fold" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Estimating <span class="math inline">\(elpd\)</span> using  k-fold-CV has the advantage of omitting one layer of approximations: the <span class="math inline">\(elpd\)</span> based on PSIS-LOO-CV is an approximation of the <span class="math inline">\(elpd\)</span> based on <em>exact</em> LOO-CV (and we saw how any cross-validation approach gave us an approximation to the true <span class="math inline">\(elpd\)</span>). This means that we don’t need to worry about <span class="math inline">\(\hat{k}\)</span>. However, K-fold also uses a reduced training set in comparison with LOO, worsening the approximation to the true generating process <span class="math inline">\(p_{t}\)</span>.</p>
<p>Before dividing our data into folds, we need to think about the way the data will be split: The data could be split randomly, but this approach would have the risk that in some of the training sets, observations from a particular subject, for example, could be completely absent. Such an omission will lead to large differences in predictive accuracy between folds. This situation can be avoided by using  stratification: split the observations into groups, ensuring that relative category frequencies are approximately preserved in the training and held-out data. This stratification can be achieved by using the  <code>kfold()</code> function, available in the package <code>brms</code>, by setting <code>folds = "stratified"</code> and <code>group = "subj"</code>, by default <code>K</code> is set to 10, but that can be changed. Additionally, since we know the models converge, we use only one chain for each model and parallelize the fitting procedure with the <code>plan(multisession)</code> function from the <code>future</code> package, returning to the default setting with <code>plan(sequential)</code>.</p>
<div class="sourceCode" id="cb933"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb933-1"><a href="ch-cv.html#cb933-1" aria-hidden="true"></a><span class="kw">library</span>(future)</span>
<span id="cb933-2"><a href="ch-cv.html#cb933-2" aria-hidden="true"></a><span class="kw">plan</span>(multisession, <span class="dt">workers =</span> <span class="dv">4</span>)</span>
<span id="cb933-3"><a href="ch-cv.html#cb933-3" aria-hidden="true"></a>kfold_sih &lt;-<span class="st"> </span><span class="kw">kfold</span>(fit_N400_sih,</span>
<span id="cb933-4"><a href="ch-cv.html#cb933-4" aria-hidden="true"></a>                   <span class="dt">folds =</span> <span class="st">&quot;stratified&quot;</span>,</span>
<span id="cb933-5"><a href="ch-cv.html#cb933-5" aria-hidden="true"></a>                   <span class="dt">group =</span> <span class="st">&quot;subj&quot;</span>,</span>
<span id="cb933-6"><a href="ch-cv.html#cb933-6" aria-hidden="true"></a>                   <span class="dt">chains =</span> <span class="dv">1</span>)</span>
<span id="cb933-7"><a href="ch-cv.html#cb933-7" aria-hidden="true"></a>kfold_sih_null &lt;-<span class="st"> </span><span class="kw">kfold</span>(fit_N400_sih_null,</span>
<span id="cb933-8"><a href="ch-cv.html#cb933-8" aria-hidden="true"></a>                         <span class="dt">folds =</span> <span class="st">&quot;stratified&quot;</span>,</span>
<span id="cb933-9"><a href="ch-cv.html#cb933-9" aria-hidden="true"></a>                         <span class="dt">group =</span> <span class="st">&quot;subj&quot;</span>)</span>
<span id="cb933-10"><a href="ch-cv.html#cb933-10" aria-hidden="true"></a><span class="kw">plan</span>(sequential)</span></code></pre></div>
<p>Running K-fold CV takes some time since each model is re-fit K times. Inspect the <span class="math inline">\(elpd\)</span> values:</p>
<div class="sourceCode" id="cb934"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb934-1"><a href="ch-cv.html#cb934-1" aria-hidden="true"></a>kfold_sih</span></code></pre></div>
<pre><code>## 
## Based on 10-fold cross-validation.
## 
##            Estimate   SE
## elpd_kfold -11094.1 46.4
## p_kfold        82.6  3.2
## kfoldic     22188.2 92.8</code></pre>
<div class="sourceCode" id="cb936"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb936-1"><a href="ch-cv.html#cb936-1" aria-hidden="true"></a>kfold_sih_null</span></code></pre></div>
<pre><code>## 
## Based on 10-fold cross-validation.
## 
##            Estimate   SE
## elpd_kfold -11094.4 46.2
## p_kfold        88.2  3.4
## kfoldic     22188.8 92.4</code></pre>
<p>Compare the two models using  <code>loo_compare</code> (this function is used for both PSIS-LOO-CV and K-fold-CV):</p>
<div class="sourceCode" id="cb938"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb938-1"><a href="ch-cv.html#cb938-1" aria-hidden="true"></a><span class="kw">loo_compare</span>(kfold_sih, kfold_sih_null)</span></code></pre></div>
<pre><code>##                   elpd_diff se_diff
## fit_N400_sih       0.0       0.0   
## fit_N400_sih_null -0.3       4.0</code></pre>
<p>Because the <span class="math inline">\(\hat{elpd}\)</span> values depend on how the folds were formed, K-fold CV introduces an additional element of randomness compared to (PSIS-)LOO-CV. To verify the robustness of the results, one can rerun the <code>kfold()</code> function so that different random configurations of the folds are selected. In this case, however, the results with K-fold-CV and PSIS-LOO-CV are quite similar: The two models can’t really be distinguished.</p>
</div>
<div id="leave-one-group-out-cross-validation" class="section level3 hasAnchor" number="16.3.3">
<h3><span class="header-section-number">16.3.3</span> Leave-one-group-out cross-validation<a href="ch-cv.html#leave-one-group-out-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An alternative to splitting the observations randomly using stratification is to treat naturally occurring clusters as folds; this is  leave-one-group-out cross-validation  (LOGO-CV). The output of LOGO-CV tells us about the capacity of the models for generalizing to unseen clusters. LOGO-CV is implemented next using subjects as the group of interest.</p>
<p>In general, there is some tension between the arguments for K-fold-CV with folds stratified by subject, which ensures data from all subjects in each fold, and for LOGO-CV, which isolates data from different subjects. However, the two approaches have different goals and merits. The first approach answers how well the model generalizes to unseen data from this particular set of participants. The second approach addresses how well the model generalizes to the behavior of unseen participants if we were to collect more data.</p>
<div class="sourceCode" id="cb940"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb940-1"><a href="ch-cv.html#cb940-1" aria-hidden="true"></a><span class="kw">plan</span>(multisession, <span class="dt">workers =</span> <span class="dv">4</span>)</span>
<span id="cb940-2"><a href="ch-cv.html#cb940-2" aria-hidden="true"></a>logo_sih &lt;-<span class="st"> </span><span class="kw">kfold</span>(fit_N400_sih,</span>
<span id="cb940-3"><a href="ch-cv.html#cb940-3" aria-hidden="true"></a>                  <span class="dt">group =</span> <span class="st">&quot;subj&quot;</span>,</span>
<span id="cb940-4"><a href="ch-cv.html#cb940-4" aria-hidden="true"></a>                  <span class="dt">chains =</span> <span class="dv">1</span>)</span>
<span id="cb940-5"><a href="ch-cv.html#cb940-5" aria-hidden="true"></a>logo_sih_null &lt;-<span class="st"> </span><span class="kw">kfold</span>(fit_N400_sih_null,</span>
<span id="cb940-6"><a href="ch-cv.html#cb940-6" aria-hidden="true"></a>                       <span class="dt">group =</span> <span class="st">&quot;subj&quot;</span>,</span>
<span id="cb940-7"><a href="ch-cv.html#cb940-7" aria-hidden="true"></a>                       <span class="dt">chains =</span> <span class="dv">1</span>)</span>
<span id="cb940-8"><a href="ch-cv.html#cb940-8" aria-hidden="true"></a><span class="kw">plan</span>(sequential)</span></code></pre></div>
<p>Running LOGO-CV with subjects takes even longer since each model is re-fit as many times as there are subjects, in this case 37 times. We can now inspect the <span class="math inline">\(elpd\)</span> estimates and evaluate which model generalizes better to unseen subjects.</p>
<p>Compare the models using <code>loo_compare</code>.</p>
<div class="sourceCode" id="cb941"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb941-1"><a href="ch-cv.html#cb941-1" aria-hidden="true"></a><span class="kw">loo_compare</span>(logo_sih, logo_sih_null)</span></code></pre></div>
<pre><code>##                   elpd_diff se_diff
## fit_N400_sih       0.0       0.0   
## fit_N400_sih_null -1.5       2.4</code></pre>
<p>As before, and as with PSIS-LOO-CV and with K-fold-CV, the two models can’t be distinguished.
Even though the full model and the null model presented earlier make different predictions, the difference in predictive accuracy is not substantial compared to the variability in the data. In these situations, cross-validation is not very useful for distinguishing very small effect sizes from zero effect sizes; see also section <a href="ch-cv.html#sec-issuesCV">16.5</a> below.</p>
</div>
</div>
<div id="sec-logcv" class="section level2 hasAnchor" number="16.4">
<h2><span class="header-section-number">16.4</span>  Comparing different likelihoods with cross-validation<a href="ch-cv.html#sec-logcv" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One can also compare two models with different likelihoods. Section <a href="ch-compbda.html#sec-lognormal">3.7.2</a> in chapter <a href="ch-compbda.html#ch-compbda">3</a> showed how a log-normal distribution was a more appropriate likelihood than a normal distribution for response times data. This was because  response times are bounded by zero and right skewed, unlike the symmetrical normal distribution. Let’s use PSIS-LOO-CV to compare the predictive accuracy of the Stroop model from section <a href="ch-hierarchical.html#sec-stroop">5.3</a> in chapter <a href="ch-hierarchical.html#ch-hierarchical">5</a>, which assumed a  log-normal likelihood to fit response times of correct responses with a similar model which assumes a normal likelihood. We can only compare models fit to the same data; we can’t compare models fit to different dependent variables (e.g., one raw dependent variable and one log-transformed).</p>
<p>Load the data from <code>bcogsci</code>, create a sum-coded predictor (see chapter <a href="ch-contr.html#ch-contr">8</a> for more details), and fit the model as in section <a href="ch-hierarchical.html#sec-stroop">5.3</a>.</p>
<div class="sourceCode" id="cb943"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb943-1"><a href="ch-cv.html#cb943-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;df_stroop&quot;</span>)</span>
<span id="cb943-2"><a href="ch-cv.html#cb943-2" aria-hidden="true"></a>df_stroop &lt;-<span class="st"> </span>df_stroop <span class="op">%&gt;%</span></span>
<span id="cb943-3"><a href="ch-cv.html#cb943-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cond =</span> <span class="kw">if_else</span>(condition <span class="op">==</span><span class="st"> &quot;Incongruent&quot;</span>, <span class="dv">1</span>, <span class="dv">-1</span>))</span></code></pre></div>
<div class="sourceCode" id="cb944"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb944-1"><a href="ch-cv.html#cb944-1" aria-hidden="true"></a>fit_stroop_log &lt;-<span class="st"> </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">|</span><span class="st"> </span>subj),</span>
<span id="cb944-2"><a href="ch-cv.html#cb944-2" aria-hidden="true"></a>  <span class="dt">family =</span> <span class="kw">lognormal</span>(),</span>
<span id="cb944-3"><a href="ch-cv.html#cb944-3" aria-hidden="true"></a>  <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb944-4"><a href="ch-cv.html#cb944-4" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b),</span>
<span id="cb944-5"><a href="ch-cv.html#cb944-5" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb944-6"><a href="ch-cv.html#cb944-6" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</span>
<span id="cb944-7"><a href="ch-cv.html#cb944-7" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)),</span>
<span id="cb944-8"><a href="ch-cv.html#cb944-8" aria-hidden="true"></a>  <span class="dt">data =</span> df_stroop)</span></code></pre></div>
<p>Calculate the <span class="math inline">\(elpd_{loo}\)</span> for the original model with the log-normal likelihood:</p>
<div class="sourceCode" id="cb945"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb945-1"><a href="ch-cv.html#cb945-1" aria-hidden="true"></a>loo_stroop_log &lt;-<span class="st"> </span><span class="kw">loo</span>(fit_stroop_log)</span></code></pre></div>
<div class="sourceCode" id="cb946"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb946-1"><a href="ch-cv.html#cb946-1" aria-hidden="true"></a>loo_stroop_log</span></code></pre></div>
<pre><code>## 
## Computed from 4000 by 3058 log-likelihood matrix.
## 
##          Estimate    SE
## elpd_loo -19860.4  93.8
## p_loo        61.9   4.2
## looic     39720.8 187.5
## ------
## MCSE of elpd_loo is 0.1.
## MCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 2.3]).
## 
## All Pareto k estimates are good (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>The summary shows that <span class="math inline">\(k\)</span> estimates are ok.</p>
<p>Now fit a similar model which assumes that the likelihood is a normal distribution. It’s important now to change the priors since they are on a different scale (namely, in milliseconds). We choose reasonable but wide priors. A  sensitivity analysis can be done if we are unsure about the priors. However, unlike what happened with the Bayes factor in chapter <a href="ch-bf.html#ch-bf">15</a>, the priors are going to affect cross-validation based model comparison only as far as they have a noticeable effect on the posterior distribution.</p>
<div class="sourceCode" id="cb948"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb948-1"><a href="ch-cv.html#cb948-1" aria-hidden="true"></a>fit_stroop_normal &lt;-<span class="st"> </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">|</span><span class="st"> </span>subj),</span>
<span id="cb948-2"><a href="ch-cv.html#cb948-2" aria-hidden="true"></a>  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb948-3"><a href="ch-cv.html#cb948-3" aria-hidden="true"></a>  <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">400</span>, <span class="dv">600</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb948-4"><a href="ch-cv.html#cb948-4" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b),</span>
<span id="cb948-5"><a href="ch-cv.html#cb948-5" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">300</span>), <span class="dt">class =</span> sigma),</span>
<span id="cb948-6"><a href="ch-cv.html#cb948-6" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">300</span>), <span class="dt">class =</span> sd),</span>
<span id="cb948-7"><a href="ch-cv.html#cb948-7" aria-hidden="true"></a>            <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)),</span>
<span id="cb948-8"><a href="ch-cv.html#cb948-8" aria-hidden="true"></a>  <span class="dt">data =</span> df_stroop)</span></code></pre></div>
<p>The <span class="math inline">\(elpd\)</span> based on PSIS-LOO-CV has several large <span class="math inline">\(\hat{k}\)</span> values.</p>
<div class="sourceCode" id="cb949"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb949-1"><a href="ch-cv.html#cb949-1" aria-hidden="true"></a>loo_stroop_normal &lt;-<span class="st"> </span><span class="kw">loo</span>(fit_stroop_normal)</span></code></pre></div>
<div class="sourceCode" id="cb950"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb950-1"><a href="ch-cv.html#cb950-1" aria-hidden="true"></a>loo_stroop_normal</span></code></pre></div>
<pre><code>## 
## Computed from 4000 by 3058 log-likelihood matrix.
## 
##          Estimate    SE
## elpd_loo -21588.3 479.8
## p_loo       107.9  58.9
## looic     43176.6 959.7
## ------
## MCSE of elpd_loo is NA.
## MCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 2.6]).
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. ESS
## (-Inf, 0.7]   (good)     3055  99.9%   119     
##    (0.7, 1]   (bad)         1   0.0%   &lt;NA&gt;    
##    (1, Inf)   (very bad)    2   0.1%   &lt;NA&gt;    
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>We use  exact LOO (rather than its approximation) for the problematic observations. By setting  <code>reloo = TRUE</code>, the 3 problematic observations with <span class="math inline">\(\hat{k}\)</span> values over <span class="math inline">\(0.7\)</span> are re-fit using exact LOO-CV.<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a></p>
<div class="sourceCode" id="cb952"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb952-1"><a href="ch-cv.html#cb952-1" aria-hidden="true"></a>loo_stroop_normal &lt;-<span class="st"> </span><span class="kw">loo</span>(fit_stroop_normal, <span class="dt">reloo =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<div class="sourceCode" id="cb953"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb953-1"><a href="ch-cv.html#cb953-1" aria-hidden="true"></a>loo_stroop_normal</span></code></pre></div>
<pre><code>## 
## Computed from 4000 by 3058 log-likelihood matrix.
## 
##          Estimate     SE
## elpd_loo -21708.7  596.6
## p_loo       228.3  179.1
## looic     43417.3 1193.2
## ------
## MCSE of elpd_loo is NaN.
## MCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 2.6]).
## 
## All Pareto k estimates are good (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>Next, compare the models.</p>
<div class="sourceCode" id="cb955"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb955-1"><a href="ch-cv.html#cb955-1" aria-hidden="true"></a><span class="kw">loo_compare</span>(loo_stroop_log, loo_stroop_normal)</span></code></pre></div>
<pre><code>##                   elpd_diff se_diff
## fit_stroop_log        0.0       0.0
## fit_stroop_normal -1848.3     540.8</code></pre>
<p>Here, cross-validation shows a clear advantage for the model with the log-normal likelihood. Figure <a href="ch-cv.html#fig:diffpredacclog">16.2</a> visualizes the pointwise predictive accuracy.</p>
<div class="sourceCode" id="cb957"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb957-1"><a href="ch-cv.html#cb957-1" aria-hidden="true"></a>df_stroop &lt;-<span class="st"> </span>df_stroop <span class="op">%&gt;%</span></span>
<span id="cb957-2"><a href="ch-cv.html#cb957-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diff_elpd =</span> loo_stroop_log<span class="op">$</span>pointwise[, <span class="st">&quot;elpd_loo&quot;</span>] <span class="op">-</span></span>
<span id="cb957-3"><a href="ch-cv.html#cb957-3" aria-hidden="true"></a><span class="st">           </span>loo_stroop_normal<span class="op">$</span>pointwise[, <span class="st">&quot;elpd_loo&quot;</span>])</span>
<span id="cb957-4"><a href="ch-cv.html#cb957-4" aria-hidden="true"></a><span class="kw">ggplot</span>(df_stroop, <span class="kw">aes</span>(<span class="dt">x =</span> RT, <span class="dt">y =</span> diff_elpd)) <span class="op">+</span></span>
<span id="cb957-5"><a href="ch-cv.html#cb957-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.4</span>) <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;RC (ms)&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:diffpredacclog"></span>
<img src="bookdown_files/figure-html/diffpredacclog-1.svg" alt="The difference in predictive accuracy between a Stroop model with a log-normal likelihood and a model with a normal likelihood. A larger (more positive) difference indicates an advantage for the model with the log-normal likelihood." width="672" />
<p class="caption">
FIGURE 16.2: The difference in predictive accuracy between a Stroop model with a log-normal likelihood and a model with a normal likelihood. A larger (more positive) difference indicates an advantage for the model with the log-normal likelihood.
</p>
</div>
<p>Figure <a href="ch-cv.html#fig:diffpredacclog">16.2</a> shows that at first glance, the advantage of the log-normal likelihood seems to lie in being able to capture extremely slow observations. Figure <a href="ch-cv.html#fig:diffpredacclog2">16.3</a> zooms in to visualize the pointwise predictive accuracy for observations with response times smaller than two seconds.</p>
<div class="sourceCode" id="cb958"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb958-1"><a href="ch-cv.html#cb958-1" aria-hidden="true"></a><span class="kw">ggplot</span>(df_stroop, <span class="kw">aes</span>(<span class="dt">x =</span> RT, <span class="dt">y =</span> diff_elpd)) <span class="op">+</span></span>
<span id="cb958-2"><a href="ch-cv.html#cb958-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.3</span>) <span class="op">+</span></span>
<span id="cb958-3"><a href="ch-cv.html#cb958-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;RC (ms)&quot;</span>) <span class="op">+</span></span>
<span id="cb958-4"><a href="ch-cv.html#cb958-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></span>
<span id="cb958-5"><a href="ch-cv.html#cb958-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">2000</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:diffpredacclog2"></span>
<img src="bookdown_files/figure-html/diffpredacclog2-1.svg" alt="The difference in predictive accuracy between a Stroop model with a log-normal likelihood and a model with a normal likelihood for observations smaller than two seconds. A larger (more positive) difference indicates an advantage for the model with the log-normal likelihood." width="672" />
<p class="caption">
FIGURE 16.3: The difference in predictive accuracy between a Stroop model with a log-normal likelihood and a model with a normal likelihood for observations smaller than two seconds. A larger (more positive) difference indicates an advantage for the model with the log-normal likelihood.
</p>
</div>
<p>Figure <a href="ch-cv.html#fig:diffpredacclog2">16.3</a> suggests that the advantage of the log-normal likelihood seems to lie in being able to account for most of the observations in the data set, which occur around the <span class="math inline">\(500\)</span> ms.</p>
</div>
<div id="sec-issuesCV" class="section level2 hasAnchor" number="16.5">
<h2><span class="header-section-number">16.5</span> Issues with cross-validation<a href="ch-cv.html#sec-issuesCV" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="citation">Sivula, Magnusson, and Vehtari (<a href="#ref-sivula2020uncertainty" role="doc-biblioref">2020</a>)</span> analyzed the behavior of the  uncertainty estimate of the <span class="math inline">\(elpd\)</span> in typical situations. Although they focus on LOO-CV, the consequences are the same for K-fold-CV (and cross-validation in a non-Bayesian context). <span class="citation">Sivula, Magnusson, and Vehtari (<a href="#ref-sivula2020uncertainty" role="doc-biblioref">2020</a>)</span> identified three cases where the uncertainty estimates can perform badly:</p>
<ol style="list-style-type: decimal">
<li>The models make very similar predictions.</li>
<li>The number of observations is small.</li>
<li>The models are misspecified with outliers (influential extreme values) in the data.</li>
</ol>
<p>When the models make similar predictions (as it is the case with nested models in earlier model comparisons), and when there is not much difference in the predictive performance of the models, the uncertainty estimates will behave badly. In these situations, cross-validation is not very useful for separating very small effect sizes from zero effect sizes.
In addition, small differences in the predictive performance cannot reliably be detected by cross-validation if the number of observations is small. However, if the predictions are very similar, <span class="citation">Sivula, Magnusson, and Vehtari (<a href="#ref-sivula2020uncertainty" role="doc-biblioref">2020</a>)</span> show that the same problems persist even with a larger data set.</p>
<p>One of the issues that cross-validation methods face when they are used to compare nested models lies in the way that the exact <span class="math inline">\(elpd\)</span> is approximated: In cross-validation approximations, out-of-sample observations are used, which are not part of the model that was fit. Every time the predictive accuracy of an observation is evaluated, modeling assumptions are ignored. One of the weaknesses of cross-validation is the high variance in the approximation of the integral over the unknown true data distribution, <span class="math inline">\(p_t\)</span> <span class="citation">(Vehtari and Ojanen <a href="#ref-VehtariOjanen2012" role="doc-biblioref">2012</a>, sec. 4)</span>.</p>
<p>Cross-validation methods are sometimes criticized because when a lot of data are available, they will give undue preference to the complex model in comparison to a true simpler model <span class="citation">(Gronau and Wagenmakers <a href="#ref-gronauLimitationsBayesianLeaveOneOut2018" role="doc-biblioref">2018</a>)</span>. This might be true for toy examples using simulated data, where it is possible to obtain essentially unlimited observations, and where a model that is known to be wrong is compared with the true model.<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a> However, the problems that occur in practice are often very different: This is because the true model is unknown and very likely not even under consideration in the model comparison <span class="citation">(see Navarro <a href="#ref-navarroDevilDeepBlue2018" role="doc-biblioref">2019</a>)</span>. In our experience, in practical settings we are very far from the asymptotic behavior of cross-validation whereby it gives undue preference to a more complex model in comparison to a true simpler model. The main weakness of cross-validation is that, as mentioned above, modeling assumptions are ignored, which prevents it from selecting a more complex model rather than a simple one when there is only a modest gain in predictive fit <span class="citation">(Vehtari et al. <a href="#ref-vehtariLimitationsLimitationsBayesian2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>An alternative to the cross-validation approach discussed here for nested models is the  projection predictive method <span class="citation">(Piironen, Paasiniemi, and Vehtari <a href="#ref-Piironenetal2020" role="doc-biblioref">2020</a>)</span>. However, this approach (which is less general since it is valid only for  generalized linear models) has a somewhat different objective. In the projection predictive method, we first build the most complete predictive model, the  <em>reference model</em>, and then we look for a simpler model that gives as similar predictions as the reference model. The idea is that for a given complexity (number of predictors), the model with the smallest predictive discrepancy to the reference model should be selected. See <a href="https://github.com/stan-dev/projpred" class="uri">https://github.com/stan-dev/projpred</a> for an implementation of this approach. Thus this approach focuses on model simplification rather than on model comparison.</p>
<p>For models that are badly misspecified, the bias in the uncertainty makes their comparison unreliable as well. In this case, posterior predictive checks and possible model refinements are worth considering before carrying out model comparison.</p>
<p>If there is a large number of observations and the models under consideration are different enough from each other, the differences in predictive accuracy will dwarf the variance in the estimate of <span class="math inline">\(elpd\)</span>, and cross-validation can be very useful <span class="citation">(see also Piironen and Vehtari <a href="#ref-piironenComparisonBayesianPredictive2017" role="doc-biblioref">2017</a>)</span>. An example of this situation appeared in section <a href="ch-cv.html#sec-logcv">16.4</a>. When models are very different, one advantage of cross-validation methods in comparison with the Bayes factor is that the selection of priors is less critical in cross-validation. It is sometimes hard to decide on priors that encode our knowledge for one model, and this difficulty is exacerbated when we want to assign comparable prior information to models with a different number of parameters that might be on a different scale. Given that cross-validation methods are less sensitive to prior specification, different models can be compared on the same footing. Another difficulty with Bayes factors’ dependency on priors is that different researchers may disagree on the priors, and selecting an uninformative prior to encompass these different views as a middle ground is not a neutral option for the Bayes factor, while it will barely affect cross-validation. See <span class="citation">Nicenboim and Vasishth (<a href="#ref-nicenboimModelsRetrievalSentence2018" role="doc-biblioref">2018</a>)</span> for an example from psycholinguistics where K-fold-CV does help in distinguishing between models.</p>
</div>
<div id="cross-validation-in-stan" class="section level2 hasAnchor" number="16.6">
<h2><span class="header-section-number">16.6</span> Cross-validation in Stan<a href="ch-cv.html#cross-validation-in-stan" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>PSIS-LOO-CV and K-fold-CV can also be used with our Stan models, but care has to be taken to store the appropriate log-likelihood in the  <code>generated quantities</code> block.<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a></p>
<div id="psis-loo-cv-in-stan" class="section level3 hasAnchor" number="16.6.1">
<h3><span class="header-section-number">16.6.1</span>  PSIS-LOO-CV in Stan<a href="ch-cv.html#psis-loo-cv-in-stan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As explained earlier, PSIS-LOO (as implemented in the package <code>loo</code>) approximates the likelihood of the held-out data based on the observed data: it’s faster (because only one model is fit), and it only requires a minimal modification of the Stan code we need to fit a model. By default, Stan only saves the sum of the log likelihood of each observation (in the parameter <code>lp__</code>). The log-likelihood of each observation needs to be stored in the generated quantities block.</p>
<p>We revisit the model implemented in section <a href="ch-introstan.html#sec-interstan">10.4.2</a>, which was evaluated using the Bayes factor in section <a href="ch-bf.html#sec-stanBF">15.4</a>. Now, we want to compare the predictive performance of a model that assumes an effect of  attentional load on  pupil size against a similar model that assumes no effect. To do this, we assume the following likelihood:</p>
<p><span class="math display">\[\begin{equation}
p\_size_n \sim \mathit{Normal}(\alpha + c\_load_n \cdot \beta_1 + c\_trial \cdot \beta_2 + c\_load \cdot c\_trial \cdot \beta_3, \sigma)
\end{equation}\]</span></p>
<p>Define priors for all the <span class="math inline">\(\beta\)</span>’s as before:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(1000, 500) \\
\beta_{\{1,2,3\}} &amp;\sim \mathit{Normal}(0, 100) \\
\sigma &amp;\sim \mathit{Normal}_+(0, 1000)
\end{aligned}
\end{equation}\]</span></p>
<p>Prepare the data as in section <a href="ch-introstan.html#sec-interstan">10.4.2</a>:</p>
<div class="sourceCode" id="cb959"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb959-1"><a href="ch-cv.html#cb959-1" aria-hidden="true"></a>df_pupil &lt;-<span class="st"> </span>df_pupil <span class="op">%&gt;%</span></span>
<span id="cb959-2"><a href="ch-cv.html#cb959-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_load =</span> load <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(load),</span>
<span id="cb959-3"><a href="ch-cv.html#cb959-3" aria-hidden="true"></a>         <span class="dt">c_trial =</span> trial <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(trial))</span>
<span id="cb959-4"><a href="ch-cv.html#cb959-4" aria-hidden="true"></a>ls_pupil &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">c_load =</span> df_pupil<span class="op">$</span>c_load,</span>
<span id="cb959-5"><a href="ch-cv.html#cb959-5" aria-hidden="true"></a>                 <span class="dt">c_trial=</span> df_pupil<span class="op">$</span>c_trial,</span>
<span id="cb959-6"><a href="ch-cv.html#cb959-6" aria-hidden="true"></a>                 <span class="dt">p_size =</span> df_pupil<span class="op">$</span>p_size,</span>
<span id="cb959-7"><a href="ch-cv.html#cb959-7" aria-hidden="true"></a>                 <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil))</span></code></pre></div>
<p>Add a <code>generated quantities</code> block to the model shown below. (It is also possible to run this block in a stand-alone file with the <code>rstan</code> function <code>gqs()</code>). If the variable name  <code>log_lik</code> is used in the Stan code, the <code>loo</code> package will know where to find the log likelihood of the observations.</p>
<p>Code the effects as <code>beta1</code>, <code>beta2</code>, <code>beta3</code> to more easily compare the model with the one used in the BF chapter, but in this case we could have used a vector or an array instead. This is the model <code>pupil_cv.stan</code> shown below:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] c_load;
  vector[N] c_trial;
  vector[N] p_size;
}
parameters {
  real alpha;
  real beta1;
  real beta2;
  real beta3;
  real&lt;lower = 0&gt; sigma;
}
model {
  // priors including all constants
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta1 | 0, 100);
  target += normal_lpdf(beta2 | 0, 100);
  target += normal_lpdf(beta3 | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  target += normal_lpdf(p_size | alpha + c_load * beta1 +
                        c_trial * beta2 +
                        c_load .* c_trial * beta3, sigma);

}
generated quantities{
  array[N] real log_lik;
  for (n in 1:N){
    log_lik[n] = normal_lpdf(p_size[n] | alpha + c_load[n] * beta1 +
                             c_trial[n] * beta2 +
                             c_load[n] * c_trial[n] * beta3,
                             sigma);

  }
}</code></pre>
<p>For the null model, just omit the term with <code>beta1</code> in both the model block and the generated quantities block. This is the model <code>pupil_null.stan</code> shown below:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] c_load;
  vector[N] c_trial;
  vector[N] p_size;
}
parameters {
  real alpha;
  real beta2;
  real beta3;
  real&lt;lower = 0&gt; sigma;
}
model {
  target += normal_lpdf(alpha | 1000, 500);
  target += normal_lpdf(beta2 | 0, 100);
  target += normal_lpdf(beta3 | 0, 100);
  target += normal_lpdf(sigma | 0, 1000)
    - normal_lccdf(0 | 0, 1000);
  target += normal_lpdf(p_size | alpha + c_trial * beta2 +
                                 c_load .* c_trial * beta3, sigma);
}
generated quantities{
  array[N] real log_lik;
  for (n in 1:N){
    log_lik[n] = normal_lpdf(p_size[n] | alpha + c_trial[n] * beta2 +
                             c_load[n] * c_trial[n] * beta3, sigma);
  }
}</code></pre>
<p>The models can be found in the <code>bcogsci</code> package:</p>
<div class="sourceCode" id="cb962"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb962-1"><a href="ch-cv.html#cb962-1" aria-hidden="true"></a>pupil_cv &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb962-2"><a href="ch-cv.html#cb962-2" aria-hidden="true"></a>                        <span class="st">&quot;pupil_cv.stan&quot;</span>,</span>
<span id="cb962-3"><a href="ch-cv.html#cb962-3" aria-hidden="true"></a>                        <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb962-4"><a href="ch-cv.html#cb962-4" aria-hidden="true"></a>pupil_null &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb962-5"><a href="ch-cv.html#cb962-5" aria-hidden="true"></a>                          <span class="st">&quot;pupil_null.stan&quot;</span>,</span>
<span id="cb962-6"><a href="ch-cv.html#cb962-6" aria-hidden="true"></a>                          <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span></code></pre></div>
<p>Fit the models:</p>
<div class="sourceCode" id="cb963"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb963-1"><a href="ch-cv.html#cb963-1" aria-hidden="true"></a>fit_pupil_int_ll &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> pupil_cv,</span>
<span id="cb963-2"><a href="ch-cv.html#cb963-2" aria-hidden="true"></a>                         <span class="dt">iter =</span> <span class="dv">3000</span>,</span>
<span id="cb963-3"><a href="ch-cv.html#cb963-3" aria-hidden="true"></a>                         <span class="dt">data =</span> ls_pupil)</span>
<span id="cb963-4"><a href="ch-cv.html#cb963-4" aria-hidden="true"></a>fit_pupil_int_null_ll &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> pupil_null,</span>
<span id="cb963-5"><a href="ch-cv.html#cb963-5" aria-hidden="true"></a>                              <span class="dt">iter =</span> <span class="dv">3000</span>,</span>
<span id="cb963-6"><a href="ch-cv.html#cb963-6" aria-hidden="true"></a>                              <span class="dt">data =</span> ls_pupil)</span></code></pre></div>
<p>Show summary of predictive accuracy of the models using the function <code>loo</code>.</p>
<div class="sourceCode" id="cb964"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb964-1"><a href="ch-cv.html#cb964-1" aria-hidden="true"></a>(loo_int &lt;-<span class="st"> </span><span class="kw">loo</span>(fit_pupil_int_ll))</span></code></pre></div>
<pre><code>## 
## Computed from 6000 by 41 log-likelihood matrix.
## 
##          Estimate   SE
## elpd_loo   -251.3  5.4
## p_loo         5.5  1.6
## looic       502.6 10.8
## ------
## MCSE of elpd_loo is 0.1.
## MCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.4]).
## 
## All Pareto k estimates are good (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode" id="cb966"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb966-1"><a href="ch-cv.html#cb966-1" aria-hidden="true"></a>(loo_null &lt;-<span class="st"> </span><span class="kw">loo</span>(fit_pupil_int_null_ll))</span></code></pre></div>
<pre><code>## 
## Computed from 6000 by 41 log-likelihood matrix.
## 
##          Estimate  SE
## elpd_loo   -255.5 4.7
## p_loo         4.5 1.2
## looic       510.9 9.3
## ------
## MCSE of elpd_loo is 0.0.
## MCSE and ESS estimates assume MCMC draws (r_eff in [0.8, 1.2]).
## 
## All Pareto k estimates are good (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode" id="cb968"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb968-1"><a href="ch-cv.html#cb968-1" aria-hidden="true"></a><span class="kw">loo_compare</span>(loo_int, loo_null)</span></code></pre></div>
<pre><code>##        elpd_diff se_diff
## model1  0.0       0.0   
## model2 -4.2       3.5</code></pre>
<p>As it happened with the cloze probability effect in the previous section, we cannot decide which model has better predictive accuracy according to PSIS-LOO.</p>
<div id="k-fold-cv-in-stan" class="section level4 hasAnchor" number="16.6.1.1">
<h4><span class="header-section-number">16.6.1.1</span>  K-fold-CV in Stan<a href="ch-cv.html#k-fold-cv-in-stan" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To use K-fold-CV (or LOGO-CV) in Stan (as opposed to PSIS-LOO), we need to be careful to store the log-likelihood of the <em>held-out data</em>, since we evaluate our model with only this subset of the data. The following example closely follows the vignette <a href="https://cran.r-project.org/web/packages/loo/vignettes/loo2-elpd.html" class="uri">https://cran.r-project.org/web/packages/loo/vignettes/loo2-elpd.html</a>.</p>
<p>The steps taken are as follows:</p>
<ol style="list-style-type: decimal">
<li>Split the data in 10 folds.</li>
</ol>
<p>Since there is only one subject, we don’t need to stratify (using  <code>kfold_split_stratified()</code>), and we use  <code>kfold_split_random()</code> from the <code>loo</code> package.</p>
<div class="sourceCode" id="cb970"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb970-1"><a href="ch-cv.html#cb970-1" aria-hidden="true"></a>df_pupil<span class="op">$</span>fold &lt;-<span class="st"> </span><span class="kw">kfold_split_random</span>(<span class="dt">K =</span> <span class="dv">10</span>, <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil))</span>
<span id="cb970-2"><a href="ch-cv.html#cb970-2" aria-hidden="true"></a><span class="co"># Show number of obs for each fold:</span></span>
<span id="cb970-3"><a href="ch-cv.html#cb970-3" aria-hidden="true"></a>df_pupil <span class="op">%&gt;%</span></span>
<span id="cb970-4"><a href="ch-cv.html#cb970-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">group_by</span>(fold) <span class="op">%&gt;%</span></span>
<span id="cb970-5"><a href="ch-cv.html#cb970-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">count</span>() <span class="op">%&gt;%</span></span>
<span id="cb970-6"><a href="ch-cv.html#cb970-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">print</span>(<span class="dt">n=</span><span class="dv">10</span>)</span></code></pre></div>
<pre><code>## # A tibble: 10 × 2
## # Groups:   fold [10]
##     fold     n
##    &lt;int&gt; &lt;int&gt;
##  1     1     4
##  2     2     4
##  3     3     4
##  4     4     4
##  5     5     4
##  6     6     4
##  7     7     4
##  8     8     4
##  9     9     4
## 10    10     5</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Fit and extract the  log pointwise predictive densities for each fold.</li>
</ol>
<p>Compile the alternative and the null models first with  <code>stan_model</code>, and prepare two matrices to store the predictive densities from the held out data. Each matrix has as many rows as post-warmup iterations in the models fit above (<span class="math inline">\(1500 \times 4\)</span>), and as many columns as observations in the data set.</p>
<div class="sourceCode" id="cb972"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb972-1"><a href="ch-cv.html#cb972-1" aria-hidden="true"></a>pupil_stanmodel &lt;-<span class="st"> </span><span class="kw">stan_model</span>(pupil_cv)</span>
<span id="cb972-2"><a href="ch-cv.html#cb972-2" aria-hidden="true"></a>pupil_null_stanmodel &lt;-<span class="st"> </span><span class="kw">stan_model</span>(pupil_null)</span>
<span id="cb972-3"><a href="ch-cv.html#cb972-3" aria-hidden="true"></a>log_pd_kfold &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> <span class="dv">6000</span>, <span class="dt">ncol =</span> <span class="kw">nrow</span>(df_pupil))</span>
<span id="cb972-4"><a href="ch-cv.html#cb972-4" aria-hidden="true"></a>log_pd_null_kfold &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> <span class="dv">6000</span>, <span class="dt">ncol =</span> <span class="kw">nrow</span>(df_pupil))</span></code></pre></div>
<p>Next, loop over the 10 folds. Each loop carries out the following steps. First, fit each model (i.e., the alternative and null model) to all the observations except the ones belonging to the held-out fold using  <code>sampling()</code>; this uses the already-compiled models. Second, compute the log pointwise predictive densities for the held-out fold with  <code>gqs()</code>. This function produces <code>generated quantities</code> based on samples from a posterior (in the <code>draw</code> argument) and ignores all the blocks except <code>generated quantities</code>.<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a> Finally, store the predictive density for the observations of the held-out fold in a matrix by extracting the log likelihood of the held-out data. The output of this loop is a matrix of the log pointwise predictive densities of all the observations.</p>
<div class="sourceCode" id="cb973"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb973-1"><a href="ch-cv.html#cb973-1" aria-hidden="true"></a><span class="co"># Loop over the folds</span></span>
<span id="cb973-2"><a href="ch-cv.html#cb973-2" aria-hidden="true"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>){</span>
<span id="cb973-3"><a href="ch-cv.html#cb973-3" aria-hidden="true"></a>  <span class="co"># Training set for k</span></span>
<span id="cb973-4"><a href="ch-cv.html#cb973-4" aria-hidden="true"></a>  df_pupil_train &lt;-<span class="st"> </span>df_pupil <span class="op">%&gt;%</span></span>
<span id="cb973-5"><a href="ch-cv.html#cb973-5" aria-hidden="true"></a><span class="st">    </span><span class="kw">filter</span>(fold <span class="op">!=</span><span class="st"> </span>k)</span>
<span id="cb973-6"><a href="ch-cv.html#cb973-6" aria-hidden="true"></a>  ls_pupil_train &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">c_load =</span> df_pupil_train<span class="op">$</span>c_load,</span>
<span id="cb973-7"><a href="ch-cv.html#cb973-7" aria-hidden="true"></a>                         <span class="dt">c_trial=</span> df_pupil_train<span class="op">$</span>c_trial,</span>
<span id="cb973-8"><a href="ch-cv.html#cb973-8" aria-hidden="true"></a>                         <span class="dt">p_size =</span> df_pupil_train<span class="op">$</span>p_size,</span>
<span id="cb973-9"><a href="ch-cv.html#cb973-9" aria-hidden="true"></a>                         <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil_train))</span>
<span id="cb973-10"><a href="ch-cv.html#cb973-10" aria-hidden="true"></a>  <span class="co"># Held out set for k</span></span>
<span id="cb973-11"><a href="ch-cv.html#cb973-11" aria-hidden="true"></a>   df_pupil_ho &lt;-<span class="st"> </span>df_pupil <span class="op">%&gt;%</span></span>
<span id="cb973-12"><a href="ch-cv.html#cb973-12" aria-hidden="true"></a><span class="st">     </span><span class="kw">filter</span>(fold <span class="op">==</span><span class="st"> </span>k)</span>
<span id="cb973-13"><a href="ch-cv.html#cb973-13" aria-hidden="true"></a>  ls_pupil_ho &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">c_load =</span> df_pupil_ho<span class="op">$</span>c_load,</span>
<span id="cb973-14"><a href="ch-cv.html#cb973-14" aria-hidden="true"></a>                      <span class="dt">c_trial=</span> df_pupil_ho<span class="op">$</span>c_trial,</span>
<span id="cb973-15"><a href="ch-cv.html#cb973-15" aria-hidden="true"></a>                      <span class="dt">p_size =</span> df_pupil_ho<span class="op">$</span>p_size,</span>
<span id="cb973-16"><a href="ch-cv.html#cb973-16" aria-hidden="true"></a>                      <span class="dt">N =</span> <span class="kw">nrow</span>(df_pupil_ho))</span>
<span id="cb973-17"><a href="ch-cv.html#cb973-17" aria-hidden="true"></a>  <span class="co"># Train the models</span></span>
<span id="cb973-18"><a href="ch-cv.html#cb973-18" aria-hidden="true"></a>  fit_train &lt;-<span class="st"> </span><span class="kw">sampling</span>(pupil_stanmodel,</span>
<span id="cb973-19"><a href="ch-cv.html#cb973-19" aria-hidden="true"></a>                        <span class="dt">iter =</span> <span class="dv">3000</span>,</span>
<span id="cb973-20"><a href="ch-cv.html#cb973-20" aria-hidden="true"></a>                        <span class="dt">data =</span> ls_pupil_train)</span>
<span id="cb973-21"><a href="ch-cv.html#cb973-21" aria-hidden="true"></a>  fit_null_train &lt;-<span class="st"> </span><span class="kw">sampling</span>(pupil_null_stanmodel,</span>
<span id="cb973-22"><a href="ch-cv.html#cb973-22" aria-hidden="true"></a>                             <span class="dt">iter =</span> <span class="dv">3000</span>,</span>
<span id="cb973-23"><a href="ch-cv.html#cb973-23" aria-hidden="true"></a>                             <span class="dt">data =</span> ls_pupil_train)</span>
<span id="cb973-24"><a href="ch-cv.html#cb973-24" aria-hidden="true"></a>  <span class="co"># Generated quantities based on the posterior from the training set</span></span>
<span id="cb973-25"><a href="ch-cv.html#cb973-25" aria-hidden="true"></a>  <span class="co"># and the data from the held out set</span></span>
<span id="cb973-26"><a href="ch-cv.html#cb973-26" aria-hidden="true"></a>  gq_ho &lt;-<span class="st"> </span><span class="kw">gqs</span>(pupil_stanmodel,</span>
<span id="cb973-27"><a href="ch-cv.html#cb973-27" aria-hidden="true"></a>               <span class="dt">draws =</span> <span class="kw">as.matrix</span>(fit_train),</span>
<span id="cb973-28"><a href="ch-cv.html#cb973-28" aria-hidden="true"></a>               <span class="dt">data =</span> ls_pupil_ho)</span>
<span id="cb973-29"><a href="ch-cv.html#cb973-29" aria-hidden="true"></a>  gq_null_ho &lt;-<span class="st"> </span><span class="kw">gqs</span>(pupil_null_stanmodel,</span>
<span id="cb973-30"><a href="ch-cv.html#cb973-30" aria-hidden="true"></a>                    <span class="dt">draws =</span> <span class="kw">as.matrix</span>(fit_null_train),</span>
<span id="cb973-31"><a href="ch-cv.html#cb973-31" aria-hidden="true"></a>                    <span class="dt">data =</span> ls_pupil_ho)</span>
<span id="cb973-32"><a href="ch-cv.html#cb973-32" aria-hidden="true"></a>  <span class="co"># Extract log likelihood which represents</span></span>
<span id="cb973-33"><a href="ch-cv.html#cb973-33" aria-hidden="true"></a>  <span class="co"># the pointwise predictive density</span></span>
<span id="cb973-34"><a href="ch-cv.html#cb973-34" aria-hidden="true"></a>  log_pd_kfold[, df_pupil<span class="op">$</span>fold <span class="op">==</span><span class="st"> </span>k] &lt;-</span>
<span id="cb973-35"><a href="ch-cv.html#cb973-35" aria-hidden="true"></a><span class="st">    </span><span class="kw">extract_log_lik</span>(gq_ho)</span>
<span id="cb973-36"><a href="ch-cv.html#cb973-36" aria-hidden="true"></a>  log_pd_null_kfold[, df_pupil<span class="op">$</span>fold <span class="op">==</span><span class="st"> </span>k] &lt;-</span>
<span id="cb973-37"><a href="ch-cv.html#cb973-37" aria-hidden="true"></a><span class="st">    </span><span class="kw">extract_log_lik</span>(gq_null_ho)</span>
<span id="cb973-38"><a href="ch-cv.html#cb973-38" aria-hidden="true"></a>}</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Compute K-fold <span class="math inline">\(\widehat{elpd}\)</span>.</li>
</ol>
<p>Next, evaluate the predictive performance of the model on the 10 folds using  <code>elpd()</code>.</p>
<div class="sourceCode" id="cb974"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb974-1"><a href="ch-cv.html#cb974-1" aria-hidden="true"></a>(elpd_pupil_kfold &lt;-<span class="st"> </span><span class="kw">elpd</span>(log_pd_kfold))</span></code></pre></div>
<pre><code>## 
## Computed from 6000 by 41 log-likelihood matrix using the generic elpd function
## 
##      Estimate   SE
## elpd   -251.2  5.0
## ic      502.5 10.0</code></pre>
<div class="sourceCode" id="cb976"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb976-1"><a href="ch-cv.html#cb976-1" aria-hidden="true"></a>(elpd_pupil_null_kfold &lt;-<span class="st"> </span><span class="kw">elpd</span>(log_pd_null_kfold))</span></code></pre></div>
<pre><code>## 
## Computed from 6000 by 41 log-likelihood matrix using the generic elpd function
## 
##      Estimate  SE
## elpd   -255.9 4.6
## ic      511.8 9.1</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Compare the <span class="math inline">\(\widehat{elpd}\)</span> estimates.</li>
</ol>
<div class="sourceCode" id="cb978"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb978-1"><a href="ch-cv.html#cb978-1" aria-hidden="true"></a><span class="kw">loo_compare</span>(elpd_pupil_kfold, elpd_pupil_null_kfold)</span></code></pre></div>
<pre><code>##        elpd_diff se_diff
## model1  0.0       0.0   
## model2 -4.7       3.3</code></pre>
<p>As with PSIS-LOO, we cannot decide which model has better predictive accuracy according to K-fold-CV.</p>
</div>
</div>
</div>
<div id="summary-13" class="section level2 hasAnchor" number="16.7">
<h2><span class="header-section-number">16.7</span> Summary<a href="ch-cv.html#summary-13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we learned how to use K-fold cross-validation and leave-one-out cross-validation, using both built-in functionality in <code>brms</code> as well as Stan, in conjunction with the <code>loo</code> package. We saw an example of model comparison where cross-validation helped distinguish between the two models (log-normal vs. normal likelihood), and another example where no important differences were found between the models being compared (the N400 data with cloze probability as predictor). In general, cross-validation will be helpful when comparing rather different models <span class="citation">(for an example from psycholinguistics, see Nicenboim and Vasishth <a href="#ref-nicenboimModelsRetrievalSentence2018" role="doc-biblioref">2018</a>)</span>; when the models are highly similar, it will be difficult to distinguish between them. In particular, for typical psychology and linguistics data sets, it will be difficult to get conclusive results from model comparisons using cross-validation that aim to find evidence for the presence of a population-level (or fixed) effect, if the effect is very small and/or the data are relatively sparse (this is often the case, especially in psycholinguistic data). In such cases, if the aim is to find evidence for a theoretical claim, other model comparison methods like Bayes factors might be more meaningful.</p>
</div>
<div id="further-reading-13" class="section level2 hasAnchor" number="16.8">
<h2><span class="header-section-number">16.8</span> Further reading<a href="ch-cv.html#further-reading-13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A technical discussion about cross-validation methods can be found in Chapter 7 of <span class="citation">Gelman et al. (<a href="#ref-Gelman14" role="doc-biblioref">2014</a>)</span>.
For a discussion about the advantages and disadvantages of (leave-one-out) cross-validation, see <span class="citation">Gronau and Wagenmakers (<a href="#ref-gronauLimitationsBayesianLeaveOneOut2018" role="doc-biblioref">2018</a>)</span>, <span class="citation">Vehtari et al. (<a href="#ref-vehtariLimitationsLimitationsBayesian2019" role="doc-biblioref">2019</a>)</span> and <span class="citation">Gronau and Wagenmakers (<a href="#ref-gronauRejoinderMoreLimitations" role="doc-biblioref">2019</a>)</span>. A LOO glossary from the <code>loo</code> package can be found in (<a href="https://mc-stan.org/loo/reference/loo-glossary.html" class="uri">https://mc-stan.org/loo/reference/loo-glossary.html</a>). Cross-validation is still an active area of research, there are multiple websites and blog posts on this topic: Aki Vehtari, the creator of the <code>loo</code> package has a comprehensive FAQ about cross-validation in <a href="https://avehtari.github.io/modelselection/CV-FAQ.html" class="uri">https://avehtari.github.io/modelselection/CV-FAQ.html</a>; on Andrew Gelman’s blog, Vehtari also discusses the situations where cross-validation can be applied: <a href="https://statmodeling.stat.columbia.edu/2018/08/03/loo-cross-validation-approaches-valid/" class="uri">https://statmodeling.stat.columbia.edu/2018/08/03/loo-cross-validation-approaches-valid/</a>.</p>
</div>
<div id="exercises-3" class="section level2 hasAnchor" number="16.9">
<h2><span class="header-section-number">16.9</span> Exercises<a href="ch-cv.html#exercises-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:logcv" class="exercise"><strong>Exercise 16.1  </strong></span>Predictive accuracy of the linear and the logarithm effect of cloze probability.</p>
</div>
<p>Is there a difference in predictive accuracy between the model that incorporates a linear effect of cloze probability and one that incorporates log-transformed cloze probabilities?</p>
<div class="exercise">
<p><span id="exr:stroopcv" class="exercise"><strong>Exercise 16.2  </strong></span>Log-normal model</p>
</div>
<p>Use PSIS-LOO to compare a model of Stroop as the one in <a href="ch-complexstan.html#exr:stroop">11.1</a> with a model that assumes no population-level effect</p>
<ol style="list-style-type: lower-alpha">
<li>in <code>brms</code>.</li>
<li>in Stan.</li>
</ol>
<div class="exercise">
<p><span id="exr:logrec" class="exercise"><strong>Exercise 16.3  </strong></span>Log-normal vs rec-normal model in Stan</p>
</div>
<p>In section <a href="ch-custom.html#sec-change">12.1</a>, we proposed a reciprocal truncated normal distribution (rec-normal) to response times data, as an alternative to the log-normal distribution. The log-likelihood (of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>) of an individual observation, <span class="math inline">\(\mathit{RT}_{n}\)</span>, for the rec-normal distribution would be the following one.</p>
<p><span class="math display">\[\begin{equation}
\log \mathcal{L} = \log(\mathit{Normal}(1/\mathit{RT}_n | \mu, \sigma)) - 2 \cdot \log(\mathit{RT}_n)
\end{equation}\]</span></p>
<p>As explained in <a href="ch-custom.html#sec-change">12.1</a>, we obtain the log-likelihood based on all the <span class="math inline">\(N\)</span> observations by summing the log-likelihood of individual observations.</p>
<p><span class="math display">\[\begin{equation}
\log \mathcal{L} = \sum_n^N \log(\mathit{Normal}(1/\mathit{RT}_n | \mu, \sigma))  - \sum_n^N 2 \cdot \log(\mathit{RT}_n)
\end{equation}\]</span></p>
<p>Since these two models assume right-skewed data with only positive values, the question that we are interested in here is if we can really distinguish between them. Investigate this in the following way:</p>
<ol style="list-style-type: lower-alpha">
<li>Generate data (N = 100 and N = 1000) with a rec-normal distribution (e.g., <code>rt = 1 / rtnorm(N, mu, sigma, a = 0)</code>).</li>
<li>Generate data (N = 100 and N = 1000) with a log-normal distribution</li>
</ol>
<p>Fit a rec-normal and a log-normal model using Stan to each of the four data sets, and use PSIS-LOO to compare the models.</p>
<p>What do you conclude?</p>

</div>
</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references hanging-indent">
<div id="ref-bernardosmith">
<p>Bernardo, José M., and Adrian F. M. Smith. 2009. <em>Bayesian Theory</em>. Vol. 405. John Wiley &amp; Sons.</p>
</div>
<div id="ref-GeisserEddy1979">
<p>Geisser, Seymour, and William F. Eddy. 1979. “A Predictive Approach to Model Selection.” <em>Journal of the American Statistical Association</em> 74 (365): 153–60.</p>
</div>
<div id="ref-Gelman14">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2014. <em>Bayesian Data Analysis</em>. Third Edition. Boca Raton, FL: Chapman; Hall/CRC Press.</p>
</div>
<div id="ref-GneitingRaftery2007">
<p>Gneiting, Tilmann, and Adrian E. Raftery. 2007. “Strictly Proper Scoring Rules, Prediction, and Estimation.” <em>Journal of the American Statistical Association</em> 102 (477): 359–78. <a href="https://doi.org/10.1198/016214506000001437">https://doi.org/10.1198/016214506000001437</a>.</p>
</div>
<div id="ref-Good1952">
<p>Good, I. J. 1952. “Rational Decisions.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 14 (1): 107–14. <a href="http://www.jstor.org/stable/2984087">http://www.jstor.org/stable/2984087</a>.</p>
</div>
<div id="ref-gronauLimitationsBayesianLeaveOneOut2018">
<p>Gronau, Quentin F., and Eric-Jan Wagenmakers. 2018. “Limitations of Bayesian Leave-One-Out Cross-Validation for Model Selection.” <em>Computational Brain &amp; Behavior</em>. <a href="https://doi.org/10.1007/s42113-018-0011-7">https://doi.org/10.1007/s42113-018-0011-7</a>.</p>
</div>
<div id="ref-gronauRejoinderMoreLimitations">
<p>Gronau, Quentin F., and Eric-Jan Wagenmakers. 2018. “Limitations of Bayesian Leave-One-Out Cross-Validation for Model Selection.” <em>Computational Brain &amp; Behavior</em>. <a href="https://doi.org/10.1007/s42113-018-0011-7">https://doi.org/10.1007/s42113-018-0011-7</a>.</p> 2019. “Rejoinder: More Limitations of Bayesian Leave-One-Out Cross-Validation.” <em>Computational Brain &amp; Behavior</em> 2 (1): 35–47. <a href="https://doi.org/%20https://doi.org/10.1007/s42113-018-0022-4">https://doi.org/ https://doi.org/10.1007/s42113-018-0022-4</a>.</p>
</div>
<div id="ref-lunn2012bugs">
<p>Lunn, David J., Chris Jackson, David J. Spiegelhalter, Nichola G. Best, and Andrew Thomas. 2012. <em>The BUGS Book: A Practical Introduction to Bayesian Analysis</em>. Vol. 98. CRC Press.</p>
</div>
<div id="ref-navarroDevilDeepBlue2018">
<p>Navarro, Danielle J. 2019. “Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection.” <em>Computational Brain &amp; Behavior</em> 2 (1): 28–34. <a href="https://doi.org/10.1007/s42113-018-0019-z">https://doi.org/10.1007/s42113-018-0019-z</a>.</p>
</div>
<div id="ref-nicenboimModelsRetrievalSentence2018">
<p>Nicenboim, Bruno, and Shravan Vasishth. 2018. “Models of Retrieval in Sentence Comprehension: A Computational Evaluation Using Bayesian Hierarchical Modeling.” <em>Journal of Memory and Language</em> 99: 1–34. <a href="https://doi.org/10.1016/j.jml.2017.08.004">https://doi.org/10.1016/j.jml.2017.08.004</a>.</p>
</div>
<div id="ref-Paananen_2021">
<p>Paananen, Topi, Juho Piironen, Paul-Christian Bürkner, and Aki Vehtari. 2021. “Implicitly Adaptive Importance Sampling.” <em>Statistics and Computing</em> 31 (2). <a href="https://doi.org/10.1007/s11222-020-09982-2">https://doi.org/10.1007/s11222-020-09982-2</a>.</p>
</div>
<div id="ref-Piironenetal2020">
<p>Piironen, Juho, Markus Paasiniemi, and Aki Vehtari. 2020. “Projective inference in high-dimensional problems: Prediction and feature selection.” <em>Electronic Journal of Statistics</em> 14 (1): 2155–97. <a href="https://doi.org/10.1214/20-EJS1711">https://doi.org/10.1214/20-EJS1711</a>.</p>
</div>
<div id="ref-piironenComparisonBayesianPredictive2017">
<p>Piironen, Juho, and Aki Vehtari. 2017. “Comparison of Bayesian Predictive Methods for Model Selection.” <em>Statistics and Computing</em> 27 (3): 711–35. <a href="https://doi.org/10.1007/s11222-016-9649-y">https://doi.org/10.1007/s11222-016-9649-y</a>.</p>
</div>
<div id="ref-sivula2020uncertainty">
<p>Sivula, Tuomas, Måns Magnusson, and Aki Vehtari. 2020. “Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison.” <em>arXiv Preprint arXiv:2008.10296</em>.</p>
</div>
<div id="ref-FAQCV">
<p>Vehtari, Aki. 2022. “Cross-validation FAQ.” <a href="https://web.archive.org/web/20221219223947/https://avehtari.github.io/modelselection/CV-FAQ.html">https://web.archive.org/web/20221219223947/https://avehtari.github.io/modelselection/CV-FAQ.html</a>.</p>
</div>
<div id="ref-vehtariPracticalBayesianModel2017">
<p>Vehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017b. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC.” <em>Statistics and Computing</em> 27 (5): 1413–32. <a href="https://doi.org/10.1007/s11222-016-9696-4">https://doi.org/10.1007/s11222-016-9696-4</a>.</p>
</div>
<div id="ref-VehtariOjanen2012">
<p>Vehtari, Aki, and Janne Ojanen. 2012. “A Survey of Bayesian Predictive Methods for Model Assessment, Selection and Comparison.” <em>Statistical Surveys</em> 6 (0): 142–228. <a href="https://doi.org/10.1214/12-ss102">https://doi.org/10.1214/12-ss102</a>.</p>
</div>
<div id="ref-VehtariGelman2015Pareto">
<p>Vehtari, Aki, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry. 2024. “Pareto Smoothed Importance Sampling.” <a href="http://arxiv.org/abs/1507.02646">http://arxiv.org/abs/1507.02646</a>.</p>
</div>
<div id="ref-vehtariLimitationsLimitationsBayesian2019">
<p>Vehtari, Aki, Daniel P. Simpson, Yuling Yao, and Andrew Gelman. 2019. “Limitations of ‘Limitations of Bayesian Leave-One-Out Cross-Validation for Model Selection’.” <em>Computational Brain &amp; Behavior</em> 2 (1): 22–27. <a href="https://doi.org/10.1007/s42113-018-0020-6">https://doi.org/10.1007/s42113-018-0020-6</a>.</p>
</div>
<div id="ref-venablesripley">
<p>Venables, William N., and Brian D. Ripley. 2002. <em>Modern Applied Statistics with S-PLUS</em>. New York: Springer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="55">
<li id="fn55"><p>Maximizing the <span class="math inline">\(elpd\)</span> in Equation <a href="ch-cv.html#eq:elpd">(16.4)</a> is also equivalent to minimizing the  Kullback–Leibler (KL) divergence from the true data generating distribution <span class="math inline">\(p_t(y_{pred})\)</span> to the <em>posterior predictive distribution</em> of the candidate model <span class="math inline">\(\mathcal{M}_1\)</span>.<a href="ch-cv.html#fnref55" class="footnote-back">↩︎</a></p></li>
<li id="fn56"><p>The  double use of the data is also a problem when one relies on information criteria like the  Akaike Information Criterion (AIC) or the  Bayesian Information Criterion (BIC).<a href="ch-cv.html#fnref56" class="footnote-back">↩︎</a></p></li>
<li id="fn57"><p>An alternative approach is to use the  model matching approximation for problematic observations <span class="citation">(Paananen et al. <a href="#ref-Paananen_2021" role="doc-biblioref">2021</a>)</span>, by setting  <code>moment_match = TRUE</code> in the <code>loo()</code> call (and we also need to fit the model with <code>save_pars = save_pars(all = TRUE)</code>). In this particular case, this approximation won’t solve our problem.<a href="ch-cv.html#fnref57" class="footnote-back">↩︎</a></p></li>
<li id="fn58"><p>If the true model is under consideration among the models being compared, we are under an <span class="math inline">\(M_{closed}\)</span> scenario. However, this is rarely realistic. The most common case is an <span class="math inline">\(M_{open}\)</span> scenario <span class="citation">(Bernardo and Smith <a href="#ref-bernardosmith" role="doc-biblioref">2009</a>)</span>, where the true model is not included in the set of models being compared.<a href="ch-cv.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p>This means the sampling notation should not be used in this block (see Box <a href="ch-introstan.html#thm:tilde">10.2</a>).<a href="ch-cv.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p>The reader using <code>cmdstanr</code> rather than <code>rstan</code> might find a cryptic error here. This is because <code>cmdstanr</code> expects the parameters not to change. A workaround can be found in <a href="https://discourse.mc-stan.org/t/generated-quantities-returns-error-mismatch-between-model-and-fitted-parameters-csv-file/17869/15" class="uri">https://discourse.mc-stan.org/t/generated-quantities-returns-error-mismatch-between-model-and-fitted-parameters-csv-file/17869/15</a><a href="ch-cv.html#fnref60" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-bf.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-cogmod.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
