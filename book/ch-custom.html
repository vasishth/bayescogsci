<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Custom distributions in Stan | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Custom distributions in Stan | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Custom distributions in Stan | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2024-08-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-complexstan.html"/>
<link rel="next" href="ch-remame.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-and-what-is-its-target-audience"><i class="fa fa-check"></i>Why read this book, and what is its target audience?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b>  Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The  law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b>  Discrete random variables: An example using the  binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b>  Continuous random variables: An example using the  normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1:  Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2:  Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate  simulated bivariate  (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The  marginal likelihood  (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b>  Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the  posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a  likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a  prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using  Bayes’ rule to compute the  posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The  posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the  posterior through  sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b>  Bayesian Regression Models using Stan:  brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b>  Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors:  sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b>  Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b>  Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b>  Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b>  Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b>  Posterior predictive distribution</a></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#the-influence-of-the-likelihood"><i class="fa fa-check"></i><b>3.7</b> The influence of the likelihood</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.7.1</b> The  log-normal likelihood</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.7.2</b> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.8</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.10</b> Further reading</a></li>
<li class="chapter" data-level="3.11" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first  linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b>  Likelihood and  priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b>  Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The  <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b>  Logistic regression: Does  set size affect  free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b>  Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b>  Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b>  No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b>  Varying intercepts and  varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A  hierarchical log-normal model: The  Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes  log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of  Prior Elicitation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English  relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the  variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b>  Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from  meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’  posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
<li class="chapter" data-level="6.7" data-path="ch-priors.html"><a href="ch-priors.html#sec-priorsexercises"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#building-a-model"><i class="fa fa-check"></i><b>7.1</b>  Building a model</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-to-ask-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions to ask on a model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#checking-whether-assumptions-are-consistent-with-domain-expertise-prior-predictive-checks"><i class="fa fa-check"></i><b>7.2.1</b>  Checking whether assumptions are consistent with  domain expertise: Prior predictive checks</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#testing-for-correct-posterior-approximations-checks-of-computational-faithfulness"><i class="fa fa-check"></i><b>7.2.2</b>  Testing for correct posterior approximations: Checks of computational faithfulness</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#sensitivity-of-the-model"><i class="fa fa-check"></i><b>7.2.3</b>  Sensitivity of the model</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#does-the-model-adequately-capture-the-dataposterior-predictive-checks"><i class="fa fa-check"></i><b>7.2.4</b>  Does the model adequately capture the data?–Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b>  Contrast coding</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding:  Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b>  Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b>  Cell means parameterization and  posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b>  Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The  hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The  <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor of four levels</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b>  Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b>  Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or  model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b>  Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts:  Monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b>  Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b>  Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the  intercept in  non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-6"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial  <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b>  Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b>  Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one  covariate</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a  group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-7"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan:  Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example:  Cloze probability with Stan with the  binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b>  Regression models in Stan</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first  linear regression in Stan: Does attentional load affect  pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b>  Interactions in Stan: Does attentional load interact with trial number affecting  pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b>  Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-8"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Hierarchical models and reparameterization </a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated  varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b>  Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-9"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b>  Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The  simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> An example where simulation-based calibration reveals a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues with and limitations of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#another-custom-distribution-the-exponential-distribution-implemented-manually"><i class="fa fa-check"></i><b>12.3</b> Another  custom distribution: The exponential distribution  implemented manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-10"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Evidence synthesis and measurements with error</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b>  Meta-analysis and  measurement error models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b>  Measurement-error models</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-11"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#prior-predictive-vs.-posterior-predictive-model-comparison"><i class="fa fa-check"></i><b>14.1</b> Prior predictive vs. posterior predictive model comparison</a></li>
<li class="chapter" data-level="14.2" data-path="ch-comparison.html"><a href="ch-comparison.html#some-important-points-to-consider-when-comparing-models"><i class="fa fa-check"></i><b>14.2</b> Some important points to consider when comparing models</a></li>
<li class="chapter" data-level="14.3" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b>  Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b>  Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and  accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
<li class="chapter" data-level="15.5.3" data-path="ch-bf.html"><a href="ch-bf.html#sec-caution"><i class="fa fa-check"></i><b>15.5.3</b> A cautionary note about Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#sample-size-determination-using-bayes-factors"><i class="fa fa-check"></i><b>15.6</b> Sample size determination using Bayes factors</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#summary-12"><i class="fa fa-check"></i><b>15.7</b> Summary</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.8</b> Further reading</a></li>
<li class="chapter" data-level="15.9" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#the-expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> The expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b>  Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#sec-issuesCV"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b>  PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-13"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to cognitive modeling</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#what-characterizes-a-computational-cognitive-model"><i class="fa fa-check"></i><b>17.1</b> What characterizes a computational cognitive model?</a></li>
<li class="chapter" data-level="17.2" data-path="ch-cogmod.html"><a href="ch-cogmod.html#some-advantages-of-taking-the-latent-variable-modeling-approach"><i class="fa fa-check"></i><b>17.2</b> Some advantages of taking the latent-variable modeling approach</a></li>
<li class="chapter" data-level="17.3" data-path="ch-cogmod.html"><a href="ch-cogmod.html#types-of-computational-cognitive-model"><i class="fa fa-check"></i><b>17.3</b> Types of computational cognitive model</a></li>
<li class="chapter" data-level="17.4" data-path="ch-cogmod.html"><a href="ch-cogmod.html#summary-14"><i class="fa fa-check"></i><b>17.4</b> Summary</a></li>
<li class="chapter" data-level="17.5" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling  multiple categorical responses</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT model assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A  hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#summary-15"><i class="fa fa-check"></i><b>18.3</b> Summary</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.4</b> Further reading</a></li>
<li class="chapter" data-level="18.5" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A  multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A  hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-16"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with  contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-17"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="ch-closing.html"><a href="ch-closing.html"><i class="fa fa-check"></i><b>21</b> In closing</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-custom" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Chapter 12</span> Custom distributions in Stan<a href="ch-custom.html#ch-custom" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p></p>
<p>Stan includes a large number of distributions, but what happens if we need a distribution that is not provided? In many cases, we can simply build a custom distribution by combining the ever-growing number of functions available in the Stan language.</p>
<div id="sec-change" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> A change of variables with the reciprocal normal distribution<a href="ch-custom.html#sec-change" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In previous chapters, when faced with response times, we assumed a log-normal distribution. The log-normal distribution moves the inferences relating to the location parameter into a multiplicative frame (see Box <a href="ch-reg.html#thm:lognormal">4.3</a>). Another alternative, however, is to assume a  “reciprocal”-normal distribution of response times. This is referred to hereafter as rec-normal in the text, and as the <span class="math inline">\(\mathit{RecNormal}\)</span> distribution in equations. That is, we may want to assume that the reciprocal of the response times are normally distributed. This assumption may be theoretically motivated <span class="citation">(Harris et al. <a href="#ref-HarrisEtAl2014" role="doc-biblioref">2014</a>; Harris and Waddington <a href="#ref-Harris2012" role="doc-biblioref">2012</a>)</span> or it may arise from the application of the Box-Cox variance stabilizing transform procedure <span class="citation">(Box and Cox <a href="#ref-box1964analysis" role="doc-biblioref">1964</a>)</span>. An example from psycholinguistics of a data analysis with a reciprocal transform of reading time data appears in <span class="citation">Wu, Kaiser, and Vasishth (<a href="#ref-WuKaiserVasishth2017" role="doc-biblioref">2017</a>)</span>.</p>
<p><span class="math display" id="eq:recip">\[\begin{equation}
\begin{aligned}
1/y &amp;\sim \mathit{Normal}(\mu, \sigma) \\
y &amp;\sim \mathit{RecNormal}(\mu, \sigma)
\end{aligned}
\tag{12.1}
\end{equation}\]</span></p>
<p>An interesting aspect of the  rec-normal is that it affords an interpretation of the location parameter in terms of  rate or speed rather than time. Analogously to the case of the log-normal, neither the location <span class="math inline">\(\mu\)</span> nor the scale <span class="math inline">\(\sigma\)</span> are in the same scale as the dependent variable <span class="math inline">\(y\)</span>. These parameters are in the same scale as the transformed dependent variable (here, <span class="math inline">\(1/y\)</span>) that is assumed to be normally distributed.</p>
<p>By setting <span class="math inline">\(\mu = 0.002\)</span> and <span class="math inline">\(\sigma = 0.0004\)</span>, data can be generated from the rec-normal that looks right-skewed and not unlike a distribution of response times. The code below shows some summary statistics of the generated data.</p>
<div class="sourceCode" id="cb762"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb762-1"><a href="ch-custom.html#cb762-1" aria-hidden="true"></a>N &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb762-2"><a href="ch-custom.html#cb762-2" aria-hidden="true"></a>mu &lt;-<span class="st"> </span><span class="fl">0.002</span></span>
<span id="cb762-3"><a href="ch-custom.html#cb762-3" aria-hidden="true"></a>sigma &lt;-<span class="st"> </span><span class="fl">0.0004</span></span>
<span id="cb762-4"><a href="ch-custom.html#cb762-4" aria-hidden="true"></a>rt &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">rnorm</span>(N, mu, sigma)</span>
<span id="cb762-5"><a href="ch-custom.html#cb762-5" aria-hidden="true"></a><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(rt), <span class="dt">sd =</span> <span class="kw">sd</span>(rt), <span class="dt">min =</span> <span class="kw">min</span>(rt), <span class="dt">max =</span> <span class="kw">max</span>(rt))</span></code></pre></div>
<pre><code>## mean   sd  min  max 
##  527  122  319 1020</code></pre>
<p>Figure <a href="ch-custom.html#fig:recnormalhist">12.1</a> shows the distribution generated with the code shown above.</p>

<div class="figure"><span style="display:block;" id="fig:recnormalhist"></span>
<img src="bookdown_files/figure-html/recnormalhist-1.svg" alt="Distribution of synthetic data with a rec-normal distribution with parameters \(\mu = 0.002\) and \(\sigma =0.0004\)." width="672" />
<p class="caption">
FIGURE 12.1: Distribution of synthetic data with a rec-normal distribution with parameters <span class="math inline">\(\mu = 0.002\)</span> and <span class="math inline">\(\sigma =0.0004\)</span>.
</p>
</div>
<p>We can fit the rec-normal distribution to the response times in a simple model with a normal likelihood, by storing the reciprocal of the  response times (<code>1/RT</code>) in the vector variable <code>recRT</code> (rather than storing the “raw” response times):</p>
<pre><code>model {
\\ priors go here
target += normal_lpdf(recRT | mu, sigma); 
}</code></pre>
<p>One issue here is that the parameters of the likelihood, <code>mu</code> and <code>sigma</code> are going to be very far away from the  unit scale (<span class="math inline">\(0.002\)</span> and <span class="math inline">\(0.0004\)</span> respectively). Due to the way Stan’s sampler is built, parameters that are too small (much smaller than 1) or too large (much larger than 1) can cause convergence problems. A straightforward solution is to fit the normal distribution to parameters in reciprocal of seconds rather than milliseconds; this would make the parameters have values that are <span class="math inline">\(1000\)</span> times larger (<span class="math inline">\(2\)</span> and <span class="math inline">\(0.4\)</span> respectively). We can do this using the  <code>transformed parameters</code> block.<a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a> Although we use <code>mu</code> and <code>sigma</code> to fit the data, the priors are defined on the parameters <code>mu_s</code> and <code>sigma_s</code> (<span class="math inline">\(\mu_s\)</span> and <span class="math inline">\(\sigma_s\)</span>).</p>
<p>Unless one can rely on previous estimates, finding good priors for <span class="math inline">\(\mu_s\)</span> and <span class="math inline">\(\sigma_s\)</span> is not trivial. To define appropriate priors, we would need to start with relatively arbitrary priors, inspect the prior predictive distributions, adjust the priors, and repeat the inspection of prior predictive distributions until these distributions start to look realistic. In the interest of conserving space, we skip this iterative process here, and assign the following priors:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mu_s &amp; \sim  \mathit{Normal}(2, 1)\\
\sigma_s &amp; \sim \mathit{Normal}_+(0.4, 0.2)
\end{aligned}
\end{equation}\]</span></p>
<p>This model is implemented in the file <code>normal_recrt.stan</code>, available in the <code>bcogsci</code> package:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] recRT;
}
parameters {
  real mu_s;
  real&lt;lower = 0&gt; sigma_s;
}
transformed parameters {
  real mu = mu_s / 1000;
  real sigma = sigma_s / 1000;
}
model {
  target += normal_lpdf(mu_s | 2, 1);
  target += normal_lpdf(sigma_s | 0.4, 0.2) +
            - normal_lccdf(0 | 0.4, 0.2);
  target += normal_lpdf(recRT | mu, sigma);
}</code></pre>
<p>Fit and display the summary of the previous model:</p>
<div class="sourceCode" id="cb766"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb766-1"><a href="ch-custom.html#cb766-1" aria-hidden="true"></a>normal_recrt &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb766-2"><a href="ch-custom.html#cb766-2" aria-hidden="true"></a>                            <span class="st">&quot;normal_recrt.stan&quot;</span>,</span>
<span id="cb766-3"><a href="ch-custom.html#cb766-3" aria-hidden="true"></a>                            <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb766-4"><a href="ch-custom.html#cb766-4" aria-hidden="true"></a>fit_rec &lt;-<span class="st"> </span><span class="kw">stan</span>(normal_recrt, <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> N, <span class="dt">recRT =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>rt))</span></code></pre></div>
<div class="sourceCode" id="cb767"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb767-1"><a href="ch-custom.html#cb767-1" aria-hidden="true"></a><span class="kw">print</span>(fit_rec, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;mu&quot;</span>, <span class="st">&quot;sigma&quot;</span>), <span class="dt">digits =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##         mean   2.5%  97.5% n_eff Rhat
## mu    0.0020 0.0019 0.0021  3878    1
## sigma 0.0004 0.0004 0.0005  3687    1</code></pre>
<p>Is a rec-normal likelihood more appropriate than a log-normal likelihood? As things stand, we cannot compare the models with these two likelihoods. This is because the dependent variables are different: we cannot compare reciprocal response times with untransformed response times on the millisecond scale.  Model comparison with the Bayes factor or cross-validation can only compare models with the same dependent variables; see chapters <a href="ch-comparison.html#ch-comparison">14</a>-<a href="ch-cv.html#ch-cv">16</a>.</p>
<p>If we do want to compare the reciprocal-normal likelihood and the log-normal likelihood, we have to set up the models with the two likelihoods in such a way that the dependent measure is on the raw millisecond scale in each model. This means that for the reciprocal normal likelihood, the model will receive as data raw reading times in milliseconds, and these will be treated as a transformed random variable from reciprocal reading times. This approach is discussed next, but requires knowledge of the Jacobian adjustment (see <span class="citation">Ross (<a href="#ref-RossProb" role="doc-biblioref">2002</a>)</span>).</p>
<div id="scaling-a-probability-density-with-the-jacobian-adjustment" class="section level3 hasAnchor" number="12.1.1">
<h3><span class="header-section-number">12.1.1</span> Scaling a probability density with the Jacobian adjustment<a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To work with the original dependent variable (<code>RT</code> rather than <code>1/RT</code>) we need to perform a  <em>change of variables</em>: If the random variable X represents the reciprocal reading times (<span class="math inline">\(1/RT\)</span>), we can transform this random variable to a new one, <span class="math inline">\(Y = 1/X = 1/(1/RT) = RT\)</span>, yielding a transformed random variable <span class="math inline">\(Y\)</span> which represents reading time.</p>
<p>This change of variables requires an adjustment to the (unnormalized log) posterior probability to account for the distortion caused by the transform.<a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a> The probability must be scaled by a  <em>Jacobian</em> adjustment, which, in the univariate case as this one, is the absolute value of the derivative of the transform.<a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a> The absolute value of any number is represented by enclosing it in two vertical bars: e.g., <span class="math inline">\(|-2| = 2\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p(\mathit{RT}_n | \mu, \sigma )&amp; = \mathit{Normal}(1/\mathit{RT}_n | \mu, \sigma) \left| \frac{d}{d\mathit{RT}_n} 1/\mathit{RT}_n \right| = \\
&amp; \mathit{Normal}(1/\mathit{RT}_n | \mu, \sigma) \cdot \left| - 1/\mathit{RT}_n^2 \right| = \\
&amp; \mathit{Normal}(1/\mathit{RT}_n | \mu, \sigma) \cdot  1/\mathit{RT}_n^2 
 \end{aligned}
\end{equation}\]</span></p>
<p>If we omit the Jacobian adjustment, we are essentially fitting a different and unnormalized probability density function (this will be shown later in this chapter). The discrepancy between the correct and incorrect posterior would depend on how large the Jacobian adjustment for our model is.</p>
<p>Because Stan works in log space, rather than multiplying the Jacobian adjustment, we add its logarithm to the log-probability density (<span class="math inline">\(\log(1/\mathit{RT}_n^2) = - 2 \cdot \log(\mathit{RT}_n)\)</span>). The contribution to the log likelihood (of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>) with its adjustment based from an individual observation, <span class="math inline">\(\mathit{RT}_{n}\)</span>, would be as follows.</p>
<p><span class="math display">\[\begin{equation}
\log(\mathit{Normal}(1/\mathit{RT}_n | \mu, \sigma)) - 2 \cdot \log(\mathit{RT}_n) 
\end{equation}\]</span></p>
<p>We obtain the log likelihood based on all the <span class="math inline">\(N\)</span> observations by summing the log likelihood of individual observations.</p>
<p><span class="math display">\[\begin{equation}
\log \mathcal{L} = \sum_{n=1}^N \log(\mathit{Normal}(1/\mathit{RT}_n | \mu, \sigma))  - \sum_{n=1}^N 2 \cdot \log(\mathit{RT}_n) 
\end{equation}\]</span></p>
<p>In Stan, this summing up is done as follows. The function <code>normal_lpdf</code> applied to a vector already returns the sum of the individual log-probability densities. The Jacobian adjustment <code>2*log(RT)</code>, which returns a vector of values, has to be summed up and added in manually (the term therefore becomes <code>-sum(2*log(RT))</code>).</p>
<pre><code>target += normal_lpdf(1 ./ RT | mu, sigma) 
          - sum(2 * log(RT));</code></pre>
<p>Before fitting the model with the change of variables, we are going to truncate the distribution.
We didn’t encounter negative values in our synthetic data, but this was because the distribution was not too spread out; that is, the scale was much smaller than the location (<span class="math inline">\(\sigma &lt;&lt; \mu\)</span>). However, in principle we could end up generating negative values. For this reason, we truncate the underlying normal distribution (for more details, see Box <a href="ch-reg.html#thm:truncation">4.1</a>). The  reciprocal truncated normal distribution has been argued to be an appropriate model of  response times, neural  inter-spike intervals, and  latency distributions of saccades in a simple optimality model in which reward is maximized to yield an optimal response rate <span class="citation">(Harris et al. <a href="#ref-HarrisEtAl2014" role="doc-biblioref">2014</a>; Harris and Waddington <a href="#ref-Harris2012" role="doc-biblioref">2012</a>)</span>.</p>
<p>Because we have <span class="math inline">\(N\)</span> observations, the truncation consists of adding <code>- N * normal_lccdf(0 | mu, sigma)</code> to <code>target</code>; also see section <a href="ch-introstan.html#sec-pupilstan">10.4.1</a>. The complete model, <code>recnormal_rt.stan</code>, including the truncation is as follows:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] RT;
}
parameters {
  real mu_s;
  real&lt;lower = 0&gt; sigma_s;
}
transformed parameters {
  real mu = mu_s / 1000;
  real sigma = sigma_s / 1000;
}
model {
  target += normal_lpdf(mu_s | 2, 1);
  target += normal_lpdf(sigma_s | 0.4, 0.2)
            - normal_lccdf(0 | 0.4, 0.2);
  target += normal_lpdf(1 ./ RT | mu, sigma)
     - N * normal_lccdf(0 | mu, sigma)
    - sum(2 * log(RT));
}</code></pre>
<p>Next, generate data from a reciprocal truncated normal distribution:</p>
<div class="sourceCode" id="cb771"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb771-1"><a href="ch-custom.html#cb771-1" aria-hidden="true"></a>N &lt;-<span class="st"> </span><span class="dv">500</span></span>
<span id="cb771-2"><a href="ch-custom.html#cb771-2" aria-hidden="true"></a>mu &lt;-<span class="st"> </span><span class="fl">.002</span></span>
<span id="cb771-3"><a href="ch-custom.html#cb771-3" aria-hidden="true"></a>sigma &lt;-<span class="st"> </span><span class="fl">.0004</span></span>
<span id="cb771-4"><a href="ch-custom.html#cb771-4" aria-hidden="true"></a>rt &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">rtnorm</span>(N, mu, sigma, <span class="dt">a =</span> <span class="dv">0</span>) </span></code></pre></div>
<p>Fit the model to the data:</p>
<div class="sourceCode" id="cb772"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb772-1"><a href="ch-custom.html#cb772-1" aria-hidden="true"></a>recnormal_rt &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb772-2"><a href="ch-custom.html#cb772-2" aria-hidden="true"></a>  <span class="st">&quot;recnormal_rt.stan&quot;</span>,</span>
<span id="cb772-3"><a href="ch-custom.html#cb772-3" aria-hidden="true"></a>  <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb772-4"><a href="ch-custom.html#cb772-4" aria-hidden="true"></a>fit_rec &lt;-<span class="st"> </span><span class="kw">stan</span>(recnormal_rt, <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> N, <span class="dt">RT =</span> rt))</span></code></pre></div>
<p>Print the posterior summary:</p>
<div class="sourceCode" id="cb773"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb773-1"><a href="ch-custom.html#cb773-1" aria-hidden="true"></a><span class="kw">print</span>(fit_rec, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;mu&quot;</span>, <span class="st">&quot;sigma&quot;</span>), <span class="dt">digits =</span> <span class="dv">4</span>) </span></code></pre></div>
<pre><code>##         mean   2.5%  97.5% n_eff Rhat
## mu    0.0020 0.0020 0.0020  3779    1
## sigma 0.0004 0.0004 0.0004  3522    1</code></pre>
<p>We get the same results as before, but now we could potentially compare this model to one that assumes a log-normal likelihood, for example.</p>
<p>An important question is the following: Does every transformation of a random variable require a Jacobian adjustment? Essentially, if we assign a distribution to a random variable and then transform it, this does not require a Jacobian adjustment.</p>
<p>Alternatively, if we apply a transformation on a random variable first, and then assign a distribution to the transformed random variable afterwards, we have a  change of variables. This requires a Jacobian adjustment. This is the reason why <code>1 ./ RT</code> requires a Jacobian adjustment, but not <code>mu_s</code>, <code>mu</code>, <code>sigma_s</code>, or <code>sigma_s</code> in the previous model.</p>
<p>As a last step, we can encapsulate our new distribution in a function, and also create a  random number generator function. This is done in a  block called  <code>functions</code>. To create a function, we need to specify the type of every argument and the type that the function returns.</p>
<p>As a simple example, if we would like a function to center a vector, we would do it as follows:
</p>
<pre><code>functions {
  vector center(vector x) {
    vector[num_elements(x)] centered;
    centered = x - mean(x);
    return centered;
  }
}
data {
...
}
parameters {
...
}
model {
...
}</code></pre>
<p>We want to create a log(PDF) function, similar to the native Stan functions that end in <code>_lpdf</code>. Our function will take as arguments a vector <code>RT</code>, and real numbers <code>mu</code> and <code>sigma</code>. In <code>_lpdf</code> functions, (some of) the arguments (e.g., <code>RT</code>, <code>mu</code>, and <code>sigma</code>) can be vectorized, but the output of the function is always a real number: the sum of the log(PDF) evaluated at every value of the random variable at the left of the pipe. As we show below, to do that we just move the right hand side of <code>target</code> in our original model inside our new function. See the section entitled Further Reading at the end of this chapter for more about Stan functions.</p>
<p>For our custom random number generator function (which should end with <code>_rng</code>), we have the added complication that the function is truncated. We simply generate random values from a normal distribution, do the reciprocal transformation and if the number is above zero, this number is returned; otherwise, a new number is drawn.
This implementation may be inefficient when rejections occur frequently. Another, more complex implementation using the inverse CDF approach can be found in <span class="citation">Stan Development Team (<a href="#ref-Stan2023" role="doc-biblioref">2024</a>)</span> (section 21.10 of the User’s guide).</p>
<p>The complete code can be found in <code>recnormal_rt_f.stan</code>, and is also shown below:</p>
<pre class="stan fold-show"><code>functions {
  real recnormal_lpdf(vector y, real mu, real sigma){
    real lpdf;
    lpdf = normal_lpdf(1 ./ y | mu, sigma)
    - num_elements(y) * normal_lccdf(0 | mu, sigma)
    - sum(2 * log(y));
    return lpdf;
  }
  real recnormal_rng(real mu, real sigma){
    real y = 0;
    while (y &lt;= 0)
      y = 1 / normal_rng(mu, sigma);
    return y;
  }
}
data {
  int&lt;lower = 1&gt; N;
  vector&lt;lower = 0&gt;[N] RT;
}
parameters {
  real mu_s;
  real&lt;lower = 0&gt; sigma_s;
}
transformed parameters {
  real mu = mu_s / 1000;
  real sigma = sigma_s / 1000;
}
model {
  target += normal_lpdf(mu_s | 2, 1);
  target += normal_lpdf(sigma_s | 0.4, 0.2)
            - normal_lccdf(0 | 0.4, 0.2);
  target += recnormal_lpdf(RT | mu, sigma);
}
generated quantities {
  array[N] real rt_pred;
  for (n in 1:N)
    rt_pred[n] = recnormal_rng(mu, sigma);
}</code></pre>
<p>Fit the model to the simulated data:</p>
<div class="sourceCode" id="cb777"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb777-1"><a href="ch-custom.html#cb777-1" aria-hidden="true"></a>recnormal_rt_f &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb777-2"><a href="ch-custom.html#cb777-2" aria-hidden="true"></a>                              <span class="st">&quot;recnormal_rt_f.stan&quot;</span>,</span>
<span id="cb777-3"><a href="ch-custom.html#cb777-3" aria-hidden="true"></a>                              <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb777-4"><a href="ch-custom.html#cb777-4" aria-hidden="true"></a>fit_rec_f &lt;-<span class="st"> </span><span class="kw">stan</span>(recnormal_rt_f, <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> N, <span class="dt">RT =</span> rt))</span></code></pre></div>
<p>Print the summary:</p>
<div class="sourceCode" id="cb778"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb778-1"><a href="ch-custom.html#cb778-1" aria-hidden="true"></a><span class="kw">print</span>(fit_rec_f, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;mu&quot;</span>, <span class="st">&quot;sigma&quot;</span>), <span class="dt">digits =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##         mean   2.5%  97.5% n_eff Rhat
## mu    0.0020 0.0020 0.0020  3299    1
## sigma 0.0004 0.0004 0.0004  3148    1</code></pre>
</div>
</div>
<div id="sec-validSBC" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span>  Validation of a computed posterior distribution<a href="ch-custom.html#sec-validSBC" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The model converged, but did it work? At the very least we expect that the simulated data that we used to test the model shows a very similar distribution to the posterior predictive distributions. This is because we are fitting the data with the same function that we use to generate the data. Figure <a href="ch-custom.html#fig:ppcheckrec">12.2</a> shows that the simulated data and the posterior predictive distributions are similar.</p>

<div class="sourceCode" id="cb780"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb780-1"><a href="ch-custom.html#cb780-1" aria-hidden="true"></a></span>
<span id="cb780-2"><a href="ch-custom.html#cb780-2" aria-hidden="true"></a><span class="kw">ppc_dens_overlay</span>(rt, <span class="dt">yrep =</span> <span class="kw">extract</span>(fit_rec_f)<span class="op">$</span>rt_pred[<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>, ]) <span class="op">+</span></span>
<span id="cb780-3"><a href="ch-custom.html#cb780-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">2000</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ppcheckrec"></span>
<img src="bookdown_files/figure-html/ppcheckrec-1.svg" alt="A posterior predictive check of fit_rec in comparison with the simulated data." width="672" />
<p class="caption">
FIGURE 12.2: A posterior predictive check of <code>fit_rec</code> in comparison with the simulated data.
</p>
</div>
<p>Our synthetic data set was generated by first defining a  <em>ground truth</em> vector of parameters <span class="math inline">\(\langle \mu; \sigma \rangle\)</span>. We would expect that the true values of the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> should be well inside the bulk of the posterior distribution of our model. We investigate this by plotting the true values of the parameters together with their posterior distributions using the function  <code>mcmc_recover_hist()</code> of <code>bayesplot</code> as shown below in Figure <a href="ch-custom.html#fig:recoveryrec">12.3</a>.</p>

<div class="sourceCode" id="cb781"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb781-1"><a href="ch-custom.html#cb781-1" aria-hidden="true"></a>post_rec &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_rec_f) <span class="op">%&gt;%</span></span>
<span id="cb781-2"><a href="ch-custom.html#cb781-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(mu, sigma)</span>
<span id="cb781-3"><a href="ch-custom.html#cb781-3" aria-hidden="true"></a><span class="kw">mcmc_recover_hist</span>(post_rec, <span class="dt">true =</span> <span class="kw">c</span>(mu, sigma))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:recoveryrec"></span>
<img src="bookdown_files/figure-html/recoveryrec-1.svg" alt="Posterior distributions of the parameters of fit_rec together with their true values." width="672" />
<p class="caption">
FIGURE 12.3: Posterior distributions of the parameters of <code>fit_rec</code> together with their true values.
</p>
</div>
<p>Even though this approach can capture serious misspecifications in our models, it has three important shortcomings.</p>
<p>First, it’s unclear what we should conclude if the true value of a parameter is not in the bulk of its posterior distribution. Even in a well-specified model, if we inspect enough parameters we will find that for some parameters, the bulks of their respective marginal posterior distributions can be relatively far away from their true values.</p>
<p>The second shortcoming is that if the posteriors that we obtain are very wide, it might not be clear what to consider the bulk of the posterior distribution that needs to be inspected.</p>
<p>The third shortcoming is that regardless of how a successful recovery of parameters is defined, we might be able to recover a posterior based on data generated from some parts of the parameter space, but not based on data generated from other parts of parameter space <span class="citation">(Talts et al. <a href="#ref-talts2018validating" role="doc-biblioref">2018</a>)</span>. However, inspecting the entire parameter space is infeasible.</p>
<p>An alternative to our approach of plotting the true values of the parameters together with their posterior distributions is simulation-based calibration <span class="citation">(Talts et al. <a href="#ref-talts2018validating" role="doc-biblioref">2018</a>; Modrák et al. <a href="#ref-ModrakEtAl2023" role="doc-biblioref">2023</a>)</span>, which extends ideas of <span class="citation">Cook, Gelman, and Rubin (<a href="#ref-Cooketal2006" role="doc-biblioref">2006</a>)</span>. This is discussed next.</p>
<div id="the-simulation-based-calibration-procedure" class="section level3 hasAnchor" number="12.2.1">
<h3><span class="header-section-number">12.2.1</span> The  simulation-based calibration procedure<a href="ch-custom.html#the-simulation-based-calibration-procedure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="citation">Talts et al. (<a href="#ref-talts2018validating" role="doc-biblioref">2018</a>)</span> suggest the simulation-based calibration as the procedure for validating a model. We check that the Bayesian computation is  <em>faithful</em> in the sense that it is not  biased. Our goal is to ensure that our model neither overestimates nor underestimates, nor does it possess incorrect precision in the parameters, given the data and priors. As initial steps, we employ two different implementations of the same statistical model: (1-2) a generator capable of directly simulating draws from the prior predictive distribution, and (3) a probabilistic program that, when combined with a posterior approximation algorithm (such as a Stan sampler), samples from the posterior distribution. Finally, (4-5) we verify that the results from both implementations have the same distribution conditional on the data.</p>
<ol style="list-style-type: decimal">
<li>Generate true values (i.e., a  ground truth) for the parameters of the model from the priors:</li>
</ol>
<p><span class="math display">\[\begin{equation}
\tilde{\Theta}_n \sim p(\boldsymbol{\Theta})
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(p(\boldsymbol{\Theta})\)</span> represents a  joint prior distributions of vector of parameters. In our previous example, the vector of parameters is <span class="math inline">\(\langle \mu; \sigma \rangle\)</span>, and the joint prior distribution is two independent distributions, a normal and a truncated normal. Crucially, the prior distributions should be meaningful; that is, one should use priors that are at least regularizing or weakly informative. The  prior space plays a crucial role because it determines the parameter space where we will verify the correctness or faithfulness of the model.</p>
<p>Assuming 150 samples (from the priors), this step for our model would be as follows:</p>
<div class="sourceCode" id="cb782"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb782-1"><a href="ch-custom.html#cb782-1" aria-hidden="true"></a>N_sim &lt;-<span class="st"> </span><span class="dv">150</span></span>
<span id="cb782-2"><a href="ch-custom.html#cb782-2" aria-hidden="true"></a>mu_tilde &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_sim, <span class="dv">2</span>, <span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">1000</span></span>
<span id="cb782-3"><a href="ch-custom.html#cb782-3" aria-hidden="true"></a>sigma_tilde &lt;-<span class="st"> </span><span class="kw">rtnorm</span>(N_sim, <span class="fl">0.4</span>, <span class="fl">0.2</span>, <span class="dt">a =</span> <span class="dv">0</span>) <span class="op">/</span><span class="st"> </span><span class="dv">1000</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Generate multiple (<code>N_sim</code>) data sets based on the probability density function used as the likelihood:</li>
</ol>
<p><span class="math display">\[\begin{equation}
\tilde{D}_n \sim p(\boldsymbol{D} | \tilde{\Theta}_n)
\end{equation}\]</span></p>
<p>The previous equation indicates that each data set <span class="math inline">\(\tilde{D}_n\)</span> is sampled from each of the generated parameters, <span class="math inline">\(\tilde{\Theta}_n\)</span>. Following our previous example, use the reciprocal truncated normal distribution to generate data sets of response times:</p>
<div class="sourceCode" id="cb783"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb783-1"><a href="ch-custom.html#cb783-1" aria-hidden="true"></a><span class="co">## Create a place holder for all the simulated datasets</span></span>
<span id="cb783-2"><a href="ch-custom.html#cb783-2" aria-hidden="true"></a>rt_tilde &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> N_sim)</span>
<span id="cb783-3"><a href="ch-custom.html#cb783-3" aria-hidden="true"></a><span class="co"># Number of observations</span></span>
<span id="cb783-4"><a href="ch-custom.html#cb783-4" aria-hidden="true"></a>N &lt;-<span class="st"> </span><span class="dv">500</span></span>
<span id="cb783-5"><a href="ch-custom.html#cb783-5" aria-hidden="true"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N_sim) {</span>
<span id="cb783-6"><a href="ch-custom.html#cb783-6" aria-hidden="true"></a>  rt_tilde[[n]] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">rtnorm</span>(N, mu_tilde[n], sigma_tilde[n], <span class="dt">a =</span> <span class="dv">0</span>)</span>
<span id="cb783-7"><a href="ch-custom.html#cb783-7" aria-hidden="true"></a>}</span></code></pre></div>
<p>Now, <code>rt_tilde</code> consists of a list with 150 vectors of 500 observations each. Each list represents a  simulated data, which corresponds to <span class="math inline">\(\tilde{D}_{n}\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>Fit the same Bayesian model to each of the generated data sets.</li>
</ol>
<p>After fitting the same model to each data set, we should compare the recovered  posterior distribution for each parameter of each simulated data set; that is, <span class="math inline">\(p(\Theta_n | \tilde{D}_{n})\)</span>, with the parameters that were used to generate the data, <span class="math inline">\(\tilde{\Theta}_{n}\)</span>. This comparison is represented by <code>...</code> in the code below (that is, it is not yet implemented in the code shown below). This raises the question of how to compare the posterior distribution with the ground truth values of the parameters.</p>
<div class="sourceCode" id="cb784"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb784-1"><a href="ch-custom.html#cb784-1" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N_sim) {</span>
<span id="cb784-2"><a href="ch-custom.html#cb784-2" aria-hidden="true"></a>  fit &lt;-<span class="st"> </span><span class="kw">stan</span>(recnormal_rt,</span>
<span id="cb784-3"><a href="ch-custom.html#cb784-3" aria-hidden="true"></a>    <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> N, <span class="dt">rt =</span> rt_tilde[[n]])</span>
<span id="cb784-4"><a href="ch-custom.html#cb784-4" aria-hidden="true"></a>  )</span>
<span id="cb784-5"><a href="ch-custom.html#cb784-5" aria-hidden="true"></a>  ...</span>
<span id="cb784-6"><a href="ch-custom.html#cb784-6" aria-hidden="true"></a>}</span></code></pre></div>
<p>The procedure shown before (1-3) defines a natural condition for assessing whether the computed posterior distributions match the exact posterior distributions <span class="citation">(Talts et al. <a href="#ref-talts2018validating" role="doc-biblioref">2018</a>; Cook, Gelman, and Rubin <a href="#ref-Cooketal2006" role="doc-biblioref">2006</a>)</span>. This condition is met if the generator, probabilistic program, and posterior approximation algorithm function match: The generator and the probabilistic program must accurately represent the same data-generating process, while the posterior approximation algorithm should yield samples that closely match the correct posterior for the probabilistic program, based on data simulated from the prior. Any discrepancy suggests a mismatch among the components <span class="citation">(Modrák et al. <a href="#ref-ModrakEtAl2023" role="doc-biblioref">2023</a>)</span>.<a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a></p>
<p>The next two steps describe a well-defined comparison of the exact posterior and prior distribution <span class="citation">(different calibration checking methods differ in how exactly they test for mismatches between the distributions; Modrák et al. <a href="#ref-ModrakEtAl2023" role="doc-biblioref">2023</a>)</span>.</p>
<ol start="4" style="list-style-type: decimal">
<li>Generate an  ensemble of rank statistics of prior samples relative to corresponding posterior samples.</li>
</ol>
<p><span class="citation">Talts et al. (<a href="#ref-talts2018validating" role="doc-biblioref">2018</a>)</span> demonstrate that the match between the exact posterior and prior can be evaluated by examining whether, for each element of <span class="math inline">\(\boldsymbol{\Theta}\)</span> (e. g., <span class="math inline">\(\langle \mu; \sigma \rangle\)</span>), the  <em>rank statistics</em> of the prior sample relative to <span class="math inline">\(S\)</span> posterior samples will be  uniformly distributed across <span class="math inline">\([0, S]\)</span>. In other words, for each sample of the prior (e.g., <span class="math inline">\(\tilde{\mu}_{n}\)</span>), we calculate the number of samples of the posterior that is smaller than the corresponding value of the parameter sampled, and we examine its distribution. A mismatch between the exact posterior and our posterior would show up as a  non-uniform distribution.</p>
<p>As a next step, we examine this new distribution visually using a histogram with <span class="math inline">\(S + 1\)</span> possible ranks (from <span class="math inline">\(0\)</span> to <span class="math inline">\(S\)</span>). There are two issues that we need to take into account <span class="citation">(Talts et al. <a href="#ref-talts2018validating" role="doc-biblioref">2018</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Regardless of our model, histograms will deviate from uniformity if the posterior samples are dependent. This can be solved by  <em>thinning</em> the samples of the posterior, that is removing a number of intermediate samples <span class="citation">(Talts et al. <a href="#ref-talts2018validating" role="doc-biblioref">2018</a> recommend between 6 and 10)</span>.</p></li>
<li><p>To reduce the noise in the histogram, we should have  bins of equal size. If <span class="math inline">\(S + 1\)</span> are divisible by a large power of <span class="math inline">\(2\)</span>, e.g, <span class="math inline">\(1024\)</span>, we will be able to re-bin the histogram easily with bins of equal size. We complete the code shown in step 3 by generating an ensemble of ranks statistics as well.</p></li>
</ol>
<p>Here we encapsulate steps 1 to 4 into a function.</p>
<div class="sourceCode" id="cb785"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb785-1"><a href="ch-custom.html#cb785-1" aria-hidden="true"></a>gen_ranks &lt;-<span class="st"> </span><span class="cf">function</span>(...,</span>
<span id="cb785-2"><a href="ch-custom.html#cb785-2" aria-hidden="true"></a>                      <span class="dt">N_sim =</span> <span class="dv">150</span>,</span>
<span id="cb785-3"><a href="ch-custom.html#cb785-3" aria-hidden="true"></a>                      <span class="dt">N_s =</span> <span class="dv">1024</span> <span class="op">*</span><span class="st"> </span><span class="dv">6</span>, <span class="co"># Total samples from the posterior</span></span>
<span id="cb785-4"><a href="ch-custom.html#cb785-4" aria-hidden="true"></a>                      <span class="dt">N =</span> <span class="dv">500</span> <span class="co"># Number of observations</span></span>
<span id="cb785-5"><a href="ch-custom.html#cb785-5" aria-hidden="true"></a>                      ){</span>
<span id="cb785-6"><a href="ch-custom.html#cb785-6" aria-hidden="true"></a>  mu_tilde &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_sim, <span class="dv">2</span>, <span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">1000</span></span>
<span id="cb785-7"><a href="ch-custom.html#cb785-7" aria-hidden="true"></a>  sigma_tilde &lt;-<span class="st"> </span><span class="kw">rtnorm</span>(N_sim, <span class="fl">0.4</span>, <span class="fl">0.2</span>, <span class="dt">a =</span> <span class="dv">0</span>) <span class="op">/</span><span class="st"> </span><span class="dv">1000</span></span>
<span id="cb785-8"><a href="ch-custom.html#cb785-8" aria-hidden="true"></a>  <span class="co"># Thin by 6, and remove one extra sample so that S + 1 = 1024</span></span>
<span id="cb785-9"><a href="ch-custom.html#cb785-9" aria-hidden="true"></a>  thinner &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> N_s, <span class="dt">by =</span> <span class="dv">6</span>)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb785-10"><a href="ch-custom.html#cb785-10" aria-hidden="true"></a>  <span class="co"># Placeholders</span></span>
<span id="cb785-11"><a href="ch-custom.html#cb785-11" aria-hidden="true"></a>  rank_mu &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, N_sim)</span>
<span id="cb785-12"><a href="ch-custom.html#cb785-12" aria-hidden="true"></a>  rank_sigma &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, N_sim)</span>
<span id="cb785-13"><a href="ch-custom.html#cb785-13" aria-hidden="true"></a>  rt_tilde &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> N_sim)</span>
<span id="cb785-14"><a href="ch-custom.html#cb785-14" aria-hidden="true"></a>  <span class="co"># Loop over the simulations</span></span>
<span id="cb785-15"><a href="ch-custom.html#cb785-15" aria-hidden="true"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N_sim) {</span>
<span id="cb785-16"><a href="ch-custom.html#cb785-16" aria-hidden="true"></a>    <span class="kw">message</span>(<span class="st">&quot;Fit number &quot;</span>, n)</span>
<span id="cb785-17"><a href="ch-custom.html#cb785-17" aria-hidden="true"></a>    <span class="co"># it will repeat the sampling unless the model converged</span></span>
<span id="cb785-18"><a href="ch-custom.html#cb785-18" aria-hidden="true"></a>    <span class="cf">repeat</span>{</span>
<span id="cb785-19"><a href="ch-custom.html#cb785-19" aria-hidden="true"></a>      rt_tilde[[n]] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">rtnorm</span>(N,</span>
<span id="cb785-20"><a href="ch-custom.html#cb785-20" aria-hidden="true"></a>                                  mu_tilde[n],</span>
<span id="cb785-21"><a href="ch-custom.html#cb785-21" aria-hidden="true"></a>                                  sigma_tilde[n],</span>
<span id="cb785-22"><a href="ch-custom.html#cb785-22" aria-hidden="true"></a>                                  <span class="dt">a =</span> <span class="dv">0</span></span>
<span id="cb785-23"><a href="ch-custom.html#cb785-23" aria-hidden="true"></a>                                  )</span>
<span id="cb785-24"><a href="ch-custom.html#cb785-24" aria-hidden="true"></a>      <span class="co"># Fit is only stored temporarily</span></span>
<span id="cb785-25"><a href="ch-custom.html#cb785-25" aria-hidden="true"></a>      fit &lt;-<span class="st"> </span><span class="kw">stan</span>(...,</span>
<span id="cb785-26"><a href="ch-custom.html#cb785-26" aria-hidden="true"></a>                  <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> N,</span>
<span id="cb785-27"><a href="ch-custom.html#cb785-27" aria-hidden="true"></a>                              <span class="dt">RT =</span> rt_tilde[[n]]),</span>
<span id="cb785-28"><a href="ch-custom.html#cb785-28" aria-hidden="true"></a>                  <span class="dt">warmup =</span> <span class="dv">2500</span>,</span>
<span id="cb785-29"><a href="ch-custom.html#cb785-29" aria-hidden="true"></a>                  <span class="dt">iter =</span> <span class="dv">2500</span> <span class="op">+</span><span class="st"> </span>N_s <span class="op">/</span><span class="st"> </span><span class="dv">4</span>,</span>
<span id="cb785-30"><a href="ch-custom.html#cb785-30" aria-hidden="true"></a>                  <span class="dt">refresh =</span> <span class="dv">-1</span>,</span>
<span id="cb785-31"><a href="ch-custom.html#cb785-31" aria-hidden="true"></a>                  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">.999</span>,</span>
<span id="cb785-32"><a href="ch-custom.html#cb785-32" aria-hidden="true"></a>                                 <span class="dt">max_treedepth =</span> <span class="dv">14</span>))</span>
<span id="cb785-33"><a href="ch-custom.html#cb785-33" aria-hidden="true"></a>      <span class="co"># continue if there are no divergent transitions</span></span>
<span id="cb785-34"><a href="ch-custom.html#cb785-34" aria-hidden="true"></a>      <span class="co"># and all Rhat are smaller than 1.05</span></span>
<span id="cb785-35"><a href="ch-custom.html#cb785-35" aria-hidden="true"></a>      <span class="cf">if</span> (<span class="op">!</span><span class="kw">get_num_divergent</span>(fit) <span class="op">&amp;</span></span>
<span id="cb785-36"><a href="ch-custom.html#cb785-36" aria-hidden="true"></a><span class="st">          </span><span class="kw">all</span>(<span class="kw">monitor</span>(fit)[,<span class="st">&quot;Rhat&quot;</span>] <span class="op">&lt;=</span><span class="st"> </span><span class="fl">1.05</span>)) <span class="cf">break</span></span>
<span id="cb785-37"><a href="ch-custom.html#cb785-37" aria-hidden="true"></a>      <span class="kw">message</span>(<span class="st">&quot;Repeating fit &quot;</span>,n)</span>
<span id="cb785-38"><a href="ch-custom.html#cb785-38" aria-hidden="true"></a>    }</span>
<span id="cb785-39"><a href="ch-custom.html#cb785-39" aria-hidden="true"></a>    post &lt;-<span class="st"> </span><span class="kw">extract</span>(fit)</span>
<span id="cb785-40"><a href="ch-custom.html#cb785-40" aria-hidden="true"></a>    <span class="co"># Number of samples of the posterior that is smaller than</span></span>
<span id="cb785-41"><a href="ch-custom.html#cb785-41" aria-hidden="true"></a>    <span class="co"># the corresponding value of the parameter sampled</span></span>
<span id="cb785-42"><a href="ch-custom.html#cb785-42" aria-hidden="true"></a>    rank_mu[n] &lt;-<span class="st"> </span><span class="kw">sum</span>(post<span class="op">$</span>mu[thinner] <span class="op">&lt;</span><span class="st"> </span>mu_tilde[n])</span>
<span id="cb785-43"><a href="ch-custom.html#cb785-43" aria-hidden="true"></a>    rank_sigma[n] &lt;-<span class="st"> </span><span class="kw">sum</span>(post<span class="op">$</span>sigma[thinner] <span class="op">&lt;</span><span class="st"> </span>sigma_tilde[n])</span>
<span id="cb785-44"><a href="ch-custom.html#cb785-44" aria-hidden="true"></a>  }</span>
<span id="cb785-45"><a href="ch-custom.html#cb785-45" aria-hidden="true"></a>  <span class="co">#data frame with ranks:</span></span>
<span id="cb785-46"><a href="ch-custom.html#cb785-46" aria-hidden="true"></a>  <span class="kw">tibble</span>(<span class="dt">sim =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>N_sim, <span class="dv">2</span>),</span>
<span id="cb785-47"><a href="ch-custom.html#cb785-47" aria-hidden="true"></a>         <span class="dt">variable =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">&quot;mu&quot;</span>, <span class="st">&quot;sigma&quot;</span>),</span>
<span id="cb785-48"><a href="ch-custom.html#cb785-48" aria-hidden="true"></a>                        <span class="dt">each =</span> N_sim),</span>
<span id="cb785-49"><a href="ch-custom.html#cb785-49" aria-hidden="true"></a>         <span class="dt">rank =</span> <span class="kw">c</span>(rank_mu, rank_sigma))</span>
<span id="cb785-50"><a href="ch-custom.html#cb785-50" aria-hidden="true"></a>}</span>
<span id="cb785-51"><a href="ch-custom.html#cb785-51" aria-hidden="true"></a></span>
<span id="cb785-52"><a href="ch-custom.html#cb785-52" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb785-53"><a href="ch-custom.html#cb785-53" aria-hidden="true"></a><span class="co"># generate ranks with the default values:</span></span>
<span id="cb785-54"><a href="ch-custom.html#cb785-54" aria-hidden="true"></a>df_rank &lt;-<span class="st"> </span><span class="kw">gen_ranks</span>(recnormal_rt) </span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>Graphically assess the  correctness of the model using  rank histograms</li>
</ol>
<p>As a last step, we use a histogram for each parameter to identify deviations of uniformity.
If the posterior estimates are correct, then each of the <span class="math inline">\(B\)</span> bins has a probability of <span class="math inline">\(1/B\)</span> that a simulation (i.e., an individual rank) falls into it: <span class="math inline">\(\mathit{Binomial}(N_{sim}, 1/B)\)</span>. This allow us to complement the histograms with confidence intervals, indicating where the variation expected from a uniform histogram should be.</p>
<p>Use the code below to build the plot shown in Figure <a href="ch-custom.html#fig:SBChand">12.4</a>. We can conclude that the implementation of our model (in the parameter space determined by the priors) is correct.</p>

<div class="sourceCode" id="cb786"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb786-1"><a href="ch-custom.html#cb786-1" aria-hidden="true"></a>B &lt;-<span class="st"> </span><span class="dv">16</span></span>
<span id="cb786-2"><a href="ch-custom.html#cb786-2" aria-hidden="true"></a>bin_size &lt;-<span class="st"> </span><span class="dv">1024</span> <span class="op">/</span><span class="st"> </span><span class="dv">16</span></span>
<span id="cb786-3"><a href="ch-custom.html#cb786-3" aria-hidden="true"></a>ci_l &lt;-<span class="st"> </span><span class="kw">qbinom</span>(<span class="fl">0.005</span>, <span class="dt">size =</span> N_sim, <span class="dt">prob =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>B)</span>
<span id="cb786-4"><a href="ch-custom.html#cb786-4" aria-hidden="true"></a>ci_u &lt;-<span class="st"> </span><span class="kw">qbinom</span>(<span class="fl">0.995</span>, <span class="dt">size =</span> N_sim, <span class="dt">prob =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>B)</span>
<span id="cb786-5"><a href="ch-custom.html#cb786-5" aria-hidden="true"></a><span class="kw">ggplot</span>(df_rank, <span class="kw">aes</span>(<span class="dt">x =</span> rank)) <span class="op">+</span></span>
<span id="cb786-6"><a href="ch-custom.html#cb786-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1023</span>, <span class="dt">length.out =</span> B <span class="op">+</span><span class="st"> </span><span class="dv">1</span>),</span>
<span id="cb786-7"><a href="ch-custom.html#cb786-7" aria-hidden="true"></a>                 <span class="dt">closed =</span> <span class="st">&quot;left&quot;</span>, <span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>) <span class="op">+</span></span>
<span id="cb786-8"><a href="ch-custom.html#cb786-8" aria-hidden="true"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;count&quot;</span>) <span class="op">+</span></span>
<span id="cb786-9"><a href="ch-custom.html#cb786-9" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="kw">c</span>(ci_l, ci_u), <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></span>
<span id="cb786-10"><a href="ch-custom.html#cb786-10" aria-hidden="true"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>variable, <span class="dt">scales =</span> <span class="st">&quot;free_y&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:SBChand"></span>
<img src="bookdown_files/figure-html/SBChand-1.svg" alt="Rank histograms of \(\mu\) and \(\sigma\) from the reciprocal truncated normal distribution. The dashed lines represent the 99% confidence interval." width="672" />
<p class="caption">
FIGURE 12.4: Rank histograms of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> from the reciprocal truncated normal distribution. The dashed lines represent the 99% confidence interval.
</p>
</div>
<p>Next, we consider an example where simulation-based calibration can reveal a problem.</p>
</div>
<div id="an-example-where-simulation-based-calibration-reveals-a-problem" class="section level3 hasAnchor" number="12.2.2">
<h3><span class="header-section-number">12.2.2</span> An example where simulation-based calibration reveals a problem<a href="ch-custom.html#an-example-where-simulation-based-calibration-reveals-a-problem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s assume that we made an error in the model implementation. We’ll fit an  <strong>incorrect</strong> model: rather than the complete likelihood including the normal PDF and the truncation and Jacobian adjustments, we will fit only the normal PDF evaluated at the reciprocal of the response times, that is <code>target += normal_lpdf(1 ./ RT | mu, sigma);</code>.</p>
<p>Assessing the correctness of the model by looking at the recovery of the parameters is misleading in this case, as it is evident from Figure <a href="ch-custom.html#fig:recoverybad">12.5</a>. One could conclude that the model is fine based on this plot, since the true values of the parameters are well inside the bulk of their posterior distributions.</p>

<div class="figure"><span style="display:block;" id="fig:recoverybad"></span>
<img src="bookdown_files/figure-html/recoverybad-1.svg" alt="Posterior distributions of the parameters of an incorrect model (truncation and Jacobian adjustments missing) together with their true values." width="672" />
<p class="caption">
FIGURE 12.5: Posterior distributions of the parameters of an incorrect model (truncation and Jacobian adjustments missing) together with their true values.
</p>
</div>
<p>However, the rank histograms produced by the simulation-based calibration procedure in Figure <a href="ch-custom.html#fig:SBCbad">12.6</a> show very tall bins at the left and at the right of each histogram, exceeding the 99% CI. This is a very clear indication that there is a problem with the  model specification: our incorrect model is overestimating <span class="math inline">\(\mu\)</span> and underestimating <span class="math inline">\(\sigma\)</span>. This example illustrates the importance of simulation-based calibration.</p>

<div class="figure"><span style="display:block;" id="fig:SBCbad"></span>
<img src="bookdown_files/figure-html/SBCbad-1.svg" alt="Rank histograms of \(\mu\) and \(\sigma\) from the incorrect implementation of the reciprocal truncated normal distribution. The dashed lines represent the 99% confidence interval." width="672" />
<p class="caption">
FIGURE 12.6: Rank histograms of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> from the <strong>incorrect</strong> implementation of the reciprocal truncated normal distribution. The dashed lines represent the 99% confidence interval.
</p>
</div>
</div>
<div id="issues-with-and-limitations-of-simulation-based-calibration" class="section level3 hasAnchor" number="12.2.3">
<h3><span class="header-section-number">12.2.3</span> Issues with and limitations of simulation-based calibration<a href="ch-custom.html#issues-with-and-limitations-of-simulation-based-calibration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the previous sections, we have used only rank histograms. Even though histograms are a very intuitive means of visualization to assess model correctness, they might not be sensitive enough for small deviations <span class="citation">(Talts et al. <a href="#ref-talts2018validating" role="doc-biblioref">2018</a>)</span>. Other visualizations might be better suited. Another limitation of histograms is that they are sensitive to the number of bins <span class="citation">(Säilynoja, Bürkner, and Vehtari <a href="#ref-sailynoja2022graphical" role="doc-biblioref">2022</a>)</span>. One could bin the histograms multiple times, but this approach is difficult to interpret when there are many parameters, and can be vulnerable to multiple testing biases <span class="citation">(Talts et al. <a href="#ref-talts2018validating" role="doc-biblioref">2018</a>)</span>. One alternative to rank histograms is visualizations based on the  empirical cumulative density function of the ranks. This is briefly presented in Box <a href="ch-custom.html#thm:sbc">12.1</a>.</p>
<div style="page-break-after: always;"></div>
<div class="extra">
<div class="theorem">
<p><span id="thm:sbc" class="theorem"><strong>Box 12.1  </strong></span><strong>Different rank visualizations and the <code>SBC</code> package.</strong></p>
</div>
<p>Implementing the simulation-based calibration algorithm “by hand” introduces a new source of potential errors. Fortunately, the R package  <code>SBC</code> <span class="citation">(Kim et al. <a href="#ref-R-SBC" role="doc-biblioref">2024</a>)</span> provides tools to validate a Stan model (or any sampling algorithm) by allowing us to run simulation-based calibrations easily.
The package is in active development at the moment<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a> and can be installed with the following command.</p>
<div class="sourceCode" id="cb787"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb787-1"><a href="ch-custom.html#cb787-1" aria-hidden="true"></a>remotes<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;hyunjimoon/SBC&quot;</span>)</span></code></pre></div>
<p>One of the main advantages of this package is that it provides several ways to visualize the results of the simulation-based calibration procedure; see <a href="https://hyunjimoon.github.io/SBC/" class="uri">https://hyunjimoon.github.io/SBC/</a>. Figure <a href="ch-custom.html#fig:sbchist">12.7</a> shows rank histograms produced by <code>SBC</code> of a correct model and several different incorrect models. An alternative to rank histograms is to use an  empirical cumulative distribution function (ECDF)-based method, as proposed by <span class="citation">Säilynoja, Bürkner, and Vehtari (<a href="#ref-sailynoja2022graphical" role="doc-biblioref">2022</a>)</span>. The idea behind this method is that if the ranks produced by the simulation-based calibration algorithm are uniform the ECDF of the ranks should be close to the CDF of a uniform distribution. Figure <a href="ch-custom.html#fig:sbcecdf">12.8</a> shows the difference between the ECDF of the ranks and the CDF of a uniform distribution together with 95% confidence bands (this is the default in the <code>SBC</code> package) for a correct model and different incorrect ones.</p>

<div class="figure"><span style="display:block;" id="fig:sbchist"></span>
<img src="bookdown_files/figure-html/sbchist-1.svg" alt="Rank histograms produced by the R package SBC showing the outcome one would expect for a correct model and for several different incorrect ones together with 95% confidence bands (these are the default bands in the package)." width="672"  />
<p class="caption">
FIGURE 12.7: Rank histograms produced by the R package <code>SBC</code> showing the outcome one would expect for a correct model and for several different incorrect ones together with 95% confidence bands (these are the default bands in the package).
</p>
</div>

<div class="figure"><span style="display:block;" id="fig:sbcecdf"></span>
<img src="bookdown_files/figure-html/sbcecdf-1.svg" alt="Difference between the perfectly uniform CDF and empirical cumulative distribution function (ECDF) of the ranks produced by the SBC R package together with 95% confidence bands. The figure shows the outcome one would expect for a correct model and for several different incorrect ones." width="672"  />
<p class="caption">
FIGURE 12.8: Difference between the perfectly uniform CDF and empirical cumulative distribution function (ECDF) of the ranks produced by the <code>SBC</code> R package together with 95% confidence bands. The figure shows the outcome one would expect for a correct model and for several different incorrect ones.
</p>
</div>
</div>
<p>Even though simulation-based calibration is a comprehensive approach to test model correctness, it has several drawbacks, regardless of the means of visualization:</p>
<ol style="list-style-type: lower-roman">
<li><p>This approach requires fitting a model many times, which can be too time intensive for complex models. It’s worth bearing in mind that code with errors can sometimes show clear  “catastrophic” failures in the recovery of the parameters. For this reason, one could start the verification of model correctness with a simple  recovery of parameters <span class="citation">(see Talts et al. <a href="#ref-talts2018validating" role="doc-biblioref">2018</a>; Gelman et al. <a href="#ref-gelman2020bayesian" role="doc-biblioref">2020</a>)</span>, and then proceed with the simulation-based calibration procedure for at least some suspect aspects of the model (e.g., when working with a custom likelihood).</p></li>
<li><p>Another major limitation of simulation-based calibration is that it is concerned exclusively with the computational aspects of the analysis and offers no guarantee for any single observation. A complete Bayesian workflow should include prior and posterior predictive checks, see chapter <a href="ch-workflow.html#ch-workflow">7</a> and <span class="citation">Schad, Betancourt, and Vasishth (<a href="#ref-schad2020toward" role="doc-biblioref">2020</a>)</span>.</p></li>
<li><p>Finally, it’s important to apply simulation-based calibration only after appropriate priors are set. This is important because only the priors determine the parameter space that will be inspected. Weak priors that may be good enough for estimation may nevertheless lead to a parameter space that is unrealistically large since, during the calibration stage, there are no independent data to constrain the space. This means that it is important to conduct prior predictive checks before assessing the correctness of the model with simulation-based calibration.</p></li>
</ol>
</div>
</div>
<div id="another-custom-distribution-the-exponential-distribution-implemented-manually" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Another  custom distribution: The exponential distribution  implemented manually<a href="ch-custom.html#another-custom-distribution-the-exponential-distribution-implemented-manually" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are cases when one needs a random variable with a distribution that is not included in Stan. Many times one can find the PDF (or a CDF) derived in a paper, and one needs to implement it manually by writing the log PDF in the Stan language.
Even though the exponential distribution is included in Stan, we demonstrate how we would include it step-by-step as if it weren’t available. This example extends what is demonstrated in <span class="citation">Stan Development Team (<a href="#ref-Stan2023" role="doc-biblioref">2024</a>)</span> (section 22.1 of the User’s guide).</p>
<p>The exponential distribution is used to model  waiting times until a certain event occurs. Its PDF is the following:</p>
<p><span class="math display">\[\begin{equation}
f(x |  \lambda )={
\begin{cases}
\lambda e^{-\lambda x} &amp; x\geq 0,\\0&amp;x&lt;0.
\end{cases}} 
\end{equation}\]</span></p>
<p>The parameter <span class="math inline">\(\lambda\)</span> is often called the rate parameter and must be positive. A higher value for the rate parameter leads to shorter waiting times on average. The mean of the distribution is <span class="math inline">\(1/\lambda\)</span>. The exponential distribution has the key property of being  <em>memoryless</em>. What this means is explained in <span class="citation">Ross (<a href="#ref-RossProb" role="doc-biblioref">2002</a>)</span> as follows: Suppose that <span class="math inline">\(X\)</span> is a random variable with an exponential distribution as a PDF; the random variable represents the lifetime of an item (e.g., <span class="math inline">\(X\)</span> could represent the time it takes for a radioactive particle to completely decay). If the item is <span class="math inline">\(t\)</span> time-units old, then the remaining life <span class="math inline">\(s\)</span> of that item has the same probability distribution as the life of a new item. Mathematically, this amounts to stating that</p>
<p><span class="math display">\[\begin{equation}
P(X&gt;s + t | X&gt; t)= P(X&gt;s)
\end{equation}\]</span></p>
<p>The implication of the memoryless property is that we do not need to know the age of an item to know what the distribution of its remaining life is.</p>
<p>To give another example of memorylessness, the conditional probability that a certain event will happen in the next 100 ms is the same regardless of whether we have been already waiting 1000 ms, 10 ms, or 0 ms. Although the exponential distribution is not commonly used for modeling response times in cognitive science, it has been used in the past <span class="citation">(Ashby and Townsend <a href="#ref-ashby1980decomposing" role="doc-biblioref">1980</a>; Ashby <a href="#ref-ashby1982testing" role="doc-biblioref">1982</a>)</span>. We focus on this distribution because of its simple analytical form.</p>
<p>As a first step, we make our own version of the  PDF in R, and then check that it integrates to 1 to verify that this is actually a proper distribution (and that we haven’t introduced a typo in the formula).</p>
<p>To avoid  underflow, that is getting a zero instead of a very small number, we’ll work in the log-scale. This will also be useful when we implement this function in Stan, thus the log PDF is</p>
<p><span class="math display">\[\begin{equation}
\log(f(x| \lambda ))= \log(\lambda) -\lambda  x
\end{equation}\]</span></p>
<p>where <span class="math inline">\(x &gt;0\)</span>.</p>
<p>Implement this function in R with the same arguments as the <code>d*</code> family of functions: if <code>log = TRUE</code> the output is a log density; call this new function <code>dexp2</code>.</p>
<div class="sourceCode" id="cb788"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb788-1"><a href="ch-custom.html#cb788-1" aria-hidden="true"></a>dexp2 &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">lambda =</span> <span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">FALSE</span>) {</span>
<span id="cb788-2"><a href="ch-custom.html#cb788-2" aria-hidden="true"></a>  log_density &lt;-<span class="st"> </span><span class="kw">log</span>(lambda) <span class="op">-</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>x</span>
<span id="cb788-3"><a href="ch-custom.html#cb788-3" aria-hidden="true"></a>  <span class="cf">if</span> (log <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) {</span>
<span id="cb788-4"><a href="ch-custom.html#cb788-4" aria-hidden="true"></a>    <span class="kw">exp</span>(log_density)</span>
<span id="cb788-5"><a href="ch-custom.html#cb788-5" aria-hidden="true"></a>  } <span class="cf">else</span> {</span>
<span id="cb788-6"><a href="ch-custom.html#cb788-6" aria-hidden="true"></a>    log_density</span>
<span id="cb788-7"><a href="ch-custom.html#cb788-7" aria-hidden="true"></a>  }</span>
<span id="cb788-8"><a href="ch-custom.html#cb788-8" aria-hidden="true"></a>}</span></code></pre></div>
<p>Verify that this function integrates to 1 for some point values of its parameter <span class="math inline">\(\lambda\)</span> (here, <span class="math inline">\(\lambda = 1\)</span> and <span class="math inline">\(\lambda = 20\)</span>):</p>
<div class="sourceCode" id="cb789"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb789-1"><a href="ch-custom.html#cb789-1" aria-hidden="true"></a>dexp2_l1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">dexp2</span>(x, <span class="dv">1</span>)</span>
<span id="cb789-2"><a href="ch-custom.html#cb789-2" aria-hidden="true"></a><span class="kw">integrate</span>(dexp2_l1, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb791"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb791-1"><a href="ch-custom.html#cb791-1" aria-hidden="true"></a>dexp2_l20 &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">dexp2</span>(x, <span class="dv">20</span>)</span>
<span id="cb791-2"><a href="ch-custom.html#cb791-2" aria-hidden="true"></a><span class="kw">integrate</span>(dexp2_l20, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>To test our function, we’ll also need to generate random values from the exponential distribution. If the  quantile function of the distribution exists, the  inverse transform sampling is a relatively straightforward way to get pseudo random numbers sampled from a target distribution <span class="citation">(for an accessible introduction to inverse sampling, see Lynch <a href="#ref-lynch2007introduction" role="doc-biblioref">2007</a>)</span>. Given a target distribution with a PDF <span class="math inline">\(f\)</span>, and a quantile function <span class="math inline">\(F^{-1}\)</span> (the inverse of the CDF), the inverse transform sampling method consists of the following:</p>
<ol style="list-style-type: decimal">
<li>Sample one number <span class="math inline">\(u\)</span> from <span class="math inline">\(\mathit{Uniform}(0,1)\)</span>.</li>
<li>Then <span class="math inline">\(z=F^{-1}(u)\)</span> is a draw from <span class="math inline">\(f(x)\)</span>.</li>
</ol>
<p>In this case, the quantile function (the  inverse of the CDF) is the following:</p>
<p><span class="math display">\[\begin{equation}
- \log(1-p)/\lambda
\end{equation}\]</span></p>
<p>Here is how one can derive this inverse of the CDF. First, consider the fact that the CDF of the exponential distribution is as follows. The term <span class="math inline">\(q\)</span> is some quantile of the distribution:</p>
<p><span class="math display">\[\begin{equation}
F(q) = \int_0^q \lambda \exp(-\lambda x)\, dx
\end{equation}\]</span></p>
<p>We can solve this  integral by using the  <span class="math inline">\(u\)</span>-substitution method <span class="citation">(Salas, Etgen, and Hille <a href="#ref-salas2003calculus" role="doc-biblioref">2003</a>)</span>. First, define</p>
<p><span class="math display">\[\begin{equation}
u = g(x) = -\lambda x
\end{equation}\]</span></p>
<p>Then, the derivative <span class="math inline">\(du/dx\)</span> is:</p>
<p><span class="math display">\[\begin{equation}
\frac{du}{dx} = -\lambda
\end{equation}\]</span></p>
<p>This implies that <span class="math inline">\(du = - \lambda dx\)</span>, or that <span class="math inline">\(- du = \lambda dx\)</span>.</p>
<p>In the CDF, replace the term <span class="math inline">\(\lambda dx\)</span> with <span class="math inline">\(-du\)</span>:</p>
<p><span class="math display">\[\begin{equation}
F(q) = \int_0^q \lambda \exp(-\lambda x)\, dx = \int_0^{-\lambda q} (- \exp(-\lambda x)\, du)
\end{equation}\]</span></p>
<p>Rewriting <span class="math inline">\(-\lambda x\)</span> as <span class="math inline">\(u\)</span>, the CDF simplifies to:</p>
<p><span class="math display">\[\begin{equation}
F(q) = \int_0^q \lambda \exp(-\lambda x)\, dx = \int_0^q (- \exp(u)\, du)
\end{equation}\]</span></p>
<p>We know from calculus that the integral of <span class="math inline">\(-\exp(u)\)</span> is <span class="math inline">\(-\exp(u)\)</span>. So, the integral becomes:</p>
<p><span class="math display">\[\begin{equation}
F(q) = \left[ - \exp(u) \right]_0^q
\end{equation}\]</span></p>
<p>Replacing <span class="math inline">\(u\)</span> with <span class="math inline">\(-\lambda x\)</span>, we get:</p>
<p><span class="math display">\[\begin{equation}
F(q) = \left[ - \exp(-\lambda x) \right]_0^q = 
1- \exp(-\lambda q)
\end{equation}\]</span></p>
<p>Thus, we know that the CDF is <span class="math inline">\(F(t) = 1- \exp(-\lambda q) = p\)</span>, where <span class="math inline">\(p\)</span> is the probability of observing the quantile <span class="math inline">\(q\)</span> or some value smaller than <span class="math inline">\(q\)</span>. To derive the inverse of the CDF, solve the equation below for <span class="math inline">\(q\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p &amp;= 1- e^{-\lambda q}\\
p-1 &amp;= - e^{-\lambda q}\\
-p+1 &amp;=  e^{-\lambda q}\\
\log(1-p) &amp;=  -\lambda q\\
- \log(1-p)/\lambda &amp;=  q\\
\end{aligned}
\end{equation}\]</span></p>
<p>Write the quantile function (the inverse of the CDF) and the random number generator for the exponential distribution in R. To differentiate it from the built-in function in R, we will call it <code>qexp2</code>:</p>
<div class="sourceCode" id="cb793"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb793-1"><a href="ch-custom.html#cb793-1" aria-hidden="true"></a>qexp2 &lt;-<span class="st"> </span><span class="cf">function</span>(p, <span class="dt">lambda =</span> <span class="dv">1</span>) {</span>
<span id="cb793-2"><a href="ch-custom.html#cb793-2" aria-hidden="true"></a>  <span class="op">-</span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p) <span class="op">/</span><span class="st"> </span>lambda</span>
<span id="cb793-3"><a href="ch-custom.html#cb793-3" aria-hidden="true"></a>}</span>
<span id="cb793-4"><a href="ch-custom.html#cb793-4" aria-hidden="true"></a>rexp2 &lt;-<span class="st"> </span><span class="cf">function</span>(n, <span class="dt">lambda =</span> <span class="dv">1</span>) {</span>
<span id="cb793-5"><a href="ch-custom.html#cb793-5" aria-hidden="true"></a>  u &lt;-<span class="st"> </span><span class="kw">runif</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb793-6"><a href="ch-custom.html#cb793-6" aria-hidden="true"></a>  <span class="kw">qexp2</span>(u, lambda)</span>
<span id="cb793-7"><a href="ch-custom.html#cb793-7" aria-hidden="true"></a>}</span></code></pre></div>
<p>The functions that we would use in a Stan model are relatively faithful to the R code, but follow Stan conventions: The expression <code>log1m(p)</code> is more arithmetically stable than <code>log(1 - .)</code> for values of <code>p</code> close to one, the function <code>exp_lpdf</code> stores the sum of the log(PDF) evaluated at each value of x, this would be analogous to doing <code>sum(dexp2(x, lambda, log = TRUE))</code>; the function <code>exp_rng</code> implements a non-vectorized version of <code>rexp2</code> and uses the auxiliary function <code>exp_icdf</code> (icdf stands for inverse CDF), which is similar to <code>qexp2</code>.</p>
<pre class="stan fold-show"><code>functions {
  real exp_lpdf(vector x, real lambda){
    vector[num_elements(x)] lpdf = log(lambda) - lambda * x;
    return sum(lpdf);
  }
  real exp_icdf(real p, real lambda){
    return -log1m(p) / lambda;
  }
  real exp_rng(real lambda){
    real u = uniform_rng(0, 1);
    return exp_icdf(u, lambda);
  }
}</code></pre>
<p>We are now ready to generate synthetic data and fit the distribution in Stan. Generate 1000 observations.</p>
<div class="sourceCode" id="cb795"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb795-1"><a href="ch-custom.html#cb795-1" aria-hidden="true"></a>N &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb795-2"><a href="ch-custom.html#cb795-2" aria-hidden="true"></a>lambda &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">200</span></span>
<span id="cb795-3"><a href="ch-custom.html#cb795-3" aria-hidden="true"></a>rt &lt;-<span class="st"> </span><span class="kw">rexp2</span>(N, lambda)</span></code></pre></div>
<p>Use <code>exponential.stan</code>, which includes the function block shown earlier, and the following obligatory blocks:</p>
<pre class="stan fold-show"><code>data {
  int&lt;lower = 1&gt; N;
  vector&lt;lower = 0&gt;[N] RT;
}
parameters {
  real&lt;lower = 0&gt; lambda;
}
model {
  target += normal_lpdf(lambda | 0, .1) -
    normal_lccdf(0 | 0, .1);
  target += exp_lpdf(RT | lambda);
}
generated quantities {
  array[N] real rt_pred;
  for (n in 1:N)
    rt_pred[n] = exp_rng(lambda);
}</code></pre>
<p>Fit the data with Stan.</p>
<div class="sourceCode" id="cb797"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb797-1"><a href="ch-custom.html#cb797-1" aria-hidden="true"></a>exponential &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb797-2"><a href="ch-custom.html#cb797-2" aria-hidden="true"></a>                           <span class="st">&quot;exponential.stan&quot;</span>,</span>
<span id="cb797-3"><a href="ch-custom.html#cb797-3" aria-hidden="true"></a>                           <span class="dt">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb797-4"><a href="ch-custom.html#cb797-4" aria-hidden="true"></a>fit_exp &lt;-<span class="st"> </span><span class="kw">stan</span>(exponential, <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> N, <span class="dt">RT =</span> rt))</span></code></pre></div>
<p>Print the summary:</p>
<div class="sourceCode" id="cb798"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb798-1"><a href="ch-custom.html#cb798-1" aria-hidden="true"></a><span class="kw">print</span>(fit_exp, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;lambda&quot;</span>))</span></code></pre></div>
<pre><code>##        mean 2.5% 97.5% n_eff Rhat
## lambda 0.01    0  0.01  1471    1</code></pre>
<p>Carry out a quick check first, verifying that the true value of the parameter <span class="math inline">\(\lambda\)</span> is reasonably inside the bulk of the posterior distribution. This is shown in Figure <a href="ch-custom.html#fig:recoveryexp">12.9</a>(a).
Figure <a href="ch-custom.html#fig:recoveryexp">12.9</a>(b) shows the results of the simulation-based calibration procedure, and shows that our implementation was correct.</p>

<div class="sourceCode" id="cb800"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb800-1"><a href="ch-custom.html#cb800-1" aria-hidden="true"></a></span>
<span id="cb800-2"><a href="ch-custom.html#cb800-2" aria-hidden="true"></a><span class="co"># Plot a</span></span>
<span id="cb800-3"><a href="ch-custom.html#cb800-3" aria-hidden="true"></a>post_exp &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_exp) <span class="op">%&gt;%</span></span>
<span id="cb800-4"><a href="ch-custom.html#cb800-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(<span class="st">&quot;lambda&quot;</span>)</span>
<span id="cb800-5"><a href="ch-custom.html#cb800-5" aria-hidden="true"></a><span class="kw">mcmc_recover_hist</span>(post_exp, <span class="dt">true =</span> lambda) <span class="op">+</span></span>
<span id="cb800-6"><a href="ch-custom.html#cb800-6" aria-hidden="true"></a><span class="st">  </span><span class="co"># make it similar to plot b:</span></span>
<span id="cb800-7"><a href="ch-custom.html#cb800-7" aria-hidden="true"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;a&quot;</span>) <span class="op">+</span></span>
<span id="cb800-8"><a href="ch-custom.html#cb800-8" aria-hidden="true"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;lamda&quot;</span>) <span class="op">+</span></span>
<span id="cb800-9"><a href="ch-custom.html#cb800-9" aria-hidden="true"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="dv">0</span>),</span>
<span id="cb800-10"><a href="ch-custom.html#cb800-10" aria-hidden="true"></a>        <span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>,</span>
<span id="cb800-11"><a href="ch-custom.html#cb800-11" aria-hidden="true"></a>        <span class="dt">strip.text.x =</span> <span class="kw">element_blank</span>())</span>
<span id="cb800-12"><a href="ch-custom.html#cb800-12" aria-hidden="true"></a><span class="co"># Plot b</span></span>
<span id="cb800-13"><a href="ch-custom.html#cb800-13" aria-hidden="true"></a>B &lt;-<span class="st"> </span><span class="dv">16</span></span>
<span id="cb800-14"><a href="ch-custom.html#cb800-14" aria-hidden="true"></a>bin_size &lt;-<span class="st"> </span><span class="dv">1024</span> <span class="op">/</span><span class="st"> </span><span class="dv">16</span></span>
<span id="cb800-15"><a href="ch-custom.html#cb800-15" aria-hidden="true"></a>N_sim =<span class="st"> </span><span class="dv">200</span></span>
<span id="cb800-16"><a href="ch-custom.html#cb800-16" aria-hidden="true"></a>ci_l &lt;-<span class="st"> </span><span class="kw">qbinom</span>(<span class="fl">0.005</span>, <span class="dt">size =</span> N_sim, <span class="dt">prob =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>B)</span>
<span id="cb800-17"><a href="ch-custom.html#cb800-17" aria-hidden="true"></a>ci_u &lt;-<span class="st"> </span><span class="kw">qbinom</span>(<span class="fl">0.995</span>, <span class="dt">size =</span> N_sim, <span class="dt">prob =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>B)</span>
<span id="cb800-18"><a href="ch-custom.html#cb800-18" aria-hidden="true"></a><span class="kw">ggplot</span>(df_rank_lambda, <span class="kw">aes</span>(<span class="dt">x =</span> rank)) <span class="op">+</span></span>
<span id="cb800-19"><a href="ch-custom.html#cb800-19" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">breaks =</span></span>
<span id="cb800-20"><a href="ch-custom.html#cb800-20" aria-hidden="true"></a>                   <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1023</span>, <span class="dt">length.out =</span> B <span class="op">+</span><span class="st"> </span><span class="dv">1</span>),</span>
<span id="cb800-21"><a href="ch-custom.html#cb800-21" aria-hidden="true"></a>                 <span class="dt">closed =</span> <span class="st">&quot;left&quot;</span>, <span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>) <span class="op">+</span></span>
<span id="cb800-22"><a href="ch-custom.html#cb800-22" aria-hidden="true"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;count&quot;</span>) <span class="op">+</span></span>
<span id="cb800-23"><a href="ch-custom.html#cb800-23" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="kw">c</span>(ci_l, ci_u), <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></span>
<span id="cb800-24"><a href="ch-custom.html#cb800-24" aria-hidden="true"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;b&quot;</span>) <span class="op">+</span></span>
<span id="cb800-25"><a href="ch-custom.html#cb800-25" aria-hidden="true"></a><span class="st">    </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="dv">0</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:recoveryexp"></span>
<img src="bookdown_files/figure-html/recoveryexp-1.svg" alt="a) Posterior distribution of the parameter \(\lambda\) of fit_exp together with its true values as a black line. b) Rank histograms of \(\lambda\) and from our hand-made implementation of the exponential distribution. The dashed lines represent the 99% confidence interval." width="45%" /><img src="bookdown_files/figure-html/recoveryexp-2.svg" alt="a) Posterior distribution of the parameter \(\lambda\) of fit_exp together with its true values as a black line. b) Rank histograms of \(\lambda\) and from our hand-made implementation of the exponential distribution. The dashed lines represent the 99% confidence interval." width="45%" />
<p class="caption">
FIGURE 12.9: a) Posterior distribution of the parameter <span class="math inline">\(\lambda\)</span> of <code>fit_exp</code> together with its true values as a black line. b) Rank histograms of <span class="math inline">\(\lambda\)</span> and from our hand-made implementation of the exponential distribution. The dashed lines represent the 99% confidence interval.
</p>
</div>
</div>
<div id="summary-10" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> Summary<a href="ch-custom.html#summary-10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we learned how to create a PDF that is not provided by Stan. We learned how to do this by doing a change of variables (with its Jacobian adjustment) and by building the distribution from scratch in a function ending in <code>_lpdf</code>. We also learned how to verify the correctness of the new functions by recovering the true values of the parameters and by using simulation-based calibration.</p>
</div>
<div id="further-reading-9" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> Further reading<a href="ch-custom.html#further-reading-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p> Jacobian adjustments in Bayesian models are an ongoing source of confusion and there are several posts in blogs and study cases that try to shed light on them:</p>
<ul>
<li><a href="https://rstudio-pubs-static.s3.amazonaws.com/486816_440106f76c944734a7d4c84761e37388.html" class="uri">https://rstudio-pubs-static.s3.amazonaws.com/486816_440106f76c944734a7d4c84761e37388.html</a></li>
<li><a href="https://betanalpha.github.io/assets/case_studies/probability_theory.html#42_probability_density_functions" class="uri">https://betanalpha.github.io/assets/case_studies/probability_theory.html#42_probability_density_functions</a></li>
<li><a href="https://jsocolar.github.io/jacobians/" class="uri">https://jsocolar.github.io/jacobians/</a></li>
<li><a href="https://mc-stan.org/documentation/case-studies/mle-params.html" class="uri">https://mc-stan.org/documentation/case-studies/mle-params.html</a></li>
</ul>
<p>Jacobian adjustments are also relevant for adjusting priors for order constraints, which is especially important for Bayes factors <span class="citation">(Heck and Wagenmakers <a href="#ref-Heck2016" role="doc-biblioref">2016</a>)</span>.</p>
<p>A complete tutorial of  simulation-based calibration using the  <code>SBC</code> package was given in the online event  Stanconnect 2021, and it is available in <a href="https://www.martinmodrak.cz/post/2021-sbc_tutorial/" class="uri">https://www.martinmodrak.cz/post/2021-sbc_tutorial/</a>. The ideas that predate this technique can be found in <span class="citation">Cook, Gelman, and Rubin (<a href="#ref-Cooketal2006" role="doc-biblioref">2006</a>)</span> and were extended in <span class="citation">Talts et al. (<a href="#ref-talts2018validating" role="doc-biblioref">2018</a>)</span>. The use of ECDF-based visualizations is discussed in <span class="citation">Säilynoja, Bürkner, and Vehtari (<a href="#ref-sailynoja2022graphical" role="doc-biblioref">2022</a>)</span>. The role of simulation-based calibration in the Bayesian workflow is discussed in <span class="citation">Schad, Betancourt, and Vasishth (<a href="#ref-schad2020toward" role="doc-biblioref">2020</a>)</span>. Examples of the use of simulation-based calibration to validate novel models used in cognitive science are <span class="citation">Hartmann, Johannsen, and Klauer (<a href="#ref-hartmann2020rtmpt" role="doc-biblioref">2020</a>)</span> and <span class="citation">Bürkner and Charpentier (<a href="#ref-burkner2020modelling" role="doc-biblioref">2020</a>)</span>. The extension of this procedure to validate Bayes factors is discussed in <span class="citation">Schad et al. (<a href="#ref-SchadEtAlBF" role="doc-biblioref">2021</a>)</span>.</p>
<p>Custom functions in general and custom probability functions are treated in <span class="citation">Stan Development Team (<a href="#ref-Stan2023" role="doc-biblioref">2024</a>)</span> (chapter 21 of the User’s guide).</p>
</div>
<div id="sec-customexercises" class="section level2 hasAnchor" number="12.6">
<h2><span class="header-section-number">12.6</span> Exercises<a href="ch-custom.html#sec-customexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:shiftedlogn" class="exercise"><strong>Exercise 12.1  </strong></span>Fitting a  shifted log-normal distribution.</p>
</div>
<p>A random variable <span class="math inline">\(Y\)</span> has a shifted log-normal distribution with shift <span class="math inline">\(\psi\)</span>, location <span class="math inline">\(\mu\)</span>, and scale <span class="math inline">\(\sigma\)</span>, if <span class="math inline">\(Z = Y-\psi\)</span> and <span class="math inline">\(Z \sim \mathit{LogNormal}(\mu,\sigma)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Implement a <code>shifted_lognormal_ldpf</code> function in Stan with three parameters, <code>mu</code>, <code>sigma</code>, and <code>psi</code>. Tip: One can use the regular log-normal distribution and apply a change of variable. In this case the adjustment of the Jacobian would be
<span class="math inline">\(|\frac{d}{dY} Y - \psi|=1\)</span>, which in log-space is conveniently zero.</p></li>
<li><p>Verify the correctness of the model by recovering the true values of (your choice) of the parameters of the model and by using simulation-based calibration. In order to use simulation-based calibration, you will need to decide on sensible priors; assume that <span class="math inline">\(\psi \sim \mathit{Normal}_+(100,50)\)</span>, and choose priors for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> so that the prior predictive distributions are adequate for response times.</p></li>
</ol>
<div class="exercise">
<p><span id="exr:wald" class="exercise"><strong>Exercise 12.2  </strong></span>Fitting a Wald distribution.</p>
</div>
<p>The  Wald distribution (or inverse Gaussian distribution) and its variants have been proposed as another useful distribution for  response times <span class="citation">(see for example Heathcote <a href="#ref-heathcote2004fitting" role="doc-biblioref">2004</a>)</span>.</p>
<p>The probability density function of the Wald distribution is the following.</p>
<p><span class="math display">\[\begin{equation}
f(x;\mu ,\lambda )={\sqrt {\frac {\lambda }{2\pi x^{3}}}}\exp {\biggl (}-{\frac {\lambda (x-\mu )^{2}}{2\mu ^{2}x}}{\biggr )}
\end{equation}\]</span></p>
<ol style="list-style-type: decimal">
<li>Implement this distribution in Stan as <code>wald_lpdf</code>. In order to do this, you will need to derive the logarithm of the PDF presented above. You can adapt the code of the following R function.</li>
</ol>
<div class="sourceCode" id="cb801"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb801-1"><a href="ch-custom.html#cb801-1" aria-hidden="true"></a>dwald &lt;-<span class="st"> </span><span class="cf">function</span>(x, lambda, mu, <span class="dt">log =</span> <span class="ot">FALSE</span>) {</span>
<span id="cb801-2"><a href="ch-custom.html#cb801-2" aria-hidden="true"></a>  log_density &lt;-<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(lambda <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pi</span>())) <span class="op">-</span></span>
<span id="cb801-3"><a href="ch-custom.html#cb801-3" aria-hidden="true"></a><span class="st">    </span><span class="fl">1.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(x) <span class="op">-</span></span>
<span id="cb801-4"><a href="ch-custom.html#cb801-4" aria-hidden="true"></a><span class="st">    </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>((x <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span>(mu <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(x)))<span class="op">^</span><span class="dv">2</span></span>
<span id="cb801-5"><a href="ch-custom.html#cb801-5" aria-hidden="true"></a>  <span class="cf">if</span> (log <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) {</span>
<span id="cb801-6"><a href="ch-custom.html#cb801-6" aria-hidden="true"></a>    <span class="kw">exp</span>(log_density)</span>
<span id="cb801-7"><a href="ch-custom.html#cb801-7" aria-hidden="true"></a>  } <span class="cf">else</span> {</span>
<span id="cb801-8"><a href="ch-custom.html#cb801-8" aria-hidden="true"></a>    log_density</span>
<span id="cb801-9"><a href="ch-custom.html#cb801-9" aria-hidden="true"></a>  }</span>
<span id="cb801-10"><a href="ch-custom.html#cb801-10" aria-hidden="true"></a>}</span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Verify the correctness of the model by recovering the true values of (your choice) of the parameters of the model and by using simulation-based calibration. As with the previous exercise, you will need to decide on sensible priors by deriving prior predictive distributions that are adequate for response times.</li>
</ol>

</div>
</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references hanging-indent">
<div id="ref-adams2018calculus">
<p>Adams, Robert A., and Christopher Essex. 2018. <em>Calculus: A Complete Course</em>. Pearson.</p>
</div>
<div id="ref-ashby1982testing">
<p>Ashby, F. Gregory. 1982. “Testing the Assumptions of Exponential, Additive Reaction Time Models.” <em>Memory &amp; Cognition</em> 10 (2): 125–34.</p>
</div>
<div id="ref-ashby1980decomposing">
<p>Ashby, F. Gregory, and James T. Townsend. 1980. “Decomposing the Reaction Time Distribution: Pure Insertion and Selective Influence Revisited.” <em>Journal of Mathematical Psychology</em> 21 (2): 93–123.</p>
</div>
<div id="ref-box1964analysis">
<p>Box, George E. P., and David R. Cox. 1964. “An Analysis of Transformations.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, 211–52.</p>
</div>
<div id="ref-burkner2020modelling">
<p>Bürkner, Paul-Christian, and Emmanuel Charpentier. 2020. “Modelling Monotonic Effects of Ordinal Predictors in Bayesian Regression Models.” <em>British Journal of Mathematical and Statistical Psychology</em>. <a href="https://doi.org/https://doi.org/10.1111/bmsp.12195">https://doi.org/https://doi.org/10.1111/bmsp.12195</a>.</p>
</div>
<div id="ref-Cooketal2006">
<p>Cook, Samantha R., Andrew Gelman, and Donald B. Rubin. 2006. “Validation of Software for Bayesian Models Using Posterior Quantiles.” <em>Journal of Computational and Graphical Statistics</em> 15 (3): 675–92. <a href="https://doi.org/10.1198/106186006X136976">https://doi.org/10.1198/106186006X136976</a>.</p>
</div>
<div id="ref-gelman2020bayesian">
<p>Gelman, Andrew, Aki Vehtari, Daniel P. Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” <em>arXiv Preprint arXiv:2011.01808</em>.</p>
</div>
<div id="ref-Harris2012">
<p>Harris, Christopher M., and Jonathan Waddington. 2012. “On the Convergence of Time Interval Moments: Caveat Sciscitator.” <em>Journal of Neuroscience Methods</em> 205 (2): 345–56. <a href="https://doi.org/https://doi.org/10.1016/j.jneumeth.2012.01.017">https://doi.org/https://doi.org/10.1016/j.jneumeth.2012.01.017</a>.</p>
</div>
<div id="ref-HarrisEtAl2014">
<p>Harris, Christopher M., Jonathan Waddington, Valerio Biscione, and Sean Manzi. 2014. “Manual Choice Reaction Times in the Rate-Domain.” <em>Frontiers in Human Neuroscience</em> 8: 418. <a href="https://doi.org/10.3389/fnhum.2014.00418">https://doi.org/10.3389/fnhum.2014.00418</a>.</p>
</div>
<div id="ref-hartmann2020rtmpt">
<p>Hartmann, Raphael, Lea Johannsen, and Karl Christoph Klauer. 2020. “rtmpt: An R Package for Fitting Response-Time Extended Multinomial Processing Tree Models.” <em>Behavior Research Methods</em> 52 (3): 1313–38.</p>
</div>
<div id="ref-heathcote2004fitting">
<p>Heathcote, Andrew. 2004. “Fitting Wald and ex-Wald Distributions to Response Time Data: An Example Using Functions for the S-Plus Package.” <em>Behavior Research Methods, Instruments, &amp; Computers</em> 36 (4): 678–94. <a href="https://doi.org/https://doi.org/10.3758/BF03206550">https://doi.org/https://doi.org/10.3758/BF03206550</a>.</p>
</div>
<div id="ref-Heck2016">
<p>Heck, Daniel W, and Eric-Jan Wagenmakers. 2016. “Adjusted Priors for Bayes Factors Involving Reparameterized Order Constraints.” <em>Journal of Mathematical Psychology</em> 73: 110–16. <a href="https://doi.org/https://doi.org/10.1016/j.jmp.2016.05.004">https://doi.org/https://doi.org/10.1016/j.jmp.2016.05.004</a>.</p>
</div>
<div id="ref-R-SBC">
<p>Kim, Shinyoung, Hyunji Moon, Martin Modrák, and Teemu Säilynoja. 2024. <em>SBC: Simulation Based Calibration for Rstan/Cmdstanr Models</em>. <a href="https://hyunjimoon.github.io/SBC/">https://hyunjimoon.github.io/SBC/</a>.</p>
</div>
<div id="ref-lynch2007introduction">
<p>Lynch, Scott Michael. 2007. <em>Introduction to Applied Bayesian Statistics and Estimation for Social Scientists</em>. New York, NY: Springer.</p>
</div>
<div id="ref-ModrakEtAl2023">
<p>Modrák, Martin, Angie H. Moon, Shinyoung Kim, Paul-Christian Bürkner, Niko Huurre, Kateřina Faltejsková, Andrew Gelman, and Aki Vehtari. 2023. “Simulation-Based Calibration Checking for Bayesian Computation: The Choice of Test Quantities Shapes Sensitivity.” <em>Bayesian Analysis</em>, 1–28. <a href="https://doi.org/10.1214/23-BA1404">https://doi.org/10.1214/23-BA1404</a>.</p>
</div>
<div id="ref-RossProb">
<p>Ross, Sheldon. 2002. <em>A First Course in Probability</em>. Pearson Education.</p>
</div>
<div id="ref-sailynoja2022graphical">
<p>Säilynoja, Teemu, Paul-Christian Bürkner, and Aki Vehtari. 2022. “Graphical Test for Discrete Uniformity and Its Applications in Goodness-of-Fit Evaluation and Multiple Sample Comparison.” <em>Statistics and Computing</em> 32 (2): 1–21. <a href="https://doi.org/https://doi.org/10.1007/s11222-022-10090-6">https://doi.org/https://doi.org/10.1007/s11222-022-10090-6</a>.</p>
</div>
<div id="ref-salas2003calculus">
<p>Salas, Saturnino L., Garret J. Etgen, and Einar Hille. 2003. <em>Calculus: One and Several Variables</em>. Ninth. John Wiley &amp; Sons.</p>
</div>
<div id="ref-schad2020toward">
<p>Schad, Daniel J., Michael J. Betancourt, and Shravan Vasishth. 2019. “Toward a Principled Bayesian Workflow in Cognitive Science.” <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/ARXIV.1904.12765">https://doi.org/10.48550/ARXIV.1904.12765</a>.</p> 2020. “Toward a Principled Bayesian Workflow in Cognitive Science.” <em>Psychological Methods</em> 26 (1): 103–26. <a href="https://doi.org/https://doi.org/10.1037/met0000275">https://doi.org/https://doi.org/10.1037/met0000275</a>.</p>
</div>
<div id="ref-SchadEtAlBF">
<p>Schad, Daniel J., Bruno Nicenboim, Paul-Christian Bürkner, Michael J. Betancourt, and Shravan Vasishth. 2021. “Workflow Techniques for the Robust Use of Bayes Factors.” <em>arXiv Preprint arXiv:2103.08744</em>.</p>
</div>
<div id="ref-Stan2023">
<p>Stan Development Team. 2024. “Stan Modeling Language Users Guide and Reference Manual, Version 2.32.” <a href="https://mc-stan.org/docs/2_35/">https://mc-stan.org/docs/2_35/</a>.</p>
</div>
<div id="ref-talts2018validating">
<p>Talts, Sean, Michael J. Betancourt, Daniel P. Simpson, Aki Vehtari, and Andrew Gelman. 2018. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” <em>arXiv Preprint arXiv:1804.06788</em>.</p>
</div>
<div id="ref-WuKaiserVasishth2017">
<p>Wu, Fuyun, Elsi Kaiser, and Shravan Vasishth. 2017. “Effects of Early Cues on the Processing of Chinese Relative Clauses: Evidence for Experience-Based Theories.” <em>Cognitive Science</em> 42: 1101–33.</p>
</div>
<div id="ref-yao2018yes">
<p>Yao, Yuling, Aki Vehtari, Daniel P. 2018. “Yes, but Did It Work?: Evaluating Variational Inference.” In <em>International Conference on Machine Learning</em>, 5581–90. PMLR.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="44">
<li id="fn44"><p>In this specific case, we could also transform the dependent variable first, but as will become clear later, we want to avoid changing our dependent variable.<a href="ch-custom.html#fnref44" class="footnote-back">↩︎</a></p></li>
<li id="fn45"><p>Not every transformation is valid, univariate changes of variables must be monotonic and differentiable, multivariate changes of variables must be surjective and differentiable.<a href="ch-custom.html#fnref45" class="footnote-back">↩︎</a></p></li>
<li id="fn46"><p>In the multivariate case, it is equal to the absolute determinant of the Jacobian, the matrix of all its first-order partial derivatives, of the transform; see section 6.7 of <span class="citation">Ross (<a href="#ref-RossProb" role="doc-biblioref">2002</a>)</span>, and p. 707 of <span class="citation">Adams and Essex (<a href="#ref-adams2018calculus" role="doc-biblioref">2018</a>)</span>.<a href="ch-custom.html#fnref46" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p>We are working under the assumption that Stan, which yields the posterior approximation, works correctly. In principle, if we assume that our model is correct, we can also use simulation-based calibration to examine whether our approximation to the posterior distribution is correct (that is, whether Stan’s sampler, or any posterior approximation method works correctly); for an example, see <span class="citation">Yao et al. (<a href="#ref-yao2018yes" role="doc-biblioref">2018</a>)</span>.<a href="ch-custom.html#fnref47" class="footnote-back">↩︎</a></p></li>
<li id="fn48"><p>Even though the package is already fully functional, function names and arguments might change by the time this book is published.<a href="ch-custom.html#fnref48" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-complexstan.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-remame.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
