<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Bayesian regression models | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Bayesian regression models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Bayesian regression models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2022-09-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-compbda.html"/>
<link rel="next" href="ch-hierarchical.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-anyway"><i class="fa fa-check"></i>Why read this book anyway?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b> Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b> Bayesian Regression Models using Stan: brms</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b> Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.6.1" data-path="ch-compbda.html"><a href="ch-compbda.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.6.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.6.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.6.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.6.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.6.3</b> Re-fitting a single subject pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.7</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.9</b> Further reading</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect response times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b> No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a><ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#model-building"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#exemplary-data-analysis"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-workflow.html"><a href="ch-workflow.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-workflow.html"><a href="ch-workflow.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a><ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-7"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-9"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-10"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a><ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b> Validation of a computed posterior distribution</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#simulation-based-calibration-revealing-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> Simulation-based calibration revealing a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-and-limitation-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues and limitation of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#a-custom-distribution-re-implementing-the-exponential-distribution-manually"><i class="fa fa-check"></i><b>12.3</b> A custom distribution: Re-implementing the exponential distribution manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-11"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Other useful models</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-12"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a><ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a><ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a><ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a><ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#summary-13"><i class="fa fa-check"></i><b>15.6</b> Summary</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.7</b> Further reading</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a><ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#issues-with-cross-validation"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a><ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-14"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a><ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.3</b> Further reading</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a><ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a><ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a><ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a><ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-reg" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 4</span> Bayesian regression models<a href="ch-reg.html#ch-reg" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We generally run experiments because we are interested in the relationship between two or more variables. A regression will tell us how our <em>dependent variable</em>, also called the <em>response</em> or <em>outcome variable</em> (e.g., pupil size, response times, accuracy, etc.) is affected by one or many <em>independent variables</em>, <em>predictors</em>, or <em>explanatory variables</em>. Predictors can be categorical (e.g., male or female), ordinal (first, second, third, etc.), or continuous (e.g., age). In this chapter we focus on simple regression models with different likelihood functions.</p>
<div id="sec-pupil" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.1</span> A first linear regression: Does attentional load affect pupil size?<a href="ch-reg.html#sec-pupil" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’ll look at the effect of cognitive processing on human pupil size to illustrate the use of Bayesian linear regression models. Although pupil size is mostly related to the amount of light that reaches the retina or the distance to a perceived object, pupil sizes are also systematically influenced by cognitive processing: Increased cognitive load leads to an increase in the pupil size <span class="citation">(for a review, see Mathot <a href="#ref-mathotPupillometryPsychologyPhysiology2018">2018</a>)</span>.</p>
<p>For this example, we’ll use the data from one subject’s pupil size of the control experiment by <span class="citation">Wahn et al. (<a href="#ref-wahnPupilSizesScale2016">2016</a>)</span>, averaged by trial. The data are available from <code>df_pupil</code> in the package <code>bcogsci</code>.
In this experiment, the subject covertly tracks between zero and five objects among several randomly moving objects on a computer screen. This task is called multiple object tracking <span class="citation">(or MOT; see Pylyshyn and Storm <a href="#ref-pylyshynTrackingMultipleIndependent1988">1988</a>)</span>. First, several objects appear on the screen, and a subset of them are indicated as “targets” at the beginning. Then, the objects start moving randomly across the screen and become indistinguishable. After several seconds, the objects stop moving and the subject need to indicate which objects were the targets. See Figure <a href="ch-reg.html#fig:mot">4.1</a>. Our research goal is to examine how the number of moving objects being tracked–that is, how the attentional load–affects pupil size.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mot"></span>
<img src="cc_figure/MOT.png" alt="Flow of events in a trial where two objects need to be tracked. Adapted from Blumberg, Peterson, and Parasuraman (2015); licensed under CC BY 4.0." width="80%" />
<p class="caption">
FIGURE 4.1: Flow of events in a trial where two objects need to be tracked. Adapted from <span class="citation">Blumberg, Peterson, and Parasuraman (<a href="#ref-Blumberg2015">2015</a>)</span>; licensed under CC BY 4.0.
</p>
</div>
<div id="likelihood-and-priors" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.1</span> Likelihood and priors<a href="ch-reg.html#likelihood-and-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will model pupil size as normally distributed, because we are not expecting a skew, and we have no further information available about the distribution of pupil sizes. (Pupil sizes cannot be of size zero or negative, so we know for sure that this choice is not exactly right.) For simplicity, we are also going to assume a linear relationship between load and the pupil size.</p>
<p>Let’s summarize our assumptions:</p>
<ol style="list-style-type: decimal">
<li>There is some average pupil size represented by <span class="math inline">\(\alpha\)</span>.</li>
<li>The increase of attentional load has a linear relationship with pupil size, determined by <span class="math inline">\(\beta\)</span>.</li>
<li>There is some noise in this process, that is, variability around the true pupil size i.e., a scale, <span class="math inline">\(\sigma\)</span>.</li>
<li>The noise is normally distributed.</li>
</ol>
<p>The generative probability density function will be as follows:</p>
<p><span class="math display">\[\begin{equation}
p\_size_n \sim \mathit{Normal}(\alpha + c\_load_n \cdot \beta,\sigma)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(n\)</span> indicates the observation number with <span class="math inline">\(n = 1, \ldots, N\)</span>.</p>
<p>This means that the formula that we’ll use in <code>brms</code> will be <code>p_size ~ 1 + c_load</code>, where <code>1</code> represents the intercept, <span class="math inline">\(\alpha\)</span>, which doesn’t depend on a predictor, and <code>c_load</code> is our predictor that is multiplied by <span class="math inline">\(\beta\)</span>. We will generally indicate with the prefix <code>c_</code>, that a predictor (in this case load) is centered (i.e., we subtract from each value the mean of all values). If load is centered, the intercept represents the pupil size at the average load in the experiment (because at the average load, the centered load is zero, and then <span class="math inline">\(\alpha + 0 \cdot \beta\)</span>). Alternatively, if the load had not been centered (i.e., starts with no load, then one, two, etc.), then the intercept would represent the pupil size when there is no load. Although we can fit a frequentist model with <code>lm(p_size ~ 1 + c_load, data set)</code>, when we fit a Bayesian model, we have to specify priors for each of the parameters.</p>
<p>For setting the priors, we need to do some research and find some information about pupil sizes. Although we might know that pupil diameters range between 2 to 4 mm in bright light to 4 to 8 mm in the dark <span class="citation">(Spector <a href="#ref-spectorPupils1990">1990</a>)</span>, this experiment was conducted with the Eyelink-II eyetracker which measures the pupils in arbitrary units <span class="citation">(Hayes and Petrov <a href="#ref-hayesMappingCorrectingInfluence2016">2016</a>)</span>. If this is our first analysis of pupil size, before setting up the priors, we’ll need to look at some measures of pupil size. (If we had analyzed this type of data before, we could also look at estimates from previous experiments). Fortunately, we have some measurements of the same subject with no attentional load for the first 100 ms, measured every 10 ms, in the data frame <code>df_pupil_pilot</code> from the package <code>bcogsci</code>: This will give us some idea about the order of magnitude of our dependent variable.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_pupil_pilot&quot;</span>)</a>
<a class="sourceLine" id="cb151-2" data-line-number="2">df_pupil_pilot<span class="op">$</span>p_size <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summary</span>()</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     852     856     862     861     866     868</code></pre>
<p>With this information we can set a regularizing prior for <span class="math inline">\(\alpha\)</span>. We center the prior around 1000 to be in the right order of magnitude.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a> Since we don’t know how much pupil sizes are going to vary by load yet, we include a rather wide prior by defining it as a normal distribution and setting its standard deviation as <span class="math inline">\(500\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\alpha \sim \mathit{Normal}(1000, 500) 
\end{equation}\]</span></p>
<p>Given that our predictor load is centered, with the prior for <span class="math inline">\(\alpha\)</span>, we are saying that we suspect that the average pupil size for the average load in the experiment will be in a 95% credible interval limited by approximately <span class="math inline">\(1000 \pm 2 \cdot 500 = [0, 2000]\)</span> units. We can calculate this with more precision in <code>R</code> using the <code>qnorm</code> function:</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb153-1" data-line-number="1"><span class="kw">qnorm</span>(<span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>), <span class="dt">mean =</span> <span class="dv">1000</span>, <span class="dt">sd =</span> <span class="dv">500</span>)</a></code></pre></div>
<pre><code>## [1]   20 1980</code></pre>
<p>We know that the measurements of the pilot data are strongly correlated because they were taken 10 milliseconds apart. For this reason, they won’t tell us how much the pupil size can vary. We set up quite an uninformative prior for <span class="math inline">\(\sigma\)</span> that encodes our lack of precise information: <span class="math inline">\(\sigma\)</span> is surely larger than zero and has to be in the order of magnitude of the pupil size with no load.</p>
<p><span class="math display">\[\begin{equation}
\sigma \sim \mathit{Normal}_+(0, 1000)
\end{equation}\]</span></p>
<p>With this prior for <span class="math inline">\(\sigma\)</span>, we are saying that we expect that the standard deviation of the pupil sizes should be in the following 95% credible interval.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" data-line-number="1">  <span class="kw">qtnorm</span>(<span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>), <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1000</span>, <span class="dt">a =</span> <span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1]   31.3 2241.4</code></pre>
<p>In order to compute the 95% credible interval, we used <code>qtnorm</code> from the <code>extraDistr</code> package rather than <code>qnorm()</code>. As mentioned earlier, the relevant command specification is <code>qtnorm(..., a = 0)</code>; recall that <code>a = 0</code> indicates a truncated normal distribution, truncated at the left by zero.</p>
<p>The mean of <span class="math inline">\(\mathit{Normal}_+\)</span>, a normal distribution truncated at zero so as to allow for only positive values, does not coincide with its location indicated with the parameter <span class="math inline">\(\mu\)</span> (and neither does the standard deviation coincide with the scale, <span class="math inline">\(\sigma\)</span>); see Box <a href="ch-reg.html#thm:truncation">4.1</a>.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb157-1" data-line-number="1">samples &lt;-<span class="st"> </span><span class="kw">rtnorm</span>(<span class="dv">20000</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1000</span>, <span class="dt">a =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb157-2" data-line-number="2"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(samples), <span class="dt">sd =</span> <span class="kw">sd</span>(samples))</a></code></pre></div>
<pre><code>## mean   sd 
##  801  601</code></pre>
<p>We still need to set a prior for <span class="math inline">\(\beta\)</span>, the change in pupil size produced by the attentional load. Given that pupil size changes are not easily perceptible (we don’t usually observe changes in pupil size in our day-to-day life), we expect them to be much smaller than the pupil size (which we assume has mean 1000 units), so we use the following prior:</p>
<p><span class="math display">\[\begin{equation}
\beta \sim \mathit{Normal}(0, 100)
\end{equation}\]</span></p>
<p>With the prior of <span class="math inline">\(\beta\)</span>, we are saying that we don’t really know if the attentional load will increase or even decrease the pupil size (it is centered at zero), but we do know that one unit of load (that is one more object to track) will potentially change the pupil size in a way that is consistent with the following 95% credible interval.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb159-1" data-line-number="1"><span class="kw">qnorm</span>(<span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>), <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">100</span>)</a></code></pre></div>
<pre><code>## [1] -196  196</code></pre>
<p>That is, we don’t expect changes in size that increase or decrease the pupil size more than 200 units for one unit increase in load.</p>
<p>The priors we have specified here are relatively uninformative; as mentioned earlier, this is because we don’t have much prior experience with pupil size studies. In other settings, we might have more prior knowledge and experience; in that case, we would use somewhat more principled priors. We will return to this point in the chapter on priors (chapter <a href="ch-priors.html#ch-priors">6</a>) and on a Bayesian workflow (chapter <a href="ch-workflow.html#ch-workflow">7</a>).</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:truncation" class="theorem"><strong>Box 4.1  </strong></span><strong>Truncated distributions</strong></p>
</div>
<p>Any distribution can be truncated. For a continuous distribution, the truncated version of the original distribution will have non-zero probability density values for a continuous subset of the original coverage. To make this more concrete, in our previous example, the normal distribution has coverage for values between minus infinity to plus infinity, and our truncated version <span class="math inline">\(Normal_+\)</span> has coverage between zero and plus infinity: all negative values have a density of zero. Let’s see how we can generalize this to be able to understand any truncation of any continuous distribution. (For the discrete case, we can simply replace the integral with a sum, and replace PDF with PMF).</p>
<p>From the axiomatic definitions of probability, we know that the area below a PDF, <span class="math inline">\(f(x)\)</span>, must be equal to one (section <a href="ch-intro.html#introprob">1.1</a>). More formally, this means that the integral of <span class="math inline">\(f\)</span> evaluated as <span class="math inline">\(f(-\infty &lt;X &lt; \infty)\)</span> should be equal to one:</p>
<p><span class="math display">\[\begin{equation}
\int_{-\infty}^{\infty} f(x) dx = 1
\end{equation}\]</span></p>
<p>But if the distribution is truncated, <span class="math inline">\(f\)</span> is going to be evaluated in some subset of its possible values, <span class="math inline">\(f(a &lt;X &lt; b)\)</span>; in the specific case of <span class="math inline">\(Normal_+\)</span>, for example, <span class="math inline">\(a = 0\)</span>, and <span class="math inline">\(b=\infty\)</span>. In the general case, this means that the integral of the PDF evaluated for <span class="math inline">\(a &lt;X &lt; b\)</span> will be lower than one, unless <span class="math inline">\(a=-\infty\)</span> and <span class="math inline">\(b=+\infty\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\int_{a}^{b} f(x) dx &lt; 1
\end{equation}\]</span></p>
<p>We want to ensure that we build a new PDF for the truncated distribution so that even though it has less coverage than the non-truncated version, it still integrates to one. To achieve this, we divide the “unnormalized” PDF by the total area of <span class="math inline">\(f(a &lt;X &lt; b)\)</span> (recall the discussion surrounding Equation <a href="ch-intro.html#eq:factork">(1.1)</a>):</p>
<p><span class="math display">\[\begin{equation}
f_{[a,b]}(x) = \frac{f(x)}{\int_{a}^{b} f(x) dx}
\end{equation}\]</span></p>
<p>The denominator of the previous equation is the difference between the CDF evaluated at <span class="math inline">\(X = b\)</span> and the CDF evaluated at <span class="math inline">\(X =a\)</span>; this can be written as <span class="math inline">\(F(b) - F(a)\)</span>:</p>
<p><span class="math display" id="eq:truncPDF">\[\begin{equation}
f_{[a,b]}(x) = \frac{f(x)}{F(b) - F(a)}
\tag{4.1}
\end{equation}\]</span></p>
<p>For the specific case, where <span class="math inline">\(f(x)\)</span> is <span class="math inline">\(Normal(x | 0, \sigma)\)</span> and we want the PDF of <span class="math inline">\(Normal_+(x | 0, \sigma)\)</span>, and thus <span class="math inline">\(a= 0\)</span> and <span class="math inline">\(b =\infty\)</span>.</p>
<p><span class="math display">\[\begin{equation}
Normal_+(x |0, \sigma) = \frac{Normal(x | 0, \sigma)}{1/2}
\end{equation}\]</span></p>
<p>Because <span class="math inline">\(F(X= b =\infty) = 1\)</span> and <span class="math inline">\(F(X = a = 0) = 1/2\)</span>.</p>
<p>You can verify this in R (and this is valid for any value of <code>sd</code>).</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb161-1" data-line-number="1"><span class="kw">dnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">0</span>) <span class="op">*</span><span class="st"> </span><span class="dv">2</span> <span class="op">==</span><span class="st"> </span><span class="kw">dtnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">a =</span> <span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Unless the truncation of the normal distribution is symmetrical, the mean <span class="math inline">\(\mu\)</span>, of the truncated normal does not coincide with the mean <span class="math inline">\(\hat{\mu}\)</span> of the parent (untruncated) normal distribution, and for any type of truncation, the standard deviation of the truncated distribution <span class="math inline">\(\sigma\)</span> does not coincide with the standard deviation <span class="math inline">\(\hat\sigma\)</span> of the parent distribution. Confusingly enough, the arguments of the family of truncated functions <code>*tnorm</code> keeps the names of the family of functions <code>*norm</code>, the terms <code>mean</code> and <code>sd</code>. So, when defining a truncated normal distribution like <code>dtnorm(mean = 300, sd = 100, a = 0, b = Inf)</code>, the <code>mean</code> and <code>sd</code> refer to the mean <span class="math inline">\(\hat{\mu}\)</span> and standard deviation <span class="math inline">\(\hat\sigma\)</span> of the untruncated parent distribution.</p>
<p>Sometimes one needs to model some observed data as coming from a truncated normal distribution. An example would be a vector of observed standard deviations; perhaps one wants to use these estimates to work out a truncated normal prior. In order to derive such an empirically motivated prior, we have to work out what mean and standard deviation we need to use in a truncated normal distribution. We could compute the mean and standard deviation from the vector of standard deviations, and then use the procedure shown below to work out the mean and standard deviation that we would need to put into the truncated normal distribution. This approach is used in chapter <a href="ch-priors.html#ch-priors">6</a>, section <a href="ch-priors.html#sec-varcomppriors">6.1.4</a> for working out a prior based on standard deviation estimates from existing data.</p>
<p>The mean and standard deviation of the parent distribution of a truncated normal (<span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>) with boundaries <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, given the mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> of the truncated normal, are computed as follows <span class="citation">(Johnson, Kotz, and Balakrishnan <a href="#ref-johnson">1995</a>)</span>. <span class="math inline">\(\phi(X)\)</span> is the PDF of the standard normal (i.e., <span class="math inline">\(\mathit{Normal}(\mu=0, \sigma=1)\)</span>) evaluated at <span class="math inline">\(X\)</span>, and <span class="math inline">\(\Phi(X)\)</span> is the CDF of the standard normal evaluated at <span class="math inline">\(X\)</span>.</p>
<p>First, define two terms <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> for convinience:</p>
<p><span class="math display">\[\begin{align}
\alpha =(a-\hat\mu )/\hat\sigma &amp;&amp; \beta =(b-\hat\mu )/\hat\sigma
\end{align}\]</span></p>
<p>Then, the mean <span class="math inline">\(\mu\)</span> of the truncated distribution can be computed as follows based on the parameters of the parent distribution:</p>
<p><span class="math display" id="eq:meantrunc">\[\begin{equation}
\mu  = \hat\mu - \hat\sigma {\frac {\phi (\beta )-\phi (\alpha )}{\Phi (\beta )-\Phi (\alpha )}} 
\tag{4.2}
\end{equation}\]</span></p>
<p>The variance <span class="math inline">\(\sigma^2\)</span> of the truncated distribution is:</p>
<p><span class="math display" id="eq:vartrunc">\[\begin{equation}
\sigma^2 = \hat\sigma^2 \times \left( 1 -  \frac{\beta \phi (\alpha )-\alpha \phi (\beta )}{\Phi (\beta )-\Phi (\alpha )}  - 
\left(\frac {\phi (\alpha )-\phi (\beta )}{\Phi (\beta )-\Phi (\alpha )}\right)^2
\right)
\tag{4.3}
\end{equation}\]</span></p>
<p>Equations <a href="ch-reg.html#eq:meantrunc">(4.2)</a> and <a href="ch-reg.html#eq:vartrunc">(4.3)</a> have two variables, so if one is given the values for the truncated distribution <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, one can solve (using algebra) for the mean and standard deviation of the untruncated distribution, <span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>.</p>
<p>For example, suppose that <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=500\)</span>, and that the mean and standard deviation of the untruncated parent distribution is <span class="math inline">\(\hat\mu=300\)</span> and <span class="math inline">\(\hat\sigma=200\)</span>. We can simulate such a situation and estimate the mean and standard deviation of the truncated distribution:</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb163-1" data-line-number="1">x &lt;-<span class="st"> </span><span class="kw">rtnorm</span>(<span class="dv">10000000</span>, <span class="dt">mean =</span> <span class="dv">300</span>, <span class="dt">sd =</span> <span class="dv">200</span>, <span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">500</span>)</a>
<a class="sourceLine" id="cb163-2" data-line-number="2"><span class="co">## the mean and sd of the truncated distributions</span></a>
<a class="sourceLine" id="cb163-3" data-line-number="3"><span class="co">## using simulation:</span></a>
<a class="sourceLine" id="cb163-4" data-line-number="4"><span class="kw">mean</span>(x)</a></code></pre></div>
<pre><code>## [1] 271</code></pre>
<div class="sourceCode" id="cb165"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb165-1" data-line-number="1"><span class="kw">sd</span>(x)</a></code></pre></div>
<pre><code>## [1] 129</code></pre>
<p>These simulated values are identical to the values computed using equations <a href="ch-reg.html#eq:meantrunc">(4.2)</a> and <a href="ch-reg.html#eq:vartrunc">(4.3)</a>:</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" data-line-number="1">a &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb167-2" data-line-number="2">b &lt;-<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb167-3" data-line-number="3">bar_x &lt;-<span class="st"> </span><span class="dv">300</span></a>
<a class="sourceLine" id="cb167-4" data-line-number="4">bar_sigma &lt;-<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb167-5" data-line-number="5">alpha &lt;-<span class="st"> </span>(a <span class="op">-</span><span class="st"> </span>bar_x) <span class="op">/</span><span class="st"> </span>bar_sigma</a>
<a class="sourceLine" id="cb167-6" data-line-number="6">beta &lt;-<span class="st"> </span>(b <span class="op">-</span><span class="st"> </span>bar_x) <span class="op">/</span><span class="st"> </span>bar_sigma</a>
<a class="sourceLine" id="cb167-7" data-line-number="7">term1 &lt;-<span class="st"> </span>((<span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span></a>
<a class="sourceLine" id="cb167-8" data-line-number="8"><span class="st">            </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)))</a>
<a class="sourceLine" id="cb167-9" data-line-number="9">term2 &lt;-<span class="st"> </span>((beta <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span>alpha <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span></a>
<a class="sourceLine" id="cb167-10" data-line-number="10"><span class="st">            </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)))</a>
<a class="sourceLine" id="cb167-11" data-line-number="11"><span class="co">## the mean and sd of the truncated distribution</span></a>
<a class="sourceLine" id="cb167-12" data-line-number="12"><span class="co">## computed analytically:</span></a>
<a class="sourceLine" id="cb167-13" data-line-number="13">(mu &lt;-<span class="st"> </span>bar_x <span class="op">-</span><span class="st"> </span>bar_sigma <span class="op">*</span><span class="st"> </span>term1)</a></code></pre></div>
<pre><code>## [1] 271</code></pre>
<div class="sourceCode" id="cb169"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" data-line-number="1">(sigma &lt;-<span class="st"> </span><span class="kw">sqrt</span>(bar_sigma<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>term2 <span class="op">-</span><span class="st"> </span>term1<span class="op">^</span><span class="dv">2</span>)))</a></code></pre></div>
<pre><code>## [1] 129</code></pre>
<p>The equations for the mean and variance of the truncated distribution (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>) can also be used to work out the mean and variance of the parent untruncated distribution (<span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>), if one has estimates for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> (from data).</p>
<p>Suppose that we have observed data with mean <span class="math inline">\(\mu = 271\)</span> and <span class="math inline">\(\sigma=129\)</span>. We want to assume that the data are coming from a truncated normal which has lower bound <span class="math inline">\(0\)</span> and upper bound <span class="math inline">\(500\)</span>. What are the mean and standard deviation of the parent distribution, <span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>?</p>
<p>To answer this question, first rewrite the equations as follows:</p>
<p><span class="math display" id="eq:meantruncrewritten">\[\begin{equation}
\mu  - \hat\mu + \hat\sigma {\frac {\phi (\beta )-\phi (\alpha )}{\Phi (\beta )-\Phi (\alpha )}} = 0
\tag{4.4}
\end{equation}\]</span></p>
<p>The variance <span class="math inline">\(\sigma^2\)</span> of the truncated distribution is:</p>
<p><span class="math display" id="eq:vartruncrewritten">\[\begin{equation}
\sigma^2 - \hat\sigma^2 \times \left( 1 -  \frac{\beta \phi (\alpha )-\alpha \phi (\beta )}{\Phi (\beta )-\Phi (\alpha )}  - 
\left(\frac {\phi (\alpha )-\phi (\beta )}{\Phi (\beta )-\Phi (\alpha )}\right)^2
\right) = 0
\tag{4.5}
\end{equation}\]</span></p>
<p>Next, solve for <span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span> given the observed mean and the standard deviation of the truncated distribution, and that one knows the boundaries (<span class="math inline">\(a\)</span>, and <span class="math inline">\(b\)</span>).</p>
<p>Define the system of equations according to the specifications of <code>multiroot</code> from the package <code>rootSolve</code>: <code>x</code> for the unknowns (<span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span>), and <code>parms</code> for the known parameters: <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and the mean and standard deviation of the truncated normal.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" data-line-number="1">eq_system &lt;-<span class="st"> </span><span class="cf">function</span>(x, parms) {</a>
<a class="sourceLine" id="cb171-2" data-line-number="2">  mu_hat &lt;-<span class="st"> </span>x[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb171-3" data-line-number="3">  sigma_hat &lt;-<span class="st"> </span>x[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb171-4" data-line-number="4">  alpha &lt;-<span class="st"> </span>(parms[<span class="st">&quot;a&quot;</span>] <span class="op">-</span><span class="st"> </span>mu_hat) <span class="op">/</span><span class="st"> </span>sigma_hat</a>
<a class="sourceLine" id="cb171-5" data-line-number="5">  beta &lt;-<span class="st"> </span>(parms[<span class="st">&quot;b&quot;</span>] <span class="op">-</span><span class="st"> </span>mu_hat) <span class="op">/</span><span class="st"> </span>sigma_hat</a>
<a class="sourceLine" id="cb171-6" data-line-number="6">  <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb171-7" data-line-number="7">    <span class="dt">F1 =</span> parms[<span class="st">&quot;mu&quot;</span>] <span class="op">-</span><span class="st"> </span>mu_hat <span class="op">+</span><span class="st"> </span>sigma_hat <span class="op">*</span></a>
<a class="sourceLine" id="cb171-8" data-line-number="8"><span class="st">   </span>(<span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span><span class="st"> </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)),</a>
<a class="sourceLine" id="cb171-9" data-line-number="9">    <span class="dt">F2 =</span> parms[<span class="st">&quot;sigma&quot;</span>] <span class="op">-</span></a>
<a class="sourceLine" id="cb171-10" data-line-number="10"><span class="st">    </span>sigma_hat <span class="op">*</span></a>
<a class="sourceLine" id="cb171-11" data-line-number="11"><span class="st">    </span><span class="kw">sqrt</span>((<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>((beta) <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span>(alpha) <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span></a>
<a class="sourceLine" id="cb171-12" data-line-number="12"><span class="st">            </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)) <span class="op">-</span></a>
<a class="sourceLine" id="cb171-13" data-line-number="13"><span class="st">            </span>((<span class="kw">dnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">dnorm</span>(alpha)) <span class="op">/</span></a>
<a class="sourceLine" id="cb171-14" data-line-number="14"><span class="st">               </span>(<span class="kw">pnorm</span>(beta) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(alpha)))<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb171-15" data-line-number="15">  )</a>
<a class="sourceLine" id="cb171-16" data-line-number="16">}</a></code></pre></div>
<p>Solving the two equations using <code>multiroot()</code> from the package <code>rootSolve</code> gives us the mean and standard deviation <span class="math inline">\(\hat\mu\)</span> and <span class="math inline">\(\hat\sigma\)</span> of the parent normal distribution. (Notice that <code>x</code> is a required parameter of the previous function so that it works with <code>multiroot()</code>, however, outside of the function the variable <code>x</code> is a vector containing the samples of the truncated normal distribution generated with <code>rtnorm()</code>).</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" data-line-number="1">soln &lt;-<span class="st"> </span><span class="kw">multiroot</span>(<span class="dt">f =</span> eq_system, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</a>
<a class="sourceLine" id="cb172-2" data-line-number="2">                  <span class="dt">parms =</span> <span class="kw">c</span>(<span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">500</span>,</a>
<a class="sourceLine" id="cb172-3" data-line-number="3">                            <span class="dt">mu =</span> <span class="kw">mean</span>(x), <span class="dt">sigma =</span> <span class="kw">sd</span>(x)))</a>
<a class="sourceLine" id="cb172-4" data-line-number="4">soln<span class="op">$</span>root</a></code></pre></div>
<pre><code>## [1] 300 200</code></pre>
</div>
</div>
<div id="the-brms-model" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.2</span> The <code>brms</code> model<a href="ch-reg.html#the-brms-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before fitting the <code>brms</code> model of the effect of load on pupil size, load the data and center the predictor <code>load</code>:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_pupil&quot;</span>)</a>
<a class="sourceLine" id="cb174-2" data-line-number="2">(df_pupil &lt;-<span class="st"> </span>df_pupil <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb174-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_load =</span> load <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(load)))</a></code></pre></div>
<pre><code>## # A tibble: 41 × 5
##    subj trial  load p_size c_load
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1   701     1     2  1021. -0.439
## 2   701     2     1   951. -1.44 
## 3   701     3     5  1064.  2.56 
## # … with 38 more rows</code></pre>
<p>Now fit the <code>brms</code> model:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" data-line-number="1">fit_pupil &lt;-<span class="st"> </span><span class="kw">brm</span>(p_size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_load,</a>
<a class="sourceLine" id="cb176-2" data-line-number="2">  <span class="dt">data =</span> df_pupil,</a>
<a class="sourceLine" id="cb176-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb176-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb176-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">1000</span>, <span class="dv">500</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb176-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb176-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_load)</a>
<a class="sourceLine" id="cb176-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb176-9" data-line-number="9">)</a></code></pre></div>
<p>The only difference from our previous models is that we now have a predictor in the formula and in the priors. Priors for predictors are indicated with <code>class = b</code>, and the specific predictor with <code>coef = c_load</code>. If we want to set the same priors to different predictors we can omit the argument <code>coef</code>. We can remove the <code>1</code> of the formula, and <code>brm()</code> will fit the exact same model as when we specify <code>1</code> explicitly. If we really want to remove the intercept we indicate this with <code>0 +...</code> or <code>-1 +...</code>. See also the Box <a href="ch-reg.html#thm:intercept">4.2</a> for more details about the treatment of the intercepts by <code>brms</code>. The priors are normal distributions for the intercept (<span class="math inline">\(\alpha\)</span>) and the slope (<span class="math inline">\(\beta\)</span>), and a truncated normal distribution for the scale of the likelihood (<span class="math inline">\(\sigma\)</span>), which in this case, since we are dealing with a normal distribution, it coincides with its the standard deviation. <code>brms</code> will automatically truncate that distribution and allow for only positive values.</p>
<p>Inspect the output of our model now. The posteriors and trace plots are shown in Figure <a href="ch-reg.html#fig:posteriorsloadpupilsize">4.2</a>; the figure is generated by typing:</p>

<div class="sourceCode" id="cb177"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" data-line-number="1"><span class="kw">plot</span>(fit_pupil)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:posteriorsloadpupilsize"></span>
<img src="bookdown_files/figure-html/posteriorsloadpupilsize-1.svg" alt="Posterior distributions of the parameters in the brms model fit_pupil, along with the corresponding trace plots." width="672" />
<p class="caption">
FIGURE 4.2: Posterior distributions of the parameters in the brms model <code>fit_pupil</code>, along with the corresponding trace plots.
</p>
</div>
<div class="sourceCode" id="cb178"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1">fit_pupil</a></code></pre></div>
<pre><code>## ...
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   701.76     20.50   662.02   742.54 1.00     3974     2833
## c_load       33.59     12.00    10.17    56.69 1.00     3739     2806
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma   128.40     15.06   103.30   161.16 1.00     3458     3057
## 
## ...</code></pre>
<p>In the next section, we discuss how one can communicate the relevant information from the model.</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:intercept" class="theorem"><strong>Box 4.2  </strong></span><strong>Intercepts in <code>brms</code></strong></p>
</div>
<p>When we set up a prior for the intercept in <code>brms</code>, we actually set a prior for an intercept assuming that all the predictors are centered. This means that when predictors are not centered (and only then), there is a mismatch between the interpretation of the intercept as returned in the output of <code>brms</code> and the interpretation of the intercept with respect to its prior specification. In this case, only the intercept in the output corresponds to the formula in the <code>brms</code> call. However, as we show below, when the intercept is much larger than the effects that we are considering in the formula (what we generally call <span class="math inline">\(\beta\)</span>), this discrepancy hardly matters.</p>
<p>The reason for this mismatch when our predictors are uncentered is that <code>brms</code> increases sampling efficiency by automatically centering all the predictors internally (that is the population-level design matrix <span class="math inline">\(X\)</span> is internally centered around its column means when <code>brms</code> fits a model). This did not matter in our previous examples because we centered our predictor (or we had none), but it might matter if we want to have uncentered predictors. In the design we are discussing, a non-centered predictor of load will mean that the intercept, <span class="math inline">\(\alpha\)</span>, has a straightforward interpretation: the <span class="math inline">\(\alpha\)</span> is the mean pupil size when there is no attention load. This is in contrast with the centered version presented before, where the intercept <span class="math inline">\(\alpha\)</span> represents the pupil size for the average load of <code>2.44</code> (<code>c_load = 0</code>). The difference between the non-centered model (below) and the centered version presented before is depicted in Figure <a href="ch-reg.html#fig:centered-non-centered">4.3</a>.</p>
<p>Suppose that we are quite sure that the prior values for the no load condition (i.e., condition is non-centered) fall between 400 and 1200 ms. In that case, the following prior could be set for <span class="math inline">\(\alpha\)</span>: <span class="math inline">\(\mathit{Normal}(800,200)\)</span>. In this case, the model becomes:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb180-1" data-line-number="1">prior_nc &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb180-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">800</span>, <span class="dv">200</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> Intercept),</a>
<a class="sourceLine" id="cb180-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb180-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> load)</a>
<a class="sourceLine" id="cb180-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb180-6" data-line-number="6"></a>
<a class="sourceLine" id="cb180-7" data-line-number="7">fit_pupil_non_centered &lt;-<span class="st"> </span><span class="kw">brm</span>(p_size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Intercept <span class="op">+</span><span class="st"> </span>load,</a>
<a class="sourceLine" id="cb180-8" data-line-number="8">  <span class="dt">data =</span> df_pupil,</a>
<a class="sourceLine" id="cb180-9" data-line-number="9">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb180-10" data-line-number="10">  <span class="dt">prior =</span> prior_nc</a>
<a class="sourceLine" id="cb180-11" data-line-number="11">)</a></code></pre></div>

<div class="figure"><span style="display:block;" id="fig:centered-non-centered"></span>
<img src="bookdown_files/figure-html/centered-non-centered-1.svg" alt="Regression lines for the non-centered and centered linear regressions. The intercept (or \(\alpha\)) represented by a circle is positioned differently depending on the centering, whereas the slope (or \(\beta\)) represented by a vertical dashed line has the same magnitude in both models." width="672"  />
<p class="caption">
FIGURE 4.3: Regression lines for the non-centered and centered linear regressions. The intercept (or <span class="math inline">\(\alpha\)</span>) represented by a circle is positioned differently depending on the centering, whereas the slope (or <span class="math inline">\(\beta\)</span>) represented by a vertical dashed line has the same magnitude in both models.
</p>
</div>
<p>When the predictor is non-centered as shown above, the regular centered intercept is removed by adding <code>0</code> to the formula, and by replacing the intercept with the “actual” intercept we want to set priors on with <code>Intercept</code>. The word <code>Intercept</code> is a reserved word; we cannot name any predictor with this name. This new parameter is also of the class <code>b</code>, so its prior needs to be defined accordingly. Once we use <code>0 + Intercept + ..</code>, the intercept is not calculated with predictors that are automatically centered any more.</p>
<p>The output below shows that, as expected, although the posterior for the intercept has changed noticeably, the posterior for the effect of load remains virtually unchanged.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_pupil_non_centered,</a>
<a class="sourceLine" id="cb181-2" data-line-number="2">                  <span class="dt">variable =</span> <span class="kw">c</span>(<span class="st">&quot;b_Intercept&quot;</span>, <span class="st">&quot;b_load&quot;</span>))</a></code></pre></div>
<pre><code>##             Estimate Est.Error   Q2.5 Q97.5
## b_Intercept    624.7      34.4 557.89 694.0
## b_load          32.2      11.6   8.98  55.2</code></pre>
<p>Notice the following potential pitfall. A model like the one below will fit a non-centered load predictor, but will assign a prior of <span class="math inline">\(\mathit{Normal}(800,200)\)</span> to the intercept of a model that assumes a centered predictor, <span class="math inline">\(\alpha_{centered}\)</span>, and not the current intercept, <span class="math inline">\(\alpha\)</span>.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" data-line-number="1">fit_pupil_wrong &lt;-<span class="st"> </span><span class="kw">brm</span>(p_size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>load,</a>
<a class="sourceLine" id="cb183-2" data-line-number="2">  <span class="dt">data =</span> df_pupil,</a>
<a class="sourceLine" id="cb183-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb183-4" data-line-number="4">  <span class="dt">prior =</span> prior_nc</a>
<a class="sourceLine" id="cb183-5" data-line-number="5">)</a></code></pre></div>
<p>What does it mean to set a prior to <span class="math inline">\(\alpha_{centered}\)</span> in a model that doesn’t include <span class="math inline">\(\alpha_{centered}\)</span>?</p>
<p>The fitted (expected) values of the non-centered model and the centered one are identical, that is, the values of the response distribution without the residual error are identical for both models:</p>
<p><span class="math display" id="eq:fitted">\[\begin{equation}
\alpha + load_n \cdot \beta = \alpha_{centered} + (load_n - mean(load)) \cdot \beta 
\tag{4.6}
\end{equation}\]</span></p>
<p>The left side of Equation <a href="ch-reg.html#eq:fitted">(4.6)</a> refers to the expected values based on our current non-centered model, and the right side refers to the expected values based on the centered model. We can re-arrange terms to understand what the effect is of a prior on <span class="math inline">\(\alpha_{centered}\)</span> in our model that doesn’t include <span class="math inline">\(\alpha_{centered}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha + load_n \cdot \beta &amp;= \alpha_{centered} + load_n\cdot \beta - mean(load) \cdot \beta\\
\alpha  &amp;= \alpha_{centered}  - mean(load) \cdot \beta\\
\alpha + mean(load) \cdot \beta  &amp;= \alpha_{centered}  
\end{aligned}
\end{equation}\]</span></p>
<p>That means that in the centered model, we are actually setting our prior to <span class="math inline">\(\alpha + mean(load) \cdot \beta\)</span>.
When <span class="math inline">\(\beta\)</span> is very small (or the means of our predictors are very small because they might be ``almost’’ centered), and the prior for <span class="math inline">\(\alpha\)</span> is very wide, we might hardly notice the difference between setting a prior to <span class="math inline">\(\alpha_{centered}\)</span> or to our actual <span class="math inline">\(\alpha\)</span> in a non-centered model (especially if the likelihood dominates anyway). But it is important to pay attention to what the parameters represent that we are setting priors on.</p>
<p>In our example analyses in this book, we will always center our predictors.</p>
</div>

</div>
<div id="how-to-communicate-the-results" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.3</span> How to communicate the results?<a href="ch-reg.html#how-to-communicate-the-results" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to answer our research question “What is the effect of attentional load on the subject’s pupil size?” For that we’ll need to examine what happens with the posterior distribution of <span class="math inline">\(\beta\)</span>, which is printed out as <code>c_load</code> in the summary of <code>brms</code>. The summary of the posterior tells us that the most likely values of <span class="math inline">\(\beta\)</span> will be around the mean of the posterior, 33.59, and we can be 95% certain that the value of <span class="math inline">\(\beta\)</span>, given the model and the data, lies between 10.17 and 56.69.</p>
<p>We see that as the attentional load increases, the pupil size of the subject becomes larger. If we want to determine how likely it is that the pupil size increased rather than decreased, we can examine the proportion of samples above zero. (The intercept and the slopes are always preceded by <code>b_</code> in <code>brms</code>. One can see all the names of parameters being estimated with <code>variables()</code>.)</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">as_draws_df</span>(fit_pupil)<span class="op">$</span>b_c_load <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] 0.995</code></pre>
<p><strong>This high probability does not mean that the effect of load is non-zero.</strong> It means instead that it’s much more likely that the effect is positive rather than negative. In order to claim that the effect is likely to be non-zero, we would have to compare the model with an alternative model in which the model assumes that the effect of load is <span class="math inline">\(0\)</span>. We’ll come back to this issue in the model comparison chapter <a href="ch-comparison.html#ch-comparison">14</a>.</p>
</div>
<div id="sec-pupiladq" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.4</span> Descriptive adequacy<a href="ch-reg.html#sec-pupiladq" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our model converged and we obtained a posterior distribution. There is, however, no guarantee that our model is good enough to represent our data. We can use posterior predictive checks to check the descriptive adequacy of the model.</p>
<p>Sometimes it’s useful to customize the posterior predictive check to visualize the fit of our model. We iterate over the different loads (e.g, 0 to 4), and we show the prior predictive distributions based on 100 simulations for each load together with the observed pupil sizes in Figure <a href="ch-reg.html#fig:postpreddens">4.4</a>. We don’t have enough data to derive a strong conclusion: both the predictive distributions and our data look very widely spread out, and it’s hard to tell if the distribution of the observations could have been generated by our model. For now we can say that it doesn’t look too bad.</p>

<div class="sourceCode" id="cb186"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb186-1" data-line-number="1"><span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">4</span>) {</a>
<a class="sourceLine" id="cb186-2" data-line-number="2">  df_sub_pupil &lt;-<span class="st"> </span><span class="kw">filter</span>(df_pupil, load <span class="op">==</span><span class="st"> </span>l)</a>
<a class="sourceLine" id="cb186-3" data-line-number="3">  p &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_pupil,</a>
<a class="sourceLine" id="cb186-4" data-line-number="4">    <span class="dt">type =</span> <span class="st">&quot;dens_overlay&quot;</span>,</a>
<a class="sourceLine" id="cb186-5" data-line-number="5">    <span class="dt">ndraws =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb186-6" data-line-number="6">    <span class="dt">newdata =</span> df_sub_pupil</a>
<a class="sourceLine" id="cb186-7" data-line-number="7">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb186-8" data-line-number="8"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">data =</span> df_sub_pupil, <span class="kw">aes</span>(<span class="dt">x =</span> p_size, <span class="dt">y =</span> <span class="fl">0.0001</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb186-9" data-line-number="9"><span class="st">    </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;load: &quot;</span>, l)) <span class="op">+</span></a>
<a class="sourceLine" id="cb186-10" data-line-number="10"><span class="st">    </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">400</span>, <span class="dv">1000</span>))</a>
<a class="sourceLine" id="cb186-11" data-line-number="11">  <span class="kw">print</span>(p)</a>
<a class="sourceLine" id="cb186-12" data-line-number="12">}</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:postpreddens"></span>
<img src="bookdown_files/figure-html/postpreddens-1.svg" alt="The plot shows 100 predicted distributions with the label \(y_{rep}\), the distribution of pupil size data in black with the label \(y\), and the observed pupil sizes in black dots for the five levels of attentional load." width="672" /><img src="bookdown_files/figure-html/postpreddens-2.svg" alt="The plot shows 100 predicted distributions with the label \(y_{rep}\), the distribution of pupil size data in black with the label \(y\), and the observed pupil sizes in black dots for the five levels of attentional load." width="672" /><img src="bookdown_files/figure-html/postpreddens-3.svg" alt="The plot shows 100 predicted distributions with the label \(y_{rep}\), the distribution of pupil size data in black with the label \(y\), and the observed pupil sizes in black dots for the five levels of attentional load." width="672" /><img src="bookdown_files/figure-html/postpreddens-4.svg" alt="The plot shows 100 predicted distributions with the label \(y_{rep}\), the distribution of pupil size data in black with the label \(y\), and the observed pupil sizes in black dots for the five levels of attentional load." width="672" /><img src="bookdown_files/figure-html/postpreddens-5.svg" alt="The plot shows 100 predicted distributions with the label \(y_{rep}\), the distribution of pupil size data in black with the label \(y\), and the observed pupil sizes in black dots for the five levels of attentional load." width="672" />
<p class="caption">
FIGURE 4.4: The plot shows 100 predicted distributions with the label <span class="math inline">\(y_{rep}\)</span>, the distribution of pupil size data in black with the label <span class="math inline">\(y\)</span>, and the observed pupil sizes in black dots for the five levels of attentional load.
</p>
</div>
<p>In Figure <a href="ch-reg.html#fig:postpredmean">4.5</a>, we look instead at the distribution of a summary statistic, such as mean pupil size by load:</p>

<div class="sourceCode" id="cb187"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" data-line-number="1"><span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">4</span>) {</a>
<a class="sourceLine" id="cb187-2" data-line-number="2">  df_sub_pupil &lt;-<span class="st"> </span><span class="kw">filter</span>(df_pupil, load <span class="op">==</span><span class="st"> </span>l)</a>
<a class="sourceLine" id="cb187-3" data-line-number="3">  p &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_pupil,</a>
<a class="sourceLine" id="cb187-4" data-line-number="4">    <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>,</a>
<a class="sourceLine" id="cb187-5" data-line-number="5">    <span class="dt">ndraws =</span> <span class="dv">1000</span>,</a>
<a class="sourceLine" id="cb187-6" data-line-number="6">    <span class="dt">newdata =</span> df_sub_pupil,</a>
<a class="sourceLine" id="cb187-7" data-line-number="7">    <span class="dt">stat =</span> <span class="st">&quot;mean&quot;</span></a>
<a class="sourceLine" id="cb187-8" data-line-number="8">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb187-9" data-line-number="9"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">data =</span> df_sub_pupil, <span class="kw">aes</span>(<span class="dt">x =</span> p_size, <span class="dt">y =</span> <span class="fl">0.001</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb187-10" data-line-number="10"><span class="st">    </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;load: &quot;</span>, l)) <span class="op">+</span></a>
<a class="sourceLine" id="cb187-11" data-line-number="11"><span class="st">    </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">400</span>, <span class="dv">1000</span>))</a>
<a class="sourceLine" id="cb187-12" data-line-number="12">  <span class="kw">print</span>(p)</a>
<a class="sourceLine" id="cb187-13" data-line-number="13">}</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:postpredmean"></span>
<img src="bookdown_files/figure-html/postpredmean-1.svg" alt="Distribution of posterior predicted means in gray and observed pupil size means in black lines by load." width="672"  /><img src="bookdown_files/figure-html/postpredmean-2.svg" alt="Distribution of posterior predicted means in gray and observed pupil size means in black lines by load." width="672"  /><img src="bookdown_files/figure-html/postpredmean-3.svg" alt="Distribution of posterior predicted means in gray and observed pupil size means in black lines by load." width="672"  /><img src="bookdown_files/figure-html/postpredmean-4.svg" alt="Distribution of posterior predicted means in gray and observed pupil size means in black lines by load." width="672"  /><img src="bookdown_files/figure-html/postpredmean-5.svg" alt="Distribution of posterior predicted means in gray and observed pupil size means in black lines by load." width="672"  />
<p class="caption">
FIGURE 4.5: Distribution of posterior predicted means in gray and observed pupil size means in black lines by load.
</p>
</div>
<p>Figure <a href="ch-reg.html#fig:postpredmean">4.5</a> shows that the observed means for no load and for a load of one are falling in the tails of the distributions. Although our model predicts a monotonic increase of pupil size, the data might be indicating that the relevant difference is simply between no load, and some load. However, given the uncertainty in the posterior predictive distributions and that the observed means are contained somewhere in the predicted distributions, it could be the case that with this model we are overinterpreting noise.</p>
</div>
</div>
<div id="sec-trial" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.2</span> Log-normal model: Does trial affect response times?<a href="ch-reg.html#sec-trial" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let us revisit the small experiment from section <a href="ch-compbda.html#sec-simplenormal">3.2.1</a>, where a subject repeatedly pressed the space bar as fast as possible, without paying attention to the stimuli. We want to know whether the subject tended to speed up (a practice effect) or slow down (a fatigue effect) while pressing the space bar. We’ll use the same data set <code>df_spacebar</code> as before, and we’ll center the column <code>trial</code>:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb188-1" data-line-number="1">df_spacebar &lt;-<span class="st"> </span>df_spacebar <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb188-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_trial =</span> trial <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(trial))</a></code></pre></div>
<div id="likelihood-and-priors-for-the-log-normal-model" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.1</span> Likelihood and priors for the log-normal model<a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we assume that response times are log-normally distributed, we could fit a likelihood such as the following:</p>
<p><span class="math display" id="eq:rtloglik">\[\begin{equation}
rt_n \sim \mathit{LogNormal}(\alpha + c\_trial_n \cdot \beta,\sigma)
\tag{4.7}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(n =1, \ldots, N\)</span>, and <span class="math inline">\(rt\)</span> is the dependent variable (response times in milliseconds). The variable <span class="math inline">\(N\)</span> represents the total number of data points.</p>
<p>We use the same priors as in section <a href="ch-compbda.html#sec-lognormal">3.6.3</a> for <span class="math inline">\(\alpha\)</span> (which is equivalent to <span class="math inline">\(\mu\)</span> in the previous model) and for <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(6, 1.5) \\
\sigma &amp;\sim \mathit{Normal}_+(0, 1)\\
\end{aligned}
\end{equation}\]</span></p>
<p>We still need a prior for <span class="math inline">\(\beta\)</span>. Effects are multiplicative rather than additive when we assume a log-normal likelihood, and that means that we need to take into account <span class="math inline">\(\alpha\)</span> in order to interpret <span class="math inline">\(\beta\)</span>; for details, see Box <a href="ch-reg.html#thm:lognormal">4.3</a>. We are going to try to understand how all our priors interact, by generating some prior predictive distributions. We start with the following prior centered in zero, a prior agnostic regarding the direction of the effect, which allows for both a slowdown (<span class="math inline">\(\beta&gt;0\)</span>) or a speedup (<span class="math inline">\(\beta&lt;0\)</span>):</p>
<p><span class="math display">\[\begin{equation}
\beta \sim \mathit{Normal}(0, 1)
\end{equation}\]</span></p>
<!-- We can edit our `normal_predictive_distribution_fast` from section \@ref(sec-ppd) and make it log-normal and dependent on trial: -->
<p>This is our first attempt at a prior predictive distribution:</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" data-line-number="1"><span class="co"># Ignore the dependent variable,</span></a>
<a class="sourceLine" id="cb189-2" data-line-number="2"><span class="co"># use a vector of ones a placeholder.</span></a>
<a class="sourceLine" id="cb189-3" data-line-number="3">df_spacebar_ref &lt;-<span class="st"> </span>df_spacebar <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb189-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">rt =</span> <span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">n</span>()))</a>
<a class="sourceLine" id="cb189-5" data-line-number="5">fit_prior_press_trial &lt;-<span class="st"> </span><span class="kw">brm</span>(rt <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_trial,</a>
<a class="sourceLine" id="cb189-6" data-line-number="6">  <span class="dt">data =</span> df_spacebar_ref,</a>
<a class="sourceLine" id="cb189-7" data-line-number="7">  <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb189-8" data-line-number="8">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb189-9" data-line-number="9">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb189-10" data-line-number="10">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb189-11" data-line-number="11">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_trial)</a>
<a class="sourceLine" id="cb189-12" data-line-number="12">  ),</a>
<a class="sourceLine" id="cb189-13" data-line-number="13">  <span class="dt">sample_prior =</span> <span class="st">&quot;only&quot;</span>,</a>
<a class="sourceLine" id="cb189-14" data-line-number="14">  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">.9</span>)</a>
<a class="sourceLine" id="cb189-15" data-line-number="15">)</a></code></pre></div>
<p>In order to understand the type of data that we are assuming a priori with the prior of the parameter <span class="math inline">\(\beta\)</span>, we’ll plot the median difference between the response times at adjacent trials. As the prior of <span class="math inline">\(\beta\)</span> gets wider we are going to observe larger differences between adjacent trials. The objective of the prior predictive checks is to calibrate the prior of <span class="math inline">\(\beta\)</span> to obtain a plausible range of differences.
We are going to plot a distribution of medians because they are less affected by the variance in the posterior predicted distribution than the distribution of mean differences; distributions of means will have much more spread. If we want to make the distribution of means more realistic, we would also need to find a more accurate prior for the scale, <span class="math inline">\(\sigma\)</span>. (Recall that the mean of log-normal distributed values depend on both the location, <span class="math inline">\(\mu\)</span> and the scale, <span class="math inline">\(\sigma\)</span>, of the distribution.) To plot the median effect, we first define a function that calculates the difference between adjacent trials, and then applies the median to the result. We use that function in <code>pp_check</code> and we show the results in Figure <a href="ch-reg.html#fig:priorbeta">4.6</a>. As expected, it is centered on zero (as our prior), but we see that the distribution of possible medians for the effect is too widely spread out and includes values that are too extreme.</p>

<div class="sourceCode" id="cb190"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" data-line-number="1">median_diff &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb190-2" data-line-number="2">  <span class="kw">median</span>(x <span class="op">-</span><span class="st"> </span><span class="kw">lag</span>(x), <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb190-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb190-4" data-line-number="4"><span class="kw">pp_check</span>(fit_prior_press_trial,</a>
<a class="sourceLine" id="cb190-5" data-line-number="5">         <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>,</a>
<a class="sourceLine" id="cb190-6" data-line-number="6">         <span class="dt">stat =</span> <span class="st">&quot;median_diff&quot;</span>,</a>
<a class="sourceLine" id="cb190-7" data-line-number="7">  <span class="co"># show only prior predictive distributions       </span></a>
<a class="sourceLine" id="cb190-8" data-line-number="8">         <span class="dt">prefix =</span> <span class="st">&quot;ppd&quot;</span>,</a>
<a class="sourceLine" id="cb190-9" data-line-number="9">  <span class="co"># each bin has a width of 500ms       </span></a>
<a class="sourceLine" id="cb190-10" data-line-number="10">         <span class="dt">binwidth =</span> <span class="dv">500</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb190-11" data-line-number="11"><span class="st">  </span><span class="co"># cut the top of the plot to improve its scale</span></a>
<a class="sourceLine" id="cb190-12" data-line-number="12"><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">50</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:priorbeta"></span>
<img src="bookdown_files/figure-html/priorbeta-1.svg" alt="Prior predictive distribution of the median effect of the model defined in 4.2 with \(\beta \sim \mathit{Normal}(0, 1)\)." width="672" />
<p class="caption">
FIGURE 4.6: Prior predictive distribution of the median effect of the model defined in <a href="ch-reg.html#sec-trial">4.2</a> with <span class="math inline">\(\beta \sim \mathit{Normal}(0, 1)\)</span>.
</p>
</div>
<p>We repeat the same procedure with <span class="math inline">\(\beta \sim \mathit{Normal}(0,0.01)\)</span>, and we plot it in Figure <a href="ch-reg.html#fig:priorbeta2">4.7</a>. The prior predictive distribution shows us that the prior is still quite vague, it is, however at least in the right order of magnitude.</p>

<div class="figure"><span style="display:block;" id="fig:priorbeta2"></span>
<img src="bookdown_files/figure-html/priorbeta2-1.svg" alt="Prior predictive distribution of the median difference in response times between adjacent trials based on the model defined in 4.2 with \(\beta \sim \mathit{Normal}(0, 0.01)\)." width="672" />
<p class="caption">
FIGURE 4.7: Prior predictive distribution of the median difference in response times between adjacent trials based on the model defined in <a href="ch-reg.html#sec-trial">4.2</a> with <span class="math inline">\(\beta \sim \mathit{Normal}(0, 0.01)\)</span>.
</p>
</div>
<p>Prior selection might look daunting and a lot of work. However, this work is usually done only the first time we encounter an experimental paradigm; besides, priors can be informed by the estimates from previous experiments (even maximum likelihood estimates from frequentist models can be useful). We will generally use very similar (or identical priors) for analyses dealing with the same type of task. When in doubt, a sensitivity analysis (see section <a href="ch-compbda.html#sec-sensitivity">3.4</a>) can tell us whether the posterior distribution depends unintentionally strongly on our prior selection. We will return to the issue of prior selection in chapter <a href="ch-priors.html#ch-priors">6</a>.</p>
<!-- \pagebreak -->

<div class="extra">
<div class="theorem">
<p><span id="thm:lognormal" class="theorem"><strong>Box 4.3  </strong></span><strong>Understanding the Log-normal likelihood</strong></p>
</div>
<p>It is important to understand what we are assuming with our log-normal likelihood. Formally, if a random variable <span class="math inline">\(Z\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the transformed random variable <span class="math inline">\(Y = \exp(Z)\)</span> is log-normally distributed and has density:</p>
<p><span class="math display">\[\begin{equation}
\mathit{LogNormal}(y|\mu,\sigma)=f(z)= \frac{1}{\sqrt{2\pi \sigma^2}y} \exp \left(-\frac{(\log(y)-\mu)^2}{2\sigma^2} \right)
\end{equation}\]</span></p>
<p>As explained in section <a href="ch-compbda.html#sec-lnfirst">3.6.2</a>, the model from Equation <a href="ch-reg.html#eq:rtloglik">(4.7)</a> is equivalent to the following:</p>
<p><span class="math display" id="eq:aX">\[\begin{equation}
\log(rt_n) \sim \mathit{Normal}(\alpha + c\_trial_n \cdot \beta,\sigma)\\
\tag{4.8}
\end{equation}\]</span></p>
<p>The family of normal distributions is closed under linear transformations: that is, if <span class="math inline">\(X\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, then (for any real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>), <span class="math inline">\(a X + b\)</span> is also normally distributed, with mean <span class="math inline">\(a \mu +b\)</span> (and standard deviation <span class="math inline">\(\sqrt{a^2\sigma^2}=|a|\sigma\)</span>).</p>
<p>This means that, assuming <span class="math inline">\(Z \sim \mathit{Normal}(\alpha, \sigma)\)</span>, Equation <a href="ch-reg.html#eq:aX">(4.8)</a> can be re-written as follows:</p>
<!-- BN: Shravan said that he would explain this property in the first section -->
<p><span class="math display" id="eq:rtlogliknoncen">\[\begin{equation}
\log(rt_n) = Z + c\_trial_n \cdot \beta
\tag{4.9}
\end{equation}\]</span></p>
<p>We exponentiate both sides, and we use the property of exponents that <span class="math inline">\(\exp(x+y)\)</span> is equal to <span class="math inline">\(\exp(x) \cdot \exp(y)\)</span>, and we set <span class="math inline">\(Y =\exp(Z)\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
rt_n &amp;= \exp \big(Z  + c\_trial_n \cdot \beta\big) \\
rt_n &amp;= \exp(Z )   \cdot \exp\big(c\_trial_n \cdot \beta\big) \\
rt_n &amp;= Y \cdot \exp\big(c\_trial_n \cdot \beta\big) 
\end{aligned}
\end{equation}\]</span></p>
<p>The last equation has two terms being multiplied, the first one, <span class="math inline">\(Y\)</span>, is telling us that we are assuming that response times are log-normally distributed with a median of <span class="math inline">\(\exp(\alpha)\)</span>, the second term, <span class="math inline">\(\exp(c\_trial_n \cdot \beta)\)</span> is telling us that the effect of trial number is multiplicative and grows or decays exponentially with the trial number. This has two important consequences:</p>
<ol style="list-style-type: decimal">
<li><p>Different values of the intercept, <span class="math inline">\(\alpha\)</span>, given the same <span class="math inline">\(\beta\)</span>, will affect the difference in response times for two adjacent trials (this is in contrast to what happens with an additive model such as normal likelihood); see Figure <a href="ch-reg.html#fig:logexp">4.8</a>. This is because, unlike in the additive case, the intercept doesn’t cancel out:</p>
<ul>
<li><p>Additive case:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
&amp; (\alpha + trial_n \cdot \beta) - (\alpha + trial_{n-1} \cdot \beta) = \\
&amp;=\alpha -\alpha + ( trial_n - trial_{n-1} ) \cdot \beta\\
&amp;= ( trial_n - trial_{n-1} ) \cdot \beta
\end{aligned}
\end{equation}\]</span></p></li>
<li><p>Multiplicative case:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
   &amp;\exp(\alpha) \cdot \exp(trial_n \cdot \beta) -\exp(\alpha) \cdot \exp(trial_{n-1} \cdot \beta) =\\ 
   &amp;= \exp(\alpha) \big(\exp(trial_n  \cdot \beta)  - \exp(trial_{n-1}\cdot \beta) \big)\\
   &amp;\neq \big(\exp(trial_n)  - \exp(trial_{n-1})  \big) \cdot \exp(\beta) 
\end{aligned}
   \end{equation}\]</span></p></li>
</ul></li>
</ol>

<div class="figure"><span style="display:block;" id="fig:logexp"></span>
<img src="bookdown_files/figure-html/logexp-1.svg" alt="Fitted value of the difference in reaction time between two adjacent trials, when \(\beta=0.01\) and \(\alpha\) lies between 0.1 and 15. The graph shows how changes in the intercept lead to changes in the difference in response times between trials, even if \(\beta\) is fixed." width="672"  />
<p class="caption">
FIGURE 4.8: Fitted value of the difference in reaction time between two adjacent trials, when <span class="math inline">\(\beta=0.01\)</span> and <span class="math inline">\(\alpha\)</span> lies between 0.1 and 15. The graph shows how changes in the intercept lead to changes in the difference in response times between trials, even if <span class="math inline">\(\beta\)</span> is fixed.
</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>As the trial number increases, the same value of <span class="math inline">\(\beta\)</span> will have a very different impact on the original scale of the dependent variable: Any (fixed) negative value for <span class="math inline">\(\beta\)</span> will lead to exponential decay and any (fixed) positive value will lead to exponential growth; see Figure <a href="ch-reg.html#fig:expgd">4.9</a>.</li>
</ol>

<div class="figure"><span style="display:block;" id="fig:expgd"></span>
<img src="bookdown_files/figure-html/expgd-1.svg" alt="Fitted value of the dependent variable (response times in ms) as function of trial number, when (A) \(\beta = -0.01\), exponential decay, and when (B) \(\beta =0.01\), exponential growth." width="672"  />
<p class="caption">
FIGURE 4.9: Fitted value of the dependent variable (response times in ms) as function of trial number, when (A) <span class="math inline">\(\beta = -0.01\)</span>, exponential decay, and when (B) <span class="math inline">\(\beta =0.01\)</span>, exponential growth.
</p>
</div>
<p>Can exponential growth or decay make sense? We need to consider that if they do make sense, they will be an approximation valid for a specific range of values, at some point we will expect a ceiling or a floor effect: response times cannot truly be 0 milliseconds, or take several minutes. However, in our specific model, exponential growth or decay <em>by trial</em> is probably a bad approximation: We will predict that our subject will take extremely long (if <span class="math inline">\(\beta &gt;0\)</span>) or extremely short (if <span class="math inline">\(\beta &lt;0\)</span>) time in pressing the space bar in a relatively low number of trials. This doesn’t mean that the likelihood is wrong by itself, but it does mean that at least we need to put a cap on the growth or decay of our experimental manipulation. We can do this if the exponential growth or decay is a function of, for example, log-transformed trial numbers:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim \mathit{LogNormal}(\alpha + c\_\log\_trial_n \cdot \beta,\sigma)\\
\end{equation}\]</span></p>

<div class="figure"><span style="display:block;" id="fig:expgd2"></span>
<img src="bookdown_files/figure-html/expgd2-1.svg" alt="Fitted value of the dependent variable (response times in ms) as function of the natural logarithm of the trial number, when (A) \(\beta=-0.01\), exponential decay, and when (B) \(\beta =.01\), exponential growth." width="672"  />
<p class="caption">
FIGURE 4.10: Fitted value of the dependent variable (response times in ms) as function of the natural logarithm of the trial number, when (A) <span class="math inline">\(\beta=-0.01\)</span>, exponential decay, and when (B) <span class="math inline">\(\beta =.01\)</span>, exponential growth.
</p>
</div>
<p><strong>Log-normal distributions everywhere</strong></p>
<p>The normal distribution is most often assumed to describe the random variation that occurs in the data from many scientific disciplines. However, most measurements actually show skewed distributions. <span class="citation">Limpert, Stahel, and Abbt (<a href="#ref-limpertLognormalDistributionsSciences2001">2001</a>)</span> discuss the log-normal distribution in scientific disciplines and how diverse type of data, from lengths of latent periods of infectious diseases to distribution of mineral resources in the Earth’s crust, including even body height–the quintessential example of a normal distribution–closely fit the log-normal distribution.</p>
<p><span class="citation">Limpert, Stahel, and Abbt (<a href="#ref-limpertLognormalDistributionsSciences2001">2001</a>)</span> point out that because a random variable that results from multiplying many independent variables has an approximate log-normal distribution, the most basic indicator of the importance of the log-normal distribution may be very general: Chemistry and physics are fundamental in life, and the prevailing operation in the laws of these disciplines is multiplication rather than addition.</p>
<p>Furthermore, at many physiological and anatomical levels in the brain, the distribution of numerous parameters is in fact strongly skewed with a heavy tail, suggesting that skewed (typically log-normal) distributions are fundamental to structural and functional brain organization. This might be explained given that the majority of interactions in highly interconnected systems, especially in biological systems, are multiplicative and synergistic rather than additive <span class="citation">(Buzsáki and Mizuseki <a href="#ref-buzsakiLogdynamicBrainHow2014">2014</a>)</span>.</p>
<p>Does the log-normal distribution make sense for response times?
It has been long noticed that the log-normal distribution often provides a good fit to response times
distributions <span class="citation">(Brée <a href="#ref-breeDistributionProblemsolvingTimes1975">1975</a>; Ulrich and Miller <a href="#ref-ulrichEffectsTruncationReaction1994">1994</a>)</span>.
One advantage of assuming log-normally distributed response times (but, in fact, this is true for many skewed distributions) is that it entails that the standard deviation of the reaction time distribution will increase with the mean, as has been observed in empirical distributions of response times <span class="citation">(Wagenmakers, Grasman, and Molenaar <a href="#ref-wagenmakersRelationMeanVariance2005">2005</a>)</span>. Interestingly, it turns out that log-normal response times are also easily generated by certain process models. <span class="citation">Ulrich and Miller (<a href="#ref-ulrichInformationProcessingModels1993">1993</a>)</span> show, for example, that models in which response times are determined by a series of processes cascading activation from an input level to an output level (usually passing through a number of intervening processing levels along the way) can generate log-normally distributed response times.</p>
</div>

</div>
<div id="the-brms-model-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.2</span> The <code>brms</code> model<a href="ch-reg.html#the-brms-model-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We are now relatively satisfied with the priors for our model, and we can fit the model of the effect of trial as a button-pressing using <code>brms</code>. We need to specify that the family is <code>lognormal()</code>.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" data-line-number="1">fit_press_trial &lt;-<span class="st"> </span><span class="kw">brm</span>(rt <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_trial,</a>
<a class="sourceLine" id="cb191-2" data-line-number="2">  <span class="dt">data =</span> df_spacebar,</a>
<a class="sourceLine" id="cb191-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb191-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb191-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb191-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb191-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">.01</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_trial)</a>
<a class="sourceLine" id="cb191-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb191-9" data-line-number="9">)</a></code></pre></div>
<p>Instead of printing out the complete output from the model, look at the estimates from the posteriors for the parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma\)</span>. These parameters are on the log scale:</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb192-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_press_trial,</a>
<a class="sourceLine" id="cb192-2" data-line-number="2">                  <span class="dt">variable =</span> <span class="kw">c</span>(<span class="st">&quot;b_Intercept&quot;</span>,</a>
<a class="sourceLine" id="cb192-3" data-line-number="3">                               <span class="st">&quot;b_c_trial&quot;</span>,</a>
<a class="sourceLine" id="cb192-4" data-line-number="4">                               <span class="st">&quot;sigma&quot;</span>))</a></code></pre></div>
<pre><code>##             Estimate Est.Error     Q2.5    Q97.5
## b_Intercept 5.118661 0.0067701 5.105434 5.132173
## b_c_trial   0.000521 0.0000634 0.000397 0.000643
## sigma       0.123109 0.0045087 0.114624 0.132112</code></pre>
<p>The posterior distributions can be plotted to obtain a graphical summary of all the parameters in the model (Figure <a href="ch-reg.html#fig:posteriorsfitpresstrial">4.11</a>:</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" data-line-number="1"><span class="kw">plot</span>(fit_press_trial)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:posteriorsfitpresstrial"></span>
<img src="bookdown_files/figure-html/posteriorsfitpresstrial-1.svg" alt="Posterior distributions of the model of the effect of trial on button-pressing." width="672" />
<p class="caption">
FIGURE 4.11: Posterior distributions of the model of the effect of trial on button-pressing.
</p>
</div>
<p>Next, we turn to the question of what we can report as our results, and what we can conclude from the data.</p>
</div>
<div id="how-to-communicate-the-results-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.3</span> How to communicate the results?<a href="ch-reg.html#how-to-communicate-the-results-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As shown above, the first step is to summarize the posteriors in a table or graphically (or both). If the research relates to the effect estimated by the model, the posterior of <span class="math inline">\(\beta\)</span> can be summarized in the following way: <span class="math inline">\(\hat\beta = 0.00052\)</span>, 95% CrI = <span class="math inline">\([ 0.0004 , 0.00064 ]\)</span>.</p>
<p>The effect is easier to interpret in milliseconds. We can transform the estimates back to the millisecond scale from the log scale, but we need to take into account that the scale is not linear, and that the effect between two button presses will differ depending on where we are in the experiment.</p>
<p>We will have a certain estimate if we consider the difference between response times in a trial at the middle of the experiment (when the centered trial number is zero) and the previous one (when the centered trial number is minus one).</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb195-1" data-line-number="1">alpha_samples &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_press_trial)<span class="op">$</span>b_Intercept</a>
<a class="sourceLine" id="cb195-2" data-line-number="2">beta_samples &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_press_trial)<span class="op">$</span>b_c_trial</a>
<a class="sourceLine" id="cb195-3" data-line-number="3">effect_middle_ms &lt;-<span class="st"> </span><span class="kw">exp</span>(alpha_samples) <span class="op">-</span></a>
<a class="sourceLine" id="cb195-4" data-line-number="4"><span class="st">  </span><span class="kw">exp</span>(alpha_samples <span class="op">-</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span>beta_samples)</a>
<a class="sourceLine" id="cb195-5" data-line-number="5"><span class="co">## ms effect in the middle of the expt</span></a>
<a class="sourceLine" id="cb195-6" data-line-number="6"><span class="co">## (mean trial vs. mean trial - 1)</span></a>
<a class="sourceLine" id="cb195-7" data-line-number="7"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_middle_ms),</a>
<a class="sourceLine" id="cb195-8" data-line-number="8">  <span class="kw">quantile</span>(effect_middle_ms, <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>##   mean   2.5%  97.5% 
## 0.0871 0.0662 0.1076</code></pre>
<p>We will obtain different estimate if we consider the difference between the second and the first trial:</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb197-1" data-line-number="1">first_trial &lt;-<span class="st"> </span><span class="kw">min</span>(df_spacebar<span class="op">$</span>c_trial)</a>
<a class="sourceLine" id="cb197-2" data-line-number="2">second_trial &lt;-<span class="st"> </span><span class="kw">min</span>(df_spacebar<span class="op">$</span>c_trial) <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb197-3" data-line-number="3">effect_beginning_ms &lt;-</a>
<a class="sourceLine" id="cb197-4" data-line-number="4"><span class="st">  </span><span class="kw">exp</span>(alpha_samples <span class="op">+</span><span class="st"> </span>second_trial <span class="op">*</span><span class="st"> </span>beta_samples) <span class="op">-</span></a>
<a class="sourceLine" id="cb197-5" data-line-number="5"><span class="st">  </span><span class="kw">exp</span>(alpha_samples <span class="op">+</span><span class="st"> </span>first_trial <span class="op">*</span><span class="st"> </span>beta_samples)</a>
<a class="sourceLine" id="cb197-6" data-line-number="6"><span class="co">## ms effect from first to second trial:</span></a>
<a class="sourceLine" id="cb197-7" data-line-number="7"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_beginning_ms),</a>
<a class="sourceLine" id="cb197-8" data-line-number="8">  <span class="kw">quantile</span>(effect_beginning_ms, <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>##   mean   2.5%  97.5% 
## 0.0792 0.0617 0.0959</code></pre>
<p>So far we converted the estimates to obtain median effects, that’s why we used <span class="math inline">\(exp(\cdot)\)</span>, if we want to obtain mean effects we need to take into account <span class="math inline">\(\sigma\)</span>, since we need to calculate <span class="math inline">\(exp(\cdot + \sigma^2/2)\)</span>. However, we can also use the built-in function <code>fitted()</code>. We can consider again the difference between the second and the first trial this time using <code>fitted()</code>.</p>
<p>First, define for which observations we want to obtain the fitted values in millisecond scale. If we are interested in the difference between the second and first trial, create a data frame with their centered versions.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb199-1" data-line-number="1">newdata_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">c_trial =</span> <span class="kw">c</span>(first_trial, second_trial))</a></code></pre></div>
<p>Second, use <code>fitted()</code> on the brms object, including the new data, and setting the <code>summary</code> parameter to <code>FALSE</code>. The first column contains the posterior samples transformed into milliseconds of the first trial, and the second column of the second trial.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb200-1" data-line-number="1">beginning &lt;-<span class="st"> </span><span class="kw">fitted</span>(fit_press_trial,</a>
<a class="sourceLine" id="cb200-2" data-line-number="2">                 <span class="dt">newdata =</span> newdata_<span class="dv">1</span>,</a>
<a class="sourceLine" id="cb200-3" data-line-number="3">                 <span class="dt">summary =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb200-4" data-line-number="4"><span class="kw">head</span>(beginning, <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]  154  154
## [2,]  150  150
## [3,]  159  159</code></pre>
<p>Last, calculate the difference between trials, and report mean and 95% quantiles.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb202-1" data-line-number="1">effect_beginning_ms &lt;-<span class="st"> </span>beginning[, <span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>beginning[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb202-2" data-line-number="2"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_beginning_ms),</a>
<a class="sourceLine" id="cb202-3" data-line-number="3">  <span class="kw">quantile</span>(effect_beginning_ms, <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>##   mean   2.5%  97.5% 
## 0.0798 0.0621 0.0966</code></pre>
<p>Given that <span class="math inline">\(\sigma\)</span> is much smaller than <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span> doesn’t have a large influence on the mean effects, and the mean and 95% CrI of the mean and median effects are quite similar.</p>
<p>We see that no matter how we calculate the trial effect, there is a slowdown. When reporting the results of these analyses, one should present the posterior mean and the 95% credible interval and then reason about whether the observed estimates are consistent with the prediction from the theory being investigated.</p>
<p>The practical relevance of the effect for the research question can be important too. For example, only after <span class="math inline">\(100\)</span> button presses do we see a barely noticeable slowdown:</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb204-1" data-line-number="1">effect_<span class="dv">100</span> &lt;-</a>
<a class="sourceLine" id="cb204-2" data-line-number="2"><span class="st">  </span><span class="kw">exp</span>(alpha_samples <span class="op">+</span><span class="st"> </span><span class="dv">100</span> <span class="op">*</span><span class="st"> </span>beta_samples) <span class="op">-</span></a>
<a class="sourceLine" id="cb204-3" data-line-number="3"><span class="st">  </span><span class="kw">exp</span>(alpha_samples)</a>
<a class="sourceLine" id="cb204-4" data-line-number="4"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_<span class="dv">100</span>),</a>
<a class="sourceLine" id="cb204-5" data-line-number="5">  <span class="kw">quantile</span>(effect_<span class="dv">100</span>, <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>##  mean  2.5% 97.5% 
##  8.95  6.76 11.11</code></pre>
<p>We need to consider whether our uncertainty of this estimate, and the estimated mean effect have any scientific relevance. Such relevance can be established by considering the previous literature, predictions from a quantitative model, or other expert domain knowledge. Sometimes, a quantitative meta-analysis is helpful; for examples, see <span class="citation">Jäger, Engelmann, and Vasishth (<a href="#ref-JaegerEngelmannVasishth2017">2017</a>)</span>, <span class="citation">Mahowald et al. (<a href="#ref-mahowald2016meta">2016</a>)</span>, <span class="citation">Nicenboim, Roettger, and Vasishth (<a href="#ref-NicenboimRoettgeretal">2018</a>)</span>, and <span class="citation">Vasishth et al. (<a href="#ref-VasishthetalPLoSOne2013">2013</a>)</span>. We will discuss meta-analysis in later in the book, in chapter <a href="ch-remame.html#ch-remame">13</a>.
<!-- SV to-do, add example case studies--></p>
<p>Sometimes, researchers are only interested in establishing that an effect is present or absent; the magnitude and uncertainty of the estimate is of secondary interest. Here, the goal is to argue that there is <strong>evidence</strong> of a slowdown. The word evidence has a special meaning in statistics <span class="citation">(Royall <a href="#ref-Royall">1997</a>)</span>, and in null hypothesis significance testing, a likelihood ratio test is the standard way to argue that one has evidence for an effect. In the Bayesian data analysis context, in order to answer such a question, a Bayes factor analysis must be carried out. We’ll come back to this issue in the model comparison chapters <a href="ch-comparison.html#ch-comparison">14</a>-<a href="ch-cv.html#ch-cv">16</a>.</p>
</div>
<div id="descriptive-adequacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.4</span> Descriptive adequacy<a href="ch-reg.html#descriptive-adequacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We look now at the predictions of the model. Since we now know that trial effects are very small, we’ll examine predictions of the model for differences in response times between 100 button presses. Similarly as we did for prior predictive checks, we define a function, this time <code>median_diff100()</code>, that calculates the median difference between a trial <span class="math inline">\(n\)</span> and a trial <span class="math inline">\(n+100\)</span>. This time we’ll compare the observed median difference against the range of predicted differences based on the model and the data rather than only the model as we did for the prior predictions. Below we use virtually the same code that we use for plotting prior predictive checks, but since we now use the fitted model, we’ll obtain posterior predictive checks; this is displayed in Figure <a href="ch-reg.html#fig:posteriorcbeta">4.12</a>.</p>

<div class="sourceCode" id="cb206"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb206-1" data-line-number="1">median_diff100 &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">median</span>(x <span class="op">-</span><span class="st"> </span><span class="kw">lag</span>(x, <span class="dv">100</span>), <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb206-2" data-line-number="2"><span class="kw">pp_check</span>(fit_press_trial,</a>
<a class="sourceLine" id="cb206-3" data-line-number="3">         <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>,</a>
<a class="sourceLine" id="cb206-4" data-line-number="4">         <span class="dt">stat =</span> <span class="st">&quot;median_diff100&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:posteriorcbeta"></span>
<img src="bookdown_files/figure-html/posteriorcbeta-1.svg" alt="Posterior predictive distribution of the median difference in response times between a trial \(n\) and a trial \(n+100\) based on the model fit_press_trial and the observed data." width="672" />
<p class="caption">
FIGURE 4.12: Posterior predictive distribution of the median difference in response times between a trial <span class="math inline">\(n\)</span> and a trial <span class="math inline">\(n+100\)</span> based on the model <code>fit_press_trial</code> and the observed data.
</p>
</div>
<p>We can conclude that model predictions for differences in response trials between trials are reasonable.</p>
</div>
</div>
<div id="sec-logistic" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.3</span> Logistic regression: Does set size affect free recall?<a href="ch-reg.html#sec-logistic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we will learn how the principles we have learned so far can naturally extend to <em>generalized</em> linear models (GLMs). We focus on one special case of GLMs that has wide application in linguistics and psychology, logistic regression.</p>
<p>As an example data set, we look at a study investigating the capacity level of working memory. The data are a subset of a data set created by <span class="citation">Oberauer (<a href="#ref-oberauerWorkingMemoryCapacity2019">2019</a>)</span>. Each subject was presented word lists of varying lengths (2, 4, 6, and 8 elements), and then was asked to recall a word given its position on the list; see Figure <a href="ch-reg.html#fig:oberauer">4.13</a>. We will focus on the data from one subject.</p>

<div class="figure"><span style="display:block;" id="fig:oberauer"></span>
<img src="cc_figure/fig1_oberauer_2019_modified.png" alt="Flow of events in a trial with memory set size 4 and free recall. Adapted from Oberauer (2019); licensed under CC BY 4.0." width="100%" />
<p class="caption">
FIGURE 4.13: Flow of events in a trial with memory set size 4 and free recall. Adapted from <span class="citation">Oberauer (<a href="#ref-oberauerWorkingMemoryCapacity2019">2019</a>)</span>; licensed under CC BY 4.0.
</p>
</div>
<p>It is well-established that as the number of items to be held in working memory increases, performance, that is accuracy, decreases <span class="citation">(see Oberauer and Kliegl <a href="#ref-oberauerkliegel2001">2001</a>, among others)</span>. We will investigate whether we can investigate this finding with data from only one subject.</p>
<p>The data can be found in <code>df_recall</code> in the package <code>bcogsci</code>. The code below loads the data, centers the predictor <code>set_size</code>, and briefly explores the data set.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb207-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_recall&quot;</span>)</a>
<a class="sourceLine" id="cb207-2" data-line-number="2">df_recall &lt;-<span class="st"> </span>df_recall <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb207-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_set_size =</span> set_size <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(set_size))</a>
<a class="sourceLine" id="cb207-4" data-line-number="4"><span class="co"># Set sizes in the data set:</span></a>
<a class="sourceLine" id="cb207-5" data-line-number="5">df_recall<span class="op">$</span>set_size <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb207-6" data-line-number="6"><span class="st">  </span><span class="kw">unique</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sort</span>()</a></code></pre></div>
<pre><code>## [1] 2 4 6 8</code></pre>
<div class="sourceCode" id="cb209"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb209-1" data-line-number="1"><span class="co"># Trials by set size</span></a>
<a class="sourceLine" id="cb209-2" data-line-number="2">df_recall <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb209-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(set_size) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb209-4" data-line-number="4"><span class="st">  </span><span class="kw">count</span>()</a></code></pre></div>
<pre><code>## # A tibble: 4 × 2
## # Groups:   set_size [4]
##   set_size     n
##      &lt;int&gt; &lt;int&gt;
## 1        2    23
## 2        4    23
## 3        6    23
## # … with 1 more row</code></pre>
<p>Here, the column <code>correct</code> records the incorrect vs. correct responses with <code>0</code> vs <code>1</code>, and the column <code>c_set_size</code> records the centered memory set size; these latter scores have continuous values -3, -1, 1, and 3. These continuous values are centered versions of 2, 4, 6, and 8.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb211-1" data-line-number="1">df_recall</a></code></pre></div>
<pre><code>## # A tibble: 92 × 8
##   subj  set_size correct trial session block tested c_set_size
##   &lt;chr&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;  &lt;int&gt;      &lt;dbl&gt;
## 1 10           4       1     1       1     1      2         -1
## 2 10           8       0     4       1     1      8          3
## 3 10           2       1     9       1     1      2         -3
## # … with 89 more rows</code></pre>
<p>We want to model the trial-by-trial accuracy and examine whether the probability of recalling a word is related to the number of words in the set that the subject needs to remember.</p>
<div id="the-likelihood-for-the-logistic-regression-model" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.1</span> The likelihood for the logistic regression model<a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that the Bernoulli likelihood generates a <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> response with a particular probability <span class="math inline">\(\theta\)</span>. For example, one can generate simulated data for <span class="math inline">\(10\)</span> trials, with a 50% probability of getting a <span class="math inline">\(1\)</span> using <code>rbern</code> from the package <code>extraDistr</code>.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb213-1" data-line-number="1"><span class="kw">rbern</span>(<span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>##  [1] 0 1 0 1 1 1 0 1 0 0</code></pre>
<p>We can therefore define each dependent value <code>correct_n</code> in the data as being generated from a Bernoulli random variable with probability of success <span class="math inline">\(\theta_n\)</span>.
Here, <span class="math inline">\(n =1, \ldots, N\)</span> indexes the trial, correct_n is the dependent variable (0 indicates an incorrect recall and 1 a correct recall), and <span class="math inline">\(\theta_n\)</span> is the probability of correctly recalling a probe in a given trial <span class="math inline">\(n\)</span>.</p>
<p><span class="math display" id="eq:bernoullilik">\[\begin{equation}
correct_n \sim \mathit{Bernoulli}(\theta_n)
\tag{4.10}
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(\theta_n\)</span> is bounded to be between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> (it is a probability), we cannot just fit a regression model using the normal (or log-normal) likelihood as we did in the preceding examples. Such a model would be inappropriate because it would assume that the data range from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span> (or from <span class="math inline">\(0\)</span> to <span class="math inline">\(+\infty\)</span>), rather than being limited to zeros and ones.</p>
<p>The generalized linear modeling framework solves this problem by defining a <em>link function</em> <span class="math inline">\(g(\cdot)\)</span> that connects the linear model to the quantity to be estimated (here, the probabilities <span class="math inline">\(\theta_n\)</span>). The link function used for <span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span> responses is called the <em>logit link</em>, and is defined as follows.</p>
<p><span class="math display">\[\begin{equation}
\eta_n = g(\theta_n) = \log\left(\frac{\theta_n}{1-\theta_n}\right)
\end{equation}\]</span></p>
<p>The term <span class="math inline">\(\frac{\theta_n}{1-\theta_n}\)</span> is called the <em>odds</em>.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> The logit link function is therefore a log-odds; it maps probability values ranging from <span class="math inline">\([0,1]\)</span> to real numbers ranging from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>. Figure <a href="ch-reg.html#fig:logisticfun">4.14</a> shows the logit link function, <span class="math inline">\(\eta = g(\theta)\)</span>, and the inverse logit, <span class="math inline">\(\theta = g^{-1}(\eta)\)</span>, which is called the <em>logistic function</em>; the relevance of this logistic function will become clear in a moment.</p>
<div class="figure"><span style="display:block;" id="fig:logisticfun"></span>
<img src="bookdown_files/figure-html/logisticfun-1.svg" alt="The logit and inverse logit (logistic) function." width="672" />
<p class="caption">
FIGURE 4.14: The logit and inverse logit (logistic) function.
</p>
</div>
<p>The linear model is now fit not to the 0,1 responses as the dependent variable, but to <span class="math inline">\(\eta_n\)</span>, i.e., log-odds, as the dependent variable:</p>
<p><span class="math display">\[\begin{equation}
\eta_n = \log\left(\frac{\theta_n}{1-\theta_n}\right) = \alpha + \beta \cdot c\_set\_size
\end{equation}\]</span></p>
<p>Unlike the linear models, the model is defined so that there is no residual error term (<span class="math inline">\(\varepsilon\)</span>) in this model. Once <span class="math inline">\(\eta_n\)</span> is estimated, one can solve the above equation for <span class="math inline">\(\theta_n\)</span> (in other words, we compute the inverse of the logit function and obtain the estimates on the probability scale). This gives the above-mentioned logistic regression function:</p>
<p><span class="math display">\[\begin{equation}
\theta_n = g^{-1}(\eta_n) =  \frac{\exp(\eta_n)}{1+\exp(\eta_n)} = \frac{1}{1+exp(-\eta_n)}
\end{equation}\]</span></p>
<p>The last equality in the equation above arises by dividing both the numerator and denominator by <span class="math inline">\(\exp(\eta_n)\)</span>.</p>
<p>In summary, the generalized linear model with the logit link fits the following Bernoulli likelihood:</p>
<p><span class="math display" id="eq:bernoullilogislik">\[\begin{equation}
correct_n \sim \mathit{Bernoulli}(\theta_n)
\tag{4.11}
\end{equation}\]</span></p>
<p>The model is fit on the log-odds scale, <span class="math inline">\(\eta_n = \alpha + c\_set\_size_n \cdot \beta\)</span>.
Once <span class="math inline">\(\eta_n\)</span> has been estimated, the inverse logit or the logistic function is used to compute the probability estimates
<span class="math inline">\(\theta_n = \frac{\exp(\eta_n)}{1+\exp(\eta_n)}\)</span>. An example of this calculations will be shown in the next section.</p>
</div>
<div id="sec-priorslogisticregression" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.2</span> Priors for the logistic regression<a href="ch-reg.html#sec-priorslogisticregression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to decide on priors for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, we need to take into account that these parameters do not represent probabilities or proportions, but <em>log-odds</em>, the x-axis in Figure <a href="ch-reg.html#fig:logisticfun">4.14</a> (right-hand side figure). As shown in the figure, the relationship between log-odds and probabilities is not linear.</p>
<p>There are two functions in R that implement the logit and inverse logit functions: <code>qlogis(p)</code> for the logit function and <code>plogis(x)</code> for the inverse logit or logistic function.</p>
<p>Now we need to set priors for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.
Given that we centered our predictor, the intercept, <span class="math inline">\(\alpha\)</span>, represents the log-odds of correctly recalling one word in a random position for the average set size of five (since <span class="math inline">\(5 = \frac{2+4+6+8}{4}\)</span>), which, incidentally, was not presented in the experiment. This is one case where the intercept doesn’t have a clear interpretation if we leave the prediction uncentered: With non-centered set size, the intercept will be the log-odds of recalling one word in a set of <em>zero</em> words.</p>
<p>The prior for <span class="math inline">\(\alpha\)</span> will depend on how difficult the recall task is. If we are not sure, we could assume that the probability of recalling a word for an average set size, <span class="math inline">\(\alpha\)</span>, is centered in .5 (a 50/50 chance) with a great deal of uncertainty. The <code>R</code> command <code>qlogis(.5)</code> tells us that .5 corresponds to zero in log-odds space. How do we include a great deal of uncertainty? We could look at Figure <a href="ch-reg.html#fig:logisticfun">4.14</a>, and decide on a standard deviation of 4 in a normal distribution centered in zero:</p>
<p><span class="math display">\[\begin{equation}
\alpha \sim \mathit{Normal}(0, 4) 
\end{equation}\]</span></p>
<p>Let’s plot this prior in log-odds and in probability scale by drawing random samples.</p>

<div class="sourceCode" id="cb215"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" data-line-number="1">samples_logodds &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">alpha =</span> <span class="kw">rnorm</span>(<span class="dv">100000</span>, <span class="dv">0</span>, <span class="dv">4</span>))</a>
<a class="sourceLine" id="cb215-2" data-line-number="2">samples_prob &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">p =</span> <span class="kw">plogis</span>(<span class="kw">rnorm</span>(<span class="dv">100000</span>, <span class="dv">0</span>, <span class="dv">4</span>)))</a>
<a class="sourceLine" id="cb215-3" data-line-number="3"><span class="kw">ggplot</span>(samples_logodds, <span class="kw">aes</span>(alpha)) <span class="op">+</span></a>
<a class="sourceLine" id="cb215-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_density</span>()</a>
<a class="sourceLine" id="cb215-5" data-line-number="5"><span class="kw">ggplot</span>(samples_prob, <span class="kw">aes</span>(p)) <span class="op">+</span></a>
<a class="sourceLine" id="cb215-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_density</span>()</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:logoddspriorsf"></span>
<img src="bookdown_files/figure-html/logoddspriorsf-1.svg" alt="Prior for \(\alpha \sim \mathit{Normal}(0, 4)\) in log-odds and in probability space." width="45%" /><img src="bookdown_files/figure-html/logoddspriorsf-2.svg" alt="Prior for \(\alpha \sim \mathit{Normal}(0, 4)\) in log-odds and in probability space." width="45%" />
<p class="caption">
FIGURE 4.15: Prior for <span class="math inline">\(\alpha \sim \mathit{Normal}(0, 4)\)</span> in log-odds and in probability space.
</p>
</div>
<p>Figure <a href="ch-reg.html#fig:logoddspriorsf">4.15</a> shows that our prior assigns more probability mass to extreme probabilities of recall than to intermediate values. Clearly, this is not what we intended.</p>
<p>We could try several values for standard deviation of the prior, until we find a prior that make sense for us. Reducing the standard deviation to 1.5 seems to make sense as shown in Figure <a href="ch-reg.html#fig:logoddspriorsf2">4.16</a>.</p>
<p><span class="math display">\[\begin{equation}
\alpha \sim \mathit{Normal}(0, 1.5) 
\end{equation}\]</span></p>

<div class="figure"><span style="display:block;" id="fig:logoddspriorsf2"></span>
<img src="bookdown_files/figure-html/logoddspriorsf2-1.svg" alt="Prior for \(\alpha \sim \mathit{Normal}(0, 1.5)\) in log-odds and in probability space." width="45%" /><img src="bookdown_files/figure-html/logoddspriorsf2-2.svg" alt="Prior for \(\alpha \sim \mathit{Normal}(0, 1.5)\) in log-odds and in probability space." width="45%" />
<p class="caption">
FIGURE 4.16: Prior for <span class="math inline">\(\alpha \sim \mathit{Normal}(0, 1.5)\)</span> in log-odds and in probability space.
</p>
</div>
<p>We need to decide now on the prior for <span class="math inline">\(\beta\)</span>, the effect in log-odds of increasing the set size. We could choose a normal distribution centered on zero, reflecting our lack of any commitment regarding the direction of the effect. Let’s get some intuitions regarding different possible standard deviations for this prior, by testing the following distributions as priors:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, 1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, .5)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, .1)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, .01)\)</span></li>
<li><span class="math inline">\(\beta \sim \mathit{Normal}(0, .001)\)</span></li>
</ol>
<p>In principle, we could produce the prior predictive distributions using <code>brms</code> with <code>sample_prior = &quot;only&quot;</code> and then <code>predict()</code>. However, as mentioned before, <code>brms</code> also uses Stan’s Hamiltonian sampler for sampling from the priors, and this leads to convergence problems when the priors are too uninformative (as in this case). We solve this issue by performing prior predictive checks directly in R using the <code>r*</code> functions (e.g., <code>rnorm()</code>, <code>rbinom()</code>, etc.) together with loops. This method is not as simple as using the convenient functions provided by <code>brms</code>, but it is very flexible and can be very powerful. We show the prior predictive distributions in Figure <a href="ch-reg.html#fig:priors4beta">4.17</a>, for the details on the implementation in R, see Box <a href="ch-reg.html#thm:priorR">4.4</a>.</p>

<div class="extra">
<div class="theorem">
<p><span id="thm:priorR" class="theorem"><strong>Box 4.4  </strong></span><strong>Prior predictive checks in R</strong></p>
</div>
<p>The following function is an edited version of the earlier <code>normal_predictive_distribution</code> from the Box <a href="ch-compbda.html#thm:efficientpriorpd">3.1</a> in section <a href="ch-compbda.html#sec-priorpred">3.3</a>; it has been edited to make it compatible with logistic regression and dependent on set size.</p>
<p>As we did before, our custom function uses the <code>purrr</code> function <code>map2_dfr()</code>, which runs an efficient for-loop, iterating over two vectors (here <code>alpha_samples</code> and <code>beta_samples</code>), and builds a data frame with the output.</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" data-line-number="1">logistic_model_pred &lt;-<span class="st"> </span><span class="cf">function</span>(alpha_samples,</a>
<a class="sourceLine" id="cb216-2" data-line-number="2">                                beta_samples,</a>
<a class="sourceLine" id="cb216-3" data-line-number="3">                                set_size,</a>
<a class="sourceLine" id="cb216-4" data-line-number="4">                                N_obs) {</a>
<a class="sourceLine" id="cb216-5" data-line-number="5">  <span class="kw">map2_dfr</span>(alpha_samples, beta_samples,</a>
<a class="sourceLine" id="cb216-6" data-line-number="6">    <span class="cf">function</span>(alpha, beta) {</a>
<a class="sourceLine" id="cb216-7" data-line-number="7">      <span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb216-8" data-line-number="8">        <span class="dt">set_size =</span> set_size,</a>
<a class="sourceLine" id="cb216-9" data-line-number="9">        <span class="co"># center size:</span></a>
<a class="sourceLine" id="cb216-10" data-line-number="10">        <span class="dt">c_set_size =</span> set_size <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(set_size),</a>
<a class="sourceLine" id="cb216-11" data-line-number="11">        <span class="co"># change the likelihood:</span></a>
<a class="sourceLine" id="cb216-12" data-line-number="12">        <span class="co"># Notice the use of a link function</span></a>
<a class="sourceLine" id="cb216-13" data-line-number="13">        <span class="co"># for alpha and beta</span></a>
<a class="sourceLine" id="cb216-14" data-line-number="14">        <span class="dt">theta =</span> <span class="kw">plogis</span>(alpha <span class="op">+</span><span class="st"> </span>c_set_size <span class="op">*</span><span class="st"> </span>beta),</a>
<a class="sourceLine" id="cb216-15" data-line-number="15">        <span class="dt">correct_pred =</span> <span class="kw">rbernoulli</span>(N_obs, <span class="dt">p =</span> theta)</a>
<a class="sourceLine" id="cb216-16" data-line-number="16">      )</a>
<a class="sourceLine" id="cb216-17" data-line-number="17">    },</a>
<a class="sourceLine" id="cb216-18" data-line-number="18">    <span class="dt">.id =</span> <span class="st">&quot;iter&quot;</span></a>
<a class="sourceLine" id="cb216-19" data-line-number="19">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb216-20" data-line-number="20"><span class="st">    </span><span class="co"># .id is always a string and needs</span></a>
<a class="sourceLine" id="cb216-21" data-line-number="21"><span class="st">    </span><span class="co"># to be converted to a number</span></a>
<a class="sourceLine" id="cb216-22" data-line-number="22"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">iter =</span> <span class="kw">as.numeric</span>(iter))</a>
<a class="sourceLine" id="cb216-23" data-line-number="23">}</a></code></pre></div>
<p>Let’s assume 800 observations with 200 observation for each set size:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" data-line-number="1">N_obs &lt;-<span class="st"> </span><span class="dv">800</span></a>
<a class="sourceLine" id="cb217-2" data-line-number="2">set_size &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>), <span class="dv">200</span>)</a></code></pre></div>
<p>Now, iterate over plausible standard deviations of <span class="math inline">\(\beta\)</span> with the <code>purrr</code> function <code>map_dfr()</code>, which iterates over one vector (here <code>sds_beta</code>), and also builds a data frame with the output.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb218-1" data-line-number="1">alpha_samples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb218-2" data-line-number="2">sds_beta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>)</a>
<a class="sourceLine" id="cb218-3" data-line-number="3">prior_pred &lt;-<span class="st"> </span><span class="kw">map_dfr</span>(sds_beta, <span class="cf">function</span>(sd) {</a>
<a class="sourceLine" id="cb218-4" data-line-number="4">  beta_samples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, sd)</a>
<a class="sourceLine" id="cb218-5" data-line-number="5">  <span class="kw">logistic_model_pred</span>(</a>
<a class="sourceLine" id="cb218-6" data-line-number="6">    <span class="dt">alpha_samples =</span> alpha_samples,</a>
<a class="sourceLine" id="cb218-7" data-line-number="7">    <span class="dt">beta_samples =</span> beta_samples,</a>
<a class="sourceLine" id="cb218-8" data-line-number="8">    <span class="dt">set_size =</span> set_size,</a>
<a class="sourceLine" id="cb218-9" data-line-number="9">    <span class="dt">N_obs =</span> N_obs</a>
<a class="sourceLine" id="cb218-10" data-line-number="10">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb218-11" data-line-number="11"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">prior_beta_sd =</span> sd)</a>
<a class="sourceLine" id="cb218-12" data-line-number="12">})</a></code></pre></div>
<p>Calculate the accuracy for each one of the priors we want to examine, for each iteration, and for each set size.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" data-line-number="1">mean_accuracy &lt;-</a>
<a class="sourceLine" id="cb219-2" data-line-number="2"><span class="st">  </span>prior_pred <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb219-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(prior_beta_sd, iter, set_size) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb219-4" data-line-number="4"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(correct_pred)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb219-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior =</span> <span class="kw">paste0</span>(<span class="st">&quot;Normal(0, &quot;</span>, prior_beta_sd, <span class="st">&quot;)&quot;</span>))</a></code></pre></div>
<p>Plot the accuracy in Figure <a href="ch-reg.html#fig:priors4beta">4.17</a> as follows.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb220-1" data-line-number="1">mean_accuracy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb220-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(accuracy)) <span class="op">+</span></a>
<a class="sourceLine" id="cb220-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb220-4" data-line-number="4"><span class="st">  </span><span class="kw">facet_grid</span>(set_size <span class="op">~</span><span class="st"> </span>prior) <span class="op">+</span></a>
<a class="sourceLine" id="cb220-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">.5</span>, <span class="dv">1</span>))</a></code></pre></div>
<p>It’s sometimes more useful to look at the predicted differences in accuracy between set sizes. We calculate them as follows, and plot them in Figure <a href="ch-reg.html#fig:priors4beta2">4.18</a>.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" data-line-number="1">diff_accuracy &lt;-<span class="st"> </span>mean_accuracy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb221-2" data-line-number="2"><span class="st">  </span><span class="kw">arrange</span>(set_size) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb221-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(iter, prior_beta_sd) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb221-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diff_accuracy =</span> accuracy <span class="op">-</span><span class="st"> </span><span class="kw">lag</span>(accuracy)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb221-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diffsize =</span> <span class="kw">paste</span>(set_size, <span class="st">&quot;-&quot;</span>, <span class="kw">lag</span>(set_size))) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb221-6" data-line-number="6"><span class="st">  </span><span class="kw">filter</span>(set_size <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span>)</a></code></pre></div>
<div class="sourceCode" id="cb222"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb222-1" data-line-number="1">diff_accuracy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb222-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(diff_accuracy)) <span class="op">+</span></a>
<a class="sourceLine" id="cb222-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb222-4" data-line-number="4"><span class="st">  </span><span class="kw">facet_grid</span>(diffsize <span class="op">~</span><span class="st"> </span>prior) <span class="op">+</span></a>
<a class="sourceLine" id="cb222-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="op">-</span>.<span class="dv">5</span>, <span class="dv">0</span>, <span class="fl">.5</span>))</a></code></pre></div>
</div>
<p>Figure <a href="ch-reg.html#fig:priors4beta">4.17</a> shows that, as expected, the priors are centered at zero. We see that the distribution of possible accuracies for the prior that has a standard deviation of <span class="math inline">\(1\)</span> is problematic: There is too much probability concentrated near <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> for set sizes of <span class="math inline">\(2\)</span> and <span class="math inline">\(8\)</span>. It’s hard to tell the differences between the other priors, and it might be more useful to look at the predicted differences in accuracy between set sizes in Figure <a href="ch-reg.html#fig:priors4beta2">4.18</a>.</p>

<div class="figure"><span style="display:block;" id="fig:priors4beta"></span>
<img src="bookdown_files/figure-html/priors4beta-1.svg" alt="Prior predictive distribution of mean accuracy of the model defined in section 4.3, for different set sizes and different priors for \(\beta\)." width="672" />
<p class="caption">
FIGURE 4.17: Prior predictive distribution of mean accuracy of the model defined in section <a href="ch-reg.html#sec-logistic">4.3</a>, for different set sizes and different priors for <span class="math inline">\(\beta\)</span>.
</p>
</div>

<div class="figure"><span style="display:block;" id="fig:priors4beta2"></span>
<img src="bookdown_files/figure-html/priors4beta2-1.svg" alt="Prior predictive distribution of differences in mean accuracy between set sizes of the model defined in 4.3 for different priors for \(\beta\)." width="672" />
<p class="caption">
FIGURE 4.18: Prior predictive distribution of differences in mean accuracy between set sizes of the model defined in <a href="ch-reg.html#sec-logistic">4.3</a> for different priors for <span class="math inline">\(\beta\)</span>.
</p>
</div>
<!-- log-odds transformation using the logit function (also known as the inverse logistic function): -->
<!-- The logit funcion maps values  -->
<!-- As before, we will assume that the relationship between set size and decrease (or increase) in acc -->
<p>If we are not sure whether the increase of set size could produce something between a null effect and a relatively large effect, we can choose the prior with a standard deviation of <span class="math inline">\(0.1\)</span>. Under this reasoning, we settle on the following priors:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(0, 1.5) \\
\beta &amp;\sim \mathit{Normal}(0, 0.1) 
\end{aligned}
\end{equation}\]</span></p>
</div>
<div id="the-brms-model-2" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.3</span> The <code>brms</code> model<a href="ch-reg.html#the-brms-model-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having decided on the likelihood, the link function, and the priors, the model can now be fit using <code>brms</code>. We need to specify that the family is <code>bernoulli()</code>, and the link is <code>logit</code>.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" data-line-number="1">fit_recall &lt;-<span class="st"> </span><span class="kw">brm</span>(correct <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_set_size,</a>
<a class="sourceLine" id="cb223-2" data-line-number="2">  <span class="dt">data =</span> df_recall,</a>
<a class="sourceLine" id="cb223-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> logit),</a>
<a class="sourceLine" id="cb223-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb223-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb223-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">.1</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_set_size)</a>
<a class="sourceLine" id="cb223-7" data-line-number="7">  )</a>
<a class="sourceLine" id="cb223-8" data-line-number="8">)</a></code></pre></div>
<p>Next, look at the summary of the posteriors of each of the parameters. Keep in mind that the parameters are in log-odds space:</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb224-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_recall,</a>
<a class="sourceLine" id="cb224-2" data-line-number="2">                  <span class="dt">variable =</span> <span class="kw">c</span>(<span class="st">&quot;b_Intercept&quot;</span>, <span class="st">&quot;b_c_set_size&quot;</span>))</a></code></pre></div>
<pre><code>##              Estimate Est.Error   Q2.5   Q97.5
## b_Intercept     1.924     0.300  1.384  2.5221
## b_c_set_size   -0.183     0.081 -0.345 -0.0279</code></pre>
<p>Inspecting <code>b_c_set_size</code>, we see that increasing the set size have a detrimental effect in recall, as we suspected.</p>
<p>Plot the posteriors as well (Figure <a href="ch-reg.html#fig:posteriorsfitrecall">4.19</a>):</p>

<div class="sourceCode" id="cb226"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb226-1" data-line-number="1"><span class="kw">plot</span>(fit_recall)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:posteriorsfitrecall"></span>
<img src="bookdown_files/figure-html/posteriorsfitrecall-1.svg" alt="Posterior distributions of the parameters in the brms model fit_recall, along with the corresponding trace plots." width="672" />
<p class="caption">
FIGURE 4.19: Posterior distributions of the parameters in the brms model <code>fit_recall</code>, along with the corresponding trace plots.
</p>
</div>
<p>Next, we turn to the question of what we can report as our results, and what we can conclude from the data.</p>
</div>
<div id="sec-comlogis" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.4</span> How to communicate the results?<a href="ch-reg.html#sec-comlogis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we are in a situation analogous to the one we saw earlier with the log-normal model. If we want to talk about the effect estimated by the model in log-odds space, we summarize the posterior of <span class="math inline">\(\beta\)</span> in the following way: <span class="math inline">\(\hat\beta = -0.183\)</span>, 95% CrI = <span class="math inline">\([ -0.345 , -0.028 ]\)</span>.</p>
<p>However, the effect is easier to understand in proportions rather than in log-odds. Let’s look at the average accuracy for the task first:</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb227-1" data-line-number="1">alpha_samples &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_recall)<span class="op">$</span>b_Intercept</a>
<a class="sourceLine" id="cb227-2" data-line-number="2">av_accuracy &lt;-<span class="st"> </span><span class="kw">plogis</span>(alpha_samples)</a>
<a class="sourceLine" id="cb227-3" data-line-number="3"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(av_accuracy), <span class="kw">quantile</span>(av_accuracy, <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>##  mean  2.5% 97.5% 
## 0.869 0.800 0.926</code></pre>
<p>As before, to transform the effect of our manipulation to an easier to interpret scale (i.e., proportion), we need to take into account that the scale is not linear, and that the effect of increasing the set size depends on the average accuracy, and the set size that we start from.</p>
<p>We can do the following calculation, similar to what we did for the trial effects experiment, to find out the decrease in accuracy in proportions or probability scale:</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb229-1" data-line-number="1">beta_samples &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_recall)<span class="op">$</span>b_c_set_size</a>
<a class="sourceLine" id="cb229-2" data-line-number="2">effect_middle &lt;-<span class="st"> </span><span class="kw">plogis</span>(alpha_samples) <span class="op">-</span></a>
<a class="sourceLine" id="cb229-3" data-line-number="3"><span class="st">  </span><span class="kw">plogis</span>(alpha_samples <span class="op">-</span><span class="st"> </span>beta_samples)</a>
<a class="sourceLine" id="cb229-4" data-line-number="4"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_middle),</a>
<a class="sourceLine" id="cb229-5" data-line-number="5">  <span class="kw">quantile</span>(effect_middle, <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>##     mean     2.5%    97.5% 
## -0.01886 -0.03690 -0.00296</code></pre>
<p>Notice the interpretation here, if we increase the set size from the average set size minus one to the average set size, we get a reduction in the accuracy of recall of <span class="math inline">\(-0.019\)</span>, 95% CrI = <span class="math inline">\([ -0.037 , -0.003 ]\)</span>. Recall that the average set size, 5, was not presented to the subject! We could alternatively look at the decrease in accuracy from a set size of 2 to 4:</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb231-1" data-line-number="1">four &lt;-<span class="st"> </span><span class="dv">4</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(df_recall<span class="op">$</span>set_size)</a>
<a class="sourceLine" id="cb231-2" data-line-number="2">two &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(df_recall<span class="op">$</span>set_size)</a>
<a class="sourceLine" id="cb231-3" data-line-number="3">effect_4m2 &lt;-</a>
<a class="sourceLine" id="cb231-4" data-line-number="4"><span class="st">  </span><span class="kw">plogis</span>(alpha_samples <span class="op">+</span><span class="st"> </span>four <span class="op">*</span><span class="st"> </span>beta_samples) <span class="op">-</span></a>
<a class="sourceLine" id="cb231-5" data-line-number="5"><span class="st">  </span><span class="kw">plogis</span>(alpha_samples <span class="op">+</span><span class="st"> </span>two <span class="op">*</span><span class="st"> </span>beta_samples)</a>
<a class="sourceLine" id="cb231-6" data-line-number="6"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_4m2), </a>
<a class="sourceLine" id="cb231-7" data-line-number="7">  <span class="kw">quantile</span>(effect_4m2, <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>##     mean     2.5%    97.5% 
## -0.02949 -0.05314 -0.00562</code></pre>
<p>We can also back-transform to probability scale using the function <code>fitted()</code> rather than using <code>plogis()</code>. One advantage is that this will work regardless of the type of scale (e.g, logit link vs. probit link).</p>
<p>Since the set size is the only predictor and it is centered, for estimating the average accuracy, we can consider an imaginary observation where the <code>c_set_size</code> is zero. (If there were more centered predictors, we would need to set all of them to zero). Now we can use the <code>summary</code> argument provided by <code>fitted()</code>.</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb233-1" data-line-number="1"><span class="kw">fitted</span>(fit_recall,</a>
<a class="sourceLine" id="cb233-2" data-line-number="2">       <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">c_set_size =</span> <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb233-3" data-line-number="3">       <span class="dt">summary =</span> <span class="ot">TRUE</span>)[,<span class="kw">c</span>(<span class="st">&quot;Estimate&quot;</span>, <span class="st">&quot;Q2.5&quot;</span>,<span class="st">&quot;Q97.5&quot;</span>)]</a></code></pre></div>
<pre><code>## Estimate     Q2.5    Q97.5 
##    0.869    0.800    0.926</code></pre>
<p>For estimating the difference in accuracy from the average set size minus one to the average set size, and from a set size of two to four, first, define <code>newdata</code> with these set sizes.</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb235-1" data-line-number="1">new_sets &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">c_set_size =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">-1</span>, four, two))</a>
<a class="sourceLine" id="cb235-2" data-line-number="2">set_sizes &lt;-<span class="st"> </span><span class="kw">fitted</span>(fit_recall,</a>
<a class="sourceLine" id="cb235-3" data-line-number="3">                 <span class="dt">newdata =</span> new_sets,</a>
<a class="sourceLine" id="cb235-4" data-line-number="4">                 <span class="dt">summary =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>Then calculate the appropriate differences considering that column one of <code>set_sizes</code> corresponds to the average set size, column two to the average set size minus one and so forth.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" data-line-number="1">effect_middle &lt;-<span class="st"> </span>set_sizes[, <span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>set_sizes[, <span class="dv">2</span>] </a>
<a class="sourceLine" id="cb236-2" data-line-number="2">effect_4m2 &lt;-<span class="st"> </span>set_sizes[, <span class="dv">3</span>] <span class="op">-</span><span class="st"> </span>set_sizes[, <span class="dv">4</span>] </a></code></pre></div>
<p>Finally, calculate the summaries.</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb237-1" data-line-number="1"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_middle), <span class="kw">quantile</span>(effect_middle, </a>
<a class="sourceLine" id="cb237-2" data-line-number="2">                                       <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>##     mean     2.5%    97.5% 
## -0.01886 -0.03690 -0.00296</code></pre>
<div class="sourceCode" id="cb239"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb239-1" data-line-number="1"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(effect_4m2), <span class="kw">quantile</span>(effect_4m2, </a>
<a class="sourceLine" id="cb239-2" data-line-number="2">                                       <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</a></code></pre></div>
<pre><code>##     mean     2.5%    97.5% 
## -0.02949 -0.05314 -0.00562</code></pre>
<p>As expected we get exactly the same values with <code>fitted()</code> than when we calculate them “by hand”.</p>
</div>
<div id="descriptive-adequacy-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.5</span> Descriptive adequacy<a href="ch-reg.html#descriptive-adequacy-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One potentially useful aspect of posterior distributions is that we could also make predictions for other conditions not presented in the actual experiment, such as set sizes that weren’t tested. We could then investigate whether our model was right using another experiment. To make predictions for other set sizes, we extend our data set, adding rows with set sizes of 3, 5, and 7. To be consistent with the data of the other set sizes in the experiment, we add 23 trials of each new set size (this is the number of trial by set size in the data set). Something important to notice is that <strong>we need to center our predictor based on the original mean set size</strong>. This is because we want to maintain our interpretation of the intercept. We extend the data as follows, and we summarize the data and plot it in Figure <a href="ch-reg.html#fig:postpredsum2">4.20</a>.</p>

<div class="sourceCode" id="cb241"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb241-1" data-line-number="1">df_recall_ext &lt;-<span class="st"> </span>df_recall <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb241-2" data-line-number="2"><span class="st">  </span><span class="kw">bind_rows</span>(<span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb241-3" data-line-number="3">    <span class="dt">set_size =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>), <span class="dv">23</span>),</a>
<a class="sourceLine" id="cb241-4" data-line-number="4">    <span class="dt">c_set_size =</span> set_size <span class="op">-</span></a>
<a class="sourceLine" id="cb241-5" data-line-number="5"><span class="st">      </span><span class="kw">mean</span>(df_recall<span class="op">$</span>set_size),</a>
<a class="sourceLine" id="cb241-6" data-line-number="6">    <span class="dt">correct =</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb241-7" data-line-number="7">  ))</a>
<a class="sourceLine" id="cb241-8" data-line-number="8"><span class="co"># nicer label for the facets:</span></a>
<a class="sourceLine" id="cb241-9" data-line-number="9">set_size &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;set size&quot;</span>, <span class="dv">2</span><span class="op">:</span><span class="dv">8</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb241-10" data-line-number="10"><span class="st">  </span><span class="kw">setNames</span>(<span class="op">-</span><span class="dv">3</span><span class="op">:</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb241-11" data-line-number="11"><span class="kw">pp_check</span>(fit_recall,</a>
<a class="sourceLine" id="cb241-12" data-line-number="12">  <span class="dt">type =</span> <span class="st">&quot;stat_grouped&quot;</span>,</a>
<a class="sourceLine" id="cb241-13" data-line-number="13">  <span class="dt">stat =</span> <span class="st">&quot;mean&quot;</span>,</a>
<a class="sourceLine" id="cb241-14" data-line-number="14">  <span class="dt">group =</span> <span class="st">&quot;c_set_size&quot;</span>,</a>
<a class="sourceLine" id="cb241-15" data-line-number="15">  <span class="dt">newdata =</span> df_recall_ext,</a>
<a class="sourceLine" id="cb241-16" data-line-number="16">  <span class="dt">facet_args =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb241-17" data-line-number="17">    <span class="dt">ncol =</span> <span class="dv">1</span>, <span class="dt">scales =</span> <span class="st">&quot;fixed&quot;</span>,</a>
<a class="sourceLine" id="cb241-18" data-line-number="18">    <span class="dt">labeller =</span> <span class="kw">as_labeller</span>(set_size)</a>
<a class="sourceLine" id="cb241-19" data-line-number="19">  ),</a>
<a class="sourceLine" id="cb241-20" data-line-number="20">  <span class="dt">binwidth =</span> <span class="fl">0.02</span></a>
<a class="sourceLine" id="cb241-21" data-line-number="21">)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postpredsum2"></span>
<img src="bookdown_files/figure-html/postpredsum2-1.svg" alt="The distribution of posterior predicted mean accuracies for tested set sizes (2, 4, 6, and 8) and untested ones (3, 5, and 7) are labeled with \(y_{rep}\). The observed mean accuracy, \(y\), are only relevant for the tested set sizes." width="672" />
<p class="caption">
FIGURE 4.20: The distribution of posterior predicted mean accuracies for tested set sizes (2, 4, 6, and 8) and untested ones (3, 5, and 7) are labeled with <span class="math inline">\(y_{rep}\)</span>. The observed mean accuracy, <span class="math inline">\(y\)</span>, are only relevant for the tested set sizes.
</p>
</div>
<p>We could now gather new data in an experiment that also shows set sizes of 3, 5, and 7. These data would be <em>held out</em> from the model <code>fit_recall</code>, since the model was fit when those data were not available. Verifying that the new observations fit in our already generated posterior predictive distribution would be a way to test genuine predictions from our model.</p>
<p>Having seen how we can fit simple regression models, we turn to hierarchical models in the next chapter.</p>
<!-- with https://osf.io/uwdcm/#! -->
<!-- accuracy in the translation from the word in Spanish to English as a function of word experience (the total number of times that a user saw a given word in Duolingo.) -->
</div>
</div>
<div id="summary-3" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.4</span> Summary<a href="ch-reg.html#summary-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we learned how to fit simple linear regression models and to fit and interpret models with a log-normal likelihood and logistic regression models. We investigated the prior specification for the models, using prior predictive checks, and the descriptive adequacy of the models using posterior predictive checks.</p>
</div>
<div id="sec-ch4furtherreading" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.5</span> Further reading<a href="ch-reg.html#sec-ch4furtherreading" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Linear regression is discussed in several classic textbooks; these have largely a frequentist orientation, but the basic theory of linear modeling presented there can easily be extended to the Bayesian framework. An accessible textbook is by <span class="citation">Dobson and Barnett (<a href="#ref-dobson2011introduction">2011</a>)</span>. Other useful textbooks on linear modeling are <span class="citation">Harrell Jr (<a href="#ref-harrell2015regression">2015</a>)</span>, <span class="citation">Faraway (<a href="#ref-faraway2016extending">2016</a>)</span>, <span class="citation">Fox (<a href="#ref-fox2015applied">2015</a>)</span>, and <span class="citation">Montgomery, Peck, and Vining (<a href="#ref-monty">2012</a>)</span>.</p>
</div>
<div id="sec-LMexercises" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.6</span> Exercises<a href="ch-reg.html#sec-LMexercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:powerposing" class="exercise"><strong>Exercise 4.1  </strong></span>A simple linear regression: Power posing and testosterone.</p>
</div>
<p>Load the following data set:</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_powerpose&quot;</span>)</a>
<a class="sourceLine" id="cb242-2" data-line-number="2"><span class="kw">head</span>(df_powerpose)</a></code></pre></div>
<pre><code>##   id hptreat female age testm1 testm2
## 2 29    High   Male  19   38.7   62.4
## 3 30     Low Female  20   32.8   29.2
## 4 31    High Female  20   32.3   27.5
## 5 32     Low Female  18   18.0   28.7
## 7 34     Low Female  21   73.6   44.7
## 8 35    High Female  20   80.7  105.5</code></pre>
<p>The data set, which was originally published in <span class="citation">Carney, Cuddy, and Yap (<a href="#ref-carney2010power">2010</a>)</span> but released in modified form by <span class="citation">Fosse (<a href="#ref-FossePowerPose">2016</a>)</span>, shows the testosterone levels of 39 different individuals, before and after treatment, where treatment refers to each individual being assigned to a high power pose or a low power pose. In the original paper by <span class="citation">Carney, Cuddy, and Yap (<a href="#ref-carney2010power">2010</a>)</span>, the unit given for testosterone measurement (estimated from saliva samples) was picograms per milliliter (pg/ml). One picogram per milliliter is 0.001 nanogram per milliliter (ng/ml).</p>
<p>The research hypothesis is that on average, assigning a subject a high power pose vs. a low power pose will lead to higher testosterone levels after treatment. Assuming that you know nothing about normal ranges of testosterone using salivary measurement, choose an appropriate Cauchy prior (e.g., <span class="math inline">\(\mathit{Cauchy}(0,2.5)\)</span>) for the target parameter(s).</p>
<p>Investigate this claim using a linear model and the default priors of <code>brms</code>. You’ll need to estimate the effect of a new variable that encodes the change in testosterone.</p>
<div class="exercise">
<p><span id="exr:pupils" class="exercise"><strong>Exercise 4.2  </strong></span>Another linear regression model: Revisiting attentional load effect on pupil size.</p>
</div>
<p>Here, we revisit the analysis shown in the chapter, on how attentional load affects pupil size.</p>
<ol style="list-style-type: lower-alpha">
<li>Our priors for this experiment were quite arbitrary. How do the prior predictive distributions look like? Do they make sense?</li>
<li>Is our posterior distribution sensitive to the priors that we selected? Perform a sensitivity analysis to find out whether the posterior is affected by our choice of prior for the <span class="math inline">\(\sigma\)</span>.</li>
<li>Our data set includes also a column that indicates the trial number. Could it be that trial has also an effect on the pupil size? As in <code>lm</code>, we indicate another main effect with a <code>+</code> sign. How would you communicate the new results?</li>
</ol>
<div class="exercise">
<p><span id="exr:lognormalm" class="exercise"><strong>Exercise 4.3  </strong></span>Log-normal model: Revisiting the effect of trial on response times.</p>
</div>
<p>We continue considering the effect of trial on response times.</p>
<ol style="list-style-type: lower-alpha">
<li>Estimate the slowdown in milliseconds between the last two times the subject pressed the space bar in the experiment.</li>
<li>How would you change your model (keeping the log-normal likelihood) so that it includes centered log-transformed trial numbers or square-root-transformed trial numbers (instead of centered trial numbers)? Does the effect in milliseconds change?</li>
</ol>
<div class="exercise">
<p><span id="exr:reg-logistic" class="exercise"><strong>Exercise 4.4  </strong></span>Logistic regression: Revisiting the effect of set size on free recall.</p>
</div>
<p>Our data set includes also a column coded as <code>tested</code> that indicates the position of the queued word. (In Figure <a href="ch-reg.html#fig:oberauer">4.13</a> <code>tested</code> would be 3). Could it be that position also has an effect on recall accuracy? How would you incorporate this in the model? Verify the descriptive adequacy of our new model.</p>
<div class="exercise">
<p><span id="exr:red" class="exercise"><strong>Exercise 4.5  </strong></span>Red is the sexiest color.</p>
</div>
<p>Load the following data set:</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r fold-show"><code class="sourceCode r"><a class="sourceLine" id="cb244-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_red&quot;</span>)</a>
<a class="sourceLine" id="cb244-2" data-line-number="2"><span class="kw">head</span>(df_red)</a></code></pre></div>
<pre><code>##    risk age red pink redorpink
## 8     0  19   0    0         0
## 9     0  25   0    0         0
## 10    0  20   0    0         0
## 11    0  20   0    0         0
## 14    0  20   0    0         0
## 15    0  18   0    0         0</code></pre>
<p>The data set is from a study that contains information about the color of the clothing worn (red, pink, or red or pink) when the subject (female) is at risk of becoming pregnant (is ovulating, self-reported). The broader issue being investigated is whether women wear red more often when they are ovulating (in order to attract a mate). Using logistic regressions, fit three different models to investigate whether being ovulating increases the probability of wearing (a) red, (b) pink, or (c) either pink or red. Use priors that are reasonable (in your opinion).</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-Blumberg2015">
<p>Blumberg, Eric J., Matthew S. Peterson, and Raja Parasuraman. 2015. “Enhancing Multiple Object Tracking Performance with Noninvasive Brain Stimulation: A Causal Role for the Anterior Intraparietal Sulcus.” <em>Frontiers in Systems Neuroscience</em> 9: 3. <a href="https://doi.org/10.3389/fnsys.2015.00003" class="uri">https://doi.org/10.3389/fnsys.2015.00003</a>.</p>
</div>
<div id="ref-breeDistributionProblemsolvingTimes1975">
<p>Brée, David S. 1975. “The Distribution of Problem-Solving Times: An Examination of the Stages Model.” <em>British Journal of Mathematical and Statistical Psychology</em> 28 (2): 177–200. <a href="https://doi.org/10/cnx3q7" class="uri">https://doi.org/10/cnx3q7</a>.</p>
</div>
<div id="ref-buzsakiLogdynamicBrainHow2014">
<p>Buzsáki, György, and Kenji Mizuseki. 2014. “The Log-Dynamic Brain: How Skewed Distributions Affect Network Operations.” <em>Nature Reviews Neuroscience</em> 15 (4): 264–78. <a href="https://doi.org/10.1038/nrn3687" class="uri">https://doi.org/10.1038/nrn3687</a>.</p>
</div>
<div id="ref-carney2010power">
<p>Carney, Dana R, Amy JC Cuddy, and Andy J Yap. 2010. “Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance.” <em>Psychological Science</em> 21 (10). Sage Publications Sage CA: Los Angeles, CA: 1363–8.</p>
</div>
<div id="ref-dobson2011introduction">
<p>Dobson, Annette J, and Adrian Barnett. 2011. <em>An Introduction to Generalized Linear Models</em>. CRC press.</p>
</div>
<div id="ref-faraway2016extending">
<p>Faraway, Julian J. 2016. <em>Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-FossePowerPose">
<p>Fosse, Nathan E. 2016. “Replication Data for ‘Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance’ by Carney, Cuddy, Yap (2010).” Harvard Dataverse. <a href="https://doi.org/10.7910/DVN/FMEGS6" class="uri">https://doi.org/10.7910/DVN/FMEGS6</a>.</p>
</div>
<div id="ref-fox2015applied">
<p>Fox, John. 2015. <em>Applied Regression Analysis and Generalized Linear Models</em>. Sage Publications.</p>
</div>
<div id="ref-harrell2015regression">
<p>Harrell Jr, Frank E. 2015. <em>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</em>. New York, NY: Springer.</p>
</div>
<div id="ref-hayesMappingCorrectingInfluence2016">
<p>Hayes, Taylor R., and Alexander A. Petrov. 2016. “Mapping and Correcting the Influence of Gaze Position on Pupil Size Measurements.” <em>Behavior Research Methods</em> 48 (2): 510–27. <a href="https://doi.org/10.3758/s13428-015-0588-x" class="uri">https://doi.org/10.3758/s13428-015-0588-x</a>.</p>
</div>
<div id="ref-JaegerEngelmannVasishth2017">
<p>Jäger, Lena A., Felix Engelmann, and Shravan Vasishth. 2017. “Similarity-Based Interference in Sentence Comprehension: Literature review and Bayesian meta-analysis.” <em>Journal of Memory and Language</em> 94: 316–39. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004" class="uri">https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004</a>.</p>
</div>
<div id="ref-johnson">
<p>Johnson, Norman L, Samuel Kotz, and Narayanaswamy Balakrishnan. 1995. <em>Continuous Univariate Distributions</em>. Vol. 289. John Wiley &amp; Sons.</p>
</div>
<div id="ref-limpertLognormalDistributionsSciences2001">
<p>Limpert, Eckhard, Werner A. Stahel, and Markus Abbt. 2001. “Log-Normal Distributions Across the Sciences: Keys and Clues.” <em>BioScience</em> 51 (5): 341. <a href="https://doi.org/10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2" class="uri">https://doi.org/10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2</a>.</p>
</div>
<div id="ref-mahowald2016meta">
<p>Mahowald, Kyle, Ariel James, Richard Futrell, and Edward Gibson. 2016. “A Meta-Analysis of Syntactic Priming in Language Production.” <em>Journal of Memory and Language</em> 91. Elsevier: 5–27.</p>
</div>
<div id="ref-mathotPupillometryPsychologyPhysiology2018">
<p>Mathot, Sebastiaan. 2018. “Pupillometry: Psychology, Physiology, and Function.” <em>Journal of Cognition</em> 1 (1): 16. <a href="https://doi.org/10.5334/joc.18" class="uri">https://doi.org/10.5334/joc.18</a>.</p>
</div>
<div id="ref-monty">
<p>Montgomery, D. C., E. A. Peck, and G. G. Vining. 2012. <em>An Introduction to Linear Regression Analysis</em>. 5th ed. Hoboken, NJ: Wiley.</p>
</div>
<div id="ref-NicenboimRoettgeretal">
<p>Nicenboim, Bruno, Timo B. Roettger, and Shravan Vasishth. 2018. “Using Meta-Analysis for Evidence Synthesis: The case of incomplete neutralization in German.” <em>Journal of Phonetics</em> 70: 39–55. <a href="https://doi.org/https://doi.org/10.1016/j.wocn.2018.06.001" class="uri">https://doi.org/https://doi.org/10.1016/j.wocn.2018.06.001</a>.</p>
</div>
<div id="ref-oberauerWorkingMemoryCapacity2019">
<p>Oberauer, Klaus. 2019. “Working Memory Capacity Limits Memory for Bindings.” <em>Journal of Cognition</em> 2 (1): 40. <a href="https://doi.org/10.5334/joc.86" class="uri">https://doi.org/10.5334/joc.86</a>.</p>
</div>
<div id="ref-oberauerkliegel2001">
<p>Oberauer, Klaus, and Reinhold Kliegl. 2001. “Beyond Resources: Formal Models of Complexity Effects and Age Differences in Working Memory.” <em>European Journal of Cognitive Psychology</em> 13 (1-2). Routledge: 187–215. <a href="https://doi.org/10.1080/09541440042000278" class="uri">https://doi.org/10.1080/09541440042000278</a>.</p>
</div>
<div id="ref-pylyshynTrackingMultipleIndependent1988">
<p>Pylyshyn, Zenon W., and Ron W. Storm. 1988. “Tracking Multiple Independent Targets: Evidence for a Parallel Tracking Mechanism.” <em>Spatial Vision</em> 3 (3): 179–97. <a href="https://doi.org/10.1163/156856888X00122" class="uri">https://doi.org/10.1163/156856888X00122</a>.</p>
</div>
<div id="ref-Royall">
<p>Royall, Richard. 1997. <em>Statistical Evidence: A Likelihood Paradigm</em>. New York: Chapman; Hall, CRC Press.</p>
</div>
<div id="ref-spectorPupils1990">
<p>Spector, Robert H. 1990. “The Pupils.” In <em>Clinical Methods: The History, Physical, and Laboratory Examinations</em>, edited by H. Kenneth Walker, W. Dallas Hall, and J. Willis Hurst, 3rd ed. Boston: Butterworths.</p>
</div>
<div id="ref-ulrichInformationProcessingModels1993">
<p>Ulrich, Rolf, and Jeff Miller. 1993. “Information Processing Models Generating Lognormally Distributed Reaction Times.” <em>Journal of Mathematical Psychology</em> 37 (4): 513–25. <a href="https://doi.org/10.1006/jmps.1993.1032" class="uri">https://doi.org/10.1006/jmps.1993.1032</a>.</p>
</div>
<div id="ref-ulrichEffectsTruncationReaction1994">
<p>Ulrich, Rolf, and Jeff Miller. 1994. “Effects of Truncation on Reaction Time Analysis.” <em>Journal of Experimental Psychology: General</em> 123 (1): 34–80. <a href="https://doi.org/10/b8tsnh" class="uri">https://doi.org/10/b8tsnh</a>.</p>
</div>
<div id="ref-VasishthetalPLoSOne2013">
<p>Vasishth, Shravan, Zhong Chen, Qiang Li, and Gueilan Guo. 2013. “Processing Chinese Relative Clauses: Evidence for the Subject-Relative Advantage.” <em>PLoS ONE</em> 8 (10). Public Library of Science: 1–14.</p>
</div>
<div id="ref-wagenmakersRelationMeanVariance2005">
<p>Wagenmakers, Eric-Jan, Raoul P. P. P. Grasman, and Peter C. M. Molenaar. 2005. “On the Relation Between the Mean and the Variance of a Diffusion Model Response Time Distribution.” <em>Journal of Mathematical Psychology</em> 49 (3): 195–204. <a href="https://doi.org/10.1016/j.jmp.2005.02.003" class="uri">https://doi.org/10.1016/j.jmp.2005.02.003</a>.</p>
</div>
<div id="ref-wahnPupilSizesScale2016">
<p>Wahn, Basil, Daniel P. Ferris, W. David Hairston, and Peter König. 2016. “Pupil Sizes Scale with Attentional Load and Task Experience in a Multiple Object Tracking Task.” <em>PLOS ONE</em> 11 (12): e0168087. <a href="https://doi.org/10.1371/journal.pone.0168087" class="uri">https://doi.org/10.1371/journal.pone.0168087</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="12">
<li id="fn12"><p>The average pupil size will probably be higher than 800, since this measurement was with no load, but, in any case, the exact number won’t matter, any mean for the prior between 500-1500 would be fine if the standard deviation is large.<a href="ch-reg.html#fnref12" class="footnote-back">↩</a></p></li>
<li id="fn13"><p>Odds are defined to be the ratio of the probability of success to the probability of failure. For example, the odds of obtaining a one in a fair six-sided die are <span class="math inline">\(\frac{1/6}{1-1/6}=1/5\)</span>. The odds of obtaining a heads in a fair coin are 1/1.<a href="ch-reg.html#fnref13" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-compbda.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-hierarchical.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
