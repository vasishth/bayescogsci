<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.3 Summarizing and evaluation the posterior distribution | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.13.2 and GitBook 2.6.7" />

  <meta property="og:title" content="4.3 Summarizing and evaluation the posterior distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bookdown.org/yihui/bookdown/" />
  <meta property="og:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="rstudio/bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.3 Summarizing and evaluation the posterior distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />

<meta name="author" content="Shravan Vasishth, Bruno Nicenboim, and Daniel Schad" />


<meta name="date" content="2019-11-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-regression-models-using-stan-brms.html"/>
<link rel="next" href="evaluating-the-priors-of-our-models.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.2</b> How to read this book</a></li>
<li class="chapter" data-level="0.3" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.3</b> Online materials</a></li>
<li class="chapter" data-level="0.4" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.4</b> Software needed</a></li>
<li class="chapter" data-level="0.5" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>0.5</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html"><i class="fa fa-check"></i><b>1.3</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.3.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.4</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.4.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><i class="fa fa-check"></i><b>1.5</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.6" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.7</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a><ul>
<li class="chapter" data-level="1.9.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.9.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.9.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.9.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.9.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.9.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.9.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.9.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.9.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayes-factor-definition.html"><a href="bayes-factor-definition.html"><i class="fa fa-check"></i><b>3</b> Bayes factor: Definition</a></li>
<li class="chapter" data-level="4" data-path="introduction-to-bayesian-data-analysis-computationally.html"><a href="introduction-to-bayesian-data-analysis-computationally.html"><i class="fa fa-check"></i><b>4</b> Introduction to Bayesian data analysis computationally</a><ul>
<li class="chapter" data-level="4.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html"><i class="fa fa-check"></i><b>4.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="4.2" data-path="bayesian-regression-models-using-stan-brms.html"><a href="bayesian-regression-models-using-stan-brms.html"><i class="fa fa-check"></i><b>4.2</b> Bayesian Regression Models using ‘Stan’: brms</a><ul>
<li class="chapter" data-level="4.2.1" data-path="bayesian-regression-models-using-stan-brms.html"><a href="bayesian-regression-models-using-stan-brms.html#a-simple-linear-model-a-single-participant-pressing-a-button-repeatedly"><i class="fa fa-check"></i><b>4.2.1</b> A simple linear model: A single participant pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="summarizing-and-evaluation-the-posterior-distribution.html"><a href="summarizing-and-evaluation-the-posterior-distribution.html"><i class="fa fa-check"></i><b>4.3</b> Summarizing and evaluation the posterior distribution</a><ul>
<li class="chapter" data-level="4.3.1" data-path="summarizing-and-evaluation-the-posterior-distribution.html"><a href="summarizing-and-evaluation-the-posterior-distribution.html#output-of-brms"><i class="fa fa-check"></i><b>4.3.1</b> Output of brms</a></li>
<li class="chapter" data-level="4.3.2" data-path="summarizing-and-evaluation-the-posterior-distribution.html"><a href="summarizing-and-evaluation-the-posterior-distribution.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>4.3.2</b> Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="evaluating-the-priors-of-our-models.html"><a href="evaluating-the-priors-of-our-models.html"><i class="fa fa-check"></i><b>4.4</b> Evaluating the priors of our models</a><ul>
<li class="chapter" data-level="4.4.1" data-path="evaluating-the-priors-of-our-models.html"><a href="evaluating-the-priors-of-our-models.html#prior-predictive-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="evaluating-the-priors-of-our-models.html"><a href="evaluating-the-priors-of-our-models.html#influence-of-priors-and-sensitivity-analysis"><i class="fa fa-check"></i><b>4.4.2</b> Influence of priors and sensitivity analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>5</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="summarizing-and-evaluation-the-posterior-distribution" class="section level2">
<h2><span class="header-section-number">4.3</span> Summarizing and evaluation the posterior distribution</h2>
<p>We fit a Bayesian model to get the posterior distribution, but since we cannot derive it analytically, fitting a brms model (or any other MCMC-based approximation) will yield samples from the posterior.</p>
<div id="output-of-brms" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Output of brms</h3>
<p>See below in listing ??, but notice that <code>b_Intercept</code> corresponds to our <span class="math inline">\(\mu\)</span> and that <code>lp</code> is not really part of the posterior, it’s the density of the unnormalized posterior for each iteration.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1"><span class="kw">posterior_samples</span>(fit_press) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">str</span>()</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    4000 obs. of  3 variables:
##  $ b_Intercept: num  169 170 170 167 168 ...
##  $ sigma      : num  25.2 25.4 25.4 23.6 24.7 ...
##  $ lp__       : num  -1688 -1688 -1688 -1689 -1688 ...</code></pre>
<p>We can plot the histogram and chains of the samples:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1"><span class="kw">plot</span>(fit_press)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-55-1.svg" width="672" /></p>
<p>And <code>brms</code> provides a nice summary:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1">fit_press</a></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: rt ~ 1 
##    Data: df_noreading_data (Number of observations: 361) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat
## Intercept   168.64      1.32   166.06   171.17 1.00
##           Bulk_ESS Tail_ESS
## Intercept     2918     2127
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat
## sigma    24.98      0.95    23.21    26.97 1.00
##       Bulk_ESS Tail_ESS
## sigma     3213     2592
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1"><span class="co"># posterior_summary(fit_press) is also useful</span></a></code></pre></div>
<p>Notice that the <code>Estimate</code> is just the mean of the posterior sample, and
CI are the 95% quantiles:</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1"><span class="kw">posterior_samples</span>(fit_press)<span class="op">$</span>b_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mean</span>()</a></code></pre></div>
<pre><code>## [1] 168.6</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" data-line-number="1"><span class="kw">posterior_samples</span>(fit_press)<span class="op">$</span>b_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">quantile</span>(<span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">.975</span>))</a></code></pre></div>
<pre><code>##  2.5% 97.5% 
## 166.1 171.2</code></pre>
<p>We see that we can fit our model without problems, and we get some
posterior distributions for our parameters. Is the model adequate to capture the data?</p>
<!-- We should ask ourselves the following questions:  -->
<!-- * What information are the priors encoding?  -->
<!-- * Do the priors make sense?  -->
<!-- * Does the likelihood make sense?  -->
<!-- We'll discuss the results of the model first, and then we'll talk about -->
<!-- descriptive adequacy (or model fit), priors, etc. -->
</div>
<div id="posterior-predictive-checks" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Posterior predictive checks</h3>
<p>We use posterior predictive checks to examine the “descriptive adequacy” of our models <span class="citation">(<span class="citeproc-not-found" data-reference-id="gelmanBayesianDataAnalysis2013"><strong>???</strong></span>, Chapter 6; <span class="citeproc-not-found" data-reference-id="shiffrinSurveyModelEvaluation2008"><strong>???</strong></span>)</span>: the observed data should look plausible under the posterior predictive distribution.</p>
<p>Similarly to the prior predictive distribution, the posterior predictive distribution is a distribution of datasets (in practice we have one dataset for each iteration of the sampler). In contrast to the prior predictive distribution, here the model generates the datasets based on the posterior distributions of its parameters. In other words, given the posterior of the parameters of the model, the posterior predictive distribution shows how other data may look like.</p>
<p>Achieving descriptive adequacy means that the current data could have been predicted with the model. While passing atest of descriptive adequacy is not strong evidence in favor of a model, a major failure in descriptive adequacy can be interpreted as strong evidence against a model <span class="citation">(<span class="citeproc-not-found" data-reference-id="shiffrinSurveyModelEvaluation2008"><strong>???</strong></span>)</span>. Thus, posterior predictive checking is an important sanity check to assess whether the model behavior is reasonable.</p>
<p>Once we have the posterior distribution <span class="math inline">\(p(\Theta\mid y)\)</span>, we can derive the
predictions based on this distribution:</p>
<p><span class="math display">\[\begin{equation}
p(y_{pred}\mid y ) = \int p(y_{pred}, \Theta\mid y)\, d\Theta= \int 
p(y_{pred}\mid \Theta,y)p(\Theta\mid y)\, d\Theta
\end{equation}\]</span></p>
<p>Assuming that past and future observations are conditionally independent given <span class="math inline">\(\Theta\)</span>, i.e., <span class="math inline">\(p(y_{pred}\mid \Theta,y)= p(y_{pred}\mid \Theta)\)</span>, we can write:</p>
<p><span class="math display">\[\begin{equation}
p(y_{pred}\mid y )=\int p(y_{pred}\mid \Theta) p(\Theta\mid y)\, d\Theta
\end{equation}\]</span></p>
<p>Note that we are conditioning <span class="math inline">\(y_{pred}\)</span> only on <span class="math inline">\(y\)</span>, we do not condition on what we don’t know (<span class="math inline">\(\Theta\)</span>); <strong>we integrate out the unknown parameters</strong>.</p>
<p>This posterior predictive distribution is different from the frequentist approach, which gives only a predictive distribution of <span class="math inline">\(y_{pred}\)</span> given our estimate of <span class="math inline">\(\theta\)</span> (a point value).</p>
<p>We can use <code>pp_check</code> then to investigate how well the observed distribution of RTs fit the predicted based on some (11 and 100) samples of the posterior; see figure <a href="summarizing-and-evaluation-the-posterior-distribution.html#fig:normalppc">4.3</a> and <a href="summarizing-and-evaluation-the-posterior-distribution.html#fig:normalppc2">4.4</a>.</p>

<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1"><span class="kw">pp_check</span>(fit_press, <span class="dt">nsamples =</span> <span class="dv">11</span>, <span class="dt">type =</span> <span class="st">&quot;hist&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:normalppc"></span>
<img src="bookdown_files/figure-html/normalppc-1.svg" alt="Posterior predictive check of the model fit_press." width="672" />
<p class="caption">
FIGURE 4.3: Posterior predictive check of the model <code>fit_press</code>.
</p>
</div>

<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1"><span class="kw">pp_check</span>(fit_press, <span class="dt">nsamples =</span> <span class="dv">100</span>)</a></code></pre></div>
<div class="figure"><span id="fig:normalppc2"></span>
<img src="bookdown_files/figure-html/normalppc2-1.svg" alt="Posterior predictive check of the model fit_press." width="672" />
<p class="caption">
FIGURE 4.4: Posterior predictive check of the model <code>fit_press</code>.
</p>
</div>
<p>Notice that the real data is slightly skewed and has no values shorter
than 100 ms, while the predictive distributions are centered and
symmetrical. We’ll come back to this issue in ??.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-regression-models-using-stan-brms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="evaluating-the-priors-of-our-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/03-brms.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
